{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains the code neccesary to create and train the Genome autoencoder (Gencoder). Gencoder is a neural network autoencoder that takes a given 1000-bp long DNA sequence and outputs a DNA sequence with similar biological characteristics but a different sequence of nucleotides. \n",
    "\n",
    "Gencoder is able to preserve biological characteristics of the input DNA sequence by learning from an auxiliary neural network that can itself predict these biological characteristics. We call this auxiliary network the biological critic. In this notebook, we use a network functionally equivalent to DeepSEA (https://www.nature.com/articles/nmeth.3547), a neural network that can \"predict the epigenetic state of a sequence, including transcription factors binding, DNase I sensitivities and histone marks in multiple cell types, and further utilize this capability to predict the chromatin effects of sequence variants and prioritize regulatory variants.\" \n",
    "\n",
    "We train Gencoder to minimize the loss between the output of running the DNA sequences it generates through DeepSEA and the output of running the training input sequence through DeepSEA. Thus, Gencoder learns to produce a DNA sequence with identical predicted epigenetic states, transcription factor binding sites, etc. as the input sequence.\n",
    "\n",
    "The use case of Gencoder is to protect the DNA sequences of individuals in publicly-accessible research databases by obfuscating their DNA sequences while preserving their research utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HNMdNj-sj3w"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uQgH_WaUoqBm"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from textwrap import wrap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rKujdKwr1dNR"
   },
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kaq6k4pJJlCy"
   },
   "source": [
    "We use DeepSEA as the biological critic. It's input is encoded A, G, C, T and takes in sequences of length 1000.\n",
    "\n",
    "Here, we generate a large number of DNA sequences of length 1000, each one-hot encoded to match the input encoding of DeepSEA.\n",
    "\n",
    "Future work will be devoted to training Gencoder on 1000-bp long chunks of real DNA sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_fragments = list(SeqIO.parse(\"code_asd_dnarna_v3/resources/hg19_UCSC.fa\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "489qIdH_ld_o"
   },
   "outputs": [],
   "source": [
    "sample_length = 1000\n",
    "nucleic_acids = \"AGCT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_samples(fragments, num_samples):\n",
    "  samples = []\n",
    "  while len(samples) < num_samples:\n",
    "    fragment_number = random.randint(0, len(fragments)-1)\n",
    "    fragment = fragments[fragment_number]\n",
    "    limit = len(fragment)\n",
    "    start = random.randint(0, limit - sample_length)\n",
    "    end = start + sample_length\n",
    "    frag = fragment.seq[start:end]\n",
    "    if all([c not in frag for c in [\"a\", \"c\", \"t\", \"g\", \"N\"]]):\n",
    "      samples.append(frag.upper())\n",
    "  return samples\n",
    "\n",
    "\n",
    "raw_training_samples = generate_samples(genome_fragments, 50000)\n",
    "raw_validation_samples = generate_samples(genome_fragments, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BQIWmsuQY67F"
   },
   "outputs": [],
   "source": [
    "\n",
    "# num_training_samples = 100000\n",
    "# validation_samples = []\n",
    "# num_validation_samples = 100\n",
    "\n",
    "# def generate_samples(num_samples):\n",
    "#   samples = []\n",
    "#   for i in range(num_samples):\n",
    "#     sample = \"\"\n",
    "#     for i in range(sample_length):\n",
    "#       base_selection = int(random.random()*4)\n",
    "#       sample += nucleic_acids[base_selection]\n",
    "#     samples.append(sample)\n",
    "#   return samples\n",
    "\n",
    "# raw_training_samples = generate_samples(num_training_samples)\n",
    "\n",
    "# raw_validation_samples = generate_samples(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T460uOphOUjX"
   },
   "source": [
    "Initialize one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOu3wO6EJgt5",
    "outputId": "4db34047-a368-4843-cef6-d5606d7d5a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "integer_encoded = label_encoder.fit_transform(list(nucleic_acids))\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder.fit(np.asarray(list(nucleic_acids)).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3l78vTb2OKMO"
   },
   "outputs": [],
   "source": [
    "# One-hot encode each input string\n",
    "batch_size = 100\n",
    "use_channels = False\n",
    "\n",
    "def encode_and_resize_samples(samples):\n",
    "  one_hot_samples = np.array([np.array(onehot_encoder.transform(np.asarray(list(sample)).reshape(-1, 1))) for sample in samples])\n",
    "  if use_channels:\n",
    "    one_hot_samples2 = np.swapaxes(np.swapaxes(np.expand_dims(one_hot_samples, axis=3), 1, 2), 2, 3)\n",
    "  else:\n",
    "    one_hot_samples2 = np.swapaxes(np.expand_dims(one_hot_samples, axis=3), 1, 3)\n",
    "\n",
    "  return one_hot_samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z8Phw0Oyy35A"
   },
   "outputs": [],
   "source": [
    "resized_training_samples = encode_and_resize_samples(raw_training_samples)\n",
    "resized_validation_samples = encode_and_resize_samples(raw_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6cGYtcRKKXU",
    "outputId": "926b0d30-015d-4384-c7c7-443833741c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_training_samples[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxP9OvEmbcXq"
   },
   "source": [
    "Reshape and batch one-hot encoded input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q7VzIsEQ-orE"
   },
   "outputs": [],
   "source": [
    "def batchify(samples: np.ndarray, batch_size: int) -> np.ndarray:\n",
    "    for i in list(range(0, len(samples), batch_size)):\n",
    "      yield samples[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5YsMdkaX-vdM"
   },
   "outputs": [],
   "source": [
    "batched_training_samples = list(batchify(resized_training_samples, batch_size))\n",
    "batched_validation_samples = list(batchify(resized_validation_samples, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3byRyOmeWhQq",
    "outputId": "91082937-e30f-4508-97b1-781bbe3f9f2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237254/3818280843.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272204863/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  training_samples = torch.as_tensor(batched_training_samples).to(device, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded genome samples to PyTorch tensors\n",
    "training_samples = torch.as_tensor(batched_training_samples).to(device, dtype=torch.float) \n",
    "validation_samples = torch.as_tensor(batched_validation_samples).to(device, dtype=torch.float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSdlYCVCspu"
   },
   "source": [
    "# Load DeepSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CEMx-FwhONir"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LambdaBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LambdaBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, input):\n",
    "        output = []\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(input))\n",
    "        return output if output else input\n",
    "\n",
    "\n",
    "class Lambda(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return self.lambda_func(self.forward_prepare(input))\n",
    "\n",
    "\n",
    "class DeepSEAasd(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DeepSEAasd, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(4, 320, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(320, 320, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d((1, 4), (1, 4)),\n",
    "                nn.Conv2d(320, 480, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(480, 480, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.MaxPool2d((1, 4), (1, 4)),\n",
    "                nn.Conv2d(480, 960, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(960, 960, (1, 8)),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "                nn.Sequential(\n",
    "                    Lambda(lambda x: x.view(1, -1)\n",
    "                           if 1 == len(x.size()) else x),\n",
    "                    nn.Linear(42240, 2003)\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.Sequential(\n",
    "                    Lambda(lambda x: x.view(1, -1)\n",
    "                           if 1 == len(x.size()) else x),\n",
    "                    nn.Linear(2003, 2002)\n",
    "                ),\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def criterion():\n",
    "    return nn.BCELoss()\n",
    "\n",
    "\n",
    "def get_optimizer(lr):\n",
    "    return (torch.optim.SGD,\n",
    "            {\"lr\": lr, \"weight_decay\": 1e-6, \"momentum\": 0.9})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "icP_n7oDXAk2"
   },
   "outputs": [],
   "source": [
    "deepsea = DeepSEAasd().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_weights = torch.load(\"code_asd_dnarna_v3/models/asd_deepsea/deepsea_asd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix misnamed weight keys\n",
    "from collections import OrderedDict\n",
    "ds_weights2 = OrderedDict((\"model.\"+k, v) for k, v in ds_weights.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepsea.load_state_dict(ds_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Frk9og9socs"
   },
   "source": [
    "# Gencoder\n",
    "\n",
    "Here, we implement Gencoder simple three-layer CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gYH0Jpkrsr9P"
   },
   "outputs": [],
   "source": [
    "num_channels = 1  # Only one channel for genomic data\n",
    "n_generator_features = 512  # Size of feature maps in generator\n",
    "n_discriminator_features = 64  # Size of feature maps in discriminator\n",
    "kernel_size = (4, 16)\n",
    "input_size = sample_length\n",
    "ngpu = 1\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Net, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.convolve = nn.Sequential(\n",
    "        # First layer\n",
    "        nn.Conv2d(num_channels, n_generator_features, kernel_size),\n",
    "        nn.LeakyReLU(0.2 ,inplace=True),\n",
    "        nn.BatchNorm2d(n_generator_features),\n",
    "        # Second layer\n",
    "        nn.Conv2d(n_generator_features, int(n_generator_features/2), (1,32)),\n",
    "        nn.LeakyReLU(0.2 ,inplace=True),\n",
    "        nn.BatchNorm2d(int(n_generator_features/2)),\n",
    "        # Third layer\n",
    "        nn.Conv2d(int(n_generator_features/2), int(n_generator_features/4), (1,32)), # <- This kernel size has to be 1,x for some reason\n",
    "        nn.LeakyReLU(0.2 ,inplace=True),\n",
    "        nn.BatchNorm2d(int(n_generator_features/4))\n",
    "    )\n",
    "\n",
    "    self.deconvolve = nn.Sequential(\n",
    "        # Fifth layer\n",
    "        nn.ConvTranspose2d(int(n_generator_features/4), int(n_generator_features/2), (1, 32)),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(int(n_generator_features/2)),\n",
    "        # Fifth layer\n",
    "        nn.ConvTranspose2d(int(n_generator_features/2), n_generator_features, (1, 32)),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(n_generator_features),\n",
    "        # Output Layer\n",
    "        nn.ConvTranspose2d(n_generator_features, num_channels, kernel_size),\n",
    "        nn.Softmax(dim=2)\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    f = self.convolve(input)\n",
    "    return self.deconvolve(f)\n",
    "\n",
    "  def print(self, input):\n",
    "    f = self.convolve(input)\n",
    "    print(f.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN3CYBtJzgHP",
    "outputId": "281fd575-78b2-47d0-cecd-831a859c63c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (convolve): Sequential(\n",
      "    (0): Conv2d(1, 512, kernel_size=(4, 16), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(512, 256, kernel_size=(1, 32), stride=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 32), stride=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconvolve): Sequential(\n",
      "    (0): ConvTranspose2d(128, 256, kernel_size=(1, 32), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ConvTranspose2d(256, 512, kernel_size=(1, 32), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(512, 1, kernel_size=(4, 16), stride=(1, 1))\n",
      "    (7): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our model with the detected GPU\n",
    "net = Net(1).to(device)\n",
    "\n",
    "# Print the model\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "jx894V0sz86X"
   },
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "loss_log = {\"val\":[], \"training\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "Pcc6Hy_pFvUa",
    "outputId": "08fb72fd-e626-46cd-eba3-e096d463e464",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model w/ loss: 0.06073909252882004\n",
      "Epoch 0, Batch: 0: Training Loss: 0.09823758155107498, Validation Loss: 0.06073909252882004\n",
      "Epoch 0, Batch: 1: Training Loss: 0.06629336625337601, Validation Loss: 0.08434315770864487\n",
      "Epoch 0, Batch: 2: Training Loss: 0.09162230044603348, Validation Loss: 0.06810101866722107\n",
      "Saving new best model w/ loss: 0.05995773524045944\n",
      "Epoch 0, Batch: 3: Training Loss: 0.059809256345033646, Validation Loss: 0.05995773524045944\n",
      "Epoch 0, Batch: 4: Training Loss: 0.06468682736158371, Validation Loss: 0.06025822088122368\n",
      "Epoch 0, Batch: 5: Training Loss: 0.06249883770942688, Validation Loss: 0.06072044000029564\n",
      "Epoch 0, Batch: 6: Training Loss: 0.06973383575677872, Validation Loss: 0.060477085411548615\n",
      "Epoch 0, Batch: 7: Training Loss: 0.05830160155892372, Validation Loss: 0.06015895679593086\n",
      "Saving new best model w/ loss: 0.05931432172656059\n",
      "Epoch 0, Batch: 8: Training Loss: 0.060447145253419876, Validation Loss: 0.05931432172656059\n",
      "Saving new best model w/ loss: 0.059259481728076935\n",
      "Epoch 0, Batch: 9: Training Loss: 0.05778117850422859, Validation Loss: 0.059259481728076935\n",
      "Saving new best model w/ loss: 0.05849974974989891\n",
      "Epoch 0, Batch: 10: Training Loss: 0.06464613229036331, Validation Loss: 0.05849974974989891\n",
      "Saving new best model w/ loss: 0.04962891712784767\n",
      "Epoch 0, Batch: 11: Training Loss: 0.059692807495594025, Validation Loss: 0.04962891712784767\n",
      "Epoch 0, Batch: 12: Training Loss: 0.05164049565792084, Validation Loss: 0.05597216263413429\n",
      "Epoch 0, Batch: 13: Training Loss: 0.05598302185535431, Validation Loss: 0.052680350840091705\n",
      "Saving new best model w/ loss: 0.04762142151594162\n",
      "Epoch 0, Batch: 14: Training Loss: 0.056312792003154755, Validation Loss: 0.04762142151594162\n",
      "Epoch 0, Batch: 15: Training Loss: 0.055669452995061874, Validation Loss: 0.04926074296236038\n",
      "Epoch 0, Batch: 16: Training Loss: 0.05125654116272926, Validation Loss: 0.05107394978404045\n",
      "Epoch 0, Batch: 17: Training Loss: 0.04756472632288933, Validation Loss: 0.048337288200855255\n",
      "Epoch 0, Batch: 18: Training Loss: 0.05159354209899902, Validation Loss: 0.048008885234594345\n",
      "Epoch 0, Batch: 19: Training Loss: 0.048014525324106216, Validation Loss: 0.049279630184173584\n",
      "Epoch 0, Batch: 20: Training Loss: 0.054801374673843384, Validation Loss: 0.048869479447603226\n",
      "Saving new best model w/ loss: 0.0465710423886776\n",
      "Epoch 0, Batch: 21: Training Loss: 0.05331290140748024, Validation Loss: 0.0465710423886776\n",
      "Saving new best model w/ loss: 0.045378975570201874\n",
      "Epoch 0, Batch: 22: Training Loss: 0.0567171685397625, Validation Loss: 0.045378975570201874\n",
      "Epoch 0, Batch: 23: Training Loss: 0.05085684731602669, Validation Loss: 0.04816480353474617\n",
      "Epoch 0, Batch: 24: Training Loss: 0.05304332822561264, Validation Loss: 0.047872498631477356\n",
      "Saving new best model w/ loss: 0.04524244740605354\n",
      "Epoch 0, Batch: 25: Training Loss: 0.044607799500226974, Validation Loss: 0.04524244740605354\n",
      "Epoch 0, Batch: 26: Training Loss: 0.04800250008702278, Validation Loss: 0.04627944156527519\n",
      "Epoch 0, Batch: 27: Training Loss: 0.051526885479688644, Validation Loss: 0.04541260004043579\n",
      "Epoch 0, Batch: 28: Training Loss: 0.05339132621884346, Validation Loss: 0.045471519231796265\n",
      "Saving new best model w/ loss: 0.04252173379063606\n",
      "Epoch 0, Batch: 29: Training Loss: 0.04945662245154381, Validation Loss: 0.04252173379063606\n",
      "Saving new best model w/ loss: 0.041786737740039825\n",
      "Epoch 0, Batch: 30: Training Loss: 0.053813450038433075, Validation Loss: 0.041786737740039825\n",
      "Saving new best model w/ loss: 0.04080667346715927\n",
      "Epoch 0, Batch: 31: Training Loss: 0.04848713055253029, Validation Loss: 0.04080667346715927\n",
      "Epoch 0, Batch: 32: Training Loss: 0.046989310532808304, Validation Loss: 0.04294116795063019\n",
      "Epoch 0, Batch: 33: Training Loss: 0.04797780141234398, Validation Loss: 0.043488338589668274\n",
      "Epoch 0, Batch: 34: Training Loss: 0.042251333594322205, Validation Loss: 0.045550670474767685\n",
      "Epoch 0, Batch: 35: Training Loss: 0.04328436776995659, Validation Loss: 0.04641448333859444\n",
      "Epoch 0, Batch: 36: Training Loss: 0.04258100315928459, Validation Loss: 0.043587349355220795\n",
      "Epoch 0, Batch: 37: Training Loss: 0.044172730296850204, Validation Loss: 0.04403501749038696\n",
      "Epoch 0, Batch: 38: Training Loss: 0.042663268744945526, Validation Loss: 0.04511850327253342\n",
      "Epoch 0, Batch: 39: Training Loss: 0.047077469527721405, Validation Loss: 0.046207454055547714\n",
      "Epoch 0, Batch: 40: Training Loss: 0.052168820053339005, Validation Loss: 0.04288205876946449\n",
      "Epoch 0, Batch: 41: Training Loss: 0.04810338094830513, Validation Loss: 0.04351712018251419\n",
      "Epoch 0, Batch: 42: Training Loss: 0.042009804397821426, Validation Loss: 0.044081855565309525\n",
      "Saving new best model w/ loss: 0.04017231613397598\n",
      "Epoch 0, Batch: 43: Training Loss: 0.04701325297355652, Validation Loss: 0.04017231613397598\n",
      "Epoch 0, Batch: 44: Training Loss: 0.047734584659338, Validation Loss: 0.04064842313528061\n",
      "Epoch 0, Batch: 45: Training Loss: 0.045189034193754196, Validation Loss: 0.04027344658970833\n",
      "Epoch 0, Batch: 46: Training Loss: 0.046876393258571625, Validation Loss: 0.0408656969666481\n",
      "Epoch 0, Batch: 47: Training Loss: 0.04511941224336624, Validation Loss: 0.04253770411014557\n",
      "Epoch 0, Batch: 48: Training Loss: 0.046057239174842834, Validation Loss: 0.04080323874950409\n",
      "Epoch 0, Batch: 49: Training Loss: 0.05313456431031227, Validation Loss: 0.044276025146245956\n",
      "Epoch 0, Batch: 50: Training Loss: 0.044839099049568176, Validation Loss: 0.04349752888083458\n",
      "Epoch 0, Batch: 51: Training Loss: 0.04317159205675125, Validation Loss: 0.047730278223752975\n",
      "Epoch 0, Batch: 52: Training Loss: 0.04861631989479065, Validation Loss: 0.044282928109169006\n",
      "Epoch 0, Batch: 53: Training Loss: 0.04356389120221138, Validation Loss: 0.04879886284470558\n",
      "Epoch 0, Batch: 54: Training Loss: 0.04633734002709389, Validation Loss: 0.041857775300741196\n",
      "Epoch 0, Batch: 55: Training Loss: 0.051152780652046204, Validation Loss: 0.04179432988166809\n",
      "Epoch 0, Batch: 56: Training Loss: 0.03889802470803261, Validation Loss: 0.04427115619182587\n",
      "Epoch 0, Batch: 57: Training Loss: 0.042024195194244385, Validation Loss: 0.04204916954040527\n",
      "Epoch 0, Batch: 58: Training Loss: 0.0478690005838871, Validation Loss: 0.04262327030301094\n",
      "Epoch 0, Batch: 59: Training Loss: 0.04391583055257797, Validation Loss: 0.040519919246435165\n",
      "Saving new best model w/ loss: 0.03941923379898071\n",
      "Epoch 0, Batch: 60: Training Loss: 0.039956822991371155, Validation Loss: 0.03941923379898071\n",
      "Epoch 0, Batch: 61: Training Loss: 0.04416545853018761, Validation Loss: 0.039566103368997574\n",
      "Saving new best model w/ loss: 0.03932533413171768\n",
      "Epoch 0, Batch: 62: Training Loss: 0.048066116869449615, Validation Loss: 0.03932533413171768\n",
      "Epoch 0, Batch: 63: Training Loss: 0.04624840244650841, Validation Loss: 0.04159624129533768\n",
      "Saving new best model w/ loss: 0.0387534499168396\n",
      "Epoch 0, Batch: 64: Training Loss: 0.04932164028286934, Validation Loss: 0.0387534499168396\n",
      "Epoch 0, Batch: 65: Training Loss: 0.04677656665444374, Validation Loss: 0.039853133261203766\n",
      "Saving new best model w/ loss: 0.03743598982691765\n",
      "Epoch 0, Batch: 66: Training Loss: 0.04548119753599167, Validation Loss: 0.03743598982691765\n",
      "Saving new best model w/ loss: 0.03661126270890236\n",
      "Epoch 0, Batch: 67: Training Loss: 0.04673732444643974, Validation Loss: 0.03661126270890236\n",
      "Epoch 0, Batch: 68: Training Loss: 0.04403121396899223, Validation Loss: 0.03747730702161789\n",
      "Epoch 0, Batch: 69: Training Loss: 0.041667208075523376, Validation Loss: 0.038048822432756424\n",
      "Epoch 0, Batch: 70: Training Loss: 0.04729916527867317, Validation Loss: 0.038730353116989136\n",
      "Epoch 0, Batch: 71: Training Loss: 0.047287873923778534, Validation Loss: 0.04113102704286575\n",
      "Epoch 0, Batch: 72: Training Loss: 0.04306638985872269, Validation Loss: 0.0375298336148262\n",
      "Epoch 0, Batch: 73: Training Loss: 0.040350835770368576, Validation Loss: 0.036717738956213\n",
      "Epoch 0, Batch: 74: Training Loss: 0.04139743000268936, Validation Loss: 0.038477785885334015\n",
      "Epoch 0, Batch: 75: Training Loss: 0.03517358750104904, Validation Loss: 0.03699870780110359\n",
      "Epoch 0, Batch: 76: Training Loss: 0.03679237887263298, Validation Loss: 0.03903818875551224\n",
      "Epoch 0, Batch: 77: Training Loss: 0.04337076097726822, Validation Loss: 0.03664786368608475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 78: Training Loss: 0.043198470026254654, Validation Loss: 0.04317120462656021\n",
      "Epoch 0, Batch: 79: Training Loss: 0.036596301943063736, Validation Loss: 0.044415611773729324\n",
      "Epoch 0, Batch: 80: Training Loss: 0.044023144990205765, Validation Loss: 0.046745508909225464\n",
      "Epoch 0, Batch: 81: Training Loss: 0.041492823511362076, Validation Loss: 0.04565875604748726\n",
      "Epoch 0, Batch: 82: Training Loss: 0.041628655046224594, Validation Loss: 0.03793913871049881\n",
      "Epoch 0, Batch: 83: Training Loss: 0.040390051901340485, Validation Loss: 0.039111386984586716\n",
      "Epoch 0, Batch: 84: Training Loss: 0.03517267853021622, Validation Loss: 0.036687467247247696\n",
      "Epoch 0, Batch: 85: Training Loss: 0.04283924028277397, Validation Loss: 0.03759484365582466\n",
      "Saving new best model w/ loss: 0.03237546235322952\n",
      "Epoch 0, Batch: 86: Training Loss: 0.04170769453048706, Validation Loss: 0.03237546235322952\n",
      "Epoch 0, Batch: 87: Training Loss: 0.04747942462563515, Validation Loss: 0.03696763142943382\n",
      "Epoch 0, Batch: 88: Training Loss: 0.04786769300699234, Validation Loss: 0.040132734924554825\n",
      "Epoch 0, Batch: 89: Training Loss: 0.03527718409895897, Validation Loss: 0.03964710980653763\n",
      "Epoch 0, Batch: 90: Training Loss: 0.04123596474528313, Validation Loss: 0.038526248186826706\n",
      "Epoch 0, Batch: 91: Training Loss: 0.041545748710632324, Validation Loss: 0.038468435406684875\n",
      "Epoch 0, Batch: 92: Training Loss: 0.043742790818214417, Validation Loss: 0.04289533570408821\n",
      "Epoch 0, Batch: 93: Training Loss: 0.04332486912608147, Validation Loss: 0.044603295624256134\n",
      "Epoch 0, Batch: 94: Training Loss: 0.04598508030176163, Validation Loss: 0.0415770597755909\n",
      "Epoch 0, Batch: 95: Training Loss: 0.04670144245028496, Validation Loss: 0.042470887303352356\n",
      "Epoch 0, Batch: 96: Training Loss: 0.04034077748656273, Validation Loss: 0.04001886397600174\n",
      "Epoch 0, Batch: 97: Training Loss: 0.04168121516704559, Validation Loss: 0.043800968676805496\n",
      "Epoch 0, Batch: 98: Training Loss: 0.04304257035255432, Validation Loss: 0.04179132729768753\n",
      "Epoch 0, Batch: 99: Training Loss: 0.04532480984926224, Validation Loss: 0.041943881660699844\n",
      "Epoch 0, Batch: 100: Training Loss: 0.04634719341993332, Validation Loss: 0.04249875247478485\n",
      "Epoch 0, Batch: 101: Training Loss: 0.043630450963974, Validation Loss: 0.04543738067150116\n",
      "Epoch 0, Batch: 102: Training Loss: 0.03746230900287628, Validation Loss: 0.043519239872694016\n",
      "Epoch 0, Batch: 103: Training Loss: 0.049734413623809814, Validation Loss: 0.04064786806702614\n",
      "Epoch 0, Batch: 104: Training Loss: 0.03129444643855095, Validation Loss: 0.04160159081220627\n",
      "Epoch 0, Batch: 105: Training Loss: 0.03885892778635025, Validation Loss: 0.04445292800664902\n",
      "Epoch 0, Batch: 106: Training Loss: 0.04249317944049835, Validation Loss: 0.04492028430104256\n",
      "Epoch 0, Batch: 107: Training Loss: 0.04422920197248459, Validation Loss: 0.043280504643917084\n",
      "Epoch 0, Batch: 108: Training Loss: 0.04282769933342934, Validation Loss: 0.04332893714308739\n",
      "Epoch 0, Batch: 109: Training Loss: 0.05042189359664917, Validation Loss: 0.04079606756567955\n",
      "Epoch 0, Batch: 110: Training Loss: 0.049920547753572464, Validation Loss: 0.0409412756562233\n",
      "Epoch 0, Batch: 111: Training Loss: 0.038150228559970856, Validation Loss: 0.039136916399002075\n",
      "Epoch 0, Batch: 112: Training Loss: 0.0369386151432991, Validation Loss: 0.03620538488030434\n",
      "Epoch 0, Batch: 113: Training Loss: 0.03673756495118141, Validation Loss: 0.03740566968917847\n",
      "Epoch 0, Batch: 114: Training Loss: 0.0421866737306118, Validation Loss: 0.03477761521935463\n",
      "Epoch 0, Batch: 115: Training Loss: 0.03841128200292587, Validation Loss: 0.037454962730407715\n",
      "Epoch 0, Batch: 116: Training Loss: 0.0357196070253849, Validation Loss: 0.03521987795829773\n",
      "Epoch 0, Batch: 117: Training Loss: 0.03499934449791908, Validation Loss: 0.03742614760994911\n",
      "Epoch 0, Batch: 118: Training Loss: 0.03918924927711487, Validation Loss: 0.036344703286886215\n",
      "Epoch 0, Batch: 119: Training Loss: 0.04302593693137169, Validation Loss: 0.03708801418542862\n",
      "Epoch 0, Batch: 120: Training Loss: 0.03884890303015709, Validation Loss: 0.03795289993286133\n",
      "Epoch 0, Batch: 121: Training Loss: 0.03875071182847023, Validation Loss: 0.03470337763428688\n",
      "Epoch 0, Batch: 122: Training Loss: 0.03915828838944435, Validation Loss: 0.03644637390971184\n",
      "Epoch 0, Batch: 123: Training Loss: 0.03685522824525833, Validation Loss: 0.03284309059381485\n",
      "Saving new best model w/ loss: 0.03159260377287865\n",
      "Epoch 0, Batch: 124: Training Loss: 0.03682324290275574, Validation Loss: 0.03159260377287865\n",
      "Epoch 0, Batch: 125: Training Loss: 0.03254448249936104, Validation Loss: 0.035494737327098846\n",
      "Epoch 0, Batch: 126: Training Loss: 0.03708604350686073, Validation Loss: 0.034522950649261475\n",
      "Epoch 0, Batch: 127: Training Loss: 0.03639116510748863, Validation Loss: 0.039336029440164566\n",
      "Epoch 0, Batch: 128: Training Loss: 0.041191499680280685, Validation Loss: 0.037929631769657135\n",
      "Epoch 0, Batch: 129: Training Loss: 0.034574177116155624, Validation Loss: 0.03682498633861542\n",
      "Epoch 0, Batch: 130: Training Loss: 0.03454558551311493, Validation Loss: 0.036183856427669525\n",
      "Saving new best model w/ loss: 0.03108319826424122\n",
      "Epoch 0, Batch: 131: Training Loss: 0.041671786457300186, Validation Loss: 0.03108319826424122\n",
      "Epoch 0, Batch: 132: Training Loss: 0.038687948137521744, Validation Loss: 0.03279171884059906\n",
      "Epoch 0, Batch: 133: Training Loss: 0.04136353358626366, Validation Loss: 0.03408944979310036\n",
      "Epoch 0, Batch: 134: Training Loss: 0.03753014653921127, Validation Loss: 0.03837718814611435\n",
      "Epoch 0, Batch: 135: Training Loss: 0.030626988038420677, Validation Loss: 0.034649528563022614\n",
      "Epoch 0, Batch: 136: Training Loss: 0.034798525273799896, Validation Loss: 0.03565923124551773\n",
      "Epoch 0, Batch: 137: Training Loss: 0.04081939533352852, Validation Loss: 0.03772147744894028\n",
      "Epoch 0, Batch: 138: Training Loss: 0.03782326728105545, Validation Loss: 0.04028325527906418\n",
      "Epoch 0, Batch: 139: Training Loss: 0.038775909692049026, Validation Loss: 0.03835825249552727\n",
      "Epoch 0, Batch: 140: Training Loss: 0.04106287658214569, Validation Loss: 0.03738510608673096\n",
      "Epoch 0, Batch: 141: Training Loss: 0.03653910383582115, Validation Loss: 0.03704220429062843\n",
      "Epoch 0, Batch: 142: Training Loss: 0.03903629630804062, Validation Loss: 0.039824794977903366\n",
      "Epoch 0, Batch: 143: Training Loss: 0.036947865039110184, Validation Loss: 0.038615573197603226\n",
      "Epoch 0, Batch: 144: Training Loss: 0.04136263579130173, Validation Loss: 0.03762923926115036\n",
      "Epoch 0, Batch: 145: Training Loss: 0.03482329100370407, Validation Loss: 0.03457088768482208\n",
      "Epoch 0, Batch: 146: Training Loss: 0.0348055437207222, Validation Loss: 0.03246007487177849\n",
      "Epoch 0, Batch: 147: Training Loss: 0.03475497290492058, Validation Loss: 0.03373950719833374\n",
      "Saving new best model w/ loss: 0.03096444346010685\n",
      "Epoch 0, Batch: 148: Training Loss: 0.03861171007156372, Validation Loss: 0.03096444346010685\n",
      "Epoch 0, Batch: 149: Training Loss: 0.035407986491918564, Validation Loss: 0.032479312270879745\n",
      "Epoch 0, Batch: 150: Training Loss: 0.04158640652894974, Validation Loss: 0.033644337207078934\n",
      "Epoch 0, Batch: 151: Training Loss: 0.037819333374500275, Validation Loss: 0.03683341667056084\n",
      "Epoch 0, Batch: 152: Training Loss: 0.04321761056780815, Validation Loss: 0.03518214821815491\n",
      "Epoch 0, Batch: 153: Training Loss: 0.041350107640028, Validation Loss: 0.03547513857483864\n",
      "Epoch 0, Batch: 154: Training Loss: 0.03665683791041374, Validation Loss: 0.03760165348649025\n",
      "Epoch 0, Batch: 155: Training Loss: 0.04692176356911659, Validation Loss: 0.03499409183859825\n",
      "Epoch 0, Batch: 156: Training Loss: 0.031478025019168854, Validation Loss: 0.036164797842502594\n",
      "Epoch 0, Batch: 157: Training Loss: 0.03300049155950546, Validation Loss: 0.03539951145648956\n",
      "Epoch 0, Batch: 158: Training Loss: 0.03711070492863655, Validation Loss: 0.03532910346984863\n",
      "Epoch 0, Batch: 159: Training Loss: 0.031014185398817062, Validation Loss: 0.03623265027999878\n",
      "Epoch 0, Batch: 160: Training Loss: 0.03592252731323242, Validation Loss: 0.03876554220914841\n",
      "Epoch 0, Batch: 161: Training Loss: 0.036018192768096924, Validation Loss: 0.039559945464134216\n",
      "Epoch 0, Batch: 162: Training Loss: 0.04197258874773979, Validation Loss: 0.03836416080594063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 163: Training Loss: 0.03525504842400551, Validation Loss: 0.03803179785609245\n",
      "Epoch 0, Batch: 164: Training Loss: 0.03231438994407654, Validation Loss: 0.03855657950043678\n",
      "Epoch 0, Batch: 165: Training Loss: 0.03482265770435333, Validation Loss: 0.038617249578237534\n",
      "Epoch 0, Batch: 166: Training Loss: 0.03221135586500168, Validation Loss: 0.036693647503852844\n",
      "Epoch 0, Batch: 167: Training Loss: 0.035088494420051575, Validation Loss: 0.03465418890118599\n",
      "Epoch 0, Batch: 168: Training Loss: 0.0375545509159565, Validation Loss: 0.03651101142168045\n",
      "Epoch 0, Batch: 169: Training Loss: 0.03894943371415138, Validation Loss: 0.03605504706501961\n",
      "Epoch 0, Batch: 170: Training Loss: 0.037339948117733, Validation Loss: 0.03457150608301163\n",
      "Epoch 0, Batch: 171: Training Loss: 0.03563569858670235, Validation Loss: 0.03876631334424019\n",
      "Epoch 0, Batch: 172: Training Loss: 0.032235607504844666, Validation Loss: 0.036039743572473526\n",
      "Epoch 0, Batch: 173: Training Loss: 0.03448181599378586, Validation Loss: 0.03589202091097832\n",
      "Epoch 0, Batch: 174: Training Loss: 0.040032729506492615, Validation Loss: 0.033294420689344406\n",
      "Epoch 0, Batch: 175: Training Loss: 0.03843293711543083, Validation Loss: 0.03438521921634674\n",
      "Epoch 0, Batch: 176: Training Loss: 0.04288442060351372, Validation Loss: 0.036814577877521515\n",
      "Epoch 0, Batch: 177: Training Loss: 0.03676048666238785, Validation Loss: 0.034277383238077164\n",
      "Epoch 0, Batch: 178: Training Loss: 0.045468248426914215, Validation Loss: 0.03945980593562126\n",
      "Epoch 0, Batch: 179: Training Loss: 0.037741780281066895, Validation Loss: 0.0360143706202507\n",
      "Epoch 0, Batch: 180: Training Loss: 0.040384043008089066, Validation Loss: 0.039349447935819626\n",
      "Epoch 0, Batch: 181: Training Loss: 0.03562299534678459, Validation Loss: 0.03569550812244415\n",
      "Epoch 0, Batch: 182: Training Loss: 0.039942171424627304, Validation Loss: 0.03687115013599396\n",
      "Epoch 0, Batch: 183: Training Loss: 0.03711424022912979, Validation Loss: 0.03955696523189545\n",
      "Epoch 0, Batch: 184: Training Loss: 0.03843693062663078, Validation Loss: 0.03692253679037094\n",
      "Epoch 0, Batch: 185: Training Loss: 0.04874745383858681, Validation Loss: 0.03660361096262932\n",
      "Epoch 0, Batch: 186: Training Loss: 0.03288663178682327, Validation Loss: 0.036011889576911926\n",
      "Epoch 0, Batch: 187: Training Loss: 0.041217390447854996, Validation Loss: 0.03593384474515915\n",
      "Epoch 0, Batch: 188: Training Loss: 0.0334724560379982, Validation Loss: 0.03170240670442581\n",
      "Epoch 0, Batch: 189: Training Loss: 0.03365228697657585, Validation Loss: 0.0386611670255661\n",
      "Epoch 0, Batch: 190: Training Loss: 0.0327516533434391, Validation Loss: 0.034341439604759216\n",
      "Epoch 0, Batch: 191: Training Loss: 0.03607878461480141, Validation Loss: 0.03527752310037613\n",
      "Epoch 0, Batch: 192: Training Loss: 0.03911059349775314, Validation Loss: 0.03669237345457077\n",
      "Epoch 0, Batch: 193: Training Loss: 0.03379227593541145, Validation Loss: 0.03559727221727371\n",
      "Epoch 0, Batch: 194: Training Loss: 0.03507055342197418, Validation Loss: 0.03802736848592758\n",
      "Epoch 0, Batch: 195: Training Loss: 0.03329385071992874, Validation Loss: 0.036090537905693054\n",
      "Epoch 0, Batch: 196: Training Loss: 0.03832876309752464, Validation Loss: 0.036112312227487564\n",
      "Epoch 0, Batch: 197: Training Loss: 0.034666988998651505, Validation Loss: 0.033401280641555786\n",
      "Epoch 0, Batch: 198: Training Loss: 0.034214042127132416, Validation Loss: 0.03338494896888733\n",
      "Epoch 0, Batch: 199: Training Loss: 0.03420740365982056, Validation Loss: 0.034227531403303146\n",
      "Epoch 0, Batch: 200: Training Loss: 0.035341329872608185, Validation Loss: 0.03400639817118645\n",
      "Epoch 0, Batch: 201: Training Loss: 0.040217604488134384, Validation Loss: 0.03839115798473358\n",
      "Epoch 0, Batch: 202: Training Loss: 0.033305272459983826, Validation Loss: 0.032505910843610764\n",
      "Epoch 0, Batch: 203: Training Loss: 0.036092113703489304, Validation Loss: 0.03757612407207489\n",
      "Epoch 0, Batch: 204: Training Loss: 0.03528118133544922, Validation Loss: 0.036745477467775345\n",
      "Epoch 0, Batch: 205: Training Loss: 0.039218414574861526, Validation Loss: 0.037159670144319534\n",
      "Epoch 0, Batch: 206: Training Loss: 0.027826489880681038, Validation Loss: 0.03444169834256172\n",
      "Epoch 0, Batch: 207: Training Loss: 0.03458130359649658, Validation Loss: 0.03694668039679527\n",
      "Epoch 0, Batch: 208: Training Loss: 0.03502286598086357, Validation Loss: 0.033616457134485245\n",
      "Epoch 0, Batch: 209: Training Loss: 0.03980801999568939, Validation Loss: 0.03367966040968895\n",
      "Epoch 0, Batch: 210: Training Loss: 0.032391250133514404, Validation Loss: 0.03410260006785393\n",
      "Epoch 0, Batch: 211: Training Loss: 0.03875778242945671, Validation Loss: 0.03236747160553932\n",
      "Epoch 0, Batch: 212: Training Loss: 0.031743235886096954, Validation Loss: 0.034007806330919266\n",
      "Saving new best model w/ loss: 0.03037315234541893\n",
      "Epoch 0, Batch: 213: Training Loss: 0.03875304013490677, Validation Loss: 0.03037315234541893\n",
      "Epoch 0, Batch: 214: Training Loss: 0.03386712819337845, Validation Loss: 0.03272317349910736\n",
      "Epoch 0, Batch: 215: Training Loss: 0.031926363706588745, Validation Loss: 0.03649957478046417\n",
      "Epoch 0, Batch: 216: Training Loss: 0.037440527230501175, Validation Loss: 0.036390792578458786\n",
      "Epoch 0, Batch: 217: Training Loss: 0.0351821593940258, Validation Loss: 0.03346814215183258\n",
      "Epoch 0, Batch: 218: Training Loss: 0.0346345491707325, Validation Loss: 0.03453126549720764\n",
      "Epoch 0, Batch: 219: Training Loss: 0.03569898009300232, Validation Loss: 0.03848792240023613\n",
      "Epoch 0, Batch: 220: Training Loss: 0.04104471579194069, Validation Loss: 0.03476376459002495\n",
      "Epoch 0, Batch: 221: Training Loss: 0.03566000610589981, Validation Loss: 0.038029372692108154\n",
      "Epoch 0, Batch: 222: Training Loss: 0.03558122739195824, Validation Loss: 0.03430073335766792\n",
      "Epoch 0, Batch: 223: Training Loss: 0.033764760941267014, Validation Loss: 0.03788965567946434\n",
      "Epoch 0, Batch: 224: Training Loss: 0.03792118281126022, Validation Loss: 0.0369834303855896\n",
      "Epoch 0, Batch: 225: Training Loss: 0.039781033992767334, Validation Loss: 0.0353248193860054\n",
      "Epoch 0, Batch: 226: Training Loss: 0.035779260098934174, Validation Loss: 0.03838646784424782\n",
      "Epoch 0, Batch: 227: Training Loss: 0.03603850305080414, Validation Loss: 0.03849014639854431\n",
      "Epoch 0, Batch: 228: Training Loss: 0.033499594777822495, Validation Loss: 0.03983370214700699\n",
      "Epoch 0, Batch: 229: Training Loss: 0.035383496433496475, Validation Loss: 0.04041265696287155\n",
      "Epoch 0, Batch: 230: Training Loss: 0.03258232772350311, Validation Loss: 0.042772140353918076\n",
      "Epoch 0, Batch: 231: Training Loss: 0.040666840970516205, Validation Loss: 0.043006252497434616\n",
      "Epoch 0, Batch: 232: Training Loss: 0.04518071189522743, Validation Loss: 0.040794987231492996\n",
      "Epoch 0, Batch: 233: Training Loss: 0.03684641793370247, Validation Loss: 0.04138714075088501\n",
      "Epoch 0, Batch: 234: Training Loss: 0.03534926474094391, Validation Loss: 0.04000912234187126\n",
      "Epoch 0, Batch: 235: Training Loss: 0.03985249623656273, Validation Loss: 0.03880744054913521\n",
      "Epoch 0, Batch: 236: Training Loss: 0.0436183325946331, Validation Loss: 0.038235343992710114\n",
      "Epoch 0, Batch: 237: Training Loss: 0.03760579600930214, Validation Loss: 0.035526879131793976\n",
      "Epoch 0, Batch: 238: Training Loss: 0.041115984320640564, Validation Loss: 0.03667006269097328\n",
      "Epoch 0, Batch: 239: Training Loss: 0.03367384895682335, Validation Loss: 0.04021589830517769\n",
      "Epoch 0, Batch: 240: Training Loss: 0.03815051540732384, Validation Loss: 0.03978680819272995\n",
      "Epoch 0, Batch: 241: Training Loss: 0.04000344127416611, Validation Loss: 0.03967602550983429\n",
      "Epoch 0, Batch: 242: Training Loss: 0.04010959342122078, Validation Loss: 0.037331677973270416\n",
      "Epoch 0, Batch: 243: Training Loss: 0.04369447007775307, Validation Loss: 0.03791585937142372\n",
      "Epoch 0, Batch: 244: Training Loss: 0.0397191122174263, Validation Loss: 0.03843013197183609\n",
      "Epoch 0, Batch: 245: Training Loss: 0.04106840491294861, Validation Loss: 0.03924129903316498\n",
      "Epoch 0, Batch: 246: Training Loss: 0.03110646642744541, Validation Loss: 0.037961460649967194\n",
      "Epoch 0, Batch: 247: Training Loss: 0.0410807728767395, Validation Loss: 0.037211060523986816\n",
      "Epoch 0, Batch: 248: Training Loss: 0.035975418984889984, Validation Loss: 0.03667871281504631\n",
      "Epoch 0, Batch: 249: Training Loss: 0.03252112492918968, Validation Loss: 0.036430761218070984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 250: Training Loss: 0.03652716800570488, Validation Loss: 0.037029799073934555\n",
      "Epoch 0, Batch: 251: Training Loss: 0.032830409705638885, Validation Loss: 0.037768542766571045\n",
      "Epoch 0, Batch: 252: Training Loss: 0.03509853780269623, Validation Loss: 0.037977106869220734\n",
      "Epoch 0, Batch: 253: Training Loss: 0.03504034876823425, Validation Loss: 0.03726319223642349\n",
      "Epoch 0, Batch: 254: Training Loss: 0.032598964869976044, Validation Loss: 0.039512429386377335\n",
      "Epoch 0, Batch: 255: Training Loss: 0.03645698353648186, Validation Loss: 0.03847692906856537\n",
      "Epoch 0, Batch: 256: Training Loss: 0.04157322272658348, Validation Loss: 0.04009182006120682\n",
      "Epoch 0, Batch: 257: Training Loss: 0.04024922847747803, Validation Loss: 0.03976575285196304\n",
      "Epoch 0, Batch: 258: Training Loss: 0.03877578303217888, Validation Loss: 0.03699431195855141\n",
      "Epoch 0, Batch: 259: Training Loss: 0.03255511447787285, Validation Loss: 0.03591567650437355\n",
      "Epoch 0, Batch: 260: Training Loss: 0.032925449311733246, Validation Loss: 0.03767199069261551\n",
      "Epoch 0, Batch: 261: Training Loss: 0.035061903297901154, Validation Loss: 0.03656880557537079\n",
      "Epoch 0, Batch: 262: Training Loss: 0.0370723120868206, Validation Loss: 0.03210456669330597\n",
      "Epoch 0, Batch: 263: Training Loss: 0.03512098640203476, Validation Loss: 0.03568389266729355\n",
      "Epoch 0, Batch: 264: Training Loss: 0.03488452732563019, Validation Loss: 0.0338749922811985\n",
      "Epoch 0, Batch: 265: Training Loss: 0.031165484338998795, Validation Loss: 0.034908536821603775\n",
      "Epoch 0, Batch: 266: Training Loss: 0.033679209649562836, Validation Loss: 0.03465339168906212\n",
      "Epoch 0, Batch: 267: Training Loss: 0.03545313701033592, Validation Loss: 0.03380163758993149\n",
      "Epoch 0, Batch: 268: Training Loss: 0.03491324186325073, Validation Loss: 0.033397648483514786\n",
      "Epoch 0, Batch: 269: Training Loss: 0.03668612986803055, Validation Loss: 0.03613590821623802\n",
      "Epoch 0, Batch: 270: Training Loss: 0.03509386256337166, Validation Loss: 0.03448198363184929\n",
      "Epoch 0, Batch: 271: Training Loss: 0.03243064880371094, Validation Loss: 0.03779903054237366\n",
      "Epoch 0, Batch: 272: Training Loss: 0.034139569848775864, Validation Loss: 0.036152493208646774\n",
      "Epoch 0, Batch: 273: Training Loss: 0.03768277168273926, Validation Loss: 0.034408316016197205\n",
      "Epoch 0, Batch: 274: Training Loss: 0.03758588060736656, Validation Loss: 0.03341959789395332\n",
      "Epoch 0, Batch: 275: Training Loss: 0.03381505236029625, Validation Loss: 0.03723391890525818\n",
      "Epoch 0, Batch: 276: Training Loss: 0.029093191027641296, Validation Loss: 0.0372476726770401\n",
      "Epoch 0, Batch: 277: Training Loss: 0.032580651342868805, Validation Loss: 0.03448982536792755\n",
      "Epoch 0, Batch: 278: Training Loss: 0.03707769513130188, Validation Loss: 0.03370773792266846\n",
      "Epoch 0, Batch: 279: Training Loss: 0.03337367996573448, Validation Loss: 0.034290019422769547\n",
      "Epoch 0, Batch: 280: Training Loss: 0.03211415186524391, Validation Loss: 0.031526125967502594\n",
      "Epoch 0, Batch: 281: Training Loss: 0.035152122378349304, Validation Loss: 0.036175165325403214\n",
      "Epoch 0, Batch: 282: Training Loss: 0.03153689205646515, Validation Loss: 0.03764655813574791\n",
      "Epoch 0, Batch: 283: Training Loss: 0.031690582633018494, Validation Loss: 0.03500492498278618\n",
      "Epoch 0, Batch: 284: Training Loss: 0.03004569374024868, Validation Loss: 0.03842756524682045\n",
      "Epoch 0, Batch: 285: Training Loss: 0.03505316376686096, Validation Loss: 0.036409080028533936\n",
      "Epoch 0, Batch: 286: Training Loss: 0.03399931639432907, Validation Loss: 0.031567011028528214\n",
      "Epoch 0, Batch: 287: Training Loss: 0.035151999443769455, Validation Loss: 0.03682833909988403\n",
      "Epoch 0, Batch: 288: Training Loss: 0.031529225409030914, Validation Loss: 0.03683369979262352\n",
      "Epoch 0, Batch: 289: Training Loss: 0.03563679754734039, Validation Loss: 0.038723696023225784\n",
      "Epoch 0, Batch: 290: Training Loss: 0.03281485289335251, Validation Loss: 0.036799680441617966\n",
      "Epoch 0, Batch: 291: Training Loss: 0.03227158635854721, Validation Loss: 0.036462679505348206\n",
      "Epoch 0, Batch: 292: Training Loss: 0.03280226141214371, Validation Loss: 0.03621383383870125\n",
      "Epoch 0, Batch: 293: Training Loss: 0.031695056706666946, Validation Loss: 0.04069405049085617\n",
      "Epoch 0, Batch: 294: Training Loss: 0.037889182567596436, Validation Loss: 0.035486750304698944\n",
      "Epoch 0, Batch: 295: Training Loss: 0.031770434230566025, Validation Loss: 0.03917737305164337\n",
      "Epoch 0, Batch: 296: Training Loss: 0.035820502787828445, Validation Loss: 0.0390855073928833\n",
      "Epoch 0, Batch: 297: Training Loss: 0.03579302132129669, Validation Loss: 0.03505188971757889\n",
      "Epoch 0, Batch: 298: Training Loss: 0.032374050468206406, Validation Loss: 0.0371575765311718\n",
      "Epoch 0, Batch: 299: Training Loss: 0.035208649933338165, Validation Loss: 0.036664191633462906\n",
      "Epoch 0, Batch: 300: Training Loss: 0.03342737257480621, Validation Loss: 0.03606994450092316\n",
      "Epoch 0, Batch: 301: Training Loss: 0.03184277191758156, Validation Loss: 0.03596150502562523\n",
      "Epoch 0, Batch: 302: Training Loss: 0.029977288097143173, Validation Loss: 0.03617889806628227\n",
      "Epoch 0, Batch: 303: Training Loss: 0.0315442755818367, Validation Loss: 0.037294644862413406\n",
      "Epoch 0, Batch: 304: Training Loss: 0.031735219061374664, Validation Loss: 0.034603334963321686\n",
      "Epoch 0, Batch: 305: Training Loss: 0.03476895019412041, Validation Loss: 0.03526412695646286\n",
      "Epoch 0, Batch: 306: Training Loss: 0.038247641175985336, Validation Loss: 0.036129940301179886\n",
      "Epoch 0, Batch: 307: Training Loss: 0.028777530416846275, Validation Loss: 0.03494025394320488\n",
      "Epoch 0, Batch: 308: Training Loss: 0.03553349897265434, Validation Loss: 0.03669288754463196\n",
      "Epoch 0, Batch: 309: Training Loss: 0.028790345415472984, Validation Loss: 0.036028821021318436\n",
      "Epoch 0, Batch: 310: Training Loss: 0.03664206713438034, Validation Loss: 0.036066196858882904\n",
      "Epoch 0, Batch: 311: Training Loss: 0.034844931215047836, Validation Loss: 0.03286829963326454\n",
      "Epoch 0, Batch: 312: Training Loss: 0.040247246623039246, Validation Loss: 0.03249019756913185\n",
      "Epoch 0, Batch: 313: Training Loss: 0.032191552221775055, Validation Loss: 0.031739793717861176\n",
      "Epoch 0, Batch: 314: Training Loss: 0.03550521656870842, Validation Loss: 0.03726242482662201\n",
      "Epoch 0, Batch: 315: Training Loss: 0.035789716988801956, Validation Loss: 0.038015179336071014\n",
      "Epoch 0, Batch: 316: Training Loss: 0.03575973957777023, Validation Loss: 0.03697523474693298\n",
      "Epoch 0, Batch: 317: Training Loss: 0.03608190268278122, Validation Loss: 0.03814929351210594\n",
      "Epoch 0, Batch: 318: Training Loss: 0.03836565092206001, Validation Loss: 0.0361565463244915\n",
      "Epoch 0, Batch: 319: Training Loss: 0.03500742465257645, Validation Loss: 0.036564186215400696\n",
      "Epoch 0, Batch: 320: Training Loss: 0.0351903960108757, Validation Loss: 0.03682400658726692\n",
      "Epoch 0, Batch: 321: Training Loss: 0.034522317349910736, Validation Loss: 0.03522290289402008\n",
      "Epoch 0, Batch: 322: Training Loss: 0.035176970064640045, Validation Loss: 0.03333968296647072\n",
      "Epoch 0, Batch: 323: Training Loss: 0.034623075276613235, Validation Loss: 0.03418591991066933\n",
      "Epoch 0, Batch: 324: Training Loss: 0.03851797431707382, Validation Loss: 0.03584320843219757\n",
      "Epoch 0, Batch: 325: Training Loss: 0.0313461571931839, Validation Loss: 0.03589056804776192\n",
      "Epoch 0, Batch: 326: Training Loss: 0.03819688409566879, Validation Loss: 0.03536294773221016\n",
      "Epoch 0, Batch: 327: Training Loss: 0.03586152195930481, Validation Loss: 0.03514809533953667\n",
      "Epoch 0, Batch: 328: Training Loss: 0.03736872971057892, Validation Loss: 0.03359914571046829\n",
      "Epoch 0, Batch: 329: Training Loss: 0.033751554787158966, Validation Loss: 0.034346211701631546\n",
      "Epoch 0, Batch: 330: Training Loss: 0.0390222892165184, Validation Loss: 0.036568693816661835\n",
      "Epoch 0, Batch: 331: Training Loss: 0.036548495292663574, Validation Loss: 0.03362327441573143\n",
      "Epoch 0, Batch: 332: Training Loss: 0.03060063347220421, Validation Loss: 0.03146839886903763\n",
      "Epoch 0, Batch: 333: Training Loss: 0.03758110851049423, Validation Loss: 0.03645341843366623\n",
      "Epoch 0, Batch: 334: Training Loss: 0.03845013678073883, Validation Loss: 0.0336533859372139\n",
      "Epoch 0, Batch: 335: Training Loss: 0.03857340291142464, Validation Loss: 0.03285182639956474\n",
      "Epoch 0, Batch: 336: Training Loss: 0.038376860320568085, Validation Loss: 0.034182555973529816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 337: Training Loss: 0.03640354052186012, Validation Loss: 0.03636828809976578\n",
      "Epoch 0, Batch: 338: Training Loss: 0.03777438402175903, Validation Loss: 0.03417973965406418\n",
      "Epoch 0, Batch: 339: Training Loss: 0.036633703857660294, Validation Loss: 0.036881137639284134\n",
      "Epoch 0, Batch: 340: Training Loss: 0.03773990273475647, Validation Loss: 0.03884318098425865\n",
      "Epoch 0, Batch: 341: Training Loss: 0.035068534314632416, Validation Loss: 0.035994093865156174\n",
      "Epoch 0, Batch: 342: Training Loss: 0.041977811604738235, Validation Loss: 0.03822001814842224\n",
      "Epoch 0, Batch: 343: Training Loss: 0.03731321543455124, Validation Loss: 0.03647283464670181\n",
      "Epoch 0, Batch: 344: Training Loss: 0.036759793758392334, Validation Loss: 0.03426104784011841\n",
      "Epoch 0, Batch: 345: Training Loss: 0.03716275468468666, Validation Loss: 0.03470008075237274\n",
      "Epoch 0, Batch: 346: Training Loss: 0.03582476079463959, Validation Loss: 0.03484347090125084\n",
      "Epoch 0, Batch: 347: Training Loss: 0.03973803669214249, Validation Loss: 0.03649028763175011\n",
      "Epoch 0, Batch: 348: Training Loss: 0.037040501832962036, Validation Loss: 0.03814123943448067\n",
      "Epoch 0, Batch: 349: Training Loss: 0.04040439799427986, Validation Loss: 0.03635966405272484\n",
      "Epoch 0, Batch: 350: Training Loss: 0.03248138353228569, Validation Loss: 0.03546769917011261\n",
      "Epoch 0, Batch: 351: Training Loss: 0.03798443824052811, Validation Loss: 0.036533161997795105\n",
      "Epoch 0, Batch: 352: Training Loss: 0.037619445472955704, Validation Loss: 0.03454074263572693\n",
      "Epoch 0, Batch: 353: Training Loss: 0.029490096494555473, Validation Loss: 0.032242100685834885\n",
      "Epoch 0, Batch: 354: Training Loss: 0.041419386863708496, Validation Loss: 0.03508060425519943\n",
      "Epoch 0, Batch: 355: Training Loss: 0.034426502883434296, Validation Loss: 0.03664974123239517\n",
      "Epoch 0, Batch: 356: Training Loss: 0.03600648418068886, Validation Loss: 0.03374244272708893\n",
      "Epoch 0, Batch: 357: Training Loss: 0.03696076571941376, Validation Loss: 0.03246408328413963\n",
      "Epoch 0, Batch: 358: Training Loss: 0.036162517964839935, Validation Loss: 0.03121274709701538\n",
      "Epoch 0, Batch: 359: Training Loss: 0.03714917227625847, Validation Loss: 0.03256700187921524\n",
      "Epoch 0, Batch: 360: Training Loss: 0.03669450432062149, Validation Loss: 0.0331377349793911\n",
      "Epoch 0, Batch: 361: Training Loss: 0.03491244837641716, Validation Loss: 0.036524876952171326\n",
      "Epoch 0, Batch: 362: Training Loss: 0.036751989275217056, Validation Loss: 0.037318818271160126\n",
      "Epoch 0, Batch: 363: Training Loss: 0.03808581084012985, Validation Loss: 0.03449975326657295\n",
      "Epoch 0, Batch: 364: Training Loss: 0.03421430662274361, Validation Loss: 0.03453464433550835\n",
      "Epoch 0, Batch: 365: Training Loss: 0.034012287855148315, Validation Loss: 0.03332528471946716\n",
      "Epoch 0, Batch: 366: Training Loss: 0.03620836138725281, Validation Loss: 0.038156162947416306\n",
      "Epoch 0, Batch: 367: Training Loss: 0.03828179091215134, Validation Loss: 0.03993798419833183\n",
      "Epoch 0, Batch: 368: Training Loss: 0.03353412076830864, Validation Loss: 0.03918682783842087\n",
      "Epoch 0, Batch: 369: Training Loss: 0.030962180346250534, Validation Loss: 0.03718610852956772\n",
      "Epoch 0, Batch: 370: Training Loss: 0.0372488833963871, Validation Loss: 0.04106929153203964\n",
      "Epoch 0, Batch: 371: Training Loss: 0.03767191991209984, Validation Loss: 0.039159707725048065\n",
      "Epoch 0, Batch: 372: Training Loss: 0.03800123557448387, Validation Loss: 0.037733301520347595\n",
      "Epoch 0, Batch: 373: Training Loss: 0.03777387738227844, Validation Loss: 0.035272397100925446\n",
      "Epoch 0, Batch: 374: Training Loss: 0.031905777752399445, Validation Loss: 0.035971034318208694\n",
      "Epoch 0, Batch: 375: Training Loss: 0.0378689169883728, Validation Loss: 0.04021506384015083\n",
      "Epoch 0, Batch: 376: Training Loss: 0.0350317656993866, Validation Loss: 0.03580815717577934\n",
      "Epoch 0, Batch: 377: Training Loss: 0.032284654676914215, Validation Loss: 0.03373124450445175\n",
      "Epoch 0, Batch: 378: Training Loss: 0.039331115782260895, Validation Loss: 0.03960493206977844\n",
      "Epoch 0, Batch: 379: Training Loss: 0.04056841507554054, Validation Loss: 0.03743089362978935\n",
      "Epoch 0, Batch: 380: Training Loss: 0.043849825859069824, Validation Loss: 0.0363800972700119\n",
      "Epoch 0, Batch: 381: Training Loss: 0.036209411919116974, Validation Loss: 0.03782956302165985\n",
      "Epoch 0, Batch: 382: Training Loss: 0.03912585228681564, Validation Loss: 0.03679352626204491\n",
      "Epoch 0, Batch: 383: Training Loss: 0.033198777586221695, Validation Loss: 0.03450452908873558\n",
      "Epoch 0, Batch: 384: Training Loss: 0.03762317821383476, Validation Loss: 0.034330472350120544\n",
      "Epoch 0, Batch: 385: Training Loss: 0.03646988794207573, Validation Loss: 0.03441602364182472\n",
      "Epoch 0, Batch: 386: Training Loss: 0.03208997845649719, Validation Loss: 0.036754269152879715\n",
      "Epoch 0, Batch: 387: Training Loss: 0.036859650164842606, Validation Loss: 0.035915669053792953\n",
      "Epoch 0, Batch: 388: Training Loss: 0.034679774194955826, Validation Loss: 0.03876486420631409\n",
      "Epoch 0, Batch: 389: Training Loss: 0.03633738309144974, Validation Loss: 0.037170130759477615\n",
      "Epoch 0, Batch: 390: Training Loss: 0.03544661030173302, Validation Loss: 0.03979148715734482\n",
      "Epoch 0, Batch: 391: Training Loss: 0.03537842631340027, Validation Loss: 0.03356856480240822\n",
      "Epoch 0, Batch: 392: Training Loss: 0.033027615398168564, Validation Loss: 0.037982046604156494\n",
      "Epoch 0, Batch: 393: Training Loss: 0.03429709002375603, Validation Loss: 0.03302226960659027\n",
      "Epoch 0, Batch: 394: Training Loss: 0.03259362652897835, Validation Loss: 0.03401976823806763\n",
      "Epoch 0, Batch: 395: Training Loss: 0.03241116926074028, Validation Loss: 0.03380485996603966\n",
      "Epoch 0, Batch: 396: Training Loss: 0.03541651368141174, Validation Loss: 0.03185563534498215\n",
      "Epoch 0, Batch: 397: Training Loss: 0.04335201531648636, Validation Loss: 0.03582445904612541\n",
      "Epoch 0, Batch: 398: Training Loss: 0.0349145270884037, Validation Loss: 0.03331650421023369\n",
      "Epoch 0, Batch: 399: Training Loss: 0.03748075291514397, Validation Loss: 0.03345585986971855\n",
      "Epoch 0, Batch: 400: Training Loss: 0.03349215164780617, Validation Loss: 0.03366643190383911\n",
      "Epoch 0, Batch: 401: Training Loss: 0.03312674164772034, Validation Loss: 0.03585069626569748\n",
      "Epoch 0, Batch: 402: Training Loss: 0.03233442083001137, Validation Loss: 0.03455739468336105\n",
      "Epoch 0, Batch: 403: Training Loss: 0.039649974554777145, Validation Loss: 0.034498732537031174\n",
      "Epoch 0, Batch: 404: Training Loss: 0.03214212879538536, Validation Loss: 0.03569483757019043\n",
      "Epoch 0, Batch: 405: Training Loss: 0.034688420593738556, Validation Loss: 0.035699401050806046\n",
      "Epoch 0, Batch: 406: Training Loss: 0.03407840058207512, Validation Loss: 0.03629051148891449\n",
      "Epoch 0, Batch: 407: Training Loss: 0.03577212244272232, Validation Loss: 0.03748532012104988\n",
      "Epoch 0, Batch: 408: Training Loss: 0.033599406480789185, Validation Loss: 0.036954957991838455\n",
      "Epoch 0, Batch: 409: Training Loss: 0.03224848210811615, Validation Loss: 0.034728821367025375\n",
      "Epoch 0, Batch: 410: Training Loss: 0.031582266092300415, Validation Loss: 0.03543504700064659\n",
      "Epoch 0, Batch: 411: Training Loss: 0.0346771739423275, Validation Loss: 0.034658756107091904\n",
      "Epoch 0, Batch: 412: Training Loss: 0.03767457604408264, Validation Loss: 0.03570720553398132\n",
      "Epoch 0, Batch: 413: Training Loss: 0.041003648191690445, Validation Loss: 0.0324484147131443\n",
      "Epoch 0, Batch: 414: Training Loss: 0.030579788610339165, Validation Loss: 0.034948959946632385\n",
      "Epoch 0, Batch: 415: Training Loss: 0.033160384744405746, Validation Loss: 0.03573651611804962\n",
      "Epoch 0, Batch: 416: Training Loss: 0.03152201324701309, Validation Loss: 0.03607066348195076\n",
      "Epoch 0, Batch: 417: Training Loss: 0.03572383150458336, Validation Loss: 0.037138260900974274\n",
      "Saving new best model w/ loss: 0.02939174510538578\n",
      "Epoch 0, Batch: 418: Training Loss: 0.03209016099572182, Validation Loss: 0.02939174510538578\n",
      "Epoch 0, Batch: 419: Training Loss: 0.035445887595415115, Validation Loss: 0.03219197317957878\n",
      "Epoch 0, Batch: 420: Training Loss: 0.03519769012928009, Validation Loss: 0.029612258076667786\n",
      "Epoch 0, Batch: 421: Training Loss: 0.04066658765077591, Validation Loss: 0.035613179206848145\n",
      "Epoch 0, Batch: 422: Training Loss: 0.04018235579133034, Validation Loss: 0.03723696619272232\n",
      "Epoch 0, Batch: 423: Training Loss: 0.03597229719161987, Validation Loss: 0.033512696623802185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 424: Training Loss: 0.03975813835859299, Validation Loss: 0.035922519862651825\n",
      "Epoch 0, Batch: 425: Training Loss: 0.035441841930150986, Validation Loss: 0.03508198633790016\n",
      "Epoch 0, Batch: 426: Training Loss: 0.034038033336400986, Validation Loss: 0.032456111162900925\n",
      "Epoch 0, Batch: 427: Training Loss: 0.03299504518508911, Validation Loss: 0.03341829031705856\n",
      "Epoch 0, Batch: 428: Training Loss: 0.035778630524873734, Validation Loss: 0.032759517431259155\n",
      "Epoch 0, Batch: 429: Training Loss: 0.02962326630949974, Validation Loss: 0.03416658565402031\n",
      "Epoch 0, Batch: 430: Training Loss: 0.031748540699481964, Validation Loss: 0.035048287361860275\n",
      "Epoch 0, Batch: 431: Training Loss: 0.03207484260201454, Validation Loss: 0.03477955609560013\n",
      "Epoch 0, Batch: 432: Training Loss: 0.03589348867535591, Validation Loss: 0.03550194948911667\n",
      "Epoch 0, Batch: 433: Training Loss: 0.03711003437638283, Validation Loss: 0.03431824594736099\n",
      "Epoch 0, Batch: 434: Training Loss: 0.03556553274393082, Validation Loss: 0.03326795622706413\n",
      "Epoch 0, Batch: 435: Training Loss: 0.036820411682128906, Validation Loss: 0.03540538251399994\n",
      "Epoch 0, Batch: 436: Training Loss: 0.03348758444190025, Validation Loss: 0.03266255930066109\n",
      "Epoch 0, Batch: 437: Training Loss: 0.033631324768066406, Validation Loss: 0.03478479012846947\n",
      "Epoch 0, Batch: 438: Training Loss: 0.031224306672811508, Validation Loss: 0.03436934947967529\n",
      "Epoch 0, Batch: 439: Training Loss: 0.03365384414792061, Validation Loss: 0.0317203626036644\n",
      "Epoch 0, Batch: 440: Training Loss: 0.03544319048523903, Validation Loss: 0.036614518612623215\n",
      "Epoch 0, Batch: 441: Training Loss: 0.03779841214418411, Validation Loss: 0.03367837890982628\n",
      "Epoch 0, Batch: 442: Training Loss: 0.03762226924300194, Validation Loss: 0.03524043411016464\n",
      "Epoch 0, Batch: 443: Training Loss: 0.03746765851974487, Validation Loss: 0.030570244416594505\n",
      "Epoch 0, Batch: 444: Training Loss: 0.034314192831516266, Validation Loss: 0.03446166217327118\n",
      "Epoch 0, Batch: 445: Training Loss: 0.029974784702062607, Validation Loss: 0.03442087024450302\n",
      "Epoch 0, Batch: 446: Training Loss: 0.04119640588760376, Validation Loss: 0.035707924515008926\n",
      "Epoch 0, Batch: 447: Training Loss: 0.035216059535741806, Validation Loss: 0.03307187557220459\n",
      "Epoch 0, Batch: 448: Training Loss: 0.03584950789809227, Validation Loss: 0.032466497272253036\n",
      "Epoch 0, Batch: 449: Training Loss: 0.03427710011601448, Validation Loss: 0.03589198738336563\n",
      "Epoch 0, Batch: 450: Training Loss: 0.032297126948833466, Validation Loss: 0.032958317548036575\n",
      "Epoch 0, Batch: 451: Training Loss: 0.033480748534202576, Validation Loss: 0.03394947946071625\n",
      "Epoch 0, Batch: 452: Training Loss: 0.03536538779735565, Validation Loss: 0.03509189561009407\n",
      "Epoch 0, Batch: 453: Training Loss: 0.03521818295121193, Validation Loss: 0.03410649299621582\n",
      "Epoch 0, Batch: 454: Training Loss: 0.033323537558317184, Validation Loss: 0.03665951266884804\n",
      "Epoch 0, Batch: 455: Training Loss: 0.032934002578258514, Validation Loss: 0.03325941041111946\n",
      "Epoch 0, Batch: 456: Training Loss: 0.036588992923498154, Validation Loss: 0.03492329269647598\n",
      "Epoch 0, Batch: 457: Training Loss: 0.03541823476552963, Validation Loss: 0.036812905222177505\n",
      "Epoch 0, Batch: 458: Training Loss: 0.028076155111193657, Validation Loss: 0.032629724591970444\n",
      "Epoch 0, Batch: 459: Training Loss: 0.03034065105021, Validation Loss: 0.037008680403232574\n",
      "Epoch 0, Batch: 460: Training Loss: 0.03543077036738396, Validation Loss: 0.03474046662449837\n",
      "Epoch 0, Batch: 461: Training Loss: 0.03191974014043808, Validation Loss: 0.03700924292206764\n",
      "Epoch 0, Batch: 462: Training Loss: 0.032066430896520615, Validation Loss: 0.03642042353749275\n",
      "Epoch 0, Batch: 463: Training Loss: 0.03942154347896576, Validation Loss: 0.03369259089231491\n",
      "Epoch 0, Batch: 464: Training Loss: 0.0319402813911438, Validation Loss: 0.035581428557634354\n",
      "Epoch 0, Batch: 465: Training Loss: 0.029591014608740807, Validation Loss: 0.03387468680739403\n",
      "Epoch 0, Batch: 466: Training Loss: 0.033940739929676056, Validation Loss: 0.033449627459049225\n",
      "Epoch 0, Batch: 467: Training Loss: 0.04383702203631401, Validation Loss: 0.030223147943615913\n",
      "Epoch 0, Batch: 468: Training Loss: 0.03974027559161186, Validation Loss: 0.032812654972076416\n",
      "Epoch 0, Batch: 469: Training Loss: 0.03473477065563202, Validation Loss: 0.030930615961551666\n",
      "Epoch 0, Batch: 470: Training Loss: 0.03467544540762901, Validation Loss: 0.032934077084064484\n",
      "Epoch 0, Batch: 471: Training Loss: 0.03423583507537842, Validation Loss: 0.03446217253804207\n",
      "Epoch 0, Batch: 472: Training Loss: 0.03691328689455986, Validation Loss: 0.032913025468587875\n",
      "Epoch 0, Batch: 473: Training Loss: 0.030021054670214653, Validation Loss: 0.0325884073972702\n",
      "Epoch 0, Batch: 474: Training Loss: 0.032757390290498734, Validation Loss: 0.031540192663669586\n",
      "Epoch 0, Batch: 475: Training Loss: 0.03675398603081703, Validation Loss: 0.035880304872989655\n",
      "Epoch 0, Batch: 476: Training Loss: 0.03571804240345955, Validation Loss: 0.039173103868961334\n",
      "Epoch 0, Batch: 477: Training Loss: 0.03732919320464134, Validation Loss: 0.036177147179841995\n",
      "Epoch 0, Batch: 478: Training Loss: 0.03280743584036827, Validation Loss: 0.034715279936790466\n",
      "Epoch 0, Batch: 479: Training Loss: 0.03579138219356537, Validation Loss: 0.03342822566628456\n",
      "Epoch 0, Batch: 480: Training Loss: 0.033383507281541824, Validation Loss: 0.03245462477207184\n",
      "Epoch 0, Batch: 481: Training Loss: 0.03604935109615326, Validation Loss: 0.034799762070178986\n",
      "Epoch 0, Batch: 482: Training Loss: 0.03783037140965462, Validation Loss: 0.03327935189008713\n",
      "Epoch 0, Batch: 483: Training Loss: 0.03565920144319534, Validation Loss: 0.031140392646193504\n",
      "Epoch 0, Batch: 484: Training Loss: 0.033753685653209686, Validation Loss: 0.03358352184295654\n",
      "Epoch 0, Batch: 485: Training Loss: 0.03175126761198044, Validation Loss: 0.03472534567117691\n",
      "Epoch 0, Batch: 486: Training Loss: 0.035098373889923096, Validation Loss: 0.03393300622701645\n",
      "Epoch 0, Batch: 487: Training Loss: 0.03221645951271057, Validation Loss: 0.03337467834353447\n",
      "Epoch 0, Batch: 488: Training Loss: 0.03461344912648201, Validation Loss: 0.03375221788883209\n",
      "Epoch 0, Batch: 489: Training Loss: 0.03655532747507095, Validation Loss: 0.03463444858789444\n",
      "Epoch 0, Batch: 490: Training Loss: 0.03528860583901405, Validation Loss: 0.03473666310310364\n",
      "Epoch 0, Batch: 491: Training Loss: 0.03529277816414833, Validation Loss: 0.03318263590335846\n",
      "Epoch 0, Batch: 492: Training Loss: 0.03349287062883377, Validation Loss: 0.0332782007753849\n",
      "Epoch 0, Batch: 493: Training Loss: 0.03695077821612358, Validation Loss: 0.033681195229291916\n",
      "Epoch 0, Batch: 494: Training Loss: 0.04090209677815437, Validation Loss: 0.034191716462373734\n",
      "Epoch 0, Batch: 495: Training Loss: 0.035529810935258865, Validation Loss: 0.03209881857037544\n",
      "Epoch 0, Batch: 496: Training Loss: 0.03278001770377159, Validation Loss: 0.037668485194444656\n",
      "Epoch 0, Batch: 497: Training Loss: 0.03277650848031044, Validation Loss: 0.03364374861121178\n",
      "Epoch 0, Batch: 498: Training Loss: 0.032707516103982925, Validation Loss: 0.03468121588230133\n",
      "Epoch 0, Batch: 499: Training Loss: 0.032430440187454224, Validation Loss: 0.034987907856702805\n",
      "Epoch 1, Batch: 0: Training Loss: 0.035060565918684006, Validation Loss: 0.03380471095442772\n",
      "Epoch 1, Batch: 1: Training Loss: 0.032905302941799164, Validation Loss: 0.03398338332772255\n",
      "Epoch 1, Batch: 2: Training Loss: 0.03257620707154274, Validation Loss: 0.0331173911690712\n",
      "Epoch 1, Batch: 3: Training Loss: 0.02847140096127987, Validation Loss: 0.03535013273358345\n",
      "Epoch 1, Batch: 4: Training Loss: 0.029847342520952225, Validation Loss: 0.034016598016023636\n",
      "Epoch 1, Batch: 5: Training Loss: 0.03265713155269623, Validation Loss: 0.0357900932431221\n",
      "Epoch 1, Batch: 6: Training Loss: 0.033799100667238235, Validation Loss: 0.031149843707680702\n",
      "Epoch 1, Batch: 7: Training Loss: 0.03248489275574684, Validation Loss: 0.03349291905760765\n",
      "Epoch 1, Batch: 8: Training Loss: 0.03267233818769455, Validation Loss: 0.032692790031433105\n",
      "Epoch 1, Batch: 9: Training Loss: 0.028742924332618713, Validation Loss: 0.03155258297920227\n",
      "Epoch 1, Batch: 10: Training Loss: 0.03463155776262283, Validation Loss: 0.032182276248931885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 11: Training Loss: 0.03381842374801636, Validation Loss: 0.032826993614435196\n",
      "Epoch 1, Batch: 12: Training Loss: 0.03521464392542839, Validation Loss: 0.032358862459659576\n",
      "Epoch 1, Batch: 13: Training Loss: 0.03311366215348244, Validation Loss: 0.03091748245060444\n",
      "Epoch 1, Batch: 14: Training Loss: 0.03565632179379463, Validation Loss: 0.03452848270535469\n",
      "Epoch 1, Batch: 15: Training Loss: 0.04155822470784187, Validation Loss: 0.031901489943265915\n",
      "Epoch 1, Batch: 16: Training Loss: 0.03676836937665939, Validation Loss: 0.03612670674920082\n",
      "Epoch 1, Batch: 17: Training Loss: 0.026409145444631577, Validation Loss: 0.031699348241090775\n",
      "Epoch 1, Batch: 18: Training Loss: 0.029385097324848175, Validation Loss: 0.03250829130411148\n",
      "Epoch 1, Batch: 19: Training Loss: 0.02658754773437977, Validation Loss: 0.033506378531455994\n",
      "Epoch 1, Batch: 20: Training Loss: 0.031020544469356537, Validation Loss: 0.032292596995830536\n",
      "Epoch 1, Batch: 21: Training Loss: 0.03513051196932793, Validation Loss: 0.03491586446762085\n",
      "Epoch 1, Batch: 22: Training Loss: 0.03878217935562134, Validation Loss: 0.03295055031776428\n",
      "Epoch 1, Batch: 23: Training Loss: 0.03310142084956169, Validation Loss: 0.03206557035446167\n",
      "Epoch 1, Batch: 24: Training Loss: 0.031544510275125504, Validation Loss: 0.03270716220140457\n",
      "Epoch 1, Batch: 25: Training Loss: 0.028712527826428413, Validation Loss: 0.03152529522776604\n",
      "Epoch 1, Batch: 26: Training Loss: 0.03307875990867615, Validation Loss: 0.032291606068611145\n",
      "Epoch 1, Batch: 27: Training Loss: 0.033564481884241104, Validation Loss: 0.03400922194123268\n",
      "Epoch 1, Batch: 28: Training Loss: 0.035928864032030106, Validation Loss: 0.03207119554281235\n",
      "Epoch 1, Batch: 29: Training Loss: 0.03179766610264778, Validation Loss: 0.03266441076993942\n",
      "Epoch 1, Batch: 30: Training Loss: 0.03402452915906906, Validation Loss: 0.03265976533293724\n",
      "Epoch 1, Batch: 31: Training Loss: 0.03602701053023338, Validation Loss: 0.030466627329587936\n",
      "Epoch 1, Batch: 32: Training Loss: 0.03760208934545517, Validation Loss: 0.031995587050914764\n",
      "Epoch 1, Batch: 33: Training Loss: 0.03148715943098068, Validation Loss: 0.032347749918699265\n",
      "Epoch 1, Batch: 34: Training Loss: 0.03149480000138283, Validation Loss: 0.03564446046948433\n",
      "Epoch 1, Batch: 35: Training Loss: 0.03476465120911598, Validation Loss: 0.03561965748667717\n",
      "Epoch 1, Batch: 36: Training Loss: 0.03171619400382042, Validation Loss: 0.03675634413957596\n",
      "Epoch 1, Batch: 37: Training Loss: 0.029976021498441696, Validation Loss: 0.035387955605983734\n",
      "Epoch 1, Batch: 38: Training Loss: 0.036184776574373245, Validation Loss: 0.03705839067697525\n",
      "Epoch 1, Batch: 39: Training Loss: 0.036832526326179504, Validation Loss: 0.03516608849167824\n",
      "Epoch 1, Batch: 40: Training Loss: 0.032246414572000504, Validation Loss: 0.03812594711780548\n",
      "Epoch 1, Batch: 41: Training Loss: 0.02933346852660179, Validation Loss: 0.03754622861742973\n",
      "Epoch 1, Batch: 42: Training Loss: 0.031318966299295425, Validation Loss: 0.03494807705283165\n",
      "Epoch 1, Batch: 43: Training Loss: 0.03414390608668327, Validation Loss: 0.03620375320315361\n",
      "Epoch 1, Batch: 44: Training Loss: 0.034488778561353683, Validation Loss: 0.03995218127965927\n",
      "Epoch 1, Batch: 45: Training Loss: 0.030837904661893845, Validation Loss: 0.03986845538020134\n",
      "Epoch 1, Batch: 46: Training Loss: 0.03214103356003761, Validation Loss: 0.03656546398997307\n",
      "Epoch 1, Batch: 47: Training Loss: 0.033890243619680405, Validation Loss: 0.03793793171644211\n",
      "Epoch 1, Batch: 48: Training Loss: 0.04823019355535507, Validation Loss: 0.03744727373123169\n",
      "Epoch 1, Batch: 49: Training Loss: 0.03433134779334068, Validation Loss: 0.035485126078128815\n",
      "Epoch 1, Batch: 50: Training Loss: 0.03689737245440483, Validation Loss: 0.035786308348178864\n",
      "Epoch 1, Batch: 51: Training Loss: 0.03455138951539993, Validation Loss: 0.039201490581035614\n",
      "Epoch 1, Batch: 52: Training Loss: 0.03560167923569679, Validation Loss: 0.042041875422000885\n",
      "Epoch 1, Batch: 53: Training Loss: 0.02765950746834278, Validation Loss: 0.04012136533856392\n",
      "Epoch 1, Batch: 54: Training Loss: 0.03457225114107132, Validation Loss: 0.04090085253119469\n",
      "Epoch 1, Batch: 55: Training Loss: 0.03602762520313263, Validation Loss: 0.04020068421959877\n",
      "Epoch 1, Batch: 56: Training Loss: 0.0323229655623436, Validation Loss: 0.04311098903417587\n",
      "Epoch 1, Batch: 57: Training Loss: 0.032873257994651794, Validation Loss: 0.041241347789764404\n",
      "Epoch 1, Batch: 58: Training Loss: 0.03300030529499054, Validation Loss: 0.0366988442838192\n",
      "Epoch 1, Batch: 59: Training Loss: 0.028219103813171387, Validation Loss: 0.03576319292187691\n",
      "Epoch 1, Batch: 60: Training Loss: 0.033007439225912094, Validation Loss: 0.034609366208314896\n",
      "Epoch 1, Batch: 61: Training Loss: 0.03541236370801926, Validation Loss: 0.034392230212688446\n",
      "Epoch 1, Batch: 62: Training Loss: 0.03435038402676582, Validation Loss: 0.03433140367269516\n",
      "Epoch 1, Batch: 63: Training Loss: 0.031078966334462166, Validation Loss: 0.03449666127562523\n",
      "Epoch 1, Batch: 64: Training Loss: 0.03583521023392677, Validation Loss: 0.036512989550828934\n",
      "Epoch 1, Batch: 65: Training Loss: 0.03784187138080597, Validation Loss: 0.03416796401143074\n",
      "Epoch 1, Batch: 66: Training Loss: 0.03412283957004547, Validation Loss: 0.034600384533405304\n",
      "Epoch 1, Batch: 67: Training Loss: 0.0322582833468914, Validation Loss: 0.03332115337252617\n",
      "Epoch 1, Batch: 68: Training Loss: 0.03132824972271919, Validation Loss: 0.03256433084607124\n",
      "Epoch 1, Batch: 69: Training Loss: 0.03260507062077522, Validation Loss: 0.03456392511725426\n",
      "Epoch 1, Batch: 70: Training Loss: 0.03881588205695152, Validation Loss: 0.03772302344441414\n",
      "Epoch 1, Batch: 71: Training Loss: 0.034132279455661774, Validation Loss: 0.03225855156779289\n",
      "Epoch 1, Batch: 72: Training Loss: 0.03124195896089077, Validation Loss: 0.03584626317024231\n",
      "Epoch 1, Batch: 73: Training Loss: 0.031091144308447838, Validation Loss: 0.034034810960292816\n",
      "Epoch 1, Batch: 74: Training Loss: 0.0277927927672863, Validation Loss: 0.03173915296792984\n",
      "Epoch 1, Batch: 75: Training Loss: 0.026513874530792236, Validation Loss: 0.03421104699373245\n",
      "Epoch 1, Batch: 76: Training Loss: 0.032890040427446365, Validation Loss: 0.03225187212228775\n",
      "Epoch 1, Batch: 77: Training Loss: 0.03842975199222565, Validation Loss: 0.03342074528336525\n",
      "Epoch 1, Batch: 78: Training Loss: 0.03674398362636566, Validation Loss: 0.03152446076273918\n",
      "Epoch 1, Batch: 79: Training Loss: 0.02964753285050392, Validation Loss: 0.03466126695275307\n",
      "Epoch 1, Batch: 80: Training Loss: 0.03207577019929886, Validation Loss: 0.03130850940942764\n",
      "Epoch 1, Batch: 81: Training Loss: 0.03249501809477806, Validation Loss: 0.030156269669532776\n",
      "Epoch 1, Batch: 82: Training Loss: 0.0350039005279541, Validation Loss: 0.03081066906452179\n",
      "Epoch 1, Batch: 83: Training Loss: 0.031535182148218155, Validation Loss: 0.030576098710298538\n",
      "Epoch 1, Batch: 84: Training Loss: 0.03253583237528801, Validation Loss: 0.02979438751935959\n",
      "Epoch 1, Batch: 85: Training Loss: 0.028207460418343544, Validation Loss: 0.031014634296298027\n",
      "Epoch 1, Batch: 86: Training Loss: 0.029709240421652794, Validation Loss: 0.0331171452999115\n",
      "Epoch 1, Batch: 87: Training Loss: 0.03302822634577751, Validation Loss: 0.03377123922109604\n",
      "Epoch 1, Batch: 88: Training Loss: 0.03652925789356232, Validation Loss: 0.03132256492972374\n",
      "Epoch 1, Batch: 89: Training Loss: 0.034407470375299454, Validation Loss: 0.03060995787382126\n",
      "Epoch 1, Batch: 90: Training Loss: 0.030318591743707657, Validation Loss: 0.03147239238023758\n",
      "Epoch 1, Batch: 91: Training Loss: 0.03142024576663971, Validation Loss: 0.029573604464530945\n",
      "Saving new best model w/ loss: 0.029294969514012337\n",
      "Epoch 1, Batch: 92: Training Loss: 0.035234272480010986, Validation Loss: 0.029294969514012337\n",
      "Saving new best model w/ loss: 0.02728916145861149\n",
      "Epoch 1, Batch: 93: Training Loss: 0.032821375876665115, Validation Loss: 0.02728916145861149\n",
      "Epoch 1, Batch: 94: Training Loss: 0.03641005605459213, Validation Loss: 0.031898383051157\n",
      "Epoch 1, Batch: 95: Training Loss: 0.030987480655312538, Validation Loss: 0.03201865404844284\n",
      "Epoch 1, Batch: 96: Training Loss: 0.029455704614520073, Validation Loss: 0.030326416715979576\n",
      "Epoch 1, Batch: 97: Training Loss: 0.03186752274632454, Validation Loss: 0.02953433245420456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 98: Training Loss: 0.034023746848106384, Validation Loss: 0.03271356597542763\n",
      "Epoch 1, Batch: 99: Training Loss: 0.036336544901132584, Validation Loss: 0.031483449041843414\n",
      "Epoch 1, Batch: 100: Training Loss: 0.03367554396390915, Validation Loss: 0.034676045179367065\n",
      "Epoch 1, Batch: 101: Training Loss: 0.03180703893303871, Validation Loss: 0.034221816807985306\n",
      "Epoch 1, Batch: 102: Training Loss: 0.03108825720846653, Validation Loss: 0.03263739123940468\n",
      "Epoch 1, Batch: 103: Training Loss: 0.03459062799811363, Validation Loss: 0.03082759492099285\n",
      "Epoch 1, Batch: 104: Training Loss: 0.03168083727359772, Validation Loss: 0.03388736769556999\n",
      "Epoch 1, Batch: 105: Training Loss: 0.029567169025540352, Validation Loss: 0.030981402844190598\n",
      "Epoch 1, Batch: 106: Training Loss: 0.03559226542711258, Validation Loss: 0.03518032282590866\n",
      "Epoch 1, Batch: 107: Training Loss: 0.032453589141368866, Validation Loss: 0.03334526717662811\n",
      "Epoch 1, Batch: 108: Training Loss: 0.03193652629852295, Validation Loss: 0.0336930938065052\n",
      "Epoch 1, Batch: 109: Training Loss: 0.03800687566399574, Validation Loss: 0.03566601499915123\n",
      "Epoch 1, Batch: 110: Training Loss: 0.03398938477039337, Validation Loss: 0.03574906289577484\n",
      "Epoch 1, Batch: 111: Training Loss: 0.03336988762021065, Validation Loss: 0.034073878079652786\n",
      "Epoch 1, Batch: 112: Training Loss: 0.029621358960866928, Validation Loss: 0.03477046266198158\n",
      "Epoch 1, Batch: 113: Training Loss: 0.02848929353058338, Validation Loss: 0.030663276091217995\n",
      "Epoch 1, Batch: 114: Training Loss: 0.03198901191353798, Validation Loss: 0.03253689035773277\n",
      "Epoch 1, Batch: 115: Training Loss: 0.03489790856838226, Validation Loss: 0.03221612796187401\n",
      "Epoch 1, Batch: 116: Training Loss: 0.02813720703125, Validation Loss: 0.03271656483411789\n",
      "Epoch 1, Batch: 117: Training Loss: 0.035708244889974594, Validation Loss: 0.032499320805072784\n",
      "Epoch 1, Batch: 118: Training Loss: 0.02954822964966297, Validation Loss: 0.03207562491297722\n",
      "Epoch 1, Batch: 119: Training Loss: 0.03578950837254524, Validation Loss: 0.032467279583215714\n",
      "Epoch 1, Batch: 120: Training Loss: 0.03028627671301365, Validation Loss: 0.030780451372265816\n",
      "Epoch 1, Batch: 121: Training Loss: 0.03195773810148239, Validation Loss: 0.031236963346600533\n",
      "Epoch 1, Batch: 122: Training Loss: 0.030155835673213005, Validation Loss: 0.032142624258995056\n",
      "Epoch 1, Batch: 123: Training Loss: 0.03279787674546242, Validation Loss: 0.032359108328819275\n",
      "Epoch 1, Batch: 124: Training Loss: 0.03373221307992935, Validation Loss: 0.03410043194890022\n",
      "Epoch 1, Batch: 125: Training Loss: 0.030852487310767174, Validation Loss: 0.032250117510557175\n",
      "Epoch 1, Batch: 126: Training Loss: 0.027563225477933884, Validation Loss: 0.030339296907186508\n",
      "Epoch 1, Batch: 127: Training Loss: 0.03278345242142677, Validation Loss: 0.030056515708565712\n",
      "Epoch 1, Batch: 128: Training Loss: 0.029249094426631927, Validation Loss: 0.033159710466861725\n",
      "Epoch 1, Batch: 129: Training Loss: 0.03045865148305893, Validation Loss: 0.030477242544293404\n",
      "Epoch 1, Batch: 130: Training Loss: 0.033257029950618744, Validation Loss: 0.03237998113036156\n",
      "Epoch 1, Batch: 131: Training Loss: 0.030612433329224586, Validation Loss: 0.03222037851810455\n",
      "Epoch 1, Batch: 132: Training Loss: 0.03265736624598503, Validation Loss: 0.028139730915427208\n",
      "Epoch 1, Batch: 133: Training Loss: 0.031106112524867058, Validation Loss: 0.03092414326965809\n",
      "Epoch 1, Batch: 134: Training Loss: 0.033905480057001114, Validation Loss: 0.029154136776924133\n",
      "Epoch 1, Batch: 135: Training Loss: 0.027889151126146317, Validation Loss: 0.02932707406580448\n",
      "Epoch 1, Batch: 136: Training Loss: 0.031914904713630676, Validation Loss: 0.029813287779688835\n",
      "Saving new best model w/ loss: 0.02674734964966774\n",
      "Epoch 1, Batch: 137: Training Loss: 0.029922660440206528, Validation Loss: 0.02674734964966774\n",
      "Epoch 1, Batch: 138: Training Loss: 0.029186096042394638, Validation Loss: 0.03128452226519585\n",
      "Epoch 1, Batch: 139: Training Loss: 0.02979215234518051, Validation Loss: 0.02916089817881584\n",
      "Epoch 1, Batch: 140: Training Loss: 0.03189835697412491, Validation Loss: 0.029714293777942657\n",
      "Epoch 1, Batch: 141: Training Loss: 0.031457096338272095, Validation Loss: 0.029262304306030273\n",
      "Epoch 1, Batch: 142: Training Loss: 0.031619843095541, Validation Loss: 0.030559122562408447\n",
      "Epoch 1, Batch: 143: Training Loss: 0.03117000125348568, Validation Loss: 0.028664708137512207\n",
      "Epoch 1, Batch: 144: Training Loss: 0.03281359374523163, Validation Loss: 0.031137607991695404\n",
      "Epoch 1, Batch: 145: Training Loss: 0.026882588863372803, Validation Loss: 0.028088903054594994\n",
      "Epoch 1, Batch: 146: Training Loss: 0.03149190545082092, Validation Loss: 0.028033867478370667\n",
      "Epoch 1, Batch: 147: Training Loss: 0.030759181827306747, Validation Loss: 0.029424922540783882\n",
      "Epoch 1, Batch: 148: Training Loss: 0.032367199659347534, Validation Loss: 0.02974293753504753\n",
      "Epoch 1, Batch: 149: Training Loss: 0.02965673990547657, Validation Loss: 0.0272492878139019\n",
      "Epoch 1, Batch: 150: Training Loss: 0.0353306345641613, Validation Loss: 0.029590370133519173\n",
      "Epoch 1, Batch: 151: Training Loss: 0.029014451429247856, Validation Loss: 0.030005164444446564\n",
      "Epoch 1, Batch: 152: Training Loss: 0.036168769001960754, Validation Loss: 0.0307509396225214\n",
      "Epoch 1, Batch: 153: Training Loss: 0.034745536744594574, Validation Loss: 0.029548197984695435\n",
      "Saving new best model w/ loss: 0.02602248825132847\n",
      "Epoch 1, Batch: 154: Training Loss: 0.03225000202655792, Validation Loss: 0.02602248825132847\n",
      "Epoch 1, Batch: 155: Training Loss: 0.03889179229736328, Validation Loss: 0.030447300523519516\n",
      "Epoch 1, Batch: 156: Training Loss: 0.032428134232759476, Validation Loss: 0.03030932880938053\n",
      "Epoch 1, Batch: 157: Training Loss: 0.028169257566332817, Validation Loss: 0.03107122890651226\n",
      "Epoch 1, Batch: 158: Training Loss: 0.033017516136169434, Validation Loss: 0.03346431255340576\n",
      "Epoch 1, Batch: 159: Training Loss: 0.03811563551425934, Validation Loss: 0.032332487404346466\n",
      "Epoch 1, Batch: 160: Training Loss: 0.02738858573138714, Validation Loss: 0.03124396502971649\n",
      "Epoch 1, Batch: 161: Training Loss: 0.032673243433237076, Validation Loss: 0.02863873355090618\n",
      "Epoch 1, Batch: 162: Training Loss: 0.03887110948562622, Validation Loss: 0.028850317001342773\n",
      "Epoch 1, Batch: 163: Training Loss: 0.032368939369916916, Validation Loss: 0.034634433686733246\n",
      "Epoch 1, Batch: 164: Training Loss: 0.03512059152126312, Validation Loss: 0.03039681538939476\n",
      "Epoch 1, Batch: 165: Training Loss: 0.03306592255830765, Validation Loss: 0.02906397357583046\n",
      "Epoch 1, Batch: 166: Training Loss: 0.029407760128378868, Validation Loss: 0.02953091822564602\n",
      "Epoch 1, Batch: 167: Training Loss: 0.03158073127269745, Validation Loss: 0.03158394247293472\n",
      "Epoch 1, Batch: 168: Training Loss: 0.029227787628769875, Validation Loss: 0.03068409115076065\n",
      "Epoch 1, Batch: 169: Training Loss: 0.03596161678433418, Validation Loss: 0.030341723933815956\n",
      "Epoch 1, Batch: 170: Training Loss: 0.033851537853479385, Validation Loss: 0.031093090772628784\n",
      "Epoch 1, Batch: 171: Training Loss: 0.026430465281009674, Validation Loss: 0.03207821771502495\n",
      "Epoch 1, Batch: 172: Training Loss: 0.03626100346446037, Validation Loss: 0.03269418701529503\n",
      "Epoch 1, Batch: 173: Training Loss: 0.030845116823911667, Validation Loss: 0.031435053795576096\n",
      "Epoch 1, Batch: 174: Training Loss: 0.035969119518995285, Validation Loss: 0.0337800532579422\n",
      "Epoch 1, Batch: 175: Training Loss: 0.034800685942173004, Validation Loss: 0.031196124851703644\n",
      "Epoch 1, Batch: 176: Training Loss: 0.03546714782714844, Validation Loss: 0.03653952106833458\n",
      "Epoch 1, Batch: 177: Training Loss: 0.03442087024450302, Validation Loss: 0.0335722416639328\n",
      "Epoch 1, Batch: 178: Training Loss: 0.03429686650633812, Validation Loss: 0.03277868404984474\n",
      "Epoch 1, Batch: 179: Training Loss: 0.03286592289805412, Validation Loss: 0.03323259577155113\n",
      "Epoch 1, Batch: 180: Training Loss: 0.03819131851196289, Validation Loss: 0.034032028168439865\n",
      "Epoch 1, Batch: 181: Training Loss: 0.03562314435839653, Validation Loss: 0.03261961787939072\n",
      "Epoch 1, Batch: 182: Training Loss: 0.03750220313668251, Validation Loss: 0.03164592385292053\n",
      "Epoch 1, Batch: 183: Training Loss: 0.03261273726820946, Validation Loss: 0.03235502913594246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 184: Training Loss: 0.038159340620040894, Validation Loss: 0.032805055379867554\n",
      "Epoch 1, Batch: 185: Training Loss: 0.038303494453430176, Validation Loss: 0.030304353684186935\n",
      "Epoch 1, Batch: 186: Training Loss: 0.03143683075904846, Validation Loss: 0.03178488835692406\n",
      "Epoch 1, Batch: 187: Training Loss: 0.03773784264922142, Validation Loss: 0.032751359045505524\n",
      "Epoch 1, Batch: 188: Training Loss: 0.033358603715896606, Validation Loss: 0.03293692693114281\n",
      "Epoch 1, Batch: 189: Training Loss: 0.030441930517554283, Validation Loss: 0.03392939642071724\n",
      "Epoch 1, Batch: 190: Training Loss: 0.034669723361730576, Validation Loss: 0.032340701669454575\n",
      "Epoch 1, Batch: 191: Training Loss: 0.033561769872903824, Validation Loss: 0.030442742630839348\n",
      "Epoch 1, Batch: 192: Training Loss: 0.03378196060657501, Validation Loss: 0.029766405001282692\n",
      "Epoch 1, Batch: 193: Training Loss: 0.02945658750832081, Validation Loss: 0.03304090350866318\n",
      "Epoch 1, Batch: 194: Training Loss: 0.034804243594408035, Validation Loss: 0.03214182332158089\n",
      "Epoch 1, Batch: 195: Training Loss: 0.03528004139661789, Validation Loss: 0.03175302594900131\n",
      "Epoch 1, Batch: 196: Training Loss: 0.035857707262039185, Validation Loss: 0.031812913715839386\n",
      "Epoch 1, Batch: 197: Training Loss: 0.032170865684747696, Validation Loss: 0.03383570536971092\n",
      "Epoch 1, Batch: 198: Training Loss: 0.02961018867790699, Validation Loss: 0.029841534793376923\n",
      "Epoch 1, Batch: 199: Training Loss: 0.028890416026115417, Validation Loss: 0.03126375004649162\n",
      "Epoch 1, Batch: 200: Training Loss: 0.030842863023281097, Validation Loss: 0.03416921943426132\n",
      "Epoch 1, Batch: 201: Training Loss: 0.03606674075126648, Validation Loss: 0.03266018256545067\n",
      "Epoch 1, Batch: 202: Training Loss: 0.03817512094974518, Validation Loss: 0.03433721140027046\n",
      "Epoch 1, Batch: 203: Training Loss: 0.03757563978433609, Validation Loss: 0.031988125294446945\n",
      "Epoch 1, Batch: 204: Training Loss: 0.03772686794400215, Validation Loss: 0.03311777859926224\n",
      "Epoch 1, Batch: 205: Training Loss: 0.03436969593167305, Validation Loss: 0.032464079558849335\n",
      "Epoch 1, Batch: 206: Training Loss: 0.03541959077119827, Validation Loss: 0.03096705488860607\n",
      "Epoch 1, Batch: 207: Training Loss: 0.03206951543688774, Validation Loss: 0.03272940218448639\n",
      "Epoch 1, Batch: 208: Training Loss: 0.027301263064146042, Validation Loss: 0.033585935831069946\n",
      "Epoch 1, Batch: 209: Training Loss: 0.03401723876595497, Validation Loss: 0.031261004507541656\n",
      "Epoch 1, Batch: 210: Training Loss: 0.03672197461128235, Validation Loss: 0.030218858271837234\n",
      "Epoch 1, Batch: 211: Training Loss: 0.03454356640577316, Validation Loss: 0.03248831257224083\n",
      "Epoch 1, Batch: 212: Training Loss: 0.032150108367204666, Validation Loss: 0.031001659110188484\n",
      "Epoch 1, Batch: 213: Training Loss: 0.03802946209907532, Validation Loss: 0.029437027871608734\n",
      "Epoch 1, Batch: 214: Training Loss: 0.03137323632836342, Validation Loss: 0.030172010883688927\n",
      "Epoch 1, Batch: 215: Training Loss: 0.03152019903063774, Validation Loss: 0.030384551733732224\n",
      "Epoch 1, Batch: 216: Training Loss: 0.02905089221894741, Validation Loss: 0.030848529189825058\n",
      "Epoch 1, Batch: 217: Training Loss: 0.032779015600681305, Validation Loss: 0.03142170235514641\n",
      "Epoch 1, Batch: 218: Training Loss: 0.03359052911400795, Validation Loss: 0.028312765061855316\n",
      "Epoch 1, Batch: 219: Training Loss: 0.031116999685764313, Validation Loss: 0.030233394354581833\n",
      "Epoch 1, Batch: 220: Training Loss: 0.036409392952919006, Validation Loss: 0.030335411429405212\n",
      "Epoch 1, Batch: 221: Training Loss: 0.030827805399894714, Validation Loss: 0.03034031391143799\n",
      "Epoch 1, Batch: 222: Training Loss: 0.03763463720679283, Validation Loss: 0.031071491539478302\n",
      "Epoch 1, Batch: 223: Training Loss: 0.028956562280654907, Validation Loss: 0.03128926083445549\n",
      "Epoch 1, Batch: 224: Training Loss: 0.03259832039475441, Validation Loss: 0.031421270221471786\n",
      "Epoch 1, Batch: 225: Training Loss: 0.027417344972491264, Validation Loss: 0.03478400036692619\n",
      "Epoch 1, Batch: 226: Training Loss: 0.03467483073472977, Validation Loss: 0.028825314715504646\n",
      "Epoch 1, Batch: 227: Training Loss: 0.030903395265340805, Validation Loss: 0.029634032398462296\n",
      "Epoch 1, Batch: 228: Training Loss: 0.030723851174116135, Validation Loss: 0.02913101576268673\n",
      "Epoch 1, Batch: 229: Training Loss: 0.027853960171341896, Validation Loss: 0.029773732647299767\n",
      "Epoch 1, Batch: 230: Training Loss: 0.02842344343662262, Validation Loss: 0.030658002942800522\n",
      "Epoch 1, Batch: 231: Training Loss: 0.03229649364948273, Validation Loss: 0.03283708542585373\n",
      "Epoch 1, Batch: 232: Training Loss: 0.033761974424123764, Validation Loss: 0.029659537598490715\n",
      "Epoch 1, Batch: 233: Training Loss: 0.02983914501965046, Validation Loss: 0.03162186220288277\n",
      "Epoch 1, Batch: 234: Training Loss: 0.035380855202674866, Validation Loss: 0.028078844770789146\n",
      "Epoch 1, Batch: 235: Training Loss: 0.039006687700748444, Validation Loss: 0.031363435089588165\n",
      "Epoch 1, Batch: 236: Training Loss: 0.038950737565755844, Validation Loss: 0.03143049776554108\n",
      "Epoch 1, Batch: 237: Training Loss: 0.031665243208408356, Validation Loss: 0.030109332874417305\n",
      "Epoch 1, Batch: 238: Training Loss: 0.036353133618831635, Validation Loss: 0.03098420612514019\n",
      "Epoch 1, Batch: 239: Training Loss: 0.031995899975299835, Validation Loss: 0.031637851148843765\n",
      "Epoch 1, Batch: 240: Training Loss: 0.027711043134331703, Validation Loss: 0.03430942818522453\n",
      "Epoch 1, Batch: 241: Training Loss: 0.031095461919903755, Validation Loss: 0.032366812229156494\n",
      "Epoch 1, Batch: 242: Training Loss: 0.03126399591565132, Validation Loss: 0.03275936841964722\n",
      "Epoch 1, Batch: 243: Training Loss: 0.03650218993425369, Validation Loss: 0.030615191906690598\n",
      "Epoch 1, Batch: 244: Training Loss: 0.03312518447637558, Validation Loss: 0.0319347009062767\n",
      "Epoch 1, Batch: 245: Training Loss: 0.03813132643699646, Validation Loss: 0.032215144485235214\n",
      "Epoch 1, Batch: 246: Training Loss: 0.02856237255036831, Validation Loss: 0.031949713826179504\n",
      "Epoch 1, Batch: 247: Training Loss: 0.033028122037649155, Validation Loss: 0.029500408098101616\n",
      "Epoch 1, Batch: 248: Training Loss: 0.035731181502342224, Validation Loss: 0.0328228585422039\n",
      "Epoch 1, Batch: 249: Training Loss: 0.03482253476977348, Validation Loss: 0.03242202475667\n",
      "Epoch 1, Batch: 250: Training Loss: 0.029407521709799767, Validation Loss: 0.03254816308617592\n",
      "Epoch 1, Batch: 251: Training Loss: 0.0294929388910532, Validation Loss: 0.0320369154214859\n",
      "Epoch 1, Batch: 252: Training Loss: 0.03394585847854614, Validation Loss: 0.032169654965400696\n",
      "Epoch 1, Batch: 253: Training Loss: 0.037795085459947586, Validation Loss: 0.03151978924870491\n",
      "Epoch 1, Batch: 254: Training Loss: 0.02778335101902485, Validation Loss: 0.031100573018193245\n",
      "Epoch 1, Batch: 255: Training Loss: 0.029683055356144905, Validation Loss: 0.028349071741104126\n",
      "Epoch 1, Batch: 256: Training Loss: 0.03432948887348175, Validation Loss: 0.030390895903110504\n",
      "Epoch 1, Batch: 257: Training Loss: 0.03432857617735863, Validation Loss: 0.031009124591946602\n",
      "Epoch 1, Batch: 258: Training Loss: 0.028418805450201035, Validation Loss: 0.03087102249264717\n",
      "Epoch 1, Batch: 259: Training Loss: 0.03088071011006832, Validation Loss: 0.031190387904644012\n",
      "Epoch 1, Batch: 260: Training Loss: 0.03439854085445404, Validation Loss: 0.030282968655228615\n",
      "Epoch 1, Batch: 261: Training Loss: 0.032988328486680984, Validation Loss: 0.028281396254897118\n",
      "Epoch 1, Batch: 262: Training Loss: 0.03301675245165825, Validation Loss: 0.03156154975295067\n",
      "Epoch 1, Batch: 263: Training Loss: 0.032480981200933456, Validation Loss: 0.029928551986813545\n",
      "Epoch 1, Batch: 264: Training Loss: 0.02846481278538704, Validation Loss: 0.03435982018709183\n",
      "Epoch 1, Batch: 265: Training Loss: 0.027872543781995773, Validation Loss: 0.031510233879089355\n",
      "Epoch 1, Batch: 266: Training Loss: 0.03295906260609627, Validation Loss: 0.030036212876439095\n",
      "Epoch 1, Batch: 267: Training Loss: 0.03206029161810875, Validation Loss: 0.032332323491573334\n",
      "Epoch 1, Batch: 268: Training Loss: 0.033084314316511154, Validation Loss: 0.031197289004921913\n",
      "Epoch 1, Batch: 269: Training Loss: 0.029312027618288994, Validation Loss: 0.029595520347356796\n",
      "Epoch 1, Batch: 270: Training Loss: 0.03190534561872482, Validation Loss: 0.029592081904411316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 271: Training Loss: 0.024072598665952682, Validation Loss: 0.030332382768392563\n",
      "Epoch 1, Batch: 272: Training Loss: 0.030379092320799828, Validation Loss: 0.03209349513053894\n",
      "Epoch 1, Batch: 273: Training Loss: 0.030311938375234604, Validation Loss: 0.03247366473078728\n",
      "Epoch 1, Batch: 274: Training Loss: 0.03479517251253128, Validation Loss: 0.03221018239855766\n",
      "Epoch 1, Batch: 275: Training Loss: 0.03355890139937401, Validation Loss: 0.03263508528470993\n",
      "Epoch 1, Batch: 276: Training Loss: 0.031059149652719498, Validation Loss: 0.03130737692117691\n",
      "Epoch 1, Batch: 277: Training Loss: 0.030287984758615494, Validation Loss: 0.03232961893081665\n",
      "Epoch 1, Batch: 278: Training Loss: 0.03269733488559723, Validation Loss: 0.034438036382198334\n",
      "Epoch 1, Batch: 279: Training Loss: 0.03185081109404564, Validation Loss: 0.02889004535973072\n",
      "Epoch 1, Batch: 280: Training Loss: 0.030250439420342445, Validation Loss: 0.033023200929164886\n",
      "Epoch 1, Batch: 281: Training Loss: 0.0326337069272995, Validation Loss: 0.034422680735588074\n",
      "Epoch 1, Batch: 282: Training Loss: 0.03154831379652023, Validation Loss: 0.033132366836071014\n",
      "Epoch 1, Batch: 283: Training Loss: 0.029267197474837303, Validation Loss: 0.03380852937698364\n",
      "Epoch 1, Batch: 284: Training Loss: 0.031394634395837784, Validation Loss: 0.034323129802942276\n",
      "Epoch 1, Batch: 285: Training Loss: 0.032886676490306854, Validation Loss: 0.03265571966767311\n",
      "Epoch 1, Batch: 286: Training Loss: 0.028084522113204002, Validation Loss: 0.03201795369386673\n",
      "Epoch 1, Batch: 287: Training Loss: 0.030974218621850014, Validation Loss: 0.03282738849520683\n",
      "Epoch 1, Batch: 288: Training Loss: 0.034725550562143326, Validation Loss: 0.0332501195371151\n",
      "Epoch 1, Batch: 289: Training Loss: 0.03452518954873085, Validation Loss: 0.03247020021080971\n",
      "Epoch 1, Batch: 290: Training Loss: 0.030503176152706146, Validation Loss: 0.03214067220687866\n",
      "Epoch 1, Batch: 291: Training Loss: 0.0314054973423481, Validation Loss: 0.03250786289572716\n",
      "Epoch 1, Batch: 292: Training Loss: 0.029548892751336098, Validation Loss: 0.036044467240571976\n",
      "Epoch 1, Batch: 293: Training Loss: 0.036685578525066376, Validation Loss: 0.03343193605542183\n",
      "Epoch 1, Batch: 294: Training Loss: 0.03403342142701149, Validation Loss: 0.0330500453710556\n",
      "Epoch 1, Batch: 295: Training Loss: 0.029197100549936295, Validation Loss: 0.033111248165369034\n",
      "Epoch 1, Batch: 296: Training Loss: 0.030106136575341225, Validation Loss: 0.03384486585855484\n",
      "Epoch 1, Batch: 297: Training Loss: 0.03489639610052109, Validation Loss: 0.03662722185254097\n",
      "Epoch 1, Batch: 298: Training Loss: 0.03022826835513115, Validation Loss: 0.03150448203086853\n",
      "Epoch 1, Batch: 299: Training Loss: 0.027883358299732208, Validation Loss: 0.033563897013664246\n",
      "Epoch 1, Batch: 300: Training Loss: 0.031328026205301285, Validation Loss: 0.03609779477119446\n",
      "Epoch 1, Batch: 301: Training Loss: 0.03247620537877083, Validation Loss: 0.034424055367708206\n",
      "Epoch 1, Batch: 302: Training Loss: 0.03029670938849449, Validation Loss: 0.03268161416053772\n",
      "Epoch 1, Batch: 303: Training Loss: 0.033508267253637314, Validation Loss: 0.031688690185546875\n",
      "Epoch 1, Batch: 304: Training Loss: 0.03251056373119354, Validation Loss: 0.0327475368976593\n",
      "Epoch 1, Batch: 305: Training Loss: 0.026924114674329758, Validation Loss: 0.031173093244433403\n",
      "Epoch 1, Batch: 306: Training Loss: 0.03264576569199562, Validation Loss: 0.03336049243807793\n",
      "Epoch 1, Batch: 307: Training Loss: 0.026057252660393715, Validation Loss: 0.036030564457178116\n",
      "Epoch 1, Batch: 308: Training Loss: 0.02907315269112587, Validation Loss: 0.03336954861879349\n",
      "Epoch 1, Batch: 309: Training Loss: 0.028219928964972496, Validation Loss: 0.033957745879888535\n",
      "Epoch 1, Batch: 310: Training Loss: 0.03010963834822178, Validation Loss: 0.03516269847750664\n",
      "Epoch 1, Batch: 311: Training Loss: 0.030012814328074455, Validation Loss: 0.03311226889491081\n",
      "Epoch 1, Batch: 312: Training Loss: 0.028754353523254395, Validation Loss: 0.03258197754621506\n",
      "Epoch 1, Batch: 313: Training Loss: 0.027730731293559074, Validation Loss: 0.03648891672492027\n",
      "Epoch 1, Batch: 314: Training Loss: 0.029863612726330757, Validation Loss: 0.03648796305060387\n",
      "Epoch 1, Batch: 315: Training Loss: 0.034388210624456406, Validation Loss: 0.032659146934747696\n",
      "Epoch 1, Batch: 316: Training Loss: 0.02697758749127388, Validation Loss: 0.03429722413420677\n",
      "Epoch 1, Batch: 317: Training Loss: 0.03199571743607521, Validation Loss: 0.03463049978017807\n",
      "Epoch 1, Batch: 318: Training Loss: 0.027164027094841003, Validation Loss: 0.03448028117418289\n",
      "Epoch 1, Batch: 319: Training Loss: 0.03289051353931427, Validation Loss: 0.03376076743006706\n",
      "Epoch 1, Batch: 320: Training Loss: 0.03506206348538399, Validation Loss: 0.0337996669113636\n",
      "Epoch 1, Batch: 321: Training Loss: 0.029016058892011642, Validation Loss: 0.03192073479294777\n",
      "Epoch 1, Batch: 322: Training Loss: 0.030811766162514687, Validation Loss: 0.03644485026597977\n",
      "Epoch 1, Batch: 323: Training Loss: 0.033175285905599594, Validation Loss: 0.03568500652909279\n",
      "Epoch 1, Batch: 324: Training Loss: 0.03315017744898796, Validation Loss: 0.035872068256139755\n",
      "Epoch 1, Batch: 325: Training Loss: 0.03327440470457077, Validation Loss: 0.0331324003636837\n",
      "Epoch 1, Batch: 326: Training Loss: 0.02915654145181179, Validation Loss: 0.03285082057118416\n",
      "Epoch 1, Batch: 327: Training Loss: 0.03200496733188629, Validation Loss: 0.03342890366911888\n",
      "Epoch 1, Batch: 328: Training Loss: 0.03282808139920235, Validation Loss: 0.03443900868296623\n",
      "Epoch 1, Batch: 329: Training Loss: 0.03473333269357681, Validation Loss: 0.0351889543235302\n",
      "Epoch 1, Batch: 330: Training Loss: 0.03458584472537041, Validation Loss: 0.033301834017038345\n",
      "Epoch 1, Batch: 331: Training Loss: 0.0317164771258831, Validation Loss: 0.034228648990392685\n",
      "Epoch 1, Batch: 332: Training Loss: 0.026680313050746918, Validation Loss: 0.032555412501096725\n",
      "Epoch 1, Batch: 333: Training Loss: 0.031005507335066795, Validation Loss: 0.034181103110313416\n",
      "Epoch 1, Batch: 334: Training Loss: 0.03578448295593262, Validation Loss: 0.03309940919280052\n",
      "Epoch 1, Batch: 335: Training Loss: 0.03360352665185928, Validation Loss: 0.032144270837306976\n",
      "Epoch 1, Batch: 336: Training Loss: 0.028643982484936714, Validation Loss: 0.03188731148838997\n",
      "Epoch 1, Batch: 337: Training Loss: 0.026808425784111023, Validation Loss: 0.032169733196496964\n",
      "Epoch 1, Batch: 338: Training Loss: 0.03371855244040489, Validation Loss: 0.03302706405520439\n",
      "Epoch 1, Batch: 339: Training Loss: 0.031547173857688904, Validation Loss: 0.030697714537382126\n",
      "Epoch 1, Batch: 340: Training Loss: 0.03033270314335823, Validation Loss: 0.03153340145945549\n",
      "Epoch 1, Batch: 341: Training Loss: 0.029844103381037712, Validation Loss: 0.029249031096696854\n",
      "Epoch 1, Batch: 342: Training Loss: 0.032139189541339874, Validation Loss: 0.029125483706593513\n",
      "Epoch 1, Batch: 343: Training Loss: 0.03517168015241623, Validation Loss: 0.02756887674331665\n",
      "Epoch 1, Batch: 344: Training Loss: 0.03462670370936394, Validation Loss: 0.03284357115626335\n",
      "Epoch 1, Batch: 345: Training Loss: 0.032597616314888, Validation Loss: 0.03434057906270027\n",
      "Epoch 1, Batch: 346: Training Loss: 0.03624938055872917, Validation Loss: 0.03621240705251694\n",
      "Epoch 1, Batch: 347: Training Loss: 0.03293094411492348, Validation Loss: 0.03148961439728737\n",
      "Epoch 1, Batch: 348: Training Loss: 0.034651484340429306, Validation Loss: 0.03064112551510334\n",
      "Epoch 1, Batch: 349: Training Loss: 0.03225046023726463, Validation Loss: 0.0296438317745924\n",
      "Epoch 1, Batch: 350: Training Loss: 0.031779684126377106, Validation Loss: 0.02967831864953041\n",
      "Epoch 1, Batch: 351: Training Loss: 0.035751596093177795, Validation Loss: 0.028513191267848015\n",
      "Epoch 1, Batch: 352: Training Loss: 0.03389200568199158, Validation Loss: 0.02821200154721737\n",
      "Epoch 1, Batch: 353: Training Loss: 0.03410501033067703, Validation Loss: 0.030361812561750412\n",
      "Epoch 1, Batch: 354: Training Loss: 0.03074932098388672, Validation Loss: 0.030660510063171387\n",
      "Epoch 1, Batch: 355: Training Loss: 0.025842394679784775, Validation Loss: 0.029634013772010803\n",
      "Epoch 1, Batch: 356: Training Loss: 0.03780659660696983, Validation Loss: 0.03223554417490959\n",
      "Epoch 1, Batch: 357: Training Loss: 0.02779299207031727, Validation Loss: 0.02959626354277134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 358: Training Loss: 0.037671491503715515, Validation Loss: 0.031170370057225227\n",
      "Epoch 1, Batch: 359: Training Loss: 0.03448719158768654, Validation Loss: 0.030135786160826683\n",
      "Epoch 1, Batch: 360: Training Loss: 0.031281229108572006, Validation Loss: 0.031821928918361664\n",
      "Epoch 1, Batch: 361: Training Loss: 0.03089664876461029, Validation Loss: 0.03023355081677437\n",
      "Epoch 1, Batch: 362: Training Loss: 0.029636656865477562, Validation Loss: 0.03070995584130287\n",
      "Epoch 1, Batch: 363: Training Loss: 0.035057611763477325, Validation Loss: 0.03038799576461315\n",
      "Epoch 1, Batch: 364: Training Loss: 0.0299394391477108, Validation Loss: 0.032621290534734726\n",
      "Epoch 1, Batch: 365: Training Loss: 0.029814576730132103, Validation Loss: 0.03297043219208717\n",
      "Epoch 1, Batch: 366: Training Loss: 0.03097127005457878, Validation Loss: 0.031356438994407654\n",
      "Epoch 1, Batch: 367: Training Loss: 0.029035605490207672, Validation Loss: 0.03332630917429924\n",
      "Epoch 1, Batch: 368: Training Loss: 0.02504771016538143, Validation Loss: 0.03346426039934158\n",
      "Epoch 1, Batch: 369: Training Loss: 0.0288571584969759, Validation Loss: 0.03193509206175804\n",
      "Epoch 1, Batch: 370: Training Loss: 0.02974390797317028, Validation Loss: 0.03369498252868652\n",
      "Epoch 1, Batch: 371: Training Loss: 0.03361789509654045, Validation Loss: 0.02990604005753994\n",
      "Epoch 1, Batch: 372: Training Loss: 0.027692724019289017, Validation Loss: 0.030490772798657417\n",
      "Epoch 1, Batch: 373: Training Loss: 0.03237928822636604, Validation Loss: 0.030865350738167763\n",
      "Epoch 1, Batch: 374: Training Loss: 0.029961900785565376, Validation Loss: 0.03226526826620102\n",
      "Epoch 1, Batch: 375: Training Loss: 0.034627679735422134, Validation Loss: 0.03176885470747948\n",
      "Epoch 1, Batch: 376: Training Loss: 0.033104974776506424, Validation Loss: 0.032044921070337296\n",
      "Epoch 1, Batch: 377: Training Loss: 0.031086722388863564, Validation Loss: 0.0366959348320961\n",
      "Epoch 1, Batch: 378: Training Loss: 0.0332006961107254, Validation Loss: 0.035226814448833466\n",
      "Epoch 1, Batch: 379: Training Loss: 0.029605837538838387, Validation Loss: 0.033322516828775406\n",
      "Epoch 1, Batch: 380: Training Loss: 0.03813306987285614, Validation Loss: 0.033047717064619064\n",
      "Epoch 1, Batch: 381: Training Loss: 0.03131657466292381, Validation Loss: 0.03352933004498482\n",
      "Epoch 1, Batch: 382: Training Loss: 0.03330695256590843, Validation Loss: 0.03287210687994957\n",
      "Epoch 1, Batch: 383: Training Loss: 0.028680535033345222, Validation Loss: 0.03523435443639755\n",
      "Epoch 1, Batch: 384: Training Loss: 0.03327370434999466, Validation Loss: 0.036788392812013626\n",
      "Epoch 1, Batch: 385: Training Loss: 0.02859474904835224, Validation Loss: 0.033747076988220215\n",
      "Epoch 1, Batch: 386: Training Loss: 0.027774129062891006, Validation Loss: 0.03475937247276306\n",
      "Epoch 1, Batch: 387: Training Loss: 0.032051824033260345, Validation Loss: 0.03213205561041832\n",
      "Epoch 1, Batch: 388: Training Loss: 0.032119520008563995, Validation Loss: 0.03595983609557152\n",
      "Epoch 1, Batch: 389: Training Loss: 0.032535091042518616, Validation Loss: 0.031683970242738724\n",
      "Epoch 1, Batch: 390: Training Loss: 0.03063427098095417, Validation Loss: 0.034009601920843124\n",
      "Epoch 1, Batch: 391: Training Loss: 0.034735001623630524, Validation Loss: 0.03414197266101837\n",
      "Epoch 1, Batch: 392: Training Loss: 0.03505692631006241, Validation Loss: 0.034836504608392715\n",
      "Epoch 1, Batch: 393: Training Loss: 0.03667785972356796, Validation Loss: 0.03296569734811783\n",
      "Epoch 1, Batch: 394: Training Loss: 0.029692670330405235, Validation Loss: 0.032403890043497086\n",
      "Epoch 1, Batch: 395: Training Loss: 0.02866356447339058, Validation Loss: 0.03566787391901016\n",
      "Epoch 1, Batch: 396: Training Loss: 0.03187458589673042, Validation Loss: 0.03382928669452667\n",
      "Epoch 1, Batch: 397: Training Loss: 0.037114568054676056, Validation Loss: 0.03287719935178757\n",
      "Epoch 1, Batch: 398: Training Loss: 0.03081703744828701, Validation Loss: 0.03309411555528641\n",
      "Epoch 1, Batch: 399: Training Loss: 0.03676531836390495, Validation Loss: 0.03331375494599342\n",
      "Epoch 1, Batch: 400: Training Loss: 0.03726520389318466, Validation Loss: 0.03410637751221657\n",
      "Epoch 1, Batch: 401: Training Loss: 0.03484326973557472, Validation Loss: 0.034350909292697906\n",
      "Epoch 1, Batch: 402: Training Loss: 0.033146269619464874, Validation Loss: 0.03154265135526657\n",
      "Epoch 1, Batch: 403: Training Loss: 0.03981328010559082, Validation Loss: 0.03196776285767555\n",
      "Epoch 1, Batch: 404: Training Loss: 0.029929911717772484, Validation Loss: 0.03231888264417648\n",
      "Epoch 1, Batch: 405: Training Loss: 0.03313308581709862, Validation Loss: 0.030785026028752327\n",
      "Epoch 1, Batch: 406: Training Loss: 0.029943112283945084, Validation Loss: 0.030671700835227966\n",
      "Epoch 1, Batch: 407: Training Loss: 0.029549619182944298, Validation Loss: 0.03255638852715492\n",
      "Epoch 1, Batch: 408: Training Loss: 0.029207292944192886, Validation Loss: 0.03179684653878212\n",
      "Epoch 1, Batch: 409: Training Loss: 0.0327145978808403, Validation Loss: 0.029781034216284752\n",
      "Epoch 1, Batch: 410: Training Loss: 0.028950441628694534, Validation Loss: 0.032921500504016876\n",
      "Epoch 1, Batch: 411: Training Loss: 0.030060403048992157, Validation Loss: 0.029689358547329903\n",
      "Epoch 1, Batch: 412: Training Loss: 0.031499091535806656, Validation Loss: 0.030507748946547508\n",
      "Epoch 1, Batch: 413: Training Loss: 0.029201718047261238, Validation Loss: 0.029225829988718033\n",
      "Epoch 1, Batch: 414: Training Loss: 0.026476586237549782, Validation Loss: 0.03348182886838913\n",
      "Epoch 1, Batch: 415: Training Loss: 0.026901762932538986, Validation Loss: 0.030018169432878494\n",
      "Epoch 1, Batch: 416: Training Loss: 0.03345581144094467, Validation Loss: 0.032057177275419235\n",
      "Epoch 1, Batch: 417: Training Loss: 0.030173221603035927, Validation Loss: 0.03194206953048706\n",
      "Epoch 1, Batch: 418: Training Loss: 0.028210066258907318, Validation Loss: 0.03244529664516449\n",
      "Epoch 1, Batch: 419: Training Loss: 0.031440701335668564, Validation Loss: 0.03246764466166496\n",
      "Epoch 1, Batch: 420: Training Loss: 0.02978317067027092, Validation Loss: 0.032854240387678146\n",
      "Epoch 1, Batch: 421: Training Loss: 0.03677381947636604, Validation Loss: 0.030502378940582275\n",
      "Epoch 1, Batch: 422: Training Loss: 0.03819923847913742, Validation Loss: 0.031228424981236458\n",
      "Epoch 1, Batch: 423: Training Loss: 0.03320048376917839, Validation Loss: 0.03288556635379791\n",
      "Epoch 1, Batch: 424: Training Loss: 0.03378140553832054, Validation Loss: 0.030097033828496933\n",
      "Epoch 1, Batch: 425: Training Loss: 0.03204430267214775, Validation Loss: 0.030703604221343994\n",
      "Epoch 1, Batch: 426: Training Loss: 0.03044125996530056, Validation Loss: 0.03282260149717331\n",
      "Epoch 1, Batch: 427: Training Loss: 0.03317175805568695, Validation Loss: 0.029721567407250404\n",
      "Epoch 1, Batch: 428: Training Loss: 0.026886826381087303, Validation Loss: 0.032629355788230896\n",
      "Epoch 1, Batch: 429: Training Loss: 0.028561603277921677, Validation Loss: 0.030378000810742378\n",
      "Epoch 1, Batch: 430: Training Loss: 0.02986254170536995, Validation Loss: 0.031727056950330734\n",
      "Epoch 1, Batch: 431: Training Loss: 0.033724453300237656, Validation Loss: 0.03246913105249405\n",
      "Epoch 1, Batch: 432: Training Loss: 0.0328858308494091, Validation Loss: 0.0336051769554615\n",
      "Epoch 1, Batch: 433: Training Loss: 0.03672344610095024, Validation Loss: 0.03282112255692482\n",
      "Epoch 1, Batch: 434: Training Loss: 0.030868854373693466, Validation Loss: 0.03210388869047165\n",
      "Epoch 1, Batch: 435: Training Loss: 0.030056869611144066, Validation Loss: 0.03295232355594635\n",
      "Epoch 1, Batch: 436: Training Loss: 0.025936869904398918, Validation Loss: 0.033788155764341354\n",
      "Epoch 1, Batch: 437: Training Loss: 0.027728524059057236, Validation Loss: 0.03194976970553398\n",
      "Epoch 1, Batch: 438: Training Loss: 0.0290847085416317, Validation Loss: 0.028932122513651848\n",
      "Epoch 1, Batch: 439: Training Loss: 0.030252519994974136, Validation Loss: 0.032780565321445465\n",
      "Epoch 1, Batch: 440: Training Loss: 0.028449075296521187, Validation Loss: 0.030471613630652428\n",
      "Epoch 1, Batch: 441: Training Loss: 0.03076881170272827, Validation Loss: 0.03105217218399048\n",
      "Epoch 1, Batch: 442: Training Loss: 0.0314149335026741, Validation Loss: 0.03155176714062691\n",
      "Epoch 1, Batch: 443: Training Loss: 0.03464258834719658, Validation Loss: 0.032610904425382614\n",
      "Epoch 1, Batch: 444: Training Loss: 0.031182071194052696, Validation Loss: 0.033411212265491486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 445: Training Loss: 0.029275033622980118, Validation Loss: 0.03291761875152588\n",
      "Epoch 1, Batch: 446: Training Loss: 0.031284399330616, Validation Loss: 0.034176722168922424\n",
      "Epoch 1, Batch: 447: Training Loss: 0.02992088347673416, Validation Loss: 0.0313822366297245\n",
      "Epoch 1, Batch: 448: Training Loss: 0.03057609312236309, Validation Loss: 0.03318493068218231\n",
      "Epoch 1, Batch: 449: Training Loss: 0.03493403643369675, Validation Loss: 0.03558863326907158\n",
      "Epoch 1, Batch: 450: Training Loss: 0.03632026165723801, Validation Loss: 0.03202185779809952\n",
      "Epoch 1, Batch: 451: Training Loss: 0.031376130878925323, Validation Loss: 0.035305205732584\n",
      "Epoch 1, Batch: 452: Training Loss: 0.034960873425006866, Validation Loss: 0.032664790749549866\n",
      "Epoch 1, Batch: 453: Training Loss: 0.03422779217362404, Validation Loss: 0.033699195832014084\n",
      "Epoch 1, Batch: 454: Training Loss: 0.028029927983880043, Validation Loss: 0.03741515427827835\n",
      "Epoch 1, Batch: 455: Training Loss: 0.03121325932443142, Validation Loss: 0.03496120497584343\n",
      "Epoch 1, Batch: 456: Training Loss: 0.027026450261473656, Validation Loss: 0.0353584848344326\n",
      "Epoch 1, Batch: 457: Training Loss: 0.03251669183373451, Validation Loss: 0.03662724420428276\n",
      "Epoch 1, Batch: 458: Training Loss: 0.024963703006505966, Validation Loss: 0.032722946256399155\n",
      "Epoch 1, Batch: 459: Training Loss: 0.03557027503848076, Validation Loss: 0.03389086574316025\n",
      "Epoch 1, Batch: 460: Training Loss: 0.03625112771987915, Validation Loss: 0.0334213450551033\n",
      "Epoch 1, Batch: 461: Training Loss: 0.029042700305581093, Validation Loss: 0.03249392658472061\n",
      "Epoch 1, Batch: 462: Training Loss: 0.030657978728413582, Validation Loss: 0.02994290553033352\n",
      "Epoch 1, Batch: 463: Training Loss: 0.03654906526207924, Validation Loss: 0.03175792098045349\n",
      "Epoch 1, Batch: 464: Training Loss: 0.026917198672890663, Validation Loss: 0.03198960795998573\n",
      "Epoch 1, Batch: 465: Training Loss: 0.030488615855574608, Validation Loss: 0.02971741557121277\n",
      "Epoch 1, Batch: 466: Training Loss: 0.03129871189594269, Validation Loss: 0.029155516996979713\n",
      "Epoch 1, Batch: 467: Training Loss: 0.04396561533212662, Validation Loss: 0.026616232469677925\n",
      "Epoch 1, Batch: 468: Training Loss: 0.034728266298770905, Validation Loss: 0.030930910259485245\n",
      "Epoch 1, Batch: 469: Training Loss: 0.029079100117087364, Validation Loss: 0.02814413420855999\n",
      "Epoch 1, Batch: 470: Training Loss: 0.032416220754384995, Validation Loss: 0.03147777169942856\n",
      "Epoch 1, Batch: 471: Training Loss: 0.032689206302165985, Validation Loss: 0.032561466097831726\n",
      "Epoch 1, Batch: 472: Training Loss: 0.03509637340903282, Validation Loss: 0.03015010431408882\n",
      "Epoch 1, Batch: 473: Training Loss: 0.02546798437833786, Validation Loss: 0.03281305730342865\n",
      "Epoch 1, Batch: 474: Training Loss: 0.029456080868840218, Validation Loss: 0.03312371298670769\n",
      "Epoch 1, Batch: 475: Training Loss: 0.03408041223883629, Validation Loss: 0.03304043039679527\n",
      "Epoch 1, Batch: 476: Training Loss: 0.035442087799310684, Validation Loss: 0.03028508834540844\n",
      "Epoch 1, Batch: 477: Training Loss: 0.03513389080762863, Validation Loss: 0.029805101454257965\n",
      "Epoch 1, Batch: 478: Training Loss: 0.03034672886133194, Validation Loss: 0.03136667609214783\n",
      "Epoch 1, Batch: 479: Training Loss: 0.03462490439414978, Validation Loss: 0.033244021236896515\n",
      "Epoch 1, Batch: 480: Training Loss: 0.02751954086124897, Validation Loss: 0.030574364587664604\n",
      "Epoch 1, Batch: 481: Training Loss: 0.03200415521860123, Validation Loss: 0.030464589595794678\n",
      "Epoch 1, Batch: 482: Training Loss: 0.03286835178732872, Validation Loss: 0.031949449330568314\n",
      "Epoch 1, Batch: 483: Training Loss: 0.030962347984313965, Validation Loss: 0.0338381789624691\n",
      "Epoch 1, Batch: 484: Training Loss: 0.03226066008210182, Validation Loss: 0.03235819935798645\n",
      "Epoch 1, Batch: 485: Training Loss: 0.03002181462943554, Validation Loss: 0.033097852021455765\n",
      "Epoch 1, Batch: 486: Training Loss: 0.03475622832775116, Validation Loss: 0.029096320271492004\n",
      "Epoch 1, Batch: 487: Training Loss: 0.0316535122692585, Validation Loss: 0.029976749792695045\n",
      "Epoch 1, Batch: 488: Training Loss: 0.028857270255684853, Validation Loss: 0.02968272566795349\n",
      "Epoch 1, Batch: 489: Training Loss: 0.03218502923846245, Validation Loss: 0.030399814248085022\n",
      "Epoch 1, Batch: 490: Training Loss: 0.03240615129470825, Validation Loss: 0.03574300557374954\n",
      "Epoch 1, Batch: 491: Training Loss: 0.027885986492037773, Validation Loss: 0.03125638887286186\n",
      "Epoch 1, Batch: 492: Training Loss: 0.03287573531270027, Validation Loss: 0.03444544970989227\n",
      "Epoch 1, Batch: 493: Training Loss: 0.03363446891307831, Validation Loss: 0.03398267179727554\n",
      "Epoch 1, Batch: 494: Training Loss: 0.034533046185970306, Validation Loss: 0.033529628068208694\n",
      "Epoch 1, Batch: 495: Training Loss: 0.032804232090711594, Validation Loss: 0.03226644918322563\n",
      "Epoch 1, Batch: 496: Training Loss: 0.030869238078594208, Validation Loss: 0.034464459866285324\n",
      "Epoch 1, Batch: 497: Training Loss: 0.027416035532951355, Validation Loss: 0.03298627585172653\n",
      "Epoch 1, Batch: 498: Training Loss: 0.029039744287729263, Validation Loss: 0.03175227716565132\n",
      "Epoch 1, Batch: 499: Training Loss: 0.02890542708337307, Validation Loss: 0.03116850182414055\n",
      "Epoch 2, Batch: 0: Training Loss: 0.027751866728067398, Validation Loss: 0.033813197165727615\n",
      "Epoch 2, Batch: 1: Training Loss: 0.03206098452210426, Validation Loss: 0.03195148706436157\n",
      "Epoch 2, Batch: 2: Training Loss: 0.032733459025621414, Validation Loss: 0.033633358776569366\n",
      "Epoch 2, Batch: 3: Training Loss: 0.03007577545940876, Validation Loss: 0.035293061286211014\n",
      "Epoch 2, Batch: 4: Training Loss: 0.02868681587278843, Validation Loss: 0.034105896949768066\n",
      "Epoch 2, Batch: 5: Training Loss: 0.02748813107609749, Validation Loss: 0.031471095979213715\n",
      "Epoch 2, Batch: 6: Training Loss: 0.027580134570598602, Validation Loss: 0.03489553928375244\n",
      "Epoch 2, Batch: 7: Training Loss: 0.030050912871956825, Validation Loss: 0.030570099130272865\n",
      "Epoch 2, Batch: 8: Training Loss: 0.034193310886621475, Validation Loss: 0.0346529483795166\n",
      "Epoch 2, Batch: 9: Training Loss: 0.027819013223052025, Validation Loss: 0.032351378351449966\n",
      "Epoch 2, Batch: 10: Training Loss: 0.02902717888355255, Validation Loss: 0.03558838739991188\n",
      "Epoch 2, Batch: 11: Training Loss: 0.03221055865287781, Validation Loss: 0.033217091113328934\n",
      "Epoch 2, Batch: 12: Training Loss: 0.03465685993432999, Validation Loss: 0.03231464698910713\n",
      "Epoch 2, Batch: 13: Training Loss: 0.03566130995750427, Validation Loss: 0.03215494379401207\n",
      "Epoch 2, Batch: 14: Training Loss: 0.030773917213082314, Validation Loss: 0.03043268993496895\n",
      "Epoch 2, Batch: 15: Training Loss: 0.030520211905241013, Validation Loss: 0.03081604093313217\n",
      "Epoch 2, Batch: 16: Training Loss: 0.03564397245645523, Validation Loss: 0.03050737828016281\n",
      "Epoch 2, Batch: 17: Training Loss: 0.032363373786211014, Validation Loss: 0.03184731677174568\n",
      "Epoch 2, Batch: 18: Training Loss: 0.02579628862440586, Validation Loss: 0.03300381451845169\n",
      "Epoch 2, Batch: 19: Training Loss: 0.02647574618458748, Validation Loss: 0.03035331331193447\n",
      "Epoch 2, Batch: 20: Training Loss: 0.030417582020163536, Validation Loss: 0.03143131360411644\n",
      "Epoch 2, Batch: 21: Training Loss: 0.027989596128463745, Validation Loss: 0.035189639776945114\n",
      "Epoch 2, Batch: 22: Training Loss: 0.03240344673395157, Validation Loss: 0.036707814782857895\n",
      "Epoch 2, Batch: 23: Training Loss: 0.03168034180998802, Validation Loss: 0.03321198746562004\n",
      "Epoch 2, Batch: 24: Training Loss: 0.027713771909475327, Validation Loss: 0.03520109876990318\n",
      "Epoch 2, Batch: 25: Training Loss: 0.028816241770982742, Validation Loss: 0.03499264270067215\n",
      "Epoch 2, Batch: 26: Training Loss: 0.028576847165822983, Validation Loss: 0.03463114798069\n",
      "Epoch 2, Batch: 27: Training Loss: 0.03149107098579407, Validation Loss: 0.03138858079910278\n",
      "Epoch 2, Batch: 28: Training Loss: 0.034353550523519516, Validation Loss: 0.031202368438243866\n",
      "Epoch 2, Batch: 29: Training Loss: 0.031012969091534615, Validation Loss: 0.029365867376327515\n",
      "Epoch 2, Batch: 30: Training Loss: 0.026748115196824074, Validation Loss: 0.027897268533706665\n",
      "Epoch 2, Batch: 31: Training Loss: 0.03189696744084358, Validation Loss: 0.03282913565635681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 32: Training Loss: 0.028944697231054306, Validation Loss: 0.030636195093393326\n",
      "Epoch 2, Batch: 33: Training Loss: 0.028504949063062668, Validation Loss: 0.028265085071325302\n",
      "Epoch 2, Batch: 34: Training Loss: 0.028353217989206314, Validation Loss: 0.031042063608765602\n",
      "Epoch 2, Batch: 35: Training Loss: 0.030754776671528816, Validation Loss: 0.029129644855856895\n",
      "Epoch 2, Batch: 36: Training Loss: 0.025429945439100266, Validation Loss: 0.03087138570845127\n",
      "Epoch 2, Batch: 37: Training Loss: 0.02703765407204628, Validation Loss: 0.03116353042423725\n",
      "Epoch 2, Batch: 38: Training Loss: 0.03178470581769943, Validation Loss: 0.032754529267549515\n",
      "Epoch 2, Batch: 39: Training Loss: 0.03197330981492996, Validation Loss: 0.032405830919742584\n",
      "Epoch 2, Batch: 40: Training Loss: 0.036742206662893295, Validation Loss: 0.03093893453478813\n",
      "Epoch 2, Batch: 41: Training Loss: 0.0318140983581543, Validation Loss: 0.030824387446045876\n",
      "Epoch 2, Batch: 42: Training Loss: 0.0310945063829422, Validation Loss: 0.03011901117861271\n",
      "Epoch 2, Batch: 43: Training Loss: 0.030791955068707466, Validation Loss: 0.029214225709438324\n",
      "Epoch 2, Batch: 44: Training Loss: 0.029886046424508095, Validation Loss: 0.03220159932971001\n",
      "Epoch 2, Batch: 45: Training Loss: 0.027978431433439255, Validation Loss: 0.03083355538547039\n",
      "Epoch 2, Batch: 46: Training Loss: 0.03010515309870243, Validation Loss: 0.03078918531537056\n",
      "Epoch 2, Batch: 47: Training Loss: 0.031074438244104385, Validation Loss: 0.031505562365055084\n",
      "Epoch 2, Batch: 48: Training Loss: 0.035298898816108704, Validation Loss: 0.0322796031832695\n",
      "Epoch 2, Batch: 49: Training Loss: 0.033747147768735886, Validation Loss: 0.02886640653014183\n",
      "Epoch 2, Batch: 50: Training Loss: 0.026518596336245537, Validation Loss: 0.02938578464090824\n",
      "Epoch 2, Batch: 51: Training Loss: 0.03329305350780487, Validation Loss: 0.029589179903268814\n",
      "Epoch 2, Batch: 52: Training Loss: 0.033078137785196304, Validation Loss: 0.031524207442998886\n",
      "Epoch 2, Batch: 53: Training Loss: 0.024789756163954735, Validation Loss: 0.028238002210855484\n",
      "Epoch 2, Batch: 54: Training Loss: 0.028334496542811394, Validation Loss: 0.03289011865854263\n",
      "Epoch 2, Batch: 55: Training Loss: 0.028956521302461624, Validation Loss: 0.035061780363321304\n",
      "Epoch 2, Batch: 56: Training Loss: 0.028245313093066216, Validation Loss: 0.03198537603020668\n",
      "Epoch 2, Batch: 57: Training Loss: 0.026561329141259193, Validation Loss: 0.029159577563405037\n",
      "Epoch 2, Batch: 58: Training Loss: 0.03434082120656967, Validation Loss: 0.03175961226224899\n",
      "Epoch 2, Batch: 59: Training Loss: 0.029568329453468323, Validation Loss: 0.03420766815543175\n",
      "Epoch 2, Batch: 60: Training Loss: 0.027709661051630974, Validation Loss: 0.03304017335176468\n",
      "Epoch 2, Batch: 61: Training Loss: 0.03389005362987518, Validation Loss: 0.031102171167731285\n",
      "Epoch 2, Batch: 62: Training Loss: 0.03160988911986351, Validation Loss: 0.029481813311576843\n",
      "Epoch 2, Batch: 63: Training Loss: 0.034817639738321304, Validation Loss: 0.030425922945141792\n",
      "Epoch 2, Batch: 64: Training Loss: 0.03245902806520462, Validation Loss: 0.03459035977721214\n",
      "Epoch 2, Batch: 65: Training Loss: 0.037788037210702896, Validation Loss: 0.034345630556344986\n",
      "Epoch 2, Batch: 66: Training Loss: 0.03488197922706604, Validation Loss: 0.03413809463381767\n",
      "Epoch 2, Batch: 67: Training Loss: 0.02716260775923729, Validation Loss: 0.03252605348825455\n",
      "Epoch 2, Batch: 68: Training Loss: 0.035227060317993164, Validation Loss: 0.032112475484609604\n",
      "Epoch 2, Batch: 69: Training Loss: 0.030838914215564728, Validation Loss: 0.035857945680618286\n",
      "Epoch 2, Batch: 70: Training Loss: 0.03621754050254822, Validation Loss: 0.034129802137613297\n",
      "Epoch 2, Batch: 71: Training Loss: 0.03216691315174103, Validation Loss: 0.03735871613025665\n",
      "Epoch 2, Batch: 72: Training Loss: 0.034928373992443085, Validation Loss: 0.035043708980083466\n",
      "Epoch 2, Batch: 73: Training Loss: 0.030395200476050377, Validation Loss: 0.03871966898441315\n",
      "Epoch 2, Batch: 74: Training Loss: 0.03140871226787567, Validation Loss: 0.03708578646183014\n",
      "Epoch 2, Batch: 75: Training Loss: 0.027184000238776207, Validation Loss: 0.03874428570270538\n",
      "Epoch 2, Batch: 76: Training Loss: 0.036091502755880356, Validation Loss: 0.03952484205365181\n",
      "Epoch 2, Batch: 77: Training Loss: 0.03677453473210335, Validation Loss: 0.03577221930027008\n",
      "Epoch 2, Batch: 78: Training Loss: 0.035568226128816605, Validation Loss: 0.038133297115564346\n",
      "Epoch 2, Batch: 79: Training Loss: 0.03716244548559189, Validation Loss: 0.03926757723093033\n",
      "Epoch 2, Batch: 80: Training Loss: 0.03245270252227783, Validation Loss: 0.03892432898283005\n",
      "Epoch 2, Batch: 81: Training Loss: 0.030105222016572952, Validation Loss: 0.03773302584886551\n",
      "Epoch 2, Batch: 82: Training Loss: 0.034436896443367004, Validation Loss: 0.03535480052232742\n",
      "Epoch 2, Batch: 83: Training Loss: 0.030417514964938164, Validation Loss: 0.03542093187570572\n",
      "Epoch 2, Batch: 84: Training Loss: 0.030340328812599182, Validation Loss: 0.035644032061100006\n",
      "Epoch 2, Batch: 85: Training Loss: 0.035277970135211945, Validation Loss: 0.032054174691438675\n",
      "Epoch 2, Batch: 86: Training Loss: 0.039087966084480286, Validation Loss: 0.034872617572546005\n",
      "Epoch 2, Batch: 87: Training Loss: 0.035821739584207535, Validation Loss: 0.03693942725658417\n",
      "Epoch 2, Batch: 88: Training Loss: 0.03638631850481033, Validation Loss: 0.03378308191895485\n",
      "Epoch 2, Batch: 89: Training Loss: 0.03759483993053436, Validation Loss: 0.03601090610027313\n",
      "Epoch 2, Batch: 90: Training Loss: 0.034333184361457825, Validation Loss: 0.03183608874678612\n",
      "Epoch 2, Batch: 91: Training Loss: 0.033753205090761185, Validation Loss: 0.03583713248372078\n",
      "Epoch 2, Batch: 92: Training Loss: 0.03754289075732231, Validation Loss: 0.036129992455244064\n",
      "Epoch 2, Batch: 93: Training Loss: 0.03459783270955086, Validation Loss: 0.034022193402051926\n",
      "Epoch 2, Batch: 94: Training Loss: 0.03625287860631943, Validation Loss: 0.031783610582351685\n",
      "Epoch 2, Batch: 95: Training Loss: 0.03258126229047775, Validation Loss: 0.03486708179116249\n",
      "Epoch 2, Batch: 96: Training Loss: 0.03669741377234459, Validation Loss: 0.034280143678188324\n",
      "Epoch 2, Batch: 97: Training Loss: 0.034653279930353165, Validation Loss: 0.03157325088977814\n",
      "Epoch 2, Batch: 98: Training Loss: 0.029393872246146202, Validation Loss: 0.0336148701608181\n",
      "Epoch 2, Batch: 99: Training Loss: 0.033732928335666656, Validation Loss: 0.03418529033660889\n",
      "Epoch 2, Batch: 100: Training Loss: 0.037245653569698334, Validation Loss: 0.033640820533037186\n",
      "Epoch 2, Batch: 101: Training Loss: 0.03394352272152901, Validation Loss: 0.0318172313272953\n",
      "Epoch 2, Batch: 102: Training Loss: 0.03112976998090744, Validation Loss: 0.03377735614776611\n",
      "Epoch 2, Batch: 103: Training Loss: 0.037570349872112274, Validation Loss: 0.032365959137678146\n",
      "Epoch 2, Batch: 104: Training Loss: 0.027206309139728546, Validation Loss: 0.03164088726043701\n",
      "Epoch 2, Batch: 105: Training Loss: 0.02807636186480522, Validation Loss: 0.03130504861474037\n",
      "Epoch 2, Batch: 106: Training Loss: 0.03618486970663071, Validation Loss: 0.032454170286655426\n",
      "Epoch 2, Batch: 107: Training Loss: 0.033578693866729736, Validation Loss: 0.02967771887779236\n",
      "Epoch 2, Batch: 108: Training Loss: 0.03055545687675476, Validation Loss: 0.029395664110779762\n",
      "Epoch 2, Batch: 109: Training Loss: 0.033971622586250305, Validation Loss: 0.02947218343615532\n",
      "Epoch 2, Batch: 110: Training Loss: 0.03960682824254036, Validation Loss: 0.03398854658007622\n",
      "Epoch 2, Batch: 111: Training Loss: 0.0316888689994812, Validation Loss: 0.03417981043457985\n",
      "Epoch 2, Batch: 112: Training Loss: 0.03735673800110817, Validation Loss: 0.03353600203990936\n",
      "Epoch 2, Batch: 113: Training Loss: 0.02997184358537197, Validation Loss: 0.03442661464214325\n",
      "Epoch 2, Batch: 114: Training Loss: 0.0363098569214344, Validation Loss: 0.03212811052799225\n",
      "Epoch 2, Batch: 115: Training Loss: 0.03731972724199295, Validation Loss: 0.03551502898335457\n",
      "Epoch 2, Batch: 116: Training Loss: 0.03444821387529373, Validation Loss: 0.03411220386624336\n",
      "Epoch 2, Batch: 117: Training Loss: 0.03459857776761055, Validation Loss: 0.031068654730916023\n",
      "Epoch 2, Batch: 118: Training Loss: 0.033029649406671524, Validation Loss: 0.030838189646601677\n",
      "Epoch 2, Batch: 119: Training Loss: 0.03629952296614647, Validation Loss: 0.032202091068029404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 120: Training Loss: 0.032963793724775314, Validation Loss: 0.03140982612967491\n",
      "Epoch 2, Batch: 121: Training Loss: 0.03322847560048103, Validation Loss: 0.0318731814622879\n",
      "Epoch 2, Batch: 122: Training Loss: 0.035097476094961166, Validation Loss: 0.0315670371055603\n",
      "Epoch 2, Batch: 123: Training Loss: 0.0339304581284523, Validation Loss: 0.030515173450112343\n",
      "Epoch 2, Batch: 124: Training Loss: 0.03244564309716225, Validation Loss: 0.03499016538262367\n",
      "Epoch 2, Batch: 125: Training Loss: 0.02905268594622612, Validation Loss: 0.034025926142930984\n",
      "Epoch 2, Batch: 126: Training Loss: 0.03127295896410942, Validation Loss: 0.03406115993857384\n",
      "Epoch 2, Batch: 127: Training Loss: 0.031048063188791275, Validation Loss: 0.035580892115831375\n",
      "Epoch 2, Batch: 128: Training Loss: 0.033059049397706985, Validation Loss: 0.036096177995204926\n",
      "Epoch 2, Batch: 129: Training Loss: 0.030367357656359673, Validation Loss: 0.03511863946914673\n",
      "Epoch 2, Batch: 130: Training Loss: 0.033380281180143356, Validation Loss: 0.031757455319166183\n",
      "Epoch 2, Batch: 131: Training Loss: 0.027950344607234, Validation Loss: 0.030234601348638535\n",
      "Epoch 2, Batch: 132: Training Loss: 0.028965696692466736, Validation Loss: 0.030524328351020813\n",
      "Epoch 2, Batch: 133: Training Loss: 0.033757954835891724, Validation Loss: 0.029691534116864204\n",
      "Epoch 2, Batch: 134: Training Loss: 0.03373502939939499, Validation Loss: 0.03150345757603645\n",
      "Epoch 2, Batch: 135: Training Loss: 0.03045508824288845, Validation Loss: 0.03322318568825722\n",
      "Epoch 2, Batch: 136: Training Loss: 0.0365314856171608, Validation Loss: 0.03204415738582611\n",
      "Epoch 2, Batch: 137: Training Loss: 0.030559806153178215, Validation Loss: 0.032318294048309326\n",
      "Epoch 2, Batch: 138: Training Loss: 0.031033916398882866, Validation Loss: 0.03240017220377922\n",
      "Epoch 2, Batch: 139: Training Loss: 0.0292905792593956, Validation Loss: 0.032169159501791\n",
      "Epoch 2, Batch: 140: Training Loss: 0.03185390308499336, Validation Loss: 0.032602597028017044\n",
      "Epoch 2, Batch: 141: Training Loss: 0.03426957130432129, Validation Loss: 0.03163783624768257\n",
      "Epoch 2, Batch: 142: Training Loss: 0.03153776749968529, Validation Loss: 0.033068086951971054\n",
      "Epoch 2, Batch: 143: Training Loss: 0.0288976039737463, Validation Loss: 0.03148256614804268\n",
      "Epoch 2, Batch: 144: Training Loss: 0.029607316479086876, Validation Loss: 0.03327380120754242\n",
      "Epoch 2, Batch: 145: Training Loss: 0.028301332145929337, Validation Loss: 0.03227299079298973\n",
      "Epoch 2, Batch: 146: Training Loss: 0.03254443779587746, Validation Loss: 0.0314469151198864\n",
      "Epoch 2, Batch: 147: Training Loss: 0.02902640961110592, Validation Loss: 0.03244095668196678\n",
      "Epoch 2, Batch: 148: Training Loss: 0.03150659054517746, Validation Loss: 0.02861734852194786\n",
      "Epoch 2, Batch: 149: Training Loss: 0.027157753705978394, Validation Loss: 0.03235866501927376\n",
      "Epoch 2, Batch: 150: Training Loss: 0.034264709800481796, Validation Loss: 0.02863459289073944\n",
      "Epoch 2, Batch: 151: Training Loss: 0.030402058735489845, Validation Loss: 0.029626071453094482\n",
      "Epoch 2, Batch: 152: Training Loss: 0.031237877905368805, Validation Loss: 0.030338123440742493\n",
      "Epoch 2, Batch: 153: Training Loss: 0.03369790315628052, Validation Loss: 0.02986697293817997\n",
      "Epoch 2, Batch: 154: Training Loss: 0.030126087367534637, Validation Loss: 0.03054068610072136\n",
      "Epoch 2, Batch: 155: Training Loss: 0.04031531512737274, Validation Loss: 0.028933733701705933\n",
      "Epoch 2, Batch: 156: Training Loss: 0.03251108154654503, Validation Loss: 0.027735630050301552\n",
      "Epoch 2, Batch: 157: Training Loss: 0.02930421009659767, Validation Loss: 0.028783388435840607\n",
      "Epoch 2, Batch: 158: Training Loss: 0.03293388709425926, Validation Loss: 0.03057790733873844\n",
      "Epoch 2, Batch: 159: Training Loss: 0.02908085472881794, Validation Loss: 0.02839624509215355\n",
      "Epoch 2, Batch: 160: Training Loss: 0.0289359949529171, Validation Loss: 0.03070027381181717\n",
      "Epoch 2, Batch: 161: Training Loss: 0.032880693674087524, Validation Loss: 0.028383681550621986\n",
      "Epoch 2, Batch: 162: Training Loss: 0.03227376565337181, Validation Loss: 0.03153857961297035\n",
      "Epoch 2, Batch: 163: Training Loss: 0.03334664925932884, Validation Loss: 0.031155869364738464\n",
      "Epoch 2, Batch: 164: Training Loss: 0.028317444026470184, Validation Loss: 0.03038446605205536\n",
      "Epoch 2, Batch: 165: Training Loss: 0.03261513262987137, Validation Loss: 0.03206203877925873\n",
      "Epoch 2, Batch: 166: Training Loss: 0.027495000511407852, Validation Loss: 0.03249699994921684\n",
      "Epoch 2, Batch: 167: Training Loss: 0.02794395573437214, Validation Loss: 0.031996678560972214\n",
      "Epoch 2, Batch: 168: Training Loss: 0.0319310799241066, Validation Loss: 0.03456913307309151\n",
      "Epoch 2, Batch: 169: Training Loss: 0.032242126762866974, Validation Loss: 0.032496336847543716\n",
      "Epoch 2, Batch: 170: Training Loss: 0.03065134584903717, Validation Loss: 0.031307052820920944\n",
      "Epoch 2, Batch: 171: Training Loss: 0.026540270075201988, Validation Loss: 0.03248334303498268\n",
      "Epoch 2, Batch: 172: Training Loss: 0.03085579164326191, Validation Loss: 0.03201548755168915\n",
      "Epoch 2, Batch: 173: Training Loss: 0.028300927951931953, Validation Loss: 0.029988247901201248\n",
      "Epoch 2, Batch: 174: Training Loss: 0.029250886291265488, Validation Loss: 0.03469560667872429\n",
      "Epoch 2, Batch: 175: Training Loss: 0.02862333506345749, Validation Loss: 0.031274449080228806\n",
      "Epoch 2, Batch: 176: Training Loss: 0.03301583603024483, Validation Loss: 0.03248055651783943\n",
      "Epoch 2, Batch: 177: Training Loss: 0.030294358730316162, Validation Loss: 0.03263140469789505\n",
      "Epoch 2, Batch: 178: Training Loss: 0.031560175120830536, Validation Loss: 0.0310031957924366\n",
      "Epoch 2, Batch: 179: Training Loss: 0.028440674766898155, Validation Loss: 0.031894709914922714\n",
      "Epoch 2, Batch: 180: Training Loss: 0.03193763270974159, Validation Loss: 0.030667033046483994\n",
      "Epoch 2, Batch: 181: Training Loss: 0.03428667038679123, Validation Loss: 0.03335442766547203\n",
      "Epoch 2, Batch: 182: Training Loss: 0.034240491688251495, Validation Loss: 0.030472969636321068\n",
      "Epoch 2, Batch: 183: Training Loss: 0.031388863921165466, Validation Loss: 0.027465835213661194\n",
      "Epoch 2, Batch: 184: Training Loss: 0.02822274900972843, Validation Loss: 0.029307078570127487\n",
      "Epoch 2, Batch: 185: Training Loss: 0.033158205449581146, Validation Loss: 0.03162764757871628\n",
      "Epoch 2, Batch: 186: Training Loss: 0.02965584024786949, Validation Loss: 0.028096351772546768\n",
      "Epoch 2, Batch: 187: Training Loss: 0.0332515686750412, Validation Loss: 0.029330190271139145\n",
      "Epoch 2, Batch: 188: Training Loss: 0.027794821187853813, Validation Loss: 0.031546518206596375\n",
      "Epoch 2, Batch: 189: Training Loss: 0.027676686644554138, Validation Loss: 0.030242616310715675\n",
      "Epoch 2, Batch: 190: Training Loss: 0.033469200134277344, Validation Loss: 0.027873868122696877\n",
      "Epoch 2, Batch: 191: Training Loss: 0.030441049486398697, Validation Loss: 0.029406169429421425\n",
      "Epoch 2, Batch: 192: Training Loss: 0.03392532095313072, Validation Loss: 0.030406326055526733\n",
      "Epoch 2, Batch: 193: Training Loss: 0.02967558242380619, Validation Loss: 0.03030228242278099\n",
      "Epoch 2, Batch: 194: Training Loss: 0.029594862833619118, Validation Loss: 0.03123387321829796\n",
      "Epoch 2, Batch: 195: Training Loss: 0.028040338307619095, Validation Loss: 0.03156197816133499\n",
      "Epoch 2, Batch: 196: Training Loss: 0.03274563327431679, Validation Loss: 0.03091001696884632\n",
      "Epoch 2, Batch: 197: Training Loss: 0.02814599685370922, Validation Loss: 0.029165280982851982\n",
      "Epoch 2, Batch: 198: Training Loss: 0.029267527163028717, Validation Loss: 0.03048328123986721\n",
      "Epoch 2, Batch: 199: Training Loss: 0.02831198461353779, Validation Loss: 0.03111291490495205\n",
      "Epoch 2, Batch: 200: Training Loss: 0.024582790210843086, Validation Loss: 0.0322447195649147\n",
      "Epoch 2, Batch: 201: Training Loss: 0.03377658128738403, Validation Loss: 0.03276684880256653\n",
      "Epoch 2, Batch: 202: Training Loss: 0.036334674805402756, Validation Loss: 0.03215493634343147\n",
      "Epoch 2, Batch: 203: Training Loss: 0.034929707646369934, Validation Loss: 0.029530230909585953\n",
      "Epoch 2, Batch: 204: Training Loss: 0.028173213824629784, Validation Loss: 0.027825459837913513\n",
      "Epoch 2, Batch: 205: Training Loss: 0.03406313434243202, Validation Loss: 0.030182084068655968\n",
      "Epoch 2, Batch: 206: Training Loss: 0.027248479425907135, Validation Loss: 0.03101692721247673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 207: Training Loss: 0.0327150858938694, Validation Loss: 0.031189415603876114\n",
      "Epoch 2, Batch: 208: Training Loss: 0.030541332438588142, Validation Loss: 0.031834352761507034\n",
      "Epoch 2, Batch: 209: Training Loss: 0.032290127128362656, Validation Loss: 0.030964625999331474\n",
      "Epoch 2, Batch: 210: Training Loss: 0.028322651982307434, Validation Loss: 0.02996029146015644\n",
      "Epoch 2, Batch: 211: Training Loss: 0.033526308834552765, Validation Loss: 0.03231819346547127\n",
      "Epoch 2, Batch: 212: Training Loss: 0.028406910598278046, Validation Loss: 0.0325496681034565\n",
      "Epoch 2, Batch: 213: Training Loss: 0.0381251685321331, Validation Loss: 0.030218416824936867\n",
      "Epoch 2, Batch: 214: Training Loss: 0.028038524091243744, Validation Loss: 0.03231348469853401\n",
      "Epoch 2, Batch: 215: Training Loss: 0.030564995482563972, Validation Loss: 0.031033780425786972\n",
      "Epoch 2, Batch: 216: Training Loss: 0.028556711971759796, Validation Loss: 0.03552401065826416\n",
      "Epoch 2, Batch: 217: Training Loss: 0.030653325840830803, Validation Loss: 0.03286224976181984\n",
      "Epoch 2, Batch: 218: Training Loss: 0.03203444555401802, Validation Loss: 0.03397229313850403\n",
      "Epoch 2, Batch: 219: Training Loss: 0.03053748980164528, Validation Loss: 0.03373080864548683\n",
      "Epoch 2, Batch: 220: Training Loss: 0.03341769427061081, Validation Loss: 0.03342973068356514\n",
      "Epoch 2, Batch: 221: Training Loss: 0.02937117964029312, Validation Loss: 0.03126918524503708\n",
      "Epoch 2, Batch: 222: Training Loss: 0.03138050064444542, Validation Loss: 0.031084252521395683\n",
      "Epoch 2, Batch: 223: Training Loss: 0.029897097498178482, Validation Loss: 0.03136806562542915\n",
      "Epoch 2, Batch: 224: Training Loss: 0.028880132362246513, Validation Loss: 0.03268822282552719\n",
      "Epoch 2, Batch: 225: Training Loss: 0.030460447072982788, Validation Loss: 0.03185215964913368\n",
      "Epoch 2, Batch: 226: Training Loss: 0.03030661679804325, Validation Loss: 0.03456616401672363\n",
      "Epoch 2, Batch: 227: Training Loss: 0.0331309512257576, Validation Loss: 0.03398425877094269\n",
      "Epoch 2, Batch: 228: Training Loss: 0.030785659328103065, Validation Loss: 0.03358995541930199\n",
      "Epoch 2, Batch: 229: Training Loss: 0.031419433653354645, Validation Loss: 0.033045586198568344\n",
      "Epoch 2, Batch: 230: Training Loss: 0.027869937941432, Validation Loss: 0.033402618020772934\n",
      "Epoch 2, Batch: 231: Training Loss: 0.04005130007863045, Validation Loss: 0.03171396255493164\n",
      "Epoch 2, Batch: 232: Training Loss: 0.03043235093355179, Validation Loss: 0.03360225632786751\n",
      "Epoch 2, Batch: 233: Training Loss: 0.02988530695438385, Validation Loss: 0.032483115792274475\n",
      "Epoch 2, Batch: 234: Training Loss: 0.02997075393795967, Validation Loss: 0.030328616499900818\n",
      "Epoch 2, Batch: 235: Training Loss: 0.03248433768749237, Validation Loss: 0.032371677458286285\n",
      "Epoch 2, Batch: 236: Training Loss: 0.033395566046237946, Validation Loss: 0.03197048604488373\n",
      "Epoch 2, Batch: 237: Training Loss: 0.029869617894291878, Validation Loss: 0.03200553357601166\n",
      "Epoch 2, Batch: 238: Training Loss: 0.033353500068187714, Validation Loss: 0.02958272211253643\n",
      "Epoch 2, Batch: 239: Training Loss: 0.031673308461904526, Validation Loss: 0.029665978625416756\n",
      "Epoch 2, Batch: 240: Training Loss: 0.02603260986506939, Validation Loss: 0.030511192977428436\n",
      "Epoch 2, Batch: 241: Training Loss: 0.03603849187493324, Validation Loss: 0.032201509922742844\n",
      "Epoch 2, Batch: 242: Training Loss: 0.02822272852063179, Validation Loss: 0.02908986061811447\n",
      "Epoch 2, Batch: 243: Training Loss: 0.0297422893345356, Validation Loss: 0.030597779899835587\n",
      "Epoch 2, Batch: 244: Training Loss: 0.0266181081533432, Validation Loss: 0.028658704832196236\n",
      "Epoch 2, Batch: 245: Training Loss: 0.033685386180877686, Validation Loss: 0.031759049743413925\n",
      "Epoch 2, Batch: 246: Training Loss: 0.02947465516626835, Validation Loss: 0.032453589141368866\n",
      "Epoch 2, Batch: 247: Training Loss: 0.033749207854270935, Validation Loss: 0.03180796653032303\n",
      "Epoch 2, Batch: 248: Training Loss: 0.028813574463129044, Validation Loss: 0.03343657776713371\n",
      "Epoch 2, Batch: 249: Training Loss: 0.028158916160464287, Validation Loss: 0.032231561839580536\n",
      "Epoch 2, Batch: 250: Training Loss: 0.02918859012424946, Validation Loss: 0.035357460379600525\n",
      "Epoch 2, Batch: 251: Training Loss: 0.027868665754795074, Validation Loss: 0.0314151868224144\n",
      "Epoch 2, Batch: 252: Training Loss: 0.03413596749305725, Validation Loss: 0.034294456243515015\n",
      "Epoch 2, Batch: 253: Training Loss: 0.035871051251888275, Validation Loss: 0.03137536719441414\n",
      "Epoch 2, Batch: 254: Training Loss: 0.028659315779805183, Validation Loss: 0.029390458017587662\n",
      "Epoch 2, Batch: 255: Training Loss: 0.026381250470876694, Validation Loss: 0.029238563030958176\n",
      "Epoch 2, Batch: 256: Training Loss: 0.0319596566259861, Validation Loss: 0.030768221244215965\n",
      "Epoch 2, Batch: 257: Training Loss: 0.034570835530757904, Validation Loss: 0.03073006123304367\n",
      "Epoch 2, Batch: 258: Training Loss: 0.03125938028097153, Validation Loss: 0.03302633762359619\n",
      "Epoch 2, Batch: 259: Training Loss: 0.02785632386803627, Validation Loss: 0.032701827585697174\n",
      "Epoch 2, Batch: 260: Training Loss: 0.0364329069852829, Validation Loss: 0.03133738785982132\n",
      "Epoch 2, Batch: 261: Training Loss: 0.03125753253698349, Validation Loss: 0.03064589574933052\n",
      "Epoch 2, Batch: 262: Training Loss: 0.029299814254045486, Validation Loss: 0.035604700446128845\n",
      "Epoch 2, Batch: 263: Training Loss: 0.031328752636909485, Validation Loss: 0.02987496368587017\n",
      "Epoch 2, Batch: 264: Training Loss: 0.0267021544277668, Validation Loss: 0.02939549833536148\n",
      "Epoch 2, Batch: 265: Training Loss: 0.032413724809885025, Validation Loss: 0.029281694442033768\n",
      "Epoch 2, Batch: 266: Training Loss: 0.030240096151828766, Validation Loss: 0.02864815853536129\n",
      "Epoch 2, Batch: 267: Training Loss: 0.030757874250411987, Validation Loss: 0.031148314476013184\n",
      "Epoch 2, Batch: 268: Training Loss: 0.03068731725215912, Validation Loss: 0.027989864349365234\n",
      "Epoch 2, Batch: 269: Training Loss: 0.03049815073609352, Validation Loss: 0.027810368686914444\n",
      "Epoch 2, Batch: 270: Training Loss: 0.03091140277683735, Validation Loss: 0.03132526949048042\n",
      "Epoch 2, Batch: 271: Training Loss: 0.027956997975707054, Validation Loss: 0.02866712398827076\n",
      "Epoch 2, Batch: 272: Training Loss: 0.02762298472225666, Validation Loss: 0.028399651870131493\n",
      "Epoch 2, Batch: 273: Training Loss: 0.027293136343359947, Validation Loss: 0.030628804117441177\n",
      "Epoch 2, Batch: 274: Training Loss: 0.028640685603022575, Validation Loss: 0.03156412020325661\n",
      "Epoch 2, Batch: 275: Training Loss: 0.02748757041990757, Validation Loss: 0.03162115812301636\n",
      "Epoch 2, Batch: 276: Training Loss: 0.03377148509025574, Validation Loss: 0.030900120735168457\n",
      "Epoch 2, Batch: 277: Training Loss: 0.02880251221358776, Validation Loss: 0.033047061413526535\n",
      "Epoch 2, Batch: 278: Training Loss: 0.030468691140413284, Validation Loss: 0.030529577285051346\n",
      "Epoch 2, Batch: 279: Training Loss: 0.03373157978057861, Validation Loss: 0.02903451956808567\n",
      "Epoch 2, Batch: 280: Training Loss: 0.0282388124614954, Validation Loss: 0.03240203112363815\n",
      "Epoch 2, Batch: 281: Training Loss: 0.031444404274225235, Validation Loss: 0.031388089060783386\n",
      "Epoch 2, Batch: 282: Training Loss: 0.028310902416706085, Validation Loss: 0.03136347979307175\n",
      "Epoch 2, Batch: 283: Training Loss: 0.027122437953948975, Validation Loss: 0.032280150800943375\n",
      "Epoch 2, Batch: 284: Training Loss: 0.03002690151333809, Validation Loss: 0.031086072325706482\n",
      "Epoch 2, Batch: 285: Training Loss: 0.02990836277604103, Validation Loss: 0.033175062388181686\n",
      "Epoch 2, Batch: 286: Training Loss: 0.026150880381464958, Validation Loss: 0.03155474737286568\n",
      "Epoch 2, Batch: 287: Training Loss: 0.03362543508410454, Validation Loss: 0.028924787417054176\n",
      "Epoch 2, Batch: 288: Training Loss: 0.03068479336798191, Validation Loss: 0.030608655884861946\n",
      "Epoch 2, Batch: 289: Training Loss: 0.03323821350932121, Validation Loss: 0.03279085084795952\n",
      "Epoch 2, Batch: 290: Training Loss: 0.03126775845885277, Validation Loss: 0.0335010290145874\n",
      "Epoch 2, Batch: 291: Training Loss: 0.030314043164253235, Validation Loss: 0.030416736379265785\n",
      "Epoch 2, Batch: 292: Training Loss: 0.03219161182641983, Validation Loss: 0.030385414138436317\n",
      "Epoch 2, Batch: 293: Training Loss: 0.0302653256803751, Validation Loss: 0.03503823280334473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 294: Training Loss: 0.03370245546102524, Validation Loss: 0.029845625162124634\n",
      "Epoch 2, Batch: 295: Training Loss: 0.0284416526556015, Validation Loss: 0.03102205879986286\n",
      "Epoch 2, Batch: 296: Training Loss: 0.031511642038822174, Validation Loss: 0.029277753084897995\n",
      "Epoch 2, Batch: 297: Training Loss: 0.031139319762587547, Validation Loss: 0.03210999071598053\n",
      "Epoch 2, Batch: 298: Training Loss: 0.03040136583149433, Validation Loss: 0.03244825452566147\n",
      "Epoch 2, Batch: 299: Training Loss: 0.027214687317609787, Validation Loss: 0.03136800602078438\n",
      "Epoch 2, Batch: 300: Training Loss: 0.030566003173589706, Validation Loss: 0.029827691614627838\n",
      "Epoch 2, Batch: 301: Training Loss: 0.03454549238085747, Validation Loss: 0.02954837866127491\n",
      "Epoch 2, Batch: 302: Training Loss: 0.030221497640013695, Validation Loss: 0.02980569750070572\n",
      "Epoch 2, Batch: 303: Training Loss: 0.027697190642356873, Validation Loss: 0.02924748882651329\n",
      "Epoch 2, Batch: 304: Training Loss: 0.0313115231692791, Validation Loss: 0.028542282059788704\n",
      "Epoch 2, Batch: 305: Training Loss: 0.024564474821090698, Validation Loss: 0.030832836404442787\n",
      "Epoch 2, Batch: 306: Training Loss: 0.03193799406290054, Validation Loss: 0.03118141181766987\n",
      "Epoch 2, Batch: 307: Training Loss: 0.027642548084259033, Validation Loss: 0.02709995023906231\n",
      "Epoch 2, Batch: 308: Training Loss: 0.028884470462799072, Validation Loss: 0.026818005368113518\n",
      "Epoch 2, Batch: 309: Training Loss: 0.028240447863936424, Validation Loss: 0.030220525339245796\n",
      "Epoch 2, Batch: 310: Training Loss: 0.03023810125887394, Validation Loss: 0.028584696352481842\n",
      "Epoch 2, Batch: 311: Training Loss: 0.028238026425242424, Validation Loss: 0.026384705677628517\n",
      "Epoch 2, Batch: 312: Training Loss: 0.030909359455108643, Validation Loss: 0.030381929129362106\n",
      "Epoch 2, Batch: 313: Training Loss: 0.025839965790510178, Validation Loss: 0.02799111418426037\n",
      "Epoch 2, Batch: 314: Training Loss: 0.028708649799227715, Validation Loss: 0.03092186152935028\n",
      "Epoch 2, Batch: 315: Training Loss: 0.03277253359556198, Validation Loss: 0.029973648488521576\n",
      "Epoch 2, Batch: 316: Training Loss: 0.02958727441728115, Validation Loss: 0.03066638484597206\n",
      "Epoch 2, Batch: 317: Training Loss: 0.027829870581626892, Validation Loss: 0.03152621164917946\n",
      "Epoch 2, Batch: 318: Training Loss: 0.02977730892598629, Validation Loss: 0.029461625963449478\n",
      "Epoch 2, Batch: 319: Training Loss: 0.031776148825883865, Validation Loss: 0.03008345700800419\n",
      "Epoch 2, Batch: 320: Training Loss: 0.03007153980433941, Validation Loss: 0.031381573528051376\n",
      "Epoch 2, Batch: 321: Training Loss: 0.029735703021287918, Validation Loss: 0.03103022463619709\n",
      "Epoch 2, Batch: 322: Training Loss: 0.03295352682471275, Validation Loss: 0.028173595666885376\n",
      "Epoch 2, Batch: 323: Training Loss: 0.03341980651021004, Validation Loss: 0.03129347041249275\n",
      "Epoch 2, Batch: 324: Training Loss: 0.03341856971383095, Validation Loss: 0.027536364272236824\n",
      "Epoch 2, Batch: 325: Training Loss: 0.028697529807686806, Validation Loss: 0.03187791258096695\n",
      "Epoch 2, Batch: 326: Training Loss: 0.02871156483888626, Validation Loss: 0.02821364626288414\n",
      "Epoch 2, Batch: 327: Training Loss: 0.028125960379838943, Validation Loss: 0.029464352875947952\n",
      "Epoch 2, Batch: 328: Training Loss: 0.03209005668759346, Validation Loss: 0.029589302837848663\n",
      "Epoch 2, Batch: 329: Training Loss: 0.02874746359884739, Validation Loss: 0.03256830945611\n",
      "Epoch 2, Batch: 330: Training Loss: 0.030996156856417656, Validation Loss: 0.02932429313659668\n",
      "Epoch 2, Batch: 331: Training Loss: 0.031490739434957504, Validation Loss: 0.029020337387919426\n",
      "Epoch 2, Batch: 332: Training Loss: 0.029263125732541084, Validation Loss: 0.03224647417664528\n",
      "Epoch 2, Batch: 333: Training Loss: 0.03268028423190117, Validation Loss: 0.028260838240385056\n",
      "Epoch 2, Batch: 334: Training Loss: 0.03484850004315376, Validation Loss: 0.03003212995827198\n",
      "Epoch 2, Batch: 335: Training Loss: 0.028337912634015083, Validation Loss: 0.026889150962233543\n",
      "Epoch 2, Batch: 336: Training Loss: 0.029433071613311768, Validation Loss: 0.02788448892533779\n",
      "Epoch 2, Batch: 337: Training Loss: 0.026967070996761322, Validation Loss: 0.03169069066643715\n",
      "Epoch 2, Batch: 338: Training Loss: 0.036042794585227966, Validation Loss: 0.03001081384718418\n",
      "Epoch 2, Batch: 339: Training Loss: 0.029571283608675003, Validation Loss: 0.027585148811340332\n",
      "Epoch 2, Batch: 340: Training Loss: 0.028390195220708847, Validation Loss: 0.031080001965165138\n",
      "Epoch 2, Batch: 341: Training Loss: 0.02768786810338497, Validation Loss: 0.027529045939445496\n",
      "Epoch 2, Batch: 342: Training Loss: 0.02830974943935871, Validation Loss: 0.028804928064346313\n",
      "Epoch 2, Batch: 343: Training Loss: 0.03329160436987877, Validation Loss: 0.02802932821214199\n",
      "Epoch 2, Batch: 344: Training Loss: 0.03417963162064552, Validation Loss: 0.027223793789744377\n",
      "Epoch 2, Batch: 345: Training Loss: 0.028150267899036407, Validation Loss: 0.02858726680278778\n",
      "Epoch 2, Batch: 346: Training Loss: 0.032670434564352036, Validation Loss: 0.0271710604429245\n",
      "Epoch 2, Batch: 347: Training Loss: 0.02824356220662594, Validation Loss: 0.028297141194343567\n",
      "Epoch 2, Batch: 348: Training Loss: 0.03179261460900307, Validation Loss: 0.029457813128829002\n",
      "Epoch 2, Batch: 349: Training Loss: 0.0315072126686573, Validation Loss: 0.02877366915345192\n",
      "Epoch 2, Batch: 350: Training Loss: 0.02805904671549797, Validation Loss: 0.029256759211421013\n",
      "Epoch 2, Batch: 351: Training Loss: 0.03260696306824684, Validation Loss: 0.03180834650993347\n",
      "Epoch 2, Batch: 352: Training Loss: 0.03288910165429115, Validation Loss: 0.027799377217888832\n",
      "Epoch 2, Batch: 353: Training Loss: 0.02951696701347828, Validation Loss: 0.026468753814697266\n",
      "Epoch 2, Batch: 354: Training Loss: 0.03236563131213188, Validation Loss: 0.026498684659600258\n",
      "Saving new best model w/ loss: 0.0258200541138649\n",
      "Epoch 2, Batch: 355: Training Loss: 0.02740613929927349, Validation Loss: 0.0258200541138649\n",
      "Epoch 2, Batch: 356: Training Loss: 0.030084580183029175, Validation Loss: 0.02690882794559002\n",
      "Epoch 2, Batch: 357: Training Loss: 0.024999286979436874, Validation Loss: 0.026760753244161606\n",
      "Epoch 2, Batch: 358: Training Loss: 0.03227127715945244, Validation Loss: 0.027213960886001587\n",
      "Epoch 2, Batch: 359: Training Loss: 0.032488398253917694, Validation Loss: 0.027767939493060112\n",
      "Saving new best model w/ loss: 0.02554054744541645\n",
      "Epoch 2, Batch: 360: Training Loss: 0.028675811365246773, Validation Loss: 0.02554054744541645\n",
      "Epoch 2, Batch: 361: Training Loss: 0.029809262603521347, Validation Loss: 0.028682125732302666\n",
      "Epoch 2, Batch: 362: Training Loss: 0.02918875776231289, Validation Loss: 0.029053928330540657\n",
      "Epoch 2, Batch: 363: Training Loss: 0.03669553995132446, Validation Loss: 0.027906734496355057\n",
      "Epoch 2, Batch: 364: Training Loss: 0.03110169619321823, Validation Loss: 0.026322422549128532\n",
      "Epoch 2, Batch: 365: Training Loss: 0.030976254492998123, Validation Loss: 0.02632148563861847\n",
      "Epoch 2, Batch: 366: Training Loss: 0.03519533947110176, Validation Loss: 0.026840902864933014\n",
      "Epoch 2, Batch: 367: Training Loss: 0.024265632033348083, Validation Loss: 0.026520079001784325\n",
      "Epoch 2, Batch: 368: Training Loss: 0.029423227533698082, Validation Loss: 0.02986994944512844\n",
      "Epoch 2, Batch: 369: Training Loss: 0.029265660792589188, Validation Loss: 0.026739750057458878\n",
      "Epoch 2, Batch: 370: Training Loss: 0.030173800885677338, Validation Loss: 0.031277287751436234\n",
      "Epoch 2, Batch: 371: Training Loss: 0.03118014708161354, Validation Loss: 0.03086923621594906\n",
      "Epoch 2, Batch: 372: Training Loss: 0.03116154856979847, Validation Loss: 0.029951244592666626\n",
      "Epoch 2, Batch: 373: Training Loss: 0.033098019659519196, Validation Loss: 0.03183884546160698\n",
      "Epoch 2, Batch: 374: Training Loss: 0.026757076382637024, Validation Loss: 0.029598746448755264\n",
      "Epoch 2, Batch: 375: Training Loss: 0.034180521965026855, Validation Loss: 0.02910354733467102\n",
      "Epoch 2, Batch: 376: Training Loss: 0.031281743198633194, Validation Loss: 0.031751472502946854\n",
      "Epoch 2, Batch: 377: Training Loss: 0.029084062203764915, Validation Loss: 0.030511830002069473\n",
      "Epoch 2, Batch: 378: Training Loss: 0.031870342791080475, Validation Loss: 0.02877187542617321\n",
      "Epoch 2, Batch: 379: Training Loss: 0.025727752596139908, Validation Loss: 0.02909105271100998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 380: Training Loss: 0.0332358293235302, Validation Loss: 0.02856932394206524\n",
      "Epoch 2, Batch: 381: Training Loss: 0.03160663694143295, Validation Loss: 0.02752992883324623\n",
      "Epoch 2, Batch: 382: Training Loss: 0.03238743543624878, Validation Loss: 0.02722540870308876\n",
      "Epoch 2, Batch: 383: Training Loss: 0.028681805357336998, Validation Loss: 0.028803329914808273\n",
      "Epoch 2, Batch: 384: Training Loss: 0.030793091282248497, Validation Loss: 0.02736690081655979\n",
      "Epoch 2, Batch: 385: Training Loss: 0.030845655128359795, Validation Loss: 0.026889599859714508\n",
      "Epoch 2, Batch: 386: Training Loss: 0.02851935848593712, Validation Loss: 0.026188528165221214\n",
      "Epoch 2, Batch: 387: Training Loss: 0.031425945460796356, Validation Loss: 0.027835708111524582\n",
      "Epoch 2, Batch: 388: Training Loss: 0.03080105409026146, Validation Loss: 0.02811296097934246\n",
      "Epoch 2, Batch: 389: Training Loss: 0.03213497996330261, Validation Loss: 0.028759341686964035\n",
      "Epoch 2, Batch: 390: Training Loss: 0.027481507509946823, Validation Loss: 0.030211804434657097\n",
      "Epoch 2, Batch: 391: Training Loss: 0.034143395721912384, Validation Loss: 0.030798498541116714\n",
      "Epoch 2, Batch: 392: Training Loss: 0.031193464994430542, Validation Loss: 0.028164133429527283\n",
      "Epoch 2, Batch: 393: Training Loss: 0.02805027738213539, Validation Loss: 0.02988862432539463\n",
      "Epoch 2, Batch: 394: Training Loss: 0.028059430420398712, Validation Loss: 0.028676357120275497\n",
      "Epoch 2, Batch: 395: Training Loss: 0.028871430084109306, Validation Loss: 0.03084012120962143\n",
      "Epoch 2, Batch: 396: Training Loss: 0.03095565363764763, Validation Loss: 0.028919320553541183\n",
      "Epoch 2, Batch: 397: Training Loss: 0.034999117255210876, Validation Loss: 0.031099429354071617\n",
      "Epoch 2, Batch: 398: Training Loss: 0.02848190627992153, Validation Loss: 0.029152745380997658\n",
      "Epoch 2, Batch: 399: Training Loss: 0.03124951757490635, Validation Loss: 0.03272402286529541\n",
      "Epoch 2, Batch: 400: Training Loss: 0.03191238269209862, Validation Loss: 0.028816964477300644\n",
      "Epoch 2, Batch: 401: Training Loss: 0.024972954764962196, Validation Loss: 0.029666759073734283\n",
      "Epoch 2, Batch: 402: Training Loss: 0.02443159557878971, Validation Loss: 0.02962503768503666\n",
      "Epoch 2, Batch: 403: Training Loss: 0.03456497937440872, Validation Loss: 0.03131045028567314\n",
      "Epoch 2, Batch: 404: Training Loss: 0.029362797737121582, Validation Loss: 0.032481081783771515\n",
      "Epoch 2, Batch: 405: Training Loss: 0.029111040756106377, Validation Loss: 0.030746007338166237\n",
      "Epoch 2, Batch: 406: Training Loss: 0.0284558292478323, Validation Loss: 0.03133488446474075\n",
      "Epoch 2, Batch: 407: Training Loss: 0.029761677607893944, Validation Loss: 0.03137441724538803\n",
      "Epoch 2, Batch: 408: Training Loss: 0.02680608630180359, Validation Loss: 0.02794235199689865\n",
      "Epoch 2, Batch: 409: Training Loss: 0.03133317828178406, Validation Loss: 0.03116205707192421\n",
      "Epoch 2, Batch: 410: Training Loss: 0.029389983043074608, Validation Loss: 0.03178897872567177\n",
      "Epoch 2, Batch: 411: Training Loss: 0.0344814732670784, Validation Loss: 0.030143894255161285\n",
      "Epoch 2, Batch: 412: Training Loss: 0.0335635282099247, Validation Loss: 0.0293722003698349\n",
      "Epoch 2, Batch: 413: Training Loss: 0.030327625572681427, Validation Loss: 0.03321898356080055\n",
      "Epoch 2, Batch: 414: Training Loss: 0.030327295884490013, Validation Loss: 0.030388085171580315\n",
      "Epoch 2, Batch: 415: Training Loss: 0.028446389362215996, Validation Loss: 0.02915322594344616\n",
      "Epoch 2, Batch: 416: Training Loss: 0.03014477714896202, Validation Loss: 0.03249373659491539\n",
      "Epoch 2, Batch: 417: Training Loss: 0.032924093306064606, Validation Loss: 0.03071550466120243\n",
      "Epoch 2, Batch: 418: Training Loss: 0.02889132872223854, Validation Loss: 0.03091631457209587\n",
      "Epoch 2, Batch: 419: Training Loss: 0.028121452778577805, Validation Loss: 0.0319393016397953\n",
      "Epoch 2, Batch: 420: Training Loss: 0.03270475193858147, Validation Loss: 0.034954410046339035\n",
      "Epoch 2, Batch: 421: Training Loss: 0.0388273261487484, Validation Loss: 0.03179268166422844\n",
      "Epoch 2, Batch: 422: Training Loss: 0.03413049876689911, Validation Loss: 0.032212015241384506\n",
      "Epoch 2, Batch: 423: Training Loss: 0.03554769977927208, Validation Loss: 0.028666233643889427\n",
      "Epoch 2, Batch: 424: Training Loss: 0.03219209983944893, Validation Loss: 0.029038354754447937\n",
      "Epoch 2, Batch: 425: Training Loss: 0.03002012148499489, Validation Loss: 0.02959013171494007\n",
      "Epoch 2, Batch: 426: Training Loss: 0.029456980526447296, Validation Loss: 0.032397083938121796\n",
      "Epoch 2, Batch: 427: Training Loss: 0.03264227509498596, Validation Loss: 0.029613662511110306\n",
      "Epoch 2, Batch: 428: Training Loss: 0.030206454917788506, Validation Loss: 0.02768028900027275\n",
      "Epoch 2, Batch: 429: Training Loss: 0.030098721385002136, Validation Loss: 0.03253854438662529\n",
      "Epoch 2, Batch: 430: Training Loss: 0.025477563962340355, Validation Loss: 0.03034106269478798\n",
      "Epoch 2, Batch: 431: Training Loss: 0.027932340279221535, Validation Loss: 0.030190452933311462\n",
      "Epoch 2, Batch: 432: Training Loss: 0.03128064423799515, Validation Loss: 0.030176227912306786\n",
      "Epoch 2, Batch: 433: Training Loss: 0.030341940000653267, Validation Loss: 0.030674679204821587\n",
      "Epoch 2, Batch: 434: Training Loss: 0.030360093340277672, Validation Loss: 0.030206359922885895\n",
      "Epoch 2, Batch: 435: Training Loss: 0.027855174615979195, Validation Loss: 0.03215893730521202\n",
      "Epoch 2, Batch: 436: Training Loss: 0.02816406637430191, Validation Loss: 0.029056061059236526\n",
      "Epoch 2, Batch: 437: Training Loss: 0.028669079765677452, Validation Loss: 0.031792089343070984\n",
      "Epoch 2, Batch: 438: Training Loss: 0.029451556503772736, Validation Loss: 0.02677948772907257\n",
      "Epoch 2, Batch: 439: Training Loss: 0.027101701125502586, Validation Loss: 0.03040287084877491\n",
      "Epoch 2, Batch: 440: Training Loss: 0.03548775985836983, Validation Loss: 0.027659917250275612\n",
      "Epoch 2, Batch: 441: Training Loss: 0.025668276473879814, Validation Loss: 0.03004992939531803\n",
      "Epoch 2, Batch: 442: Training Loss: 0.032617684453725815, Validation Loss: 0.02912294678390026\n",
      "Epoch 2, Batch: 443: Training Loss: 0.029403071850538254, Validation Loss: 0.030117252841591835\n",
      "Epoch 2, Batch: 444: Training Loss: 0.028108680620789528, Validation Loss: 0.0292457677423954\n",
      "Epoch 2, Batch: 445: Training Loss: 0.025939112529158592, Validation Loss: 0.02917415462434292\n",
      "Epoch 2, Batch: 446: Training Loss: 0.031250834465026855, Validation Loss: 0.028866171836853027\n",
      "Epoch 2, Batch: 447: Training Loss: 0.02687903307378292, Validation Loss: 0.02993110939860344\n",
      "Epoch 2, Batch: 448: Training Loss: 0.029398323968052864, Validation Loss: 0.029664846137166023\n",
      "Epoch 2, Batch: 449: Training Loss: 0.029936552047729492, Validation Loss: 0.030733641237020493\n",
      "Epoch 2, Batch: 450: Training Loss: 0.030339298769831657, Validation Loss: 0.03203544765710831\n",
      "Epoch 2, Batch: 451: Training Loss: 0.031377892941236496, Validation Loss: 0.02887023612856865\n",
      "Epoch 2, Batch: 452: Training Loss: 0.03147996962070465, Validation Loss: 0.028368523344397545\n",
      "Epoch 2, Batch: 453: Training Loss: 0.03223370388150215, Validation Loss: 0.028263624757528305\n",
      "Epoch 2, Batch: 454: Training Loss: 0.027416206896305084, Validation Loss: 0.031335823237895966\n",
      "Epoch 2, Batch: 455: Training Loss: 0.027794944122433662, Validation Loss: 0.029146986082196236\n",
      "Epoch 2, Batch: 456: Training Loss: 0.02645127847790718, Validation Loss: 0.028555497527122498\n",
      "Epoch 2, Batch: 457: Training Loss: 0.030497850850224495, Validation Loss: 0.0289914533495903\n",
      "Epoch 2, Batch: 458: Training Loss: 0.029469875618815422, Validation Loss: 0.030054789036512375\n",
      "Epoch 2, Batch: 459: Training Loss: 0.02914932742714882, Validation Loss: 0.029921291396021843\n",
      "Epoch 2, Batch: 460: Training Loss: 0.030882902443408966, Validation Loss: 0.029404670000076294\n",
      "Epoch 2, Batch: 461: Training Loss: 0.0236213319003582, Validation Loss: 0.029534686356782913\n",
      "Epoch 2, Batch: 462: Training Loss: 0.027444947510957718, Validation Loss: 0.030916724354028702\n",
      "Epoch 2, Batch: 463: Training Loss: 0.030761435627937317, Validation Loss: 0.028869789093732834\n",
      "Epoch 2, Batch: 464: Training Loss: 0.02398873120546341, Validation Loss: 0.02922785095870495\n",
      "Epoch 2, Batch: 465: Training Loss: 0.027876555919647217, Validation Loss: 0.03552671894431114\n",
      "Epoch 2, Batch: 466: Training Loss: 0.02992505580186844, Validation Loss: 0.028943374752998352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch: 467: Training Loss: 0.03728567808866501, Validation Loss: 0.02914881892502308\n",
      "Epoch 2, Batch: 468: Training Loss: 0.03521706908941269, Validation Loss: 0.027527062222361565\n",
      "Epoch 2, Batch: 469: Training Loss: 0.02899167500436306, Validation Loss: 0.02918383851647377\n",
      "Epoch 2, Batch: 470: Training Loss: 0.02774074487388134, Validation Loss: 0.0331689789891243\n",
      "Epoch 2, Batch: 471: Training Loss: 0.03120369277894497, Validation Loss: 0.0307545717805624\n",
      "Epoch 2, Batch: 472: Training Loss: 0.0348714180290699, Validation Loss: 0.030496563762426376\n",
      "Epoch 2, Batch: 473: Training Loss: 0.028299463912844658, Validation Loss: 0.030754538252949715\n",
      "Epoch 2, Batch: 474: Training Loss: 0.028695493936538696, Validation Loss: 0.030712218955159187\n",
      "Epoch 2, Batch: 475: Training Loss: 0.029414556920528412, Validation Loss: 0.03387895226478577\n",
      "Epoch 2, Batch: 476: Training Loss: 0.0327318049967289, Validation Loss: 0.029492946341633797\n",
      "Epoch 2, Batch: 477: Training Loss: 0.03485593572258949, Validation Loss: 0.031139196828007698\n",
      "Epoch 2, Batch: 478: Training Loss: 0.027812514454126358, Validation Loss: 0.03246164321899414\n",
      "Epoch 2, Batch: 479: Training Loss: 0.03270099312067032, Validation Loss: 0.029519444331526756\n",
      "Epoch 2, Batch: 480: Training Loss: 0.02638857625424862, Validation Loss: 0.031034346669912338\n",
      "Epoch 2, Batch: 481: Training Loss: 0.02857772260904312, Validation Loss: 0.031870096921920776\n",
      "Epoch 2, Batch: 482: Training Loss: 0.0280535276979208, Validation Loss: 0.03207468241453171\n",
      "Epoch 2, Batch: 483: Training Loss: 0.026952538639307022, Validation Loss: 0.03514751046895981\n",
      "Epoch 2, Batch: 484: Training Loss: 0.031828682869672775, Validation Loss: 0.0372585691511631\n",
      "Epoch 2, Batch: 485: Training Loss: 0.02868012525141239, Validation Loss: 0.03096427023410797\n",
      "Epoch 2, Batch: 486: Training Loss: 0.030093319714069366, Validation Loss: 0.03372248634696007\n",
      "Epoch 2, Batch: 487: Training Loss: 0.031128035858273506, Validation Loss: 0.030835995450615883\n",
      "Epoch 2, Batch: 488: Training Loss: 0.02915584109723568, Validation Loss: 0.03067038394510746\n",
      "Epoch 2, Batch: 489: Training Loss: 0.033200886100530624, Validation Loss: 0.033345580101013184\n",
      "Epoch 2, Batch: 490: Training Loss: 0.033304836601018906, Validation Loss: 0.030528435483574867\n",
      "Epoch 2, Batch: 491: Training Loss: 0.02380559593439102, Validation Loss: 0.030428512021899223\n",
      "Epoch 2, Batch: 492: Training Loss: 0.032926131039857864, Validation Loss: 0.03007565811276436\n",
      "Epoch 2, Batch: 493: Training Loss: 0.02855605073273182, Validation Loss: 0.029585272073745728\n",
      "Epoch 2, Batch: 494: Training Loss: 0.03483803570270538, Validation Loss: 0.029514171183109283\n",
      "Epoch 2, Batch: 495: Training Loss: 0.02803386002779007, Validation Loss: 0.030648784711956978\n",
      "Epoch 2, Batch: 496: Training Loss: 0.02908899635076523, Validation Loss: 0.02986319735646248\n",
      "Epoch 2, Batch: 497: Training Loss: 0.027231978252530098, Validation Loss: 0.02887025475502014\n",
      "Epoch 2, Batch: 498: Training Loss: 0.029673699289560318, Validation Loss: 0.029516926035284996\n",
      "Epoch 2, Batch: 499: Training Loss: 0.028076710179448128, Validation Loss: 0.029670830816030502\n",
      "Epoch 3, Batch: 0: Training Loss: 0.030424723401665688, Validation Loss: 0.03263140469789505\n",
      "Epoch 3, Batch: 1: Training Loss: 0.025952311232686043, Validation Loss: 0.026978904381394386\n",
      "Epoch 3, Batch: 2: Training Loss: 0.03380066901445389, Validation Loss: 0.028032217174768448\n",
      "Epoch 3, Batch: 3: Training Loss: 0.028394922614097595, Validation Loss: 0.027161503210663795\n",
      "Epoch 3, Batch: 4: Training Loss: 0.025960227474570274, Validation Loss: 0.03176174685359001\n",
      "Epoch 3, Batch: 5: Training Loss: 0.027659203857183456, Validation Loss: 0.02911873534321785\n",
      "Epoch 3, Batch: 6: Training Loss: 0.029379790648818016, Validation Loss: 0.02950296178460121\n",
      "Epoch 3, Batch: 7: Training Loss: 0.029353277757763863, Validation Loss: 0.031385768204927444\n",
      "Epoch 3, Batch: 8: Training Loss: 0.0253693088889122, Validation Loss: 0.029063086956739426\n",
      "Epoch 3, Batch: 9: Training Loss: 0.030441753566265106, Validation Loss: 0.031231554225087166\n",
      "Epoch 3, Batch: 10: Training Loss: 0.030679013580083847, Validation Loss: 0.028635060414671898\n",
      "Epoch 3, Batch: 11: Training Loss: 0.035271238535642624, Validation Loss: 0.02959509752690792\n",
      "Epoch 3, Batch: 12: Training Loss: 0.030834399163722992, Validation Loss: 0.031439851969480515\n",
      "Epoch 3, Batch: 13: Training Loss: 0.032574791461229324, Validation Loss: 0.028503820300102234\n",
      "Epoch 3, Batch: 14: Training Loss: 0.03191811218857765, Validation Loss: 0.028522878885269165\n",
      "Epoch 3, Batch: 15: Training Loss: 0.030658980831503868, Validation Loss: 0.027407661080360413\n",
      "Epoch 3, Batch: 16: Training Loss: 0.0357414148747921, Validation Loss: 0.027119886130094528\n",
      "Epoch 3, Batch: 17: Training Loss: 0.026203103363513947, Validation Loss: 0.029571982100605965\n",
      "Epoch 3, Batch: 18: Training Loss: 0.02919188141822815, Validation Loss: 0.02921166643500328\n",
      "Epoch 3, Batch: 19: Training Loss: 0.028229938820004463, Validation Loss: 0.027780607342720032\n",
      "Epoch 3, Batch: 20: Training Loss: 0.0339042991399765, Validation Loss: 0.028658172115683556\n",
      "Epoch 3, Batch: 21: Training Loss: 0.029112644493579865, Validation Loss: 0.027883004397153854\n",
      "Epoch 3, Batch: 22: Training Loss: 0.03332110866904259, Validation Loss: 0.03108317404985428\n",
      "Epoch 3, Batch: 23: Training Loss: 0.027988865971565247, Validation Loss: 0.027453074231743813\n",
      "Epoch 3, Batch: 24: Training Loss: 0.02875300496816635, Validation Loss: 0.028345363214612007\n",
      "Epoch 3, Batch: 25: Training Loss: 0.030866164714097977, Validation Loss: 0.030004048720002174\n",
      "Epoch 3, Batch: 26: Training Loss: 0.031482189893722534, Validation Loss: 0.029115835204720497\n",
      "Epoch 3, Batch: 27: Training Loss: 0.030573280528187752, Validation Loss: 0.03039289079606533\n",
      "Epoch 3, Batch: 28: Training Loss: 0.03303113207221031, Validation Loss: 0.03070632368326187\n",
      "Epoch 3, Batch: 29: Training Loss: 0.033118486404418945, Validation Loss: 0.030109407380223274\n",
      "Epoch 3, Batch: 30: Training Loss: 0.03203676640987396, Validation Loss: 0.03222488984465599\n",
      "Epoch 3, Batch: 31: Training Loss: 0.03346465528011322, Validation Loss: 0.031334489583969116\n",
      "Epoch 3, Batch: 32: Training Loss: 0.029208755120635033, Validation Loss: 0.028442157432436943\n",
      "Epoch 3, Batch: 33: Training Loss: 0.02626350149512291, Validation Loss: 0.027998710051178932\n",
      "Epoch 3, Batch: 34: Training Loss: 0.026015961542725563, Validation Loss: 0.03150016814470291\n",
      "Epoch 3, Batch: 35: Training Loss: 0.02728007361292839, Validation Loss: 0.029444236308336258\n",
      "Epoch 3, Batch: 36: Training Loss: 0.03254103660583496, Validation Loss: 0.029412737116217613\n",
      "Epoch 3, Batch: 37: Training Loss: 0.029137566685676575, Validation Loss: 0.0318785198032856\n",
      "Epoch 3, Batch: 38: Training Loss: 0.03571806475520134, Validation Loss: 0.030713509768247604\n",
      "Epoch 3, Batch: 39: Training Loss: 0.033046044409275055, Validation Loss: 0.028977345675230026\n",
      "Epoch 3, Batch: 40: Training Loss: 0.036040980368852615, Validation Loss: 0.02881646901369095\n",
      "Epoch 3, Batch: 41: Training Loss: 0.03296989947557449, Validation Loss: 0.032213836908340454\n",
      "Epoch 3, Batch: 42: Training Loss: 0.028614791110157967, Validation Loss: 0.03119119629263878\n",
      "Epoch 3, Batch: 43: Training Loss: 0.030687110498547554, Validation Loss: 0.03196026757359505\n",
      "Epoch 3, Batch: 44: Training Loss: 0.035706743597984314, Validation Loss: 0.02862127311527729\n",
      "Saving new best model w/ loss: 0.025124860927462578\n",
      "Epoch 3, Batch: 45: Training Loss: 0.02932123653590679, Validation Loss: 0.025124860927462578\n",
      "Epoch 3, Batch: 46: Training Loss: 0.030705736950039864, Validation Loss: 0.03064088150858879\n",
      "Epoch 3, Batch: 47: Training Loss: 0.03163614496588707, Validation Loss: 0.03208295628428459\n",
      "Epoch 3, Batch: 48: Training Loss: 0.034464821219444275, Validation Loss: 0.03039584681391716\n",
      "Epoch 3, Batch: 49: Training Loss: 0.0345417857170105, Validation Loss: 0.029564451426267624\n",
      "Epoch 3, Batch: 50: Training Loss: 0.03190652281045914, Validation Loss: 0.03243245556950569\n",
      "Epoch 3, Batch: 51: Training Loss: 0.030150577425956726, Validation Loss: 0.03266396373510361\n",
      "Epoch 3, Batch: 52: Training Loss: 0.03211037069559097, Validation Loss: 0.03455411270260811\n",
      "Epoch 3, Batch: 53: Training Loss: 0.02725629135966301, Validation Loss: 0.031753018498420715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 54: Training Loss: 0.029243052005767822, Validation Loss: 0.032130707055330276\n",
      "Epoch 3, Batch: 55: Training Loss: 0.031008483842015266, Validation Loss: 0.030932346358895302\n",
      "Epoch 3, Batch: 56: Training Loss: 0.032458994537591934, Validation Loss: 0.03151457756757736\n",
      "Epoch 3, Batch: 57: Training Loss: 0.024604711681604385, Validation Loss: 0.0312831774353981\n",
      "Epoch 3, Batch: 58: Training Loss: 0.030127190053462982, Validation Loss: 0.03346765413880348\n",
      "Epoch 3, Batch: 59: Training Loss: 0.025016730651259422, Validation Loss: 0.029462678357958794\n",
      "Epoch 3, Batch: 60: Training Loss: 0.03237473964691162, Validation Loss: 0.032775431871414185\n",
      "Epoch 3, Batch: 61: Training Loss: 0.03656025975942612, Validation Loss: 0.0320018008351326\n",
      "Epoch 3, Batch: 62: Training Loss: 0.031047619879245758, Validation Loss: 0.030640561133623123\n",
      "Epoch 3, Batch: 63: Training Loss: 0.03503166884183884, Validation Loss: 0.028721444308757782\n",
      "Epoch 3, Batch: 64: Training Loss: 0.02987942285835743, Validation Loss: 0.03341830521821976\n",
      "Epoch 3, Batch: 65: Training Loss: 0.031064020469784737, Validation Loss: 0.027837255969643593\n",
      "Epoch 3, Batch: 66: Training Loss: 0.03582640737295151, Validation Loss: 0.032300468534231186\n",
      "Epoch 3, Batch: 67: Training Loss: 0.02995876967906952, Validation Loss: 0.029821818694472313\n",
      "Epoch 3, Batch: 68: Training Loss: 0.03154706582427025, Validation Loss: 0.02989490143954754\n",
      "Epoch 3, Batch: 69: Training Loss: 0.027288630604743958, Validation Loss: 0.02872668392956257\n",
      "Epoch 3, Batch: 70: Training Loss: 0.035586316138505936, Validation Loss: 0.030941499397158623\n",
      "Epoch 3, Batch: 71: Training Loss: 0.029693106189370155, Validation Loss: 0.031015146523714066\n",
      "Epoch 3, Batch: 72: Training Loss: 0.027615126222372055, Validation Loss: 0.026997294276952744\n",
      "Epoch 3, Batch: 73: Training Loss: 0.025409823283553123, Validation Loss: 0.029371310025453568\n",
      "Epoch 3, Batch: 74: Training Loss: 0.03031873144209385, Validation Loss: 0.02881559543311596\n",
      "Epoch 3, Batch: 75: Training Loss: 0.02364795282483101, Validation Loss: 0.030113935470581055\n",
      "Epoch 3, Batch: 76: Training Loss: 0.02760975994169712, Validation Loss: 0.030335932970046997\n",
      "Epoch 3, Batch: 77: Training Loss: 0.03655415028333664, Validation Loss: 0.03158401697874069\n",
      "Epoch 3, Batch: 78: Training Loss: 0.03405676409602165, Validation Loss: 0.03424837067723274\n",
      "Epoch 3, Batch: 79: Training Loss: 0.027901234105229378, Validation Loss: 0.03320331498980522\n",
      "Epoch 3, Batch: 80: Training Loss: 0.03415917605161667, Validation Loss: 0.03280119225382805\n",
      "Epoch 3, Batch: 81: Training Loss: 0.030278611928224564, Validation Loss: 0.030279232189059258\n",
      "Epoch 3, Batch: 82: Training Loss: 0.03129260241985321, Validation Loss: 0.032104793936014175\n",
      "Epoch 3, Batch: 83: Training Loss: 0.030561761930584908, Validation Loss: 0.03343484178185463\n",
      "Epoch 3, Batch: 84: Training Loss: 0.030625995248556137, Validation Loss: 0.03141036629676819\n",
      "Epoch 3, Batch: 85: Training Loss: 0.02879904955625534, Validation Loss: 0.03173237293958664\n",
      "Epoch 3, Batch: 86: Training Loss: 0.03162190690636635, Validation Loss: 0.03385341167449951\n",
      "Epoch 3, Batch: 87: Training Loss: 0.032723817974328995, Validation Loss: 0.03005060739815235\n",
      "Epoch 3, Batch: 88: Training Loss: 0.030967120081186295, Validation Loss: 0.031957659870386124\n",
      "Epoch 3, Batch: 89: Training Loss: 0.035428550094366074, Validation Loss: 0.033550702035427094\n",
      "Epoch 3, Batch: 90: Training Loss: 0.030370673164725304, Validation Loss: 0.031349506229162216\n",
      "Epoch 3, Batch: 91: Training Loss: 0.03005637601017952, Validation Loss: 0.033055346459150314\n",
      "Epoch 3, Batch: 92: Training Loss: 0.03386006876826286, Validation Loss: 0.031055530533194542\n",
      "Epoch 3, Batch: 93: Training Loss: 0.029548591002821922, Validation Loss: 0.030996117740869522\n",
      "Epoch 3, Batch: 94: Training Loss: 0.03130963817238808, Validation Loss: 0.026480086147785187\n",
      "Epoch 3, Batch: 95: Training Loss: 0.03167029097676277, Validation Loss: 0.03179170563817024\n",
      "Epoch 3, Batch: 96: Training Loss: 0.03231314942240715, Validation Loss: 0.03313339129090309\n",
      "Epoch 3, Batch: 97: Training Loss: 0.02826678566634655, Validation Loss: 0.03172898665070534\n",
      "Epoch 3, Batch: 98: Training Loss: 0.030847467482089996, Validation Loss: 0.031542614102363586\n",
      "Epoch 3, Batch: 99: Training Loss: 0.031516168266534805, Validation Loss: 0.02943466603755951\n",
      "Epoch 3, Batch: 100: Training Loss: 0.029692957177758217, Validation Loss: 0.028957678005099297\n",
      "Epoch 3, Batch: 101: Training Loss: 0.030470987781882286, Validation Loss: 0.030794091522693634\n",
      "Epoch 3, Batch: 102: Training Loss: 0.03127794712781906, Validation Loss: 0.03193003311753273\n",
      "Epoch 3, Batch: 103: Training Loss: 0.03310215473175049, Validation Loss: 0.031686704605817795\n",
      "Epoch 3, Batch: 104: Training Loss: 0.026694949716329575, Validation Loss: 0.031535081565380096\n",
      "Epoch 3, Batch: 105: Training Loss: 0.031319763511419296, Validation Loss: 0.02939765714108944\n",
      "Epoch 3, Batch: 106: Training Loss: 0.029172755777835846, Validation Loss: 0.030852438881993294\n",
      "Epoch 3, Batch: 107: Training Loss: 0.03335731476545334, Validation Loss: 0.03493019938468933\n",
      "Epoch 3, Batch: 108: Training Loss: 0.03133775293827057, Validation Loss: 0.03206142038106918\n",
      "Epoch 3, Batch: 109: Training Loss: 0.034510333091020584, Validation Loss: 0.029745936393737793\n",
      "Epoch 3, Batch: 110: Training Loss: 0.03713414818048477, Validation Loss: 0.028535334393382072\n",
      "Epoch 3, Batch: 111: Training Loss: 0.030535396188497543, Validation Loss: 0.031054843217134476\n",
      "Epoch 3, Batch: 112: Training Loss: 0.027399631217122078, Validation Loss: 0.031142668798565865\n",
      "Epoch 3, Batch: 113: Training Loss: 0.02755732834339142, Validation Loss: 0.03283720090985298\n",
      "Epoch 3, Batch: 114: Training Loss: 0.030337097123265266, Validation Loss: 0.03185759484767914\n",
      "Epoch 3, Batch: 115: Training Loss: 0.03395850211381912, Validation Loss: 0.03006957843899727\n",
      "Epoch 3, Batch: 116: Training Loss: 0.025847941637039185, Validation Loss: 0.030297115445137024\n",
      "Epoch 3, Batch: 117: Training Loss: 0.031283095479011536, Validation Loss: 0.030290236696600914\n",
      "Epoch 3, Batch: 118: Training Loss: 0.02695385552942753, Validation Loss: 0.03207467496395111\n",
      "Epoch 3, Batch: 119: Training Loss: 0.03658991679549217, Validation Loss: 0.029232043772935867\n",
      "Epoch 3, Batch: 120: Training Loss: 0.024853724986314774, Validation Loss: 0.028185924515128136\n",
      "Epoch 3, Batch: 121: Training Loss: 0.03146723657846451, Validation Loss: 0.02901434525847435\n",
      "Epoch 3, Batch: 122: Training Loss: 0.029596511274576187, Validation Loss: 0.026179607957601547\n",
      "Epoch 3, Batch: 123: Training Loss: 0.03253903239965439, Validation Loss: 0.029796672984957695\n",
      "Epoch 3, Batch: 124: Training Loss: 0.0290029626339674, Validation Loss: 0.02969544008374214\n",
      "Epoch 3, Batch: 125: Training Loss: 0.026212437078356743, Validation Loss: 0.029613452032208443\n",
      "Epoch 3, Batch: 126: Training Loss: 0.027444375678896904, Validation Loss: 0.03258383274078369\n",
      "Epoch 3, Batch: 127: Training Loss: 0.030613405629992485, Validation Loss: 0.03238016366958618\n",
      "Epoch 3, Batch: 128: Training Loss: 0.03333016112446785, Validation Loss: 0.03135702386498451\n",
      "Epoch 3, Batch: 129: Training Loss: 0.02656167559325695, Validation Loss: 0.030469045042991638\n",
      "Epoch 3, Batch: 130: Training Loss: 0.03142876550555229, Validation Loss: 0.03248217701911926\n",
      "Epoch 3, Batch: 131: Training Loss: 0.030986595898866653, Validation Loss: 0.03146255016326904\n",
      "Epoch 3, Batch: 132: Training Loss: 0.033939067274332047, Validation Loss: 0.031079106032848358\n",
      "Epoch 3, Batch: 133: Training Loss: 0.037045668810606, Validation Loss: 0.030523942783474922\n",
      "Epoch 3, Batch: 134: Training Loss: 0.034594081342220306, Validation Loss: 0.03102361038327217\n",
      "Epoch 3, Batch: 135: Training Loss: 0.03171534836292267, Validation Loss: 0.03352557122707367\n",
      "Epoch 3, Batch: 136: Training Loss: 0.030480675399303436, Validation Loss: 0.03157299757003784\n",
      "Epoch 3, Batch: 137: Training Loss: 0.030634967610239983, Validation Loss: 0.03247788920998573\n",
      "Epoch 3, Batch: 138: Training Loss: 0.033600494265556335, Validation Loss: 0.03429737314581871\n",
      "Epoch 3, Batch: 139: Training Loss: 0.02787790820002556, Validation Loss: 0.031130734831094742\n",
      "Epoch 3, Batch: 140: Training Loss: 0.03698418661952019, Validation Loss: 0.0312805250287056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 141: Training Loss: 0.0323614776134491, Validation Loss: 0.029856186360120773\n",
      "Epoch 3, Batch: 142: Training Loss: 0.03158644959330559, Validation Loss: 0.027801210060715675\n",
      "Epoch 3, Batch: 143: Training Loss: 0.030953682959079742, Validation Loss: 0.02972385659813881\n",
      "Epoch 3, Batch: 144: Training Loss: 0.0287286639213562, Validation Loss: 0.028829745948314667\n",
      "Epoch 3, Batch: 145: Training Loss: 0.030721094459295273, Validation Loss: 0.02986104227602482\n",
      "Epoch 3, Batch: 146: Training Loss: 0.03226313367486, Validation Loss: 0.02938194014132023\n",
      "Epoch 3, Batch: 147: Training Loss: 0.0312609001994133, Validation Loss: 0.029823698103427887\n",
      "Epoch 3, Batch: 148: Training Loss: 0.03287377208471298, Validation Loss: 0.03166084364056587\n",
      "Epoch 3, Batch: 149: Training Loss: 0.034353017807006836, Validation Loss: 0.029612403362989426\n",
      "Epoch 3, Batch: 150: Training Loss: 0.03095877170562744, Validation Loss: 0.02684415876865387\n",
      "Epoch 3, Batch: 151: Training Loss: 0.027385704219341278, Validation Loss: 0.030054284259676933\n",
      "Epoch 3, Batch: 152: Training Loss: 0.031241528689861298, Validation Loss: 0.028913244605064392\n",
      "Epoch 3, Batch: 153: Training Loss: 0.03633926063776016, Validation Loss: 0.02844485640525818\n",
      "Epoch 3, Batch: 154: Training Loss: 0.029383057728409767, Validation Loss: 0.027515988796949387\n",
      "Epoch 3, Batch: 155: Training Loss: 0.03732779994606972, Validation Loss: 0.027292221784591675\n",
      "Epoch 3, Batch: 156: Training Loss: 0.02873733825981617, Validation Loss: 0.029401972889900208\n",
      "Epoch 3, Batch: 157: Training Loss: 0.027613922953605652, Validation Loss: 0.028384096920490265\n",
      "Epoch 3, Batch: 158: Training Loss: 0.030719200149178505, Validation Loss: 0.030386770144104958\n",
      "Epoch 3, Batch: 159: Training Loss: 0.029911119490861893, Validation Loss: 0.026987474411725998\n",
      "Epoch 3, Batch: 160: Training Loss: 0.029214588925242424, Validation Loss: 0.030400121584534645\n",
      "Epoch 3, Batch: 161: Training Loss: 0.028346462175250053, Validation Loss: 0.02753426507115364\n",
      "Epoch 3, Batch: 162: Training Loss: 0.032916754484176636, Validation Loss: 0.027973022311925888\n",
      "Epoch 3, Batch: 163: Training Loss: 0.02773429825901985, Validation Loss: 0.02908596582710743\n",
      "Epoch 3, Batch: 164: Training Loss: 0.026710281148552895, Validation Loss: 0.028952032327651978\n",
      "Epoch 3, Batch: 165: Training Loss: 0.032648880034685135, Validation Loss: 0.03122289851307869\n",
      "Epoch 3, Batch: 166: Training Loss: 0.029748836532235146, Validation Loss: 0.03186144307255745\n",
      "Epoch 3, Batch: 167: Training Loss: 0.026413166895508766, Validation Loss: 0.030879752710461617\n",
      "Epoch 3, Batch: 168: Training Loss: 0.02804049663245678, Validation Loss: 0.031027937307953835\n",
      "Epoch 3, Batch: 169: Training Loss: 0.03184862062335014, Validation Loss: 0.030900225043296814\n",
      "Epoch 3, Batch: 170: Training Loss: 0.031673308461904526, Validation Loss: 0.03329741582274437\n",
      "Epoch 3, Batch: 171: Training Loss: 0.026973221451044083, Validation Loss: 0.03273153305053711\n",
      "Epoch 3, Batch: 172: Training Loss: 0.030498076230287552, Validation Loss: 0.030826333910226822\n",
      "Epoch 3, Batch: 173: Training Loss: 0.028079340234398842, Validation Loss: 0.0310975294560194\n",
      "Epoch 3, Batch: 174: Training Loss: 0.03349199891090393, Validation Loss: 0.030014023184776306\n",
      "Epoch 3, Batch: 175: Training Loss: 0.028562594205141068, Validation Loss: 0.029901109635829926\n",
      "Epoch 3, Batch: 176: Training Loss: 0.03296833112835884, Validation Loss: 0.029529232531785965\n",
      "Epoch 3, Batch: 177: Training Loss: 0.027348676696419716, Validation Loss: 0.030513275414705276\n",
      "Epoch 3, Batch: 178: Training Loss: 0.033920060843229294, Validation Loss: 0.028316380456089973\n",
      "Epoch 3, Batch: 179: Training Loss: 0.030375655740499496, Validation Loss: 0.02914522960782051\n",
      "Epoch 3, Batch: 180: Training Loss: 0.02890460006892681, Validation Loss: 0.03046671487390995\n",
      "Epoch 3, Batch: 181: Training Loss: 0.036866676062345505, Validation Loss: 0.030625242739915848\n",
      "Epoch 3, Batch: 182: Training Loss: 0.03269914537668228, Validation Loss: 0.030071968212723732\n",
      "Epoch 3, Batch: 183: Training Loss: 0.030910251662135124, Validation Loss: 0.0278034508228302\n",
      "Epoch 3, Batch: 184: Training Loss: 0.030854275450110435, Validation Loss: 0.028131578117609024\n",
      "Epoch 3, Batch: 185: Training Loss: 0.03427601978182793, Validation Loss: 0.028229819610714912\n",
      "Epoch 3, Batch: 186: Training Loss: 0.028822876513004303, Validation Loss: 0.028026163578033447\n",
      "Epoch 3, Batch: 187: Training Loss: 0.03177711367607117, Validation Loss: 0.031795646995306015\n",
      "Epoch 3, Batch: 188: Training Loss: 0.032273486256599426, Validation Loss: 0.02874710224568844\n",
      "Epoch 3, Batch: 189: Training Loss: 0.02567538060247898, Validation Loss: 0.026086946949362755\n",
      "Epoch 3, Batch: 190: Training Loss: 0.031609468162059784, Validation Loss: 0.026956653222441673\n",
      "Epoch 3, Batch: 191: Training Loss: 0.02689279615879059, Validation Loss: 0.02805601991713047\n",
      "Epoch 3, Batch: 192: Training Loss: 0.028180742636322975, Validation Loss: 0.025932030752301216\n",
      "Epoch 3, Batch: 193: Training Loss: 0.028548894450068474, Validation Loss: 0.02627411112189293\n",
      "Epoch 3, Batch: 194: Training Loss: 0.02874814346432686, Validation Loss: 0.025176478549838066\n",
      "Epoch 3, Batch: 195: Training Loss: 0.03078961931169033, Validation Loss: 0.029651930555701256\n",
      "Epoch 3, Batch: 196: Training Loss: 0.035260919481515884, Validation Loss: 0.028439559042453766\n",
      "Epoch 3, Batch: 197: Training Loss: 0.03157668560743332, Validation Loss: 0.02990761399269104\n",
      "Epoch 3, Batch: 198: Training Loss: 0.030895384028553963, Validation Loss: 0.030872691422700882\n",
      "Epoch 3, Batch: 199: Training Loss: 0.027364030480384827, Validation Loss: 0.029464036226272583\n",
      "Epoch 3, Batch: 200: Training Loss: 0.024563077837228775, Validation Loss: 0.0276318546384573\n",
      "Epoch 3, Batch: 201: Training Loss: 0.034106869250535965, Validation Loss: 0.030198292806744576\n",
      "Epoch 3, Batch: 202: Training Loss: 0.03408744931221008, Validation Loss: 0.030833154916763306\n",
      "Epoch 3, Batch: 203: Training Loss: 0.03376347944140434, Validation Loss: 0.027947038412094116\n",
      "Epoch 3, Batch: 204: Training Loss: 0.030964482575654984, Validation Loss: 0.032146867364645004\n",
      "Epoch 3, Batch: 205: Training Loss: 0.034793920814991, Validation Loss: 0.028292298316955566\n",
      "Epoch 3, Batch: 206: Training Loss: 0.03039461188018322, Validation Loss: 0.029134362936019897\n",
      "Epoch 3, Batch: 207: Training Loss: 0.03135400265455246, Validation Loss: 0.030970342457294464\n",
      "Epoch 3, Batch: 208: Training Loss: 0.03178713098168373, Validation Loss: 0.030091162770986557\n",
      "Epoch 3, Batch: 209: Training Loss: 0.03180975466966629, Validation Loss: 0.02924298867583275\n",
      "Epoch 3, Batch: 210: Training Loss: 0.032048117369413376, Validation Loss: 0.03132982552051544\n",
      "Epoch 3, Batch: 211: Training Loss: 0.031740255653858185, Validation Loss: 0.02859354205429554\n",
      "Epoch 3, Batch: 212: Training Loss: 0.030731452628970146, Validation Loss: 0.03008751943707466\n",
      "Epoch 3, Batch: 213: Training Loss: 0.03184967860579491, Validation Loss: 0.028766609728336334\n",
      "Epoch 3, Batch: 214: Training Loss: 0.03138739988207817, Validation Loss: 0.029684992507100105\n",
      "Epoch 3, Batch: 215: Training Loss: 0.0337919220328331, Validation Loss: 0.027449743822216988\n",
      "Epoch 3, Batch: 216: Training Loss: 0.031687214970588684, Validation Loss: 0.02730460651218891\n",
      "Epoch 3, Batch: 217: Training Loss: 0.028971804305911064, Validation Loss: 0.028254322707653046\n",
      "Epoch 3, Batch: 218: Training Loss: 0.030732333660125732, Validation Loss: 0.030591439455747604\n",
      "Epoch 3, Batch: 219: Training Loss: 0.032010674476623535, Validation Loss: 0.030495572835206985\n",
      "Epoch 3, Batch: 220: Training Loss: 0.03411082178354263, Validation Loss: 0.03197763115167618\n",
      "Epoch 3, Batch: 221: Training Loss: 0.030762553215026855, Validation Loss: 0.031206456944346428\n",
      "Epoch 3, Batch: 222: Training Loss: 0.033988915383815765, Validation Loss: 0.029647251591086388\n",
      "Epoch 3, Batch: 223: Training Loss: 0.03206288442015648, Validation Loss: 0.02780010923743248\n",
      "Epoch 3, Batch: 224: Training Loss: 0.029073724523186684, Validation Loss: 0.02856554090976715\n",
      "Epoch 3, Batch: 225: Training Loss: 0.03087366186082363, Validation Loss: 0.030415039509534836\n",
      "Epoch 3, Batch: 226: Training Loss: 0.030039189383387566, Validation Loss: 0.03174850344657898\n",
      "Epoch 3, Batch: 227: Training Loss: 0.03674644976854324, Validation Loss: 0.02969454973936081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 228: Training Loss: 0.029805447906255722, Validation Loss: 0.03259766474366188\n",
      "Epoch 3, Batch: 229: Training Loss: 0.031579334288835526, Validation Loss: 0.031243309378623962\n",
      "Epoch 3, Batch: 230: Training Loss: 0.03113754838705063, Validation Loss: 0.03194791451096535\n",
      "Epoch 3, Batch: 231: Training Loss: 0.03454628959298134, Validation Loss: 0.031209707260131836\n",
      "Epoch 3, Batch: 232: Training Loss: 0.03674931824207306, Validation Loss: 0.029271502047777176\n",
      "Epoch 3, Batch: 233: Training Loss: 0.029973307624459267, Validation Loss: 0.0288410447537899\n",
      "Epoch 3, Batch: 234: Training Loss: 0.03303752839565277, Validation Loss: 0.03163343295454979\n",
      "Epoch 3, Batch: 235: Training Loss: 0.036984581500291824, Validation Loss: 0.0292553398758173\n",
      "Epoch 3, Batch: 236: Training Loss: 0.03501070663332939, Validation Loss: 0.028688298538327217\n",
      "Epoch 3, Batch: 237: Training Loss: 0.02932756580412388, Validation Loss: 0.028685782104730606\n",
      "Epoch 3, Batch: 238: Training Loss: 0.035804633051157, Validation Loss: 0.032468076795339584\n",
      "Epoch 3, Batch: 239: Training Loss: 0.030038533732295036, Validation Loss: 0.028906570747494698\n",
      "Epoch 3, Batch: 240: Training Loss: 0.024910584092140198, Validation Loss: 0.02709711715579033\n",
      "Epoch 3, Batch: 241: Training Loss: 0.029959505423903465, Validation Loss: 0.029369067400693893\n",
      "Epoch 3, Batch: 242: Training Loss: 0.03211439028382301, Validation Loss: 0.0295096542686224\n",
      "Epoch 3, Batch: 243: Training Loss: 0.036464858800172806, Validation Loss: 0.027398187667131424\n",
      "Epoch 3, Batch: 244: Training Loss: 0.02984560653567314, Validation Loss: 0.02809583954513073\n",
      "Epoch 3, Batch: 245: Training Loss: 0.0345601961016655, Validation Loss: 0.03137676417827606\n",
      "Epoch 3, Batch: 246: Training Loss: 0.027846021577715874, Validation Loss: 0.030754640698432922\n",
      "Epoch 3, Batch: 247: Training Loss: 0.0322217121720314, Validation Loss: 0.028083013370633125\n",
      "Epoch 3, Batch: 248: Training Loss: 0.029828131198883057, Validation Loss: 0.029457127675414085\n",
      "Epoch 3, Batch: 249: Training Loss: 0.032563745975494385, Validation Loss: 0.031003136187791824\n",
      "Epoch 3, Batch: 250: Training Loss: 0.03390384092926979, Validation Loss: 0.030632978305220604\n",
      "Epoch 3, Batch: 251: Training Loss: 0.029127992689609528, Validation Loss: 0.030924612656235695\n",
      "Epoch 3, Batch: 252: Training Loss: 0.02899034507572651, Validation Loss: 0.030756615102291107\n",
      "Epoch 3, Batch: 253: Training Loss: 0.036022983491420746, Validation Loss: 0.02916722185909748\n",
      "Epoch 3, Batch: 254: Training Loss: 0.027264047414064407, Validation Loss: 0.028541935607790947\n",
      "Epoch 3, Batch: 255: Training Loss: 0.02678154781460762, Validation Loss: 0.028163159266114235\n",
      "Epoch 3, Batch: 256: Training Loss: 0.03241443261504173, Validation Loss: 0.03150162473320961\n",
      "Epoch 3, Batch: 257: Training Loss: 0.035790614783763885, Validation Loss: 0.03184949606657028\n",
      "Epoch 3, Batch: 258: Training Loss: 0.02967633120715618, Validation Loss: 0.030003324151039124\n",
      "Epoch 3, Batch: 259: Training Loss: 0.027556832879781723, Validation Loss: 0.028922483325004578\n",
      "Epoch 3, Batch: 260: Training Loss: 0.03589075431227684, Validation Loss: 0.028149135410785675\n",
      "Epoch 3, Batch: 261: Training Loss: 0.03234211727976799, Validation Loss: 0.029423993080854416\n",
      "Epoch 3, Batch: 262: Training Loss: 0.029049478471279144, Validation Loss: 0.02677127532660961\n",
      "Epoch 3, Batch: 263: Training Loss: 0.030773740261793137, Validation Loss: 0.028873011469841003\n",
      "Epoch 3, Batch: 264: Training Loss: 0.027414465323090553, Validation Loss: 0.028886372223496437\n",
      "Epoch 3, Batch: 265: Training Loss: 0.03192140907049179, Validation Loss: 0.02981114573776722\n",
      "Epoch 3, Batch: 266: Training Loss: 0.030921127647161484, Validation Loss: 0.027869248762726784\n",
      "Epoch 3, Batch: 267: Training Loss: 0.03247855603694916, Validation Loss: 0.030410919338464737\n",
      "Epoch 3, Batch: 268: Training Loss: 0.03215179592370987, Validation Loss: 0.03020629659295082\n",
      "Epoch 3, Batch: 269: Training Loss: 0.03045433945953846, Validation Loss: 0.029869165271520615\n",
      "Epoch 3, Batch: 270: Training Loss: 0.028137024492025375, Validation Loss: 0.030039889737963676\n",
      "Epoch 3, Batch: 271: Training Loss: 0.029232803732156754, Validation Loss: 0.029949460178613663\n",
      "Epoch 3, Batch: 272: Training Loss: 0.028556635603308678, Validation Loss: 0.027402294799685478\n",
      "Epoch 3, Batch: 273: Training Loss: 0.027795961126685143, Validation Loss: 0.029243310913443565\n",
      "Epoch 3, Batch: 274: Training Loss: 0.030911095440387726, Validation Loss: 0.02953270822763443\n",
      "Epoch 3, Batch: 275: Training Loss: 0.028671137988567352, Validation Loss: 0.03050350397825241\n",
      "Epoch 3, Batch: 276: Training Loss: 0.028381559997797012, Validation Loss: 0.029801808297634125\n",
      "Epoch 3, Batch: 277: Training Loss: 0.028913917019963264, Validation Loss: 0.02703736163675785\n",
      "Epoch 3, Batch: 278: Training Loss: 0.02973264455795288, Validation Loss: 0.0292014479637146\n",
      "Epoch 3, Batch: 279: Training Loss: 0.030510136857628822, Validation Loss: 0.027440445497632027\n",
      "Epoch 3, Batch: 280: Training Loss: 0.03140619397163391, Validation Loss: 0.03007870726287365\n",
      "Epoch 3, Batch: 281: Training Loss: 0.03463026508688927, Validation Loss: 0.02954232506453991\n",
      "Epoch 3, Batch: 282: Training Loss: 0.0319981724023819, Validation Loss: 0.02931210771203041\n",
      "Epoch 3, Batch: 283: Training Loss: 0.030234444886446, Validation Loss: 0.029983682557940483\n",
      "Epoch 3, Batch: 284: Training Loss: 0.03069467283785343, Validation Loss: 0.02997119538486004\n",
      "Epoch 3, Batch: 285: Training Loss: 0.031087661162018776, Validation Loss: 0.028879420831799507\n",
      "Epoch 3, Batch: 286: Training Loss: 0.02880607172846794, Validation Loss: 0.03217439725995064\n",
      "Epoch 3, Batch: 287: Training Loss: 0.031078793108463287, Validation Loss: 0.0315387137234211\n",
      "Epoch 3, Batch: 288: Training Loss: 0.0321221686899662, Validation Loss: 0.03083849512040615\n",
      "Epoch 3, Batch: 289: Training Loss: 0.032606735825538635, Validation Loss: 0.030420565977692604\n",
      "Epoch 3, Batch: 290: Training Loss: 0.03164290636777878, Validation Loss: 0.03039509244263172\n",
      "Epoch 3, Batch: 291: Training Loss: 0.030231645330786705, Validation Loss: 0.030097410082817078\n",
      "Epoch 3, Batch: 292: Training Loss: 0.031212111935019493, Validation Loss: 0.029882289469242096\n",
      "Epoch 3, Batch: 293: Training Loss: 0.0322195328772068, Validation Loss: 0.029888072982430458\n",
      "Epoch 3, Batch: 294: Training Loss: 0.03234175965189934, Validation Loss: 0.029705248773097992\n",
      "Epoch 3, Batch: 295: Training Loss: 0.0298334788531065, Validation Loss: 0.030117537826299667\n",
      "Epoch 3, Batch: 296: Training Loss: 0.02824852056801319, Validation Loss: 0.028090063482522964\n",
      "Epoch 3, Batch: 297: Training Loss: 0.030045954510569572, Validation Loss: 0.02822304144501686\n",
      "Epoch 3, Batch: 298: Training Loss: 0.032695453613996506, Validation Loss: 0.03004854917526245\n",
      "Epoch 3, Batch: 299: Training Loss: 0.030166946351528168, Validation Loss: 0.02794775739312172\n",
      "Epoch 3, Batch: 300: Training Loss: 0.03011276200413704, Validation Loss: 0.028678348287940025\n",
      "Epoch 3, Batch: 301: Training Loss: 0.031206030398607254, Validation Loss: 0.026306388899683952\n",
      "Epoch 3, Batch: 302: Training Loss: 0.028175266459584236, Validation Loss: 0.030783191323280334\n",
      "Epoch 3, Batch: 303: Training Loss: 0.028792623430490494, Validation Loss: 0.027930615469813347\n",
      "Epoch 3, Batch: 304: Training Loss: 0.03119107149541378, Validation Loss: 0.025995926931500435\n",
      "Epoch 3, Batch: 305: Training Loss: 0.029683858156204224, Validation Loss: 0.027537507936358452\n",
      "Epoch 3, Batch: 306: Training Loss: 0.03225649893283844, Validation Loss: 0.028277797624468803\n",
      "Epoch 3, Batch: 307: Training Loss: 0.027103092521429062, Validation Loss: 0.029048705473542213\n",
      "Epoch 3, Batch: 308: Training Loss: 0.0303301140666008, Validation Loss: 0.028263676911592484\n",
      "Epoch 3, Batch: 309: Training Loss: 0.029146187007427216, Validation Loss: 0.030949145555496216\n",
      "Epoch 3, Batch: 310: Training Loss: 0.028622914105653763, Validation Loss: 0.02781156823039055\n",
      "Epoch 3, Batch: 311: Training Loss: 0.029082298278808594, Validation Loss: 0.02579093538224697\n",
      "Epoch 3, Batch: 312: Training Loss: 0.03237657994031906, Validation Loss: 0.02562309242784977\n",
      "Epoch 3, Batch: 313: Training Loss: 0.023344965651631355, Validation Loss: 0.02729741483926773\n",
      "Epoch 3, Batch: 314: Training Loss: 0.028160618618130684, Validation Loss: 0.02775014005601406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 315: Training Loss: 0.03282851353287697, Validation Loss: 0.02907259203493595\n",
      "Epoch 3, Batch: 316: Training Loss: 0.029652392491698265, Validation Loss: 0.02831961028277874\n",
      "Epoch 3, Batch: 317: Training Loss: 0.029124515131115913, Validation Loss: 0.0267013106495142\n",
      "Epoch 3, Batch: 318: Training Loss: 0.02884874865412712, Validation Loss: 0.027137335389852524\n",
      "Saving new best model w/ loss: 0.024705709889531136\n",
      "Epoch 3, Batch: 319: Training Loss: 0.03009830415248871, Validation Loss: 0.024705709889531136\n",
      "Epoch 3, Batch: 320: Training Loss: 0.02930022031068802, Validation Loss: 0.025746064260601997\n",
      "Epoch 3, Batch: 321: Training Loss: 0.02539614960551262, Validation Loss: 0.026482827961444855\n",
      "Epoch 3, Batch: 322: Training Loss: 0.02750435099005699, Validation Loss: 0.02512049861252308\n",
      "Epoch 3, Batch: 323: Training Loss: 0.028998691588640213, Validation Loss: 0.02570934407413006\n",
      "Epoch 3, Batch: 324: Training Loss: 0.03387410193681717, Validation Loss: 0.02616060897707939\n",
      "Epoch 3, Batch: 325: Training Loss: 0.024832691997289658, Validation Loss: 0.02822752483189106\n",
      "Epoch 3, Batch: 326: Training Loss: 0.032298047095537186, Validation Loss: 0.025820055976510048\n",
      "Epoch 3, Batch: 327: Training Loss: 0.02750479243695736, Validation Loss: 0.028781769797205925\n",
      "Epoch 3, Batch: 328: Training Loss: 0.03355199098587036, Validation Loss: 0.028366519138216972\n",
      "Epoch 3, Batch: 329: Training Loss: 0.03130852431058884, Validation Loss: 0.029531165957450867\n",
      "Epoch 3, Batch: 330: Training Loss: 0.0315057747066021, Validation Loss: 0.027177484706044197\n",
      "Epoch 3, Batch: 331: Training Loss: 0.029726069420576096, Validation Loss: 0.02673967368900776\n",
      "Epoch 3, Batch: 332: Training Loss: 0.026900671422481537, Validation Loss: 0.028067823499441147\n",
      "Epoch 3, Batch: 333: Training Loss: 0.02930617146193981, Validation Loss: 0.02912888303399086\n",
      "Epoch 3, Batch: 334: Training Loss: 0.032698482275009155, Validation Loss: 0.026087699458003044\n",
      "Epoch 3, Batch: 335: Training Loss: 0.031594473868608475, Validation Loss: 0.02930910512804985\n",
      "Epoch 3, Batch: 336: Training Loss: 0.027139605954289436, Validation Loss: 0.027507785707712173\n",
      "Epoch 3, Batch: 337: Training Loss: 0.02927212417125702, Validation Loss: 0.02664606273174286\n",
      "Epoch 3, Batch: 338: Training Loss: 0.03283112868666649, Validation Loss: 0.028800442814826965\n",
      "Epoch 3, Batch: 339: Training Loss: 0.02813638001680374, Validation Loss: 0.03019692935049534\n",
      "Epoch 3, Batch: 340: Training Loss: 0.030210448428988457, Validation Loss: 0.028902586549520493\n",
      "Epoch 3, Batch: 341: Training Loss: 0.030477257445454597, Validation Loss: 0.02737310342490673\n",
      "Epoch 3, Batch: 342: Training Loss: 0.03128397464752197, Validation Loss: 0.027284810319542885\n",
      "Epoch 3, Batch: 343: Training Loss: 0.030649086460471153, Validation Loss: 0.028322920203208923\n",
      "Epoch 3, Batch: 344: Training Loss: 0.03261558338999748, Validation Loss: 0.02970089390873909\n",
      "Epoch 3, Batch: 345: Training Loss: 0.028606167063117027, Validation Loss: 0.02758876420557499\n",
      "Epoch 3, Batch: 346: Training Loss: 0.03569977730512619, Validation Loss: 0.026040539145469666\n",
      "Epoch 3, Batch: 347: Training Loss: 0.028404295444488525, Validation Loss: 0.029093194752931595\n",
      "Epoch 3, Batch: 348: Training Loss: 0.03486623242497444, Validation Loss: 0.0289560928940773\n",
      "Epoch 3, Batch: 349: Training Loss: 0.02992311865091324, Validation Loss: 0.029761996120214462\n",
      "Epoch 3, Batch: 350: Training Loss: 0.028996091336011887, Validation Loss: 0.02700565569102764\n",
      "Epoch 3, Batch: 351: Training Loss: 0.03211859613656998, Validation Loss: 0.026340285316109657\n",
      "Epoch 3, Batch: 352: Training Loss: 0.032895006239414215, Validation Loss: 0.027591168880462646\n",
      "Epoch 3, Batch: 353: Training Loss: 0.034272681921720505, Validation Loss: 0.02867523953318596\n",
      "Epoch 3, Batch: 354: Training Loss: 0.029862303286790848, Validation Loss: 0.026423972100019455\n",
      "Epoch 3, Batch: 355: Training Loss: 0.02616281621158123, Validation Loss: 0.026828426867723465\n",
      "Epoch 3, Batch: 356: Training Loss: 0.02880474179983139, Validation Loss: 0.0278348159044981\n",
      "Saving new best model w/ loss: 0.022911958396434784\n",
      "Epoch 3, Batch: 357: Training Loss: 0.02634933404624462, Validation Loss: 0.022911958396434784\n",
      "Epoch 3, Batch: 358: Training Loss: 0.03041180968284607, Validation Loss: 0.026340875774621964\n",
      "Epoch 3, Batch: 359: Training Loss: 0.03241512179374695, Validation Loss: 0.026118595153093338\n",
      "Epoch 3, Batch: 360: Training Loss: 0.03316528722643852, Validation Loss: 0.028864119201898575\n",
      "Epoch 3, Batch: 361: Training Loss: 0.030096134170889854, Validation Loss: 0.028844280168414116\n",
      "Epoch 3, Batch: 362: Training Loss: 0.03038221038877964, Validation Loss: 0.02765873447060585\n",
      "Epoch 3, Batch: 363: Training Loss: 0.03144153580069542, Validation Loss: 0.028179427608847618\n",
      "Epoch 3, Batch: 364: Training Loss: 0.028463928028941154, Validation Loss: 0.02828487940132618\n",
      "Epoch 3, Batch: 365: Training Loss: 0.03205891698598862, Validation Loss: 0.028507741168141365\n",
      "Epoch 3, Batch: 366: Training Loss: 0.033739618957042694, Validation Loss: 0.026415320113301277\n",
      "Epoch 3, Batch: 367: Training Loss: 0.025913218036293983, Validation Loss: 0.027512365952134132\n",
      "Epoch 3, Batch: 368: Training Loss: 0.03393404558300972, Validation Loss: 0.03034471906721592\n",
      "Epoch 3, Batch: 369: Training Loss: 0.02709820494055748, Validation Loss: 0.028429768979549408\n",
      "Epoch 3, Batch: 370: Training Loss: 0.030152084305882454, Validation Loss: 0.02638765051960945\n",
      "Epoch 3, Batch: 371: Training Loss: 0.02842174470424652, Validation Loss: 0.02674664743244648\n",
      "Epoch 3, Batch: 372: Training Loss: 0.02959810011088848, Validation Loss: 0.02848019450902939\n",
      "Epoch 3, Batch: 373: Training Loss: 0.035448119044303894, Validation Loss: 0.02773934043943882\n",
      "Epoch 3, Batch: 374: Training Loss: 0.029325926676392555, Validation Loss: 0.030525073409080505\n",
      "Epoch 3, Batch: 375: Training Loss: 0.033414579927921295, Validation Loss: 0.03130890429019928\n",
      "Epoch 3, Batch: 376: Training Loss: 0.028567207977175713, Validation Loss: 0.028248831629753113\n",
      "Epoch 3, Batch: 377: Training Loss: 0.032303180545568466, Validation Loss: 0.029148707166314125\n",
      "Epoch 3, Batch: 378: Training Loss: 0.0316099114716053, Validation Loss: 0.025793157517910004\n",
      "Epoch 3, Batch: 379: Training Loss: 0.02937105856835842, Validation Loss: 0.026796549558639526\n",
      "Epoch 3, Batch: 380: Training Loss: 0.03607717156410217, Validation Loss: 0.029586531221866608\n",
      "Epoch 3, Batch: 381: Training Loss: 0.03411019593477249, Validation Loss: 0.027011409401893616\n",
      "Epoch 3, Batch: 382: Training Loss: 0.03329719975590706, Validation Loss: 0.030372517183423042\n",
      "Epoch 3, Batch: 383: Training Loss: 0.03303978592157364, Validation Loss: 0.028481684625148773\n",
      "Epoch 3, Batch: 384: Training Loss: 0.026490293443202972, Validation Loss: 0.027757061645388603\n",
      "Epoch 3, Batch: 385: Training Loss: 0.030186370015144348, Validation Loss: 0.028503118082880974\n",
      "Epoch 3, Batch: 386: Training Loss: 0.02521776594221592, Validation Loss: 0.028959883376955986\n",
      "Epoch 3, Batch: 387: Training Loss: 0.02918320894241333, Validation Loss: 0.026431985199451447\n",
      "Epoch 3, Batch: 388: Training Loss: 0.02778351493179798, Validation Loss: 0.028697410598397255\n",
      "Epoch 3, Batch: 389: Training Loss: 0.0317913293838501, Validation Loss: 0.027821648865938187\n",
      "Epoch 3, Batch: 390: Training Loss: 0.030157484114170074, Validation Loss: 0.026867806911468506\n",
      "Epoch 3, Batch: 391: Training Loss: 0.03386847302317619, Validation Loss: 0.028803378343582153\n",
      "Epoch 3, Batch: 392: Training Loss: 0.03034544549882412, Validation Loss: 0.029215900227427483\n",
      "Epoch 3, Batch: 393: Training Loss: 0.0302865132689476, Validation Loss: 0.02964925207197666\n",
      "Epoch 3, Batch: 394: Training Loss: 0.02853665128350258, Validation Loss: 0.031657639890909195\n",
      "Epoch 3, Batch: 395: Training Loss: 0.028798995539546013, Validation Loss: 0.029444139450788498\n",
      "Epoch 3, Batch: 396: Training Loss: 0.028275417163968086, Validation Loss: 0.030335135757923126\n",
      "Epoch 3, Batch: 397: Training Loss: 0.029641231521964073, Validation Loss: 0.03217663615942001\n",
      "Epoch 3, Batch: 398: Training Loss: 0.028074827045202255, Validation Loss: 0.03202559053897858\n",
      "Epoch 3, Batch: 399: Training Loss: 0.031036674976348877, Validation Loss: 0.027697928249835968\n",
      "Epoch 3, Batch: 400: Training Loss: 0.03210833668708801, Validation Loss: 0.026145553216338158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 401: Training Loss: 0.027536330744624138, Validation Loss: 0.029584087431430817\n",
      "Epoch 3, Batch: 402: Training Loss: 0.02616121619939804, Validation Loss: 0.029922788962721825\n",
      "Epoch 3, Batch: 403: Training Loss: 0.03535046800971031, Validation Loss: 0.028927944600582123\n",
      "Epoch 3, Batch: 404: Training Loss: 0.029867876321077347, Validation Loss: 0.02958986721932888\n",
      "Epoch 3, Batch: 405: Training Loss: 0.026961328461766243, Validation Loss: 0.030494943261146545\n",
      "Epoch 3, Batch: 406: Training Loss: 0.03177203983068466, Validation Loss: 0.030328402295708656\n",
      "Epoch 3, Batch: 407: Training Loss: 0.029448863118886948, Validation Loss: 0.02973652072250843\n",
      "Epoch 3, Batch: 408: Training Loss: 0.02680274285376072, Validation Loss: 0.030292095616459846\n",
      "Epoch 3, Batch: 409: Training Loss: 0.03432762622833252, Validation Loss: 0.024972403421998024\n",
      "Epoch 3, Batch: 410: Training Loss: 0.03307963162660599, Validation Loss: 0.03063473105430603\n",
      "Epoch 3, Batch: 411: Training Loss: 0.03011641837656498, Validation Loss: 0.02815948612987995\n",
      "Epoch 3, Batch: 412: Training Loss: 0.03400864079594612, Validation Loss: 0.028263607993721962\n",
      "Epoch 3, Batch: 413: Training Loss: 0.03259296342730522, Validation Loss: 0.02865605056285858\n",
      "Epoch 3, Batch: 414: Training Loss: 0.029338127002120018, Validation Loss: 0.029617467895150185\n",
      "Epoch 3, Batch: 415: Training Loss: 0.030494533479213715, Validation Loss: 0.028649650514125824\n",
      "Epoch 3, Batch: 416: Training Loss: 0.02875555120408535, Validation Loss: 0.026875844225287437\n",
      "Epoch 3, Batch: 417: Training Loss: 0.02816076949238777, Validation Loss: 0.028513610363006592\n",
      "Epoch 3, Batch: 418: Training Loss: 0.03008832037448883, Validation Loss: 0.027032846584916115\n",
      "Epoch 3, Batch: 419: Training Loss: 0.028098775073885918, Validation Loss: 0.023998023942112923\n",
      "Epoch 3, Batch: 420: Training Loss: 0.030166761949658394, Validation Loss: 0.02627601847052574\n",
      "Epoch 3, Batch: 421: Training Loss: 0.03706897422671318, Validation Loss: 0.027166904881596565\n",
      "Epoch 3, Batch: 422: Training Loss: 0.031765155494213104, Validation Loss: 0.027217699214816093\n",
      "Epoch 3, Batch: 423: Training Loss: 0.03498516604304314, Validation Loss: 0.02700580470263958\n",
      "Epoch 3, Batch: 424: Training Loss: 0.03225260600447655, Validation Loss: 0.026887984946370125\n",
      "Epoch 3, Batch: 425: Training Loss: 0.02900739759206772, Validation Loss: 0.028286518529057503\n",
      "Epoch 3, Batch: 426: Training Loss: 0.027248071506619453, Validation Loss: 0.026462195441126823\n",
      "Epoch 3, Batch: 427: Training Loss: 0.030259396880865097, Validation Loss: 0.02779313363134861\n",
      "Epoch 3, Batch: 428: Training Loss: 0.02868267148733139, Validation Loss: 0.025413753464818\n",
      "Epoch 3, Batch: 429: Training Loss: 0.027124734595417976, Validation Loss: 0.026339417323470116\n",
      "Epoch 3, Batch: 430: Training Loss: 0.025111133232712746, Validation Loss: 0.026557117700576782\n",
      "Epoch 3, Batch: 431: Training Loss: 0.03154377266764641, Validation Loss: 0.027842143550515175\n",
      "Epoch 3, Batch: 432: Training Loss: 0.03268354758620262, Validation Loss: 0.025902671739459038\n",
      "Epoch 3, Batch: 433: Training Loss: 0.029722949489951134, Validation Loss: 0.028149841353297234\n",
      "Epoch 3, Batch: 434: Training Loss: 0.031511690467596054, Validation Loss: 0.027568232268095016\n",
      "Epoch 3, Batch: 435: Training Loss: 0.0287628211081028, Validation Loss: 0.030020447447896004\n",
      "Epoch 3, Batch: 436: Training Loss: 0.026899755001068115, Validation Loss: 0.028934022411704063\n",
      "Epoch 3, Batch: 437: Training Loss: 0.024437524378299713, Validation Loss: 0.026786187663674355\n",
      "Epoch 3, Batch: 438: Training Loss: 0.030689137056469917, Validation Loss: 0.02575715258717537\n",
      "Epoch 3, Batch: 439: Training Loss: 0.02975134737789631, Validation Loss: 0.028433095663785934\n",
      "Epoch 3, Batch: 440: Training Loss: 0.03198837488889694, Validation Loss: 0.02636237069964409\n",
      "Epoch 3, Batch: 441: Training Loss: 0.03243983909487724, Validation Loss: 0.026376571506261826\n",
      "Epoch 3, Batch: 442: Training Loss: 0.029592635110020638, Validation Loss: 0.028020670637488365\n",
      "Epoch 3, Batch: 443: Training Loss: 0.029422558844089508, Validation Loss: 0.02882329560816288\n",
      "Epoch 3, Batch: 444: Training Loss: 0.028926119208335876, Validation Loss: 0.029872998595237732\n",
      "Epoch 3, Batch: 445: Training Loss: 0.027551326900720596, Validation Loss: 0.02872025966644287\n",
      "Epoch 3, Batch: 446: Training Loss: 0.031915757805109024, Validation Loss: 0.028076842427253723\n",
      "Epoch 3, Batch: 447: Training Loss: 0.031621646136045456, Validation Loss: 0.027014872059226036\n",
      "Epoch 3, Batch: 448: Training Loss: 0.02791353315114975, Validation Loss: 0.029820622876286507\n",
      "Epoch 3, Batch: 449: Training Loss: 0.0318814292550087, Validation Loss: 0.028683669865131378\n",
      "Epoch 3, Batch: 450: Training Loss: 0.02472388930618763, Validation Loss: 0.03307751193642616\n",
      "Epoch 3, Batch: 451: Training Loss: 0.031001223251223564, Validation Loss: 0.030366839841008186\n",
      "Epoch 3, Batch: 452: Training Loss: 0.03301888331770897, Validation Loss: 0.03068469651043415\n",
      "Epoch 3, Batch: 453: Training Loss: 0.030491817742586136, Validation Loss: 0.03058444894850254\n",
      "Epoch 3, Batch: 454: Training Loss: 0.029380187392234802, Validation Loss: 0.032077498733997345\n",
      "Epoch 3, Batch: 455: Training Loss: 0.03255833685398102, Validation Loss: 0.02909216284751892\n",
      "Epoch 3, Batch: 456: Training Loss: 0.02782878652215004, Validation Loss: 0.029618486762046814\n",
      "Epoch 3, Batch: 457: Training Loss: 0.02788902446627617, Validation Loss: 0.029168084263801575\n",
      "Epoch 3, Batch: 458: Training Loss: 0.02378597855567932, Validation Loss: 0.02794952504336834\n",
      "Epoch 3, Batch: 459: Training Loss: 0.031035462394356728, Validation Loss: 0.028783520683646202\n",
      "Epoch 3, Batch: 460: Training Loss: 0.03144228458404541, Validation Loss: 0.03025158867239952\n",
      "Epoch 3, Batch: 461: Training Loss: 0.02745196223258972, Validation Loss: 0.029705485329031944\n",
      "Epoch 3, Batch: 462: Training Loss: 0.028942039236426353, Validation Loss: 0.030610639601945877\n",
      "Epoch 3, Batch: 463: Training Loss: 0.032220259308815, Validation Loss: 0.029896045103669167\n",
      "Epoch 3, Batch: 464: Training Loss: 0.025555351749062538, Validation Loss: 0.027854830026626587\n",
      "Epoch 3, Batch: 465: Training Loss: 0.025904109701514244, Validation Loss: 0.028366394340991974\n",
      "Epoch 3, Batch: 466: Training Loss: 0.02859930880367756, Validation Loss: 0.03396807610988617\n",
      "Epoch 3, Batch: 467: Training Loss: 0.039617497473955154, Validation Loss: 0.032987046986818314\n",
      "Epoch 3, Batch: 468: Training Loss: 0.03344782441854477, Validation Loss: 0.031459707766771317\n",
      "Epoch 3, Batch: 469: Training Loss: 0.026676950976252556, Validation Loss: 0.030047299340367317\n",
      "Epoch 3, Batch: 470: Training Loss: 0.03139679133892059, Validation Loss: 0.02855560928583145\n",
      "Epoch 3, Batch: 471: Training Loss: 0.030966196209192276, Validation Loss: 0.030284255743026733\n",
      "Epoch 3, Batch: 472: Training Loss: 0.03441011160612106, Validation Loss: 0.030628282576799393\n",
      "Epoch 3, Batch: 473: Training Loss: 0.027929523959755898, Validation Loss: 0.027126237750053406\n",
      "Epoch 3, Batch: 474: Training Loss: 0.03164377436041832, Validation Loss: 0.02940727211534977\n",
      "Epoch 3, Batch: 475: Training Loss: 0.0319904200732708, Validation Loss: 0.028660086914896965\n",
      "Epoch 3, Batch: 476: Training Loss: 0.033256448805332184, Validation Loss: 0.028230231255292892\n",
      "Epoch 3, Batch: 477: Training Loss: 0.03817538544535637, Validation Loss: 0.02858031541109085\n",
      "Epoch 3, Batch: 478: Training Loss: 0.02811569534242153, Validation Loss: 0.027944542467594147\n",
      "Epoch 3, Batch: 479: Training Loss: 0.029957488179206848, Validation Loss: 0.027307569980621338\n",
      "Epoch 3, Batch: 480: Training Loss: 0.028015777468681335, Validation Loss: 0.026959069073200226\n",
      "Epoch 3, Batch: 481: Training Loss: 0.028009478002786636, Validation Loss: 0.028865020722150803\n",
      "Epoch 3, Batch: 482: Training Loss: 0.02973976731300354, Validation Loss: 0.03181316703557968\n",
      "Epoch 3, Batch: 483: Training Loss: 0.029227137565612793, Validation Loss: 0.029952166602015495\n",
      "Epoch 3, Batch: 484: Training Loss: 0.029495298862457275, Validation Loss: 0.030830804258584976\n",
      "Epoch 3, Batch: 485: Training Loss: 0.02795119397342205, Validation Loss: 0.02774525061249733\n",
      "Epoch 3, Batch: 486: Training Loss: 0.02650143764913082, Validation Loss: 0.03021852672100067\n",
      "Epoch 3, Batch: 487: Training Loss: 0.030860671773552895, Validation Loss: 0.029469214379787445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch: 488: Training Loss: 0.028961585834622383, Validation Loss: 0.03183484077453613\n",
      "Epoch 3, Batch: 489: Training Loss: 0.02910761535167694, Validation Loss: 0.030400168150663376\n",
      "Epoch 3, Batch: 490: Training Loss: 0.03025560826063156, Validation Loss: 0.03241905942559242\n",
      "Epoch 3, Batch: 491: Training Loss: 0.029086029157042503, Validation Loss: 0.029341571033000946\n",
      "Epoch 3, Batch: 492: Training Loss: 0.03041895106434822, Validation Loss: 0.029092280194163322\n",
      "Epoch 3, Batch: 493: Training Loss: 0.02888864278793335, Validation Loss: 0.03099815733730793\n",
      "Epoch 3, Batch: 494: Training Loss: 0.03491613641381264, Validation Loss: 0.031403347849845886\n",
      "Epoch 3, Batch: 495: Training Loss: 0.029898567125201225, Validation Loss: 0.02898307703435421\n",
      "Epoch 3, Batch: 496: Training Loss: 0.027035241946578026, Validation Loss: 0.02891729772090912\n",
      "Epoch 3, Batch: 497: Training Loss: 0.025625521317124367, Validation Loss: 0.027665793895721436\n",
      "Epoch 3, Batch: 498: Training Loss: 0.02930782549083233, Validation Loss: 0.028074217960238457\n",
      "Epoch 3, Batch: 499: Training Loss: 0.028208129107952118, Validation Loss: 0.026537582278251648\n",
      "Epoch 4, Batch: 0: Training Loss: 0.03132392466068268, Validation Loss: 0.02582205832004547\n",
      "Epoch 4, Batch: 1: Training Loss: 0.03003542311489582, Validation Loss: 0.027967138215899467\n",
      "Epoch 4, Batch: 2: Training Loss: 0.0348319336771965, Validation Loss: 0.030469324439764023\n",
      "Epoch 4, Batch: 3: Training Loss: 0.02943873219192028, Validation Loss: 0.02952483482658863\n",
      "Epoch 4, Batch: 4: Training Loss: 0.02674574963748455, Validation Loss: 0.027422118932008743\n",
      "Epoch 4, Batch: 5: Training Loss: 0.026448387652635574, Validation Loss: 0.02755344659090042\n",
      "Epoch 4, Batch: 6: Training Loss: 0.026716845110058784, Validation Loss: 0.02849121019244194\n",
      "Epoch 4, Batch: 7: Training Loss: 0.02721075341105461, Validation Loss: 0.028500612825155258\n",
      "Epoch 4, Batch: 8: Training Loss: 0.03084729239344597, Validation Loss: 0.029294054955244064\n",
      "Epoch 4, Batch: 9: Training Loss: 0.026646343991160393, Validation Loss: 0.028478367254137993\n",
      "Epoch 4, Batch: 10: Training Loss: 0.03194134309887886, Validation Loss: 0.025924377143383026\n",
      "Epoch 4, Batch: 11: Training Loss: 0.03308065980672836, Validation Loss: 0.028036318719387054\n",
      "Epoch 4, Batch: 12: Training Loss: 0.029025018215179443, Validation Loss: 0.02897040732204914\n",
      "Epoch 4, Batch: 13: Training Loss: 0.036050163209438324, Validation Loss: 0.030828163027763367\n",
      "Epoch 4, Batch: 14: Training Loss: 0.03271134942770004, Validation Loss: 0.031217537820339203\n",
      "Epoch 4, Batch: 15: Training Loss: 0.03362847492098808, Validation Loss: 0.02752380073070526\n",
      "Epoch 4, Batch: 16: Training Loss: 0.033227384090423584, Validation Loss: 0.028639843687415123\n",
      "Epoch 4, Batch: 17: Training Loss: 0.029667982831597328, Validation Loss: 0.027669530361890793\n",
      "Epoch 4, Batch: 18: Training Loss: 0.027117259800434113, Validation Loss: 0.029013818129897118\n",
      "Epoch 4, Batch: 19: Training Loss: 0.029300488531589508, Validation Loss: 0.03143935278058052\n",
      "Epoch 4, Batch: 20: Training Loss: 0.02858491614460945, Validation Loss: 0.0291048064827919\n",
      "Epoch 4, Batch: 21: Training Loss: 0.03608081862330437, Validation Loss: 0.031321991235017776\n",
      "Epoch 4, Batch: 22: Training Loss: 0.03089337795972824, Validation Loss: 0.027828969061374664\n",
      "Epoch 4, Batch: 23: Training Loss: 0.025981420651078224, Validation Loss: 0.029465924948453903\n",
      "Epoch 4, Batch: 24: Training Loss: 0.029639607295393944, Validation Loss: 0.03295118734240532\n",
      "Epoch 4, Batch: 25: Training Loss: 0.031403880566358566, Validation Loss: 0.027689771726727486\n",
      "Epoch 4, Batch: 26: Training Loss: 0.03254474326968193, Validation Loss: 0.028638314455747604\n",
      "Epoch 4, Batch: 27: Training Loss: 0.028755778446793556, Validation Loss: 0.02945130504667759\n",
      "Epoch 4, Batch: 28: Training Loss: 0.031217053532600403, Validation Loss: 0.029713565483689308\n",
      "Epoch 4, Batch: 29: Training Loss: 0.0313507542014122, Validation Loss: 0.027819206938147545\n",
      "Epoch 4, Batch: 30: Training Loss: 0.026441318914294243, Validation Loss: 0.02913646213710308\n",
      "Epoch 4, Batch: 31: Training Loss: 0.03327200561761856, Validation Loss: 0.02904948964715004\n",
      "Epoch 4, Batch: 32: Training Loss: 0.03526759520173073, Validation Loss: 0.027995022013783455\n",
      "Epoch 4, Batch: 33: Training Loss: 0.027948157861828804, Validation Loss: 0.029631389304995537\n",
      "Epoch 4, Batch: 34: Training Loss: 0.028347427025437355, Validation Loss: 0.028754882514476776\n",
      "Epoch 4, Batch: 35: Training Loss: 0.02954913303256035, Validation Loss: 0.028063766658306122\n",
      "Epoch 4, Batch: 36: Training Loss: 0.030588090419769287, Validation Loss: 0.02892090380191803\n",
      "Epoch 4, Batch: 37: Training Loss: 0.030622541904449463, Validation Loss: 0.028620507568120956\n",
      "Epoch 4, Batch: 38: Training Loss: 0.029114002361893654, Validation Loss: 0.027951447293162346\n",
      "Epoch 4, Batch: 39: Training Loss: 0.03103780187666416, Validation Loss: 0.03006972000002861\n",
      "Epoch 4, Batch: 40: Training Loss: 0.03508954495191574, Validation Loss: 0.02873346023261547\n",
      "Epoch 4, Batch: 41: Training Loss: 0.028040923178195953, Validation Loss: 0.030199335888028145\n",
      "Epoch 4, Batch: 42: Training Loss: 0.027727408334612846, Validation Loss: 0.029639218002557755\n",
      "Epoch 4, Batch: 43: Training Loss: 0.026029912754893303, Validation Loss: 0.029565947130322456\n",
      "Epoch 4, Batch: 44: Training Loss: 0.02761034294962883, Validation Loss: 0.02898351661860943\n",
      "Epoch 4, Batch: 45: Training Loss: 0.028651559725403786, Validation Loss: 0.02687874622642994\n",
      "Epoch 4, Batch: 46: Training Loss: 0.02947492152452469, Validation Loss: 0.02784370817244053\n",
      "Epoch 4, Batch: 47: Training Loss: 0.02971220202744007, Validation Loss: 0.02905532903969288\n",
      "Epoch 4, Batch: 48: Training Loss: 0.034433260560035706, Validation Loss: 0.029538961127400398\n",
      "Epoch 4, Batch: 49: Training Loss: 0.029597872868180275, Validation Loss: 0.02855759672820568\n",
      "Epoch 4, Batch: 50: Training Loss: 0.0310694370418787, Validation Loss: 0.029640749096870422\n",
      "Epoch 4, Batch: 51: Training Loss: 0.028941156342625618, Validation Loss: 0.030198292806744576\n",
      "Epoch 4, Batch: 52: Training Loss: 0.027424408122897148, Validation Loss: 0.02971825562417507\n",
      "Epoch 4, Batch: 53: Training Loss: 0.026192128658294678, Validation Loss: 0.030548937618732452\n",
      "Epoch 4, Batch: 54: Training Loss: 0.029171746224164963, Validation Loss: 0.02864251658320427\n",
      "Epoch 4, Batch: 55: Training Loss: 0.029748572036623955, Validation Loss: 0.03206547722220421\n",
      "Epoch 4, Batch: 56: Training Loss: 0.031232334673404694, Validation Loss: 0.032575298100709915\n",
      "Epoch 4, Batch: 57: Training Loss: 0.029750164598226547, Validation Loss: 0.028290191665291786\n",
      "Epoch 4, Batch: 58: Training Loss: 0.02963823825120926, Validation Loss: 0.0285864919424057\n",
      "Epoch 4, Batch: 59: Training Loss: 0.027028808370232582, Validation Loss: 0.02870030328631401\n",
      "Epoch 4, Batch: 60: Training Loss: 0.028614912182092667, Validation Loss: 0.027953265234827995\n",
      "Epoch 4, Batch: 61: Training Loss: 0.034742746502161026, Validation Loss: 0.027563607320189476\n",
      "Epoch 4, Batch: 62: Training Loss: 0.031867653131484985, Validation Loss: 0.02936476655304432\n",
      "Epoch 4, Batch: 63: Training Loss: 0.0342862606048584, Validation Loss: 0.027768081054091454\n",
      "Epoch 4, Batch: 64: Training Loss: 0.027768949046730995, Validation Loss: 0.027511784806847572\n",
      "Epoch 4, Batch: 65: Training Loss: 0.033261436969041824, Validation Loss: 0.02856733649969101\n",
      "Epoch 4, Batch: 66: Training Loss: 0.02867230400443077, Validation Loss: 0.026554156094789505\n",
      "Epoch 4, Batch: 67: Training Loss: 0.02939649671316147, Validation Loss: 0.02741706557571888\n",
      "Epoch 4, Batch: 68: Training Loss: 0.03258015960454941, Validation Loss: 0.02976127155125141\n",
      "Epoch 4, Batch: 69: Training Loss: 0.029338037595152855, Validation Loss: 0.026985278353095055\n",
      "Epoch 4, Batch: 70: Training Loss: 0.03092372417449951, Validation Loss: 0.0296618714928627\n",
      "Epoch 4, Batch: 71: Training Loss: 0.027661478146910667, Validation Loss: 0.02953278459608555\n",
      "Epoch 4, Batch: 72: Training Loss: 0.027882171794772148, Validation Loss: 0.029672197997570038\n",
      "Epoch 4, Batch: 73: Training Loss: 0.029177697375416756, Validation Loss: 0.03207631781697273\n",
      "Epoch 4, Batch: 74: Training Loss: 0.02859351970255375, Validation Loss: 0.030322028324007988\n",
      "Epoch 4, Batch: 75: Training Loss: 0.025839634239673615, Validation Loss: 0.031091077253222466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch: 76: Training Loss: 0.025327542796730995, Validation Loss: 0.02903617173433304\n",
      "Epoch 4, Batch: 77: Training Loss: 0.0344325453042984, Validation Loss: 0.030514484271407127\n",
      "Epoch 4, Batch: 78: Training Loss: 0.033632852137088776, Validation Loss: 0.029941530898213387\n",
      "Epoch 4, Batch: 79: Training Loss: 0.026118773967027664, Validation Loss: 0.029317112639546394\n",
      "Epoch 4, Batch: 80: Training Loss: 0.02719440869987011, Validation Loss: 0.0265515074133873\n",
      "Epoch 4, Batch: 81: Training Loss: 0.03131578862667084, Validation Loss: 0.030555645003914833\n",
      "Epoch 4, Batch: 82: Training Loss: 0.03099835477769375, Validation Loss: 0.031153008341789246\n",
      "Epoch 4, Batch: 83: Training Loss: 0.027351664379239082, Validation Loss: 0.033065419644117355\n",
      "Epoch 4, Batch: 84: Training Loss: 0.03355308622121811, Validation Loss: 0.0316140353679657\n",
      "Epoch 4, Batch: 85: Training Loss: 0.028571385890245438, Validation Loss: 0.03011634759604931\n",
      "Epoch 4, Batch: 86: Training Loss: 0.030830401927232742, Validation Loss: 0.028751911595463753\n",
      "Epoch 4, Batch: 87: Training Loss: 0.0324401929974556, Validation Loss: 0.030983727425336838\n",
      "Epoch 4, Batch: 88: Training Loss: 0.03217362239956856, Validation Loss: 0.029259616509079933\n",
      "Epoch 4, Batch: 89: Training Loss: 0.03039785847067833, Validation Loss: 0.032408032566308975\n",
      "Epoch 4, Batch: 90: Training Loss: 0.03025873191654682, Validation Loss: 0.029307078570127487\n",
      "Epoch 4, Batch: 91: Training Loss: 0.03237617760896683, Validation Loss: 0.029567409306764603\n",
      "Epoch 4, Batch: 92: Training Loss: 0.031293343752622604, Validation Loss: 0.03282668814063072\n",
      "Epoch 4, Batch: 93: Training Loss: 0.027949420735239983, Validation Loss: 0.030400961637496948\n",
      "Epoch 4, Batch: 94: Training Loss: 0.03129008412361145, Validation Loss: 0.03087019920349121\n",
      "Epoch 4, Batch: 95: Training Loss: 0.030599132180213928, Validation Loss: 0.028634903952479362\n",
      "Epoch 4, Batch: 96: Training Loss: 0.027007808908820152, Validation Loss: 0.02745022065937519\n",
      "Epoch 4, Batch: 97: Training Loss: 0.03119407407939434, Validation Loss: 0.02799806371331215\n",
      "Epoch 4, Batch: 98: Training Loss: 0.03353571519255638, Validation Loss: 0.028293531388044357\n",
      "Epoch 4, Batch: 99: Training Loss: 0.029615912586450577, Validation Loss: 0.030745411291718483\n",
      "Epoch 4, Batch: 100: Training Loss: 0.028629343956708908, Validation Loss: 0.029891211539506912\n",
      "Epoch 4, Batch: 101: Training Loss: 0.02772434987127781, Validation Loss: 0.030216911807656288\n",
      "Epoch 4, Batch: 102: Training Loss: 0.02942798286676407, Validation Loss: 0.029505576938390732\n",
      "Epoch 4, Batch: 103: Training Loss: 0.03326169401407242, Validation Loss: 0.030888399109244347\n",
      "Epoch 4, Batch: 104: Training Loss: 0.029380330815911293, Validation Loss: 0.026206692680716515\n",
      "Epoch 4, Batch: 105: Training Loss: 0.02718084305524826, Validation Loss: 0.02634689211845398\n",
      "Epoch 4, Batch: 106: Training Loss: 0.02674846351146698, Validation Loss: 0.026965398341417313\n",
      "Epoch 4, Batch: 107: Training Loss: 0.03512550890445709, Validation Loss: 0.02902417816221714\n",
      "Epoch 4, Batch: 108: Training Loss: 0.029434580355882645, Validation Loss: 0.031307730823755264\n",
      "Epoch 4, Batch: 109: Training Loss: 0.031087510287761688, Validation Loss: 0.03052479960024357\n",
      "Epoch 4, Batch: 110: Training Loss: 0.03658688813447952, Validation Loss: 0.02782052382826805\n",
      "Epoch 4, Batch: 111: Training Loss: 0.031233808025717735, Validation Loss: 0.03050879016518593\n",
      "Epoch 4, Batch: 112: Training Loss: 0.027356693521142006, Validation Loss: 0.02954377606511116\n",
      "Epoch 4, Batch: 113: Training Loss: 0.03329652175307274, Validation Loss: 0.030026772990822792\n",
      "Epoch 4, Batch: 114: Training Loss: 0.030893145129084587, Validation Loss: 0.027345973998308182\n",
      "Epoch 4, Batch: 115: Training Loss: 0.027645405381917953, Validation Loss: 0.027988480404019356\n",
      "Epoch 4, Batch: 116: Training Loss: 0.026185205206274986, Validation Loss: 0.029266424477100372\n",
      "Epoch 4, Batch: 117: Training Loss: 0.03172589838504791, Validation Loss: 0.030867569148540497\n",
      "Epoch 4, Batch: 118: Training Loss: 0.025961725041270256, Validation Loss: 0.032766468822956085\n",
      "Epoch 4, Batch: 119: Training Loss: 0.02796352095901966, Validation Loss: 0.02699645608663559\n",
      "Epoch 4, Batch: 120: Training Loss: 0.030358746647834778, Validation Loss: 0.02844046801328659\n",
      "Epoch 4, Batch: 121: Training Loss: 0.03374670818448067, Validation Loss: 0.03095737285912037\n",
      "Epoch 4, Batch: 122: Training Loss: 0.029671290889382362, Validation Loss: 0.028221827000379562\n",
      "Epoch 4, Batch: 123: Training Loss: 0.02746749296784401, Validation Loss: 0.030368613079190254\n",
      "Epoch 4, Batch: 124: Training Loss: 0.02889571338891983, Validation Loss: 0.030257588252425194\n",
      "Epoch 4, Batch: 125: Training Loss: 0.02780694141983986, Validation Loss: 0.03178014978766441\n",
      "Epoch 4, Batch: 126: Training Loss: 0.02775159291923046, Validation Loss: 0.029097996652126312\n",
      "Epoch 4, Batch: 127: Training Loss: 0.029753902927041054, Validation Loss: 0.029790252447128296\n",
      "Epoch 4, Batch: 128: Training Loss: 0.029978090897202492, Validation Loss: 0.029000017791986465\n",
      "Epoch 4, Batch: 129: Training Loss: 0.029041310772299767, Validation Loss: 0.030420612543821335\n",
      "Epoch 4, Batch: 130: Training Loss: 0.026969026774168015, Validation Loss: 0.028843121603131294\n",
      "Epoch 4, Batch: 131: Training Loss: 0.028903717175126076, Validation Loss: 0.031276244670152664\n",
      "Epoch 4, Batch: 132: Training Loss: 0.02912202477455139, Validation Loss: 0.030273564159870148\n",
      "Epoch 4, Batch: 133: Training Loss: 0.03475652262568474, Validation Loss: 0.028382612392306328\n",
      "Epoch 4, Batch: 134: Training Loss: 0.029058340936899185, Validation Loss: 0.02890470065176487\n",
      "Epoch 4, Batch: 135: Training Loss: 0.027834465727210045, Validation Loss: 0.03072284534573555\n",
      "Epoch 4, Batch: 136: Training Loss: 0.028898777440190315, Validation Loss: 0.02979377470910549\n",
      "Epoch 4, Batch: 137: Training Loss: 0.03008878603577614, Validation Loss: 0.030598683282732964\n",
      "Epoch 4, Batch: 138: Training Loss: 0.030089382082223892, Validation Loss: 0.030274836346507072\n",
      "Epoch 4, Batch: 139: Training Loss: 0.024256834760308266, Validation Loss: 0.028545092791318893\n",
      "Epoch 4, Batch: 140: Training Loss: 0.033918872475624084, Validation Loss: 0.029877470806241035\n",
      "Epoch 4, Batch: 141: Training Loss: 0.027985403314232826, Validation Loss: 0.030080923810601234\n",
      "Epoch 4, Batch: 142: Training Loss: 0.03669741004705429, Validation Loss: 0.028569940477609634\n",
      "Epoch 4, Batch: 143: Training Loss: 0.030483558773994446, Validation Loss: 0.02784767374396324\n",
      "Epoch 4, Batch: 144: Training Loss: 0.030845843255519867, Validation Loss: 0.028043095022439957\n",
      "Epoch 4, Batch: 145: Training Loss: 0.02964387647807598, Validation Loss: 0.028216885402798653\n",
      "Epoch 4, Batch: 146: Training Loss: 0.02526792697608471, Validation Loss: 0.029534591361880302\n",
      "Epoch 4, Batch: 147: Training Loss: 0.025006858631968498, Validation Loss: 0.029953930526971817\n",
      "Epoch 4, Batch: 148: Training Loss: 0.02948174998164177, Validation Loss: 0.032170042395591736\n",
      "Epoch 4, Batch: 149: Training Loss: 0.028869394212961197, Validation Loss: 0.031359292566776276\n",
      "Epoch 4, Batch: 150: Training Loss: 0.03489378094673157, Validation Loss: 0.03267846256494522\n",
      "Epoch 4, Batch: 151: Training Loss: 0.03356980159878731, Validation Loss: 0.031035102903842926\n",
      "Epoch 4, Batch: 152: Training Loss: 0.03336785361170769, Validation Loss: 0.02843138948082924\n",
      "Epoch 4, Batch: 153: Training Loss: 0.03940371051430702, Validation Loss: 0.03557252138853073\n",
      "Epoch 4, Batch: 154: Training Loss: 0.03527922183275223, Validation Loss: 0.03176499158143997\n",
      "Epoch 4, Batch: 155: Training Loss: 0.04106200486421585, Validation Loss: 0.031737472862005234\n",
      "Epoch 4, Batch: 156: Training Loss: 0.03501957654953003, Validation Loss: 0.033117473125457764\n",
      "Epoch 4, Batch: 157: Training Loss: 0.03100559487938881, Validation Loss: 0.031780924648046494\n",
      "Epoch 4, Batch: 158: Training Loss: 0.03170696273446083, Validation Loss: 0.030200155451893806\n",
      "Epoch 4, Batch: 159: Training Loss: 0.032015591859817505, Validation Loss: 0.030919330194592476\n",
      "Epoch 4, Batch: 160: Training Loss: 0.028086278587579727, Validation Loss: 0.029462704434990883\n",
      "Epoch 4, Batch: 161: Training Loss: 0.03149694576859474, Validation Loss: 0.030878139659762383\n",
      "Epoch 4, Batch: 162: Training Loss: 0.03390934318304062, Validation Loss: 0.031883373856544495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch: 163: Training Loss: 0.027999229729175568, Validation Loss: 0.030918961390852928\n",
      "Epoch 4, Batch: 164: Training Loss: 0.02956373617053032, Validation Loss: 0.028539741411805153\n",
      "Epoch 4, Batch: 165: Training Loss: 0.033477045595645905, Validation Loss: 0.03333001583814621\n",
      "Epoch 4, Batch: 166: Training Loss: 0.029761165380477905, Validation Loss: 0.03186619654297829\n",
      "Epoch 4, Batch: 167: Training Loss: 0.025393623858690262, Validation Loss: 0.03417133912444115\n",
      "Epoch 4, Batch: 168: Training Loss: 0.036027830094099045, Validation Loss: 0.030594350770115852\n",
      "Epoch 4, Batch: 169: Training Loss: 0.03227093815803528, Validation Loss: 0.03218043968081474\n",
      "Epoch 4, Batch: 170: Training Loss: 0.03231377154588699, Validation Loss: 0.028729498386383057\n",
      "Epoch 4, Batch: 171: Training Loss: 0.030714411288499832, Validation Loss: 0.03003225289285183\n",
      "Epoch 4, Batch: 172: Training Loss: 0.028012234717607498, Validation Loss: 0.030805248767137527\n",
      "Epoch 4, Batch: 173: Training Loss: 0.028518088161945343, Validation Loss: 0.029561126604676247\n",
      "Epoch 4, Batch: 174: Training Loss: 0.03257489204406738, Validation Loss: 0.029368385672569275\n",
      "Epoch 4, Batch: 175: Training Loss: 0.03017331473529339, Validation Loss: 0.03132963180541992\n",
      "Epoch 4, Batch: 176: Training Loss: 0.032110217958688736, Validation Loss: 0.03164463862776756\n",
      "Epoch 4, Batch: 177: Training Loss: 0.028475569561123848, Validation Loss: 0.029364902526140213\n",
      "Epoch 4, Batch: 178: Training Loss: 0.034428179264068604, Validation Loss: 0.031509075313806534\n",
      "Epoch 4, Batch: 179: Training Loss: 0.03155975416302681, Validation Loss: 0.03185424581170082\n",
      "Epoch 4, Batch: 180: Training Loss: 0.026923755183815956, Validation Loss: 0.03030172362923622\n",
      "Epoch 4, Batch: 181: Training Loss: 0.027980120852589607, Validation Loss: 0.030327681452035904\n",
      "Epoch 4, Batch: 182: Training Loss: 0.03179362416267395, Validation Loss: 0.032101407647132874\n",
      "Epoch 4, Batch: 183: Training Loss: 0.03249146044254303, Validation Loss: 0.027241626754403114\n",
      "Epoch 4, Batch: 184: Training Loss: 0.029457416385412216, Validation Loss: 0.028252901509404182\n",
      "Epoch 4, Batch: 185: Training Loss: 0.035583898425102234, Validation Loss: 0.03135986998677254\n",
      "Epoch 4, Batch: 186: Training Loss: 0.029772302135825157, Validation Loss: 0.030199328437447548\n",
      "Epoch 4, Batch: 187: Training Loss: 0.029049856588244438, Validation Loss: 0.031176377087831497\n",
      "Epoch 4, Batch: 188: Training Loss: 0.031699419021606445, Validation Loss: 0.03021169640123844\n",
      "Epoch 4, Batch: 189: Training Loss: 0.027126910164952278, Validation Loss: 0.03209954500198364\n",
      "Epoch 4, Batch: 190: Training Loss: 0.030852552503347397, Validation Loss: 0.030255451798439026\n",
      "Epoch 4, Batch: 191: Training Loss: 0.030673429369926453, Validation Loss: 0.03264617174863815\n",
      "Epoch 4, Batch: 192: Training Loss: 0.029428305104374886, Validation Loss: 0.032113783061504364\n",
      "Epoch 4, Batch: 193: Training Loss: 0.028588738292455673, Validation Loss: 0.03218678757548332\n",
      "Epoch 4, Batch: 194: Training Loss: 0.027541736140847206, Validation Loss: 0.03292166441679001\n",
      "Epoch 4, Batch: 195: Training Loss: 0.03180862218141556, Validation Loss: 0.030100097879767418\n",
      "Epoch 4, Batch: 196: Training Loss: 0.03313040733337402, Validation Loss: 0.031204791739583015\n",
      "Epoch 4, Batch: 197: Training Loss: 0.03190416470170021, Validation Loss: 0.029852988198399544\n",
      "Epoch 4, Batch: 198: Training Loss: 0.025356026366353035, Validation Loss: 0.032186612486839294\n",
      "Epoch 4, Batch: 199: Training Loss: 0.02553696557879448, Validation Loss: 0.03279987722635269\n",
      "Epoch 4, Batch: 200: Training Loss: 0.025088628754019737, Validation Loss: 0.032719749957323074\n",
      "Epoch 4, Batch: 201: Training Loss: 0.033486708998680115, Validation Loss: 0.03265923634171486\n",
      "Epoch 4, Batch: 202: Training Loss: 0.028406910598278046, Validation Loss: 0.03238396719098091\n",
      "Epoch 4, Batch: 203: Training Loss: 0.034511830657720566, Validation Loss: 0.0322754792869091\n",
      "Epoch 4, Batch: 204: Training Loss: 0.029868368059396744, Validation Loss: 0.0364953875541687\n",
      "Epoch 4, Batch: 205: Training Loss: 0.03592989593744278, Validation Loss: 0.03491181135177612\n",
      "Epoch 4, Batch: 206: Training Loss: 0.02645653672516346, Validation Loss: 0.03094383142888546\n",
      "Epoch 4, Batch: 207: Training Loss: 0.031077688559889793, Validation Loss: 0.033536750823259354\n",
      "Epoch 4, Batch: 208: Training Loss: 0.030999805778265, Validation Loss: 0.03294629231095314\n",
      "Epoch 4, Batch: 209: Training Loss: 0.030872125178575516, Validation Loss: 0.03162000700831413\n",
      "Epoch 4, Batch: 210: Training Loss: 0.029706547036767006, Validation Loss: 0.031386006623506546\n",
      "Epoch 4, Batch: 211: Training Loss: 0.030742453411221504, Validation Loss: 0.028982046991586685\n",
      "Epoch 4, Batch: 212: Training Loss: 0.028982248157262802, Validation Loss: 0.030289970338344574\n",
      "Epoch 4, Batch: 213: Training Loss: 0.033758051693439484, Validation Loss: 0.03335157409310341\n",
      "Epoch 4, Batch: 214: Training Loss: 0.02595914714038372, Validation Loss: 0.02918771654367447\n",
      "Epoch 4, Batch: 215: Training Loss: 0.02966701239347458, Validation Loss: 0.03007773868739605\n",
      "Epoch 4, Batch: 216: Training Loss: 0.029933713376522064, Validation Loss: 0.029052656143903732\n",
      "Epoch 4, Batch: 217: Training Loss: 0.03012322634458542, Validation Loss: 0.028250031173229218\n",
      "Epoch 4, Batch: 218: Training Loss: 0.0271292757242918, Validation Loss: 0.029438311234116554\n",
      "Epoch 4, Batch: 219: Training Loss: 0.02823895774781704, Validation Loss: 0.03244131803512573\n",
      "Epoch 4, Batch: 220: Training Loss: 0.0342259407043457, Validation Loss: 0.03377801924943924\n",
      "Epoch 4, Batch: 221: Training Loss: 0.03138723224401474, Validation Loss: 0.030523674562573433\n",
      "Epoch 4, Batch: 222: Training Loss: 0.03045797348022461, Validation Loss: 0.029307730495929718\n",
      "Epoch 4, Batch: 223: Training Loss: 0.028653202578425407, Validation Loss: 0.028558550402522087\n",
      "Epoch 4, Batch: 224: Training Loss: 0.03201797232031822, Validation Loss: 0.026300234720110893\n",
      "Epoch 4, Batch: 225: Training Loss: 0.03177163004875183, Validation Loss: 0.029651962220668793\n",
      "Epoch 4, Batch: 226: Training Loss: 0.03270108625292778, Validation Loss: 0.02809756062924862\n",
      "Epoch 4, Batch: 227: Training Loss: 0.030911993235349655, Validation Loss: 0.029713628813624382\n",
      "Epoch 4, Batch: 228: Training Loss: 0.03290009871125221, Validation Loss: 0.027708645910024643\n",
      "Epoch 4, Batch: 229: Training Loss: 0.03133898600935936, Validation Loss: 0.02666451968252659\n",
      "Epoch 4, Batch: 230: Training Loss: 0.030615586787462234, Validation Loss: 0.028082747012376785\n",
      "Epoch 4, Batch: 231: Training Loss: 0.03518029674887657, Validation Loss: 0.028840133920311928\n",
      "Epoch 4, Batch: 232: Training Loss: 0.032850999385118484, Validation Loss: 0.02930000051856041\n",
      "Epoch 4, Batch: 233: Training Loss: 0.02827240340411663, Validation Loss: 0.030288217589259148\n",
      "Epoch 4, Batch: 234: Training Loss: 0.02856454811990261, Validation Loss: 0.029298938810825348\n",
      "Epoch 4, Batch: 235: Training Loss: 0.03473367914557457, Validation Loss: 0.0306369848549366\n",
      "Epoch 4, Batch: 236: Training Loss: 0.03265766799449921, Validation Loss: 0.029696067795157433\n",
      "Epoch 4, Batch: 237: Training Loss: 0.029794462025165558, Validation Loss: 0.029515475034713745\n",
      "Epoch 4, Batch: 238: Training Loss: 0.030491046607494354, Validation Loss: 0.029693396762013435\n",
      "Epoch 4, Batch: 239: Training Loss: 0.029456105083227158, Validation Loss: 0.03393637016415596\n",
      "Epoch 4, Batch: 240: Training Loss: 0.026557087898254395, Validation Loss: 0.029339302331209183\n",
      "Epoch 4, Batch: 241: Training Loss: 0.02679554373025894, Validation Loss: 0.031147219240665436\n",
      "Epoch 4, Batch: 242: Training Loss: 0.03164161369204521, Validation Loss: 0.030792566016316414\n",
      "Epoch 4, Batch: 243: Training Loss: 0.03117287904024124, Validation Loss: 0.03033541701734066\n",
      "Epoch 4, Batch: 244: Training Loss: 0.027548279613256454, Validation Loss: 0.03113819658756256\n",
      "Epoch 4, Batch: 245: Training Loss: 0.03358926996588707, Validation Loss: 0.02720455825328827\n",
      "Epoch 4, Batch: 246: Training Loss: 0.02596932277083397, Validation Loss: 0.03092913329601288\n",
      "Epoch 4, Batch: 247: Training Loss: 0.030755113810300827, Validation Loss: 0.029787657782435417\n",
      "Epoch 4, Batch: 248: Training Loss: 0.02909740060567856, Validation Loss: 0.02913619950413704\n",
      "Epoch 4, Batch: 249: Training Loss: 0.030375370755791664, Validation Loss: 0.02985306642949581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch: 250: Training Loss: 0.02872079238295555, Validation Loss: 0.027441568672657013\n",
      "Epoch 4, Batch: 251: Training Loss: 0.027734627947211266, Validation Loss: 0.03090703673660755\n",
      "Epoch 4, Batch: 252: Training Loss: 0.032353006303310394, Validation Loss: 0.028422174975275993\n",
      "Epoch 4, Batch: 253: Training Loss: 0.03515992313623428, Validation Loss: 0.028804603964090347\n",
      "Epoch 4, Batch: 254: Training Loss: 0.02609173022210598, Validation Loss: 0.027574729174375534\n",
      "Epoch 4, Batch: 255: Training Loss: 0.023798422887921333, Validation Loss: 0.025764796882867813\n",
      "Epoch 4, Batch: 256: Training Loss: 0.03184468299150467, Validation Loss: 0.027943965047597885\n",
      "Epoch 4, Batch: 257: Training Loss: 0.035359758883714676, Validation Loss: 0.0282472912222147\n",
      "Epoch 4, Batch: 258: Training Loss: 0.027216186746954918, Validation Loss: 0.02916618064045906\n",
      "Epoch 4, Batch: 259: Training Loss: 0.028167834505438805, Validation Loss: 0.028111686930060387\n",
      "Epoch 4, Batch: 260: Training Loss: 0.03191173076629639, Validation Loss: 0.028328262269496918\n",
      "Epoch 4, Batch: 261: Training Loss: 0.030536776408553123, Validation Loss: 0.028669854626059532\n",
      "Epoch 4, Batch: 262: Training Loss: 0.03538987785577774, Validation Loss: 0.029461724683642387\n",
      "Epoch 4, Batch: 263: Training Loss: 0.0313064269721508, Validation Loss: 0.02576339617371559\n",
      "Epoch 4, Batch: 264: Training Loss: 0.02994813211262226, Validation Loss: 0.028039662167429924\n",
      "Epoch 4, Batch: 265: Training Loss: 0.030020590871572495, Validation Loss: 0.028318634256720543\n",
      "Epoch 4, Batch: 266: Training Loss: 0.02988681010901928, Validation Loss: 0.028583042323589325\n",
      "Epoch 4, Batch: 267: Training Loss: 0.02989579178392887, Validation Loss: 0.025813141837716103\n",
      "Epoch 4, Batch: 268: Training Loss: 0.03216787055134773, Validation Loss: 0.028529761359095573\n",
      "Epoch 4, Batch: 269: Training Loss: 0.026917677372694016, Validation Loss: 0.02854500152170658\n",
      "Epoch 4, Batch: 270: Training Loss: 0.024915728718042374, Validation Loss: 0.02672255039215088\n",
      "Epoch 4, Batch: 271: Training Loss: 0.026195582002401352, Validation Loss: 0.028412995859980583\n",
      "Epoch 4, Batch: 272: Training Loss: 0.028778186067938805, Validation Loss: 0.02943292260169983\n",
      "Epoch 4, Batch: 273: Training Loss: 0.02922951430082321, Validation Loss: 0.025692615658044815\n",
      "Epoch 4, Batch: 274: Training Loss: 0.030961066484451294, Validation Loss: 0.02915683574974537\n",
      "Epoch 4, Batch: 275: Training Loss: 0.03068089298903942, Validation Loss: 0.026417024433612823\n",
      "Epoch 4, Batch: 276: Training Loss: 0.029544979333877563, Validation Loss: 0.02813437208533287\n",
      "Epoch 4, Batch: 277: Training Loss: 0.026546409353613853, Validation Loss: 0.02851998619735241\n",
      "Epoch 4, Batch: 278: Training Loss: 0.030056647956371307, Validation Loss: 0.02850211225450039\n",
      "Epoch 4, Batch: 279: Training Loss: 0.02545717917382717, Validation Loss: 0.029682952910661697\n",
      "Epoch 4, Batch: 280: Training Loss: 0.029445812106132507, Validation Loss: 0.02941964752972126\n",
      "Epoch 4, Batch: 281: Training Loss: 0.02992514707148075, Validation Loss: 0.02802753448486328\n",
      "Epoch 4, Batch: 282: Training Loss: 0.027561061084270477, Validation Loss: 0.031342677772045135\n",
      "Epoch 4, Batch: 283: Training Loss: 0.029808536171913147, Validation Loss: 0.028518816456198692\n",
      "Epoch 4, Batch: 284: Training Loss: 0.031454600393772125, Validation Loss: 0.026011556386947632\n",
      "Epoch 4, Batch: 285: Training Loss: 0.032355595380067825, Validation Loss: 0.02660674788057804\n",
      "Epoch 4, Batch: 286: Training Loss: 0.026681676506996155, Validation Loss: 0.028051426634192467\n",
      "Epoch 4, Batch: 287: Training Loss: 0.029152527451515198, Validation Loss: 0.026873433962464333\n",
      "Epoch 4, Batch: 288: Training Loss: 0.026344245299696922, Validation Loss: 0.02880975417792797\n",
      "Epoch 4, Batch: 289: Training Loss: 0.030601410195231438, Validation Loss: 0.02848043479025364\n",
      "Epoch 4, Batch: 290: Training Loss: 0.024678824469447136, Validation Loss: 0.02901819348335266\n",
      "Epoch 4, Batch: 291: Training Loss: 0.03063952550292015, Validation Loss: 0.0304800346493721\n",
      "Epoch 4, Batch: 292: Training Loss: 0.02714572474360466, Validation Loss: 0.03092864155769348\n",
      "Epoch 4, Batch: 293: Training Loss: 0.029681187123060226, Validation Loss: 0.03102034702897072\n",
      "Epoch 4, Batch: 294: Training Loss: 0.030816109851002693, Validation Loss: 0.03273247182369232\n",
      "Epoch 4, Batch: 295: Training Loss: 0.029769180342555046, Validation Loss: 0.03063577227294445\n",
      "Epoch 4, Batch: 296: Training Loss: 0.031091967597603798, Validation Loss: 0.03105596825480461\n",
      "Epoch 4, Batch: 297: Training Loss: 0.029207047075033188, Validation Loss: 0.029209410771727562\n",
      "Epoch 4, Batch: 298: Training Loss: 0.02842661924660206, Validation Loss: 0.02812279388308525\n",
      "Epoch 4, Batch: 299: Training Loss: 0.02684398740530014, Validation Loss: 0.03065655194222927\n",
      "Epoch 4, Batch: 300: Training Loss: 0.024714386090636253, Validation Loss: 0.02760082669556141\n",
      "Epoch 4, Batch: 301: Training Loss: 0.029665807262063026, Validation Loss: 0.02702614665031433\n",
      "Epoch 4, Batch: 302: Training Loss: 0.02779764123260975, Validation Loss: 0.02988196350634098\n",
      "Epoch 4, Batch: 303: Training Loss: 0.02695019729435444, Validation Loss: 0.02871393784880638\n",
      "Epoch 4, Batch: 304: Training Loss: 0.02866356447339058, Validation Loss: 0.029121454805135727\n",
      "Epoch 4, Batch: 305: Training Loss: 0.029469924047589302, Validation Loss: 0.02819533459842205\n",
      "Epoch 4, Batch: 306: Training Loss: 0.03261962905526161, Validation Loss: 0.026406103745102882\n",
      "Epoch 4, Batch: 307: Training Loss: 0.026605650782585144, Validation Loss: 0.028383871540427208\n",
      "Epoch 4, Batch: 308: Training Loss: 0.027550108730793, Validation Loss: 0.02706092782318592\n",
      "Epoch 4, Batch: 309: Training Loss: 0.029400929808616638, Validation Loss: 0.02760832943022251\n",
      "Epoch 4, Batch: 310: Training Loss: 0.026750022545456886, Validation Loss: 0.027741089463233948\n",
      "Epoch 4, Batch: 311: Training Loss: 0.02591228112578392, Validation Loss: 0.02633802965283394\n",
      "Epoch 4, Batch: 312: Training Loss: 0.029817352071404457, Validation Loss: 0.027142832055687904\n",
      "Epoch 4, Batch: 313: Training Loss: 0.02201334945857525, Validation Loss: 0.026145601645112038\n",
      "Epoch 4, Batch: 314: Training Loss: 0.029261158779263496, Validation Loss: 0.026940127834677696\n",
      "Epoch 4, Batch: 315: Training Loss: 0.030205974355340004, Validation Loss: 0.02629837952554226\n",
      "Epoch 4, Batch: 316: Training Loss: 0.024749768897891045, Validation Loss: 0.028228215873241425\n",
      "Epoch 4, Batch: 317: Training Loss: 0.025369171053171158, Validation Loss: 0.025124751031398773\n",
      "Epoch 4, Batch: 318: Training Loss: 0.027184875681996346, Validation Loss: 0.027244234457612038\n",
      "Epoch 4, Batch: 319: Training Loss: 0.03359254077076912, Validation Loss: 0.028194859623908997\n",
      "Epoch 4, Batch: 320: Training Loss: 0.032526060938835144, Validation Loss: 0.02800535410642624\n",
      "Epoch 4, Batch: 321: Training Loss: 0.027831606566905975, Validation Loss: 0.029181256890296936\n",
      "Epoch 4, Batch: 322: Training Loss: 0.029297323897480965, Validation Loss: 0.029414106160402298\n",
      "Epoch 4, Batch: 323: Training Loss: 0.025195900350809097, Validation Loss: 0.027705224230885506\n",
      "Epoch 4, Batch: 324: Training Loss: 0.028851868584752083, Validation Loss: 0.028885850682854652\n",
      "Epoch 4, Batch: 325: Training Loss: 0.029683182016015053, Validation Loss: 0.02778596431016922\n",
      "Epoch 4, Batch: 326: Training Loss: 0.03191489353775978, Validation Loss: 0.027597250416874886\n",
      "Epoch 4, Batch: 327: Training Loss: 0.02823219634592533, Validation Loss: 0.02851988561451435\n",
      "Epoch 4, Batch: 328: Training Loss: 0.03135387971997261, Validation Loss: 0.028504259884357452\n",
      "Epoch 4, Batch: 329: Training Loss: 0.029380785301327705, Validation Loss: 0.028469786047935486\n",
      "Epoch 4, Batch: 330: Training Loss: 0.03051992878317833, Validation Loss: 0.027478985488414764\n",
      "Epoch 4, Batch: 331: Training Loss: 0.03081701695919037, Validation Loss: 0.02707548998296261\n",
      "Epoch 4, Batch: 332: Training Loss: 0.027979573234915733, Validation Loss: 0.02883840724825859\n",
      "Epoch 4, Batch: 333: Training Loss: 0.031869564205408096, Validation Loss: 0.03076961822807789\n",
      "Epoch 4, Batch: 334: Training Loss: 0.030589845031499863, Validation Loss: 0.026341214776039124\n",
      "Epoch 4, Batch: 335: Training Loss: 0.02671935223042965, Validation Loss: 0.028554916381835938\n",
      "Epoch 4, Batch: 336: Training Loss: 0.024950925260782242, Validation Loss: 0.030248058959841728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch: 337: Training Loss: 0.02717634290456772, Validation Loss: 0.028878403827548027\n",
      "Epoch 4, Batch: 338: Training Loss: 0.03027432970702648, Validation Loss: 0.026517657563090324\n",
      "Epoch 4, Batch: 339: Training Loss: 0.026626773178577423, Validation Loss: 0.02437509223818779\n",
      "Epoch 4, Batch: 340: Training Loss: 0.02439422532916069, Validation Loss: 0.02509603463113308\n",
      "Epoch 4, Batch: 341: Training Loss: 0.027022331953048706, Validation Loss: 0.027898453176021576\n",
      "Epoch 4, Batch: 342: Training Loss: 0.02820940501987934, Validation Loss: 0.028605367988348007\n",
      "Epoch 4, Batch: 343: Training Loss: 0.028229258954524994, Validation Loss: 0.028329458087682724\n",
      "Epoch 4, Batch: 344: Training Loss: 0.026353761553764343, Validation Loss: 0.026838142424821854\n",
      "Epoch 4, Batch: 345: Training Loss: 0.02570304088294506, Validation Loss: 0.02842453122138977\n",
      "Epoch 4, Batch: 346: Training Loss: 0.029277542605996132, Validation Loss: 0.02636520192027092\n",
      "Epoch 4, Batch: 347: Training Loss: 0.02747071161866188, Validation Loss: 0.028181280940771103\n",
      "Epoch 4, Batch: 348: Training Loss: 0.03051120787858963, Validation Loss: 0.025791455060243607\n",
      "Epoch 4, Batch: 349: Training Loss: 0.027784526348114014, Validation Loss: 0.026879332959651947\n",
      "Epoch 4, Batch: 350: Training Loss: 0.029255665838718414, Validation Loss: 0.026158103719353676\n",
      "Epoch 4, Batch: 351: Training Loss: 0.03325464576482773, Validation Loss: 0.029691392555832863\n",
      "Epoch 4, Batch: 352: Training Loss: 0.029790004715323448, Validation Loss: 0.029468709602952003\n",
      "Epoch 4, Batch: 353: Training Loss: 0.030715953558683395, Validation Loss: 0.03088797815144062\n",
      "Epoch 4, Batch: 354: Training Loss: 0.03308357298374176, Validation Loss: 0.03189611807465553\n",
      "Epoch 4, Batch: 355: Training Loss: 0.02418777532875538, Validation Loss: 0.030412014573812485\n",
      "Epoch 4, Batch: 356: Training Loss: 0.027449779212474823, Validation Loss: 0.032457269728183746\n",
      "Epoch 4, Batch: 357: Training Loss: 0.029678089544177055, Validation Loss: 0.0320938304066658\n",
      "Epoch 4, Batch: 358: Training Loss: 0.034179214388132095, Validation Loss: 0.030647531151771545\n",
      "Epoch 4, Batch: 359: Training Loss: 0.02762746997177601, Validation Loss: 0.028643565252423286\n",
      "Epoch 4, Batch: 360: Training Loss: 0.031945277005434036, Validation Loss: 0.030112374573946\n",
      "Epoch 4, Batch: 361: Training Loss: 0.02828531712293625, Validation Loss: 0.028383033350110054\n",
      "Epoch 4, Batch: 362: Training Loss: 0.025124942883849144, Validation Loss: 0.029587222263216972\n",
      "Epoch 4, Batch: 363: Training Loss: 0.03351094573736191, Validation Loss: 0.029935000464320183\n",
      "Epoch 4, Batch: 364: Training Loss: 0.027305282652378082, Validation Loss: 0.030514011159539223\n",
      "Epoch 4, Batch: 365: Training Loss: 0.02940499223768711, Validation Loss: 0.028690868988633156\n",
      "Epoch 4, Batch: 366: Training Loss: 0.027374165132641792, Validation Loss: 0.032524511218070984\n",
      "Epoch 4, Batch: 367: Training Loss: 0.027393586933612823, Validation Loss: 0.0292830653488636\n",
      "Epoch 4, Batch: 368: Training Loss: 0.02653972990810871, Validation Loss: 0.029262002557516098\n",
      "Epoch 4, Batch: 369: Training Loss: 0.028641285374760628, Validation Loss: 0.028600582852959633\n",
      "Epoch 4, Batch: 370: Training Loss: 0.02971605770289898, Validation Loss: 0.029305612668395042\n",
      "Epoch 4, Batch: 371: Training Loss: 0.029196878895163536, Validation Loss: 0.03026028349995613\n",
      "Epoch 4, Batch: 372: Training Loss: 0.027607666328549385, Validation Loss: 0.030817346647381783\n",
      "Epoch 4, Batch: 373: Training Loss: 0.029434356838464737, Validation Loss: 0.02951393462717533\n",
      "Epoch 4, Batch: 374: Training Loss: 0.02658778615295887, Validation Loss: 0.031011473387479782\n",
      "Epoch 4, Batch: 375: Training Loss: 0.033263787627220154, Validation Loss: 0.03143797069787979\n",
      "Epoch 4, Batch: 376: Training Loss: 0.033042315393686295, Validation Loss: 0.02791084162890911\n",
      "Epoch 4, Batch: 377: Training Loss: 0.030656320974230766, Validation Loss: 0.03145666420459747\n",
      "Epoch 4, Batch: 378: Training Loss: 0.027747726067900658, Validation Loss: 0.027907729148864746\n",
      "Epoch 4, Batch: 379: Training Loss: 0.02815265767276287, Validation Loss: 0.02849394641816616\n",
      "Epoch 4, Batch: 380: Training Loss: 0.027981504797935486, Validation Loss: 0.027774853631854057\n",
      "Epoch 4, Batch: 381: Training Loss: 0.029349742457270622, Validation Loss: 0.028333811089396477\n",
      "Epoch 4, Batch: 382: Training Loss: 0.02929692342877388, Validation Loss: 0.029747946187853813\n",
      "Epoch 4, Batch: 383: Training Loss: 0.026575621217489243, Validation Loss: 0.029843969270586967\n",
      "Epoch 4, Batch: 384: Training Loss: 0.028283195570111275, Validation Loss: 0.029913760721683502\n",
      "Epoch 4, Batch: 385: Training Loss: 0.028730949386954308, Validation Loss: 0.030786147341132164\n",
      "Epoch 4, Batch: 386: Training Loss: 0.026126587763428688, Validation Loss: 0.027038700878620148\n",
      "Epoch 4, Batch: 387: Training Loss: 0.02879299782216549, Validation Loss: 0.027201706543564796\n",
      "Epoch 4, Batch: 388: Training Loss: 0.026192722842097282, Validation Loss: 0.028386494144797325\n",
      "Epoch 4, Batch: 389: Training Loss: 0.02796575240790844, Validation Loss: 0.02853609248995781\n",
      "Epoch 4, Batch: 390: Training Loss: 0.03132273629307747, Validation Loss: 0.02903287671506405\n",
      "Epoch 4, Batch: 391: Training Loss: 0.028484076261520386, Validation Loss: 0.028649577870965004\n",
      "Epoch 4, Batch: 392: Training Loss: 0.03105989098548889, Validation Loss: 0.03127852454781532\n",
      "Epoch 4, Batch: 393: Training Loss: 0.028887255117297173, Validation Loss: 0.02622966840863228\n",
      "Epoch 4, Batch: 394: Training Loss: 0.027527745813131332, Validation Loss: 0.02629251778125763\n",
      "Epoch 4, Batch: 395: Training Loss: 0.02591678872704506, Validation Loss: 0.0289134718477726\n",
      "Epoch 4, Batch: 396: Training Loss: 0.027937909588217735, Validation Loss: 0.02910670079290867\n",
      "Epoch 4, Batch: 397: Training Loss: 0.035390306264162064, Validation Loss: 0.02753211185336113\n",
      "Epoch 4, Batch: 398: Training Loss: 0.026715286076068878, Validation Loss: 0.029331820085644722\n",
      "Epoch 4, Batch: 399: Training Loss: 0.03145008161664009, Validation Loss: 0.028570648282766342\n",
      "Epoch 4, Batch: 400: Training Loss: 0.028106166049838066, Validation Loss: 0.027608675882220268\n",
      "Epoch 4, Batch: 401: Training Loss: 0.025288311764597893, Validation Loss: 0.028919575735926628\n",
      "Epoch 4, Batch: 402: Training Loss: 0.02645806595683098, Validation Loss: 0.024630963802337646\n",
      "Epoch 4, Batch: 403: Training Loss: 0.030559280887246132, Validation Loss: 0.028608910739421844\n",
      "Epoch 4, Batch: 404: Training Loss: 0.02531636878848076, Validation Loss: 0.030156880617141724\n",
      "Epoch 4, Batch: 405: Training Loss: 0.02526598423719406, Validation Loss: 0.027193238958716393\n",
      "Epoch 4, Batch: 406: Training Loss: 0.030789878219366074, Validation Loss: 0.027561763301491737\n",
      "Epoch 4, Batch: 407: Training Loss: 0.023611346259713173, Validation Loss: 0.025794383138418198\n",
      "Epoch 4, Batch: 408: Training Loss: 0.022793050855398178, Validation Loss: 0.027473973110318184\n",
      "Epoch 4, Batch: 409: Training Loss: 0.02788769267499447, Validation Loss: 0.026777930557727814\n",
      "Epoch 4, Batch: 410: Training Loss: 0.027713719755411148, Validation Loss: 0.028006264939904213\n",
      "Epoch 4, Batch: 411: Training Loss: 0.02968944050371647, Validation Loss: 0.025825412943959236\n",
      "Epoch 4, Batch: 412: Training Loss: 0.03614510968327522, Validation Loss: 0.02673623152077198\n",
      "Epoch 4, Batch: 413: Training Loss: 0.03147812932729721, Validation Loss: 0.025791041553020477\n",
      "Epoch 4, Batch: 414: Training Loss: 0.025352543219923973, Validation Loss: 0.026914872229099274\n",
      "Epoch 4, Batch: 415: Training Loss: 0.027229351922869682, Validation Loss: 0.025895994156599045\n",
      "Epoch 4, Batch: 416: Training Loss: 0.029255744069814682, Validation Loss: 0.028431227430701256\n",
      "Epoch 4, Batch: 417: Training Loss: 0.0273800827562809, Validation Loss: 0.029374828562140465\n",
      "Epoch 4, Batch: 418: Training Loss: 0.024334684014320374, Validation Loss: 0.03109612502157688\n",
      "Epoch 4, Batch: 419: Training Loss: 0.02619733288884163, Validation Loss: 0.030762366950511932\n",
      "Epoch 4, Batch: 420: Training Loss: 0.030808687210083008, Validation Loss: 0.027989327907562256\n",
      "Epoch 4, Batch: 421: Training Loss: 0.039539992809295654, Validation Loss: 0.029498416930437088\n",
      "Epoch 4, Batch: 422: Training Loss: 0.03500567749142647, Validation Loss: 0.02776291035115719\n",
      "Epoch 4, Batch: 423: Training Loss: 0.03162798285484314, Validation Loss: 0.02681455761194229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch: 424: Training Loss: 0.03180322051048279, Validation Loss: 0.0299055278301239\n",
      "Epoch 4, Batch: 425: Training Loss: 0.023340750485658646, Validation Loss: 0.027907254174351692\n",
      "Epoch 4, Batch: 426: Training Loss: 0.02921571582555771, Validation Loss: 0.026656661182641983\n",
      "Epoch 4, Batch: 427: Training Loss: 0.02964385226368904, Validation Loss: 0.027623357251286507\n",
      "Epoch 4, Batch: 428: Training Loss: 0.028193894773721695, Validation Loss: 0.02766421064734459\n",
      "Epoch 4, Batch: 429: Training Loss: 0.025989916175603867, Validation Loss: 0.030188994482159615\n",
      "Epoch 4, Batch: 430: Training Loss: 0.025675062090158463, Validation Loss: 0.030644819140434265\n",
      "Epoch 4, Batch: 431: Training Loss: 0.031127354130148888, Validation Loss: 0.027495043352246284\n",
      "Epoch 4, Batch: 432: Training Loss: 0.03147682920098305, Validation Loss: 0.02628396265208721\n",
      "Epoch 4, Batch: 433: Training Loss: 0.029637053608894348, Validation Loss: 0.0279195848852396\n",
      "Epoch 4, Batch: 434: Training Loss: 0.02975751832127571, Validation Loss: 0.026783259585499763\n",
      "Epoch 4, Batch: 435: Training Loss: 0.026964480057358742, Validation Loss: 0.02762610837817192\n",
      "Epoch 4, Batch: 436: Training Loss: 0.025283224880695343, Validation Loss: 0.029033049941062927\n",
      "Epoch 4, Batch: 437: Training Loss: 0.02434750832617283, Validation Loss: 0.026883887127041817\n",
      "Epoch 4, Batch: 438: Training Loss: 0.028118668124079704, Validation Loss: 0.025459861382842064\n",
      "Epoch 4, Batch: 439: Training Loss: 0.029203593730926514, Validation Loss: 0.029130449518561363\n",
      "Epoch 4, Batch: 440: Training Loss: 0.03181643411517143, Validation Loss: 0.02824394404888153\n",
      "Epoch 4, Batch: 441: Training Loss: 0.030815834179520607, Validation Loss: 0.028037723153829575\n",
      "Epoch 4, Batch: 442: Training Loss: 0.03190700709819794, Validation Loss: 0.025513065978884697\n",
      "Epoch 4, Batch: 443: Training Loss: 0.029422888532280922, Validation Loss: 0.027750272303819656\n",
      "Epoch 4, Batch: 444: Training Loss: 0.029551230370998383, Validation Loss: 0.024357058107852936\n",
      "Epoch 4, Batch: 445: Training Loss: 0.02702525444328785, Validation Loss: 0.02899324521422386\n",
      "Epoch 4, Batch: 446: Training Loss: 0.033355358988046646, Validation Loss: 0.02661743201315403\n",
      "Epoch 4, Batch: 447: Training Loss: 0.028507430106401443, Validation Loss: 0.025475535541772842\n",
      "Epoch 4, Batch: 448: Training Loss: 0.02918819896876812, Validation Loss: 0.02774973213672638\n",
      "Epoch 4, Batch: 449: Training Loss: 0.025980787351727486, Validation Loss: 0.027084097266197205\n",
      "Epoch 4, Batch: 450: Training Loss: 0.024862870573997498, Validation Loss: 0.02978760004043579\n",
      "Epoch 4, Batch: 451: Training Loss: 0.02759191021323204, Validation Loss: 0.030209647491574287\n",
      "Epoch 4, Batch: 452: Training Loss: 0.033948928117752075, Validation Loss: 0.027600206434726715\n",
      "Epoch 4, Batch: 453: Training Loss: 0.02987375669181347, Validation Loss: 0.027824679389595985\n",
      "Epoch 4, Batch: 454: Training Loss: 0.02810092270374298, Validation Loss: 0.030030759051442146\n",
      "Epoch 4, Batch: 455: Training Loss: 0.02560598962008953, Validation Loss: 0.027000512927770615\n",
      "Epoch 4, Batch: 456: Training Loss: 0.025193287059664726, Validation Loss: 0.027937989681959152\n",
      "Epoch 4, Batch: 457: Training Loss: 0.03274178504943848, Validation Loss: 0.027618318796157837\n",
      "Epoch 4, Batch: 458: Training Loss: 0.02601686865091324, Validation Loss: 0.026752520352602005\n",
      "Epoch 4, Batch: 459: Training Loss: 0.026511995121836662, Validation Loss: 0.028423888608813286\n",
      "Epoch 4, Batch: 460: Training Loss: 0.02929757721722126, Validation Loss: 0.026437873020768166\n",
      "Epoch 4, Batch: 461: Training Loss: 0.024506086483597755, Validation Loss: 0.027963828295469284\n",
      "Epoch 4, Batch: 462: Training Loss: 0.024302469566464424, Validation Loss: 0.02911396138370037\n",
      "Epoch 4, Batch: 463: Training Loss: 0.03181740641593933, Validation Loss: 0.029922625049948692\n",
      "Epoch 4, Batch: 464: Training Loss: 0.025001108646392822, Validation Loss: 0.02894730679690838\n",
      "Epoch 4, Batch: 465: Training Loss: 0.031231477856636047, Validation Loss: 0.02601734735071659\n",
      "Epoch 4, Batch: 466: Training Loss: 0.024355793371796608, Validation Loss: 0.027723632752895355\n",
      "Epoch 4, Batch: 467: Training Loss: 0.03685038164258003, Validation Loss: 0.02822459675371647\n",
      "Epoch 4, Batch: 468: Training Loss: 0.030186673626303673, Validation Loss: 0.0274392981082201\n",
      "Epoch 4, Batch: 469: Training Loss: 0.028895413503050804, Validation Loss: 0.027509493753314018\n",
      "Epoch 4, Batch: 470: Training Loss: 0.02582065388560295, Validation Loss: 0.029223239049315453\n",
      "Epoch 4, Batch: 471: Training Loss: 0.030580682680010796, Validation Loss: 0.031201673671603203\n",
      "Epoch 4, Batch: 472: Training Loss: 0.03258361294865608, Validation Loss: 0.026307720690965652\n",
      "Epoch 4, Batch: 473: Training Loss: 0.025002779439091682, Validation Loss: 0.02710404247045517\n",
      "Epoch 4, Batch: 474: Training Loss: 0.028190432116389275, Validation Loss: 0.028034726157784462\n",
      "Epoch 4, Batch: 475: Training Loss: 0.030689988285303116, Validation Loss: 0.027811285108327866\n",
      "Epoch 4, Batch: 476: Training Loss: 0.028985198587179184, Validation Loss: 0.029564635828137398\n",
      "Epoch 4, Batch: 477: Training Loss: 0.034781210124492645, Validation Loss: 0.025987805798649788\n",
      "Epoch 4, Batch: 478: Training Loss: 0.027950365096330643, Validation Loss: 0.026680173352360725\n",
      "Epoch 4, Batch: 479: Training Loss: 0.027836501598358154, Validation Loss: 0.025604212656617165\n",
      "Epoch 4, Batch: 480: Training Loss: 0.024648573249578476, Validation Loss: 0.028110571205615997\n",
      "Epoch 4, Batch: 481: Training Loss: 0.026498859748244286, Validation Loss: 0.026797033846378326\n",
      "Epoch 4, Batch: 482: Training Loss: 0.028903350234031677, Validation Loss: 0.0253925658762455\n",
      "Epoch 4, Batch: 483: Training Loss: 0.02821970172226429, Validation Loss: 0.025270910933613777\n",
      "Epoch 4, Batch: 484: Training Loss: 0.027952374890446663, Validation Loss: 0.02775155007839203\n",
      "Epoch 4, Batch: 485: Training Loss: 0.026637304574251175, Validation Loss: 0.02938907779753208\n",
      "Epoch 4, Batch: 486: Training Loss: 0.031176095828413963, Validation Loss: 0.026597894728183746\n",
      "Epoch 4, Batch: 487: Training Loss: 0.029805613681674004, Validation Loss: 0.0280714463442564\n",
      "Epoch 4, Batch: 488: Training Loss: 0.029749678447842598, Validation Loss: 0.026686104014515877\n",
      "Epoch 4, Batch: 489: Training Loss: 0.030521074309945107, Validation Loss: 0.027748998254537582\n",
      "Epoch 4, Batch: 490: Training Loss: 0.03120771236717701, Validation Loss: 0.025865137577056885\n",
      "Epoch 4, Batch: 491: Training Loss: 0.02457323856651783, Validation Loss: 0.030737344175577164\n",
      "Epoch 4, Batch: 492: Training Loss: 0.02898934856057167, Validation Loss: 0.02726302295923233\n",
      "Epoch 4, Batch: 493: Training Loss: 0.030096029862761497, Validation Loss: 0.02712753601372242\n",
      "Epoch 4, Batch: 494: Training Loss: 0.033605001866817474, Validation Loss: 0.027148673310875893\n",
      "Epoch 4, Batch: 495: Training Loss: 0.028540395200252533, Validation Loss: 0.031037641689181328\n",
      "Epoch 4, Batch: 496: Training Loss: 0.02556607499718666, Validation Loss: 0.027833562344312668\n",
      "Epoch 4, Batch: 497: Training Loss: 0.02752825990319252, Validation Loss: 0.027567777782678604\n",
      "Epoch 4, Batch: 498: Training Loss: 0.027371149510145187, Validation Loss: 0.026198893785476685\n",
      "Epoch 4, Batch: 499: Training Loss: 0.028755169361829758, Validation Loss: 0.025930875912308693\n",
      "Epoch 5, Batch: 0: Training Loss: 0.027933845296502113, Validation Loss: 0.024104442447423935\n",
      "Epoch 5, Batch: 1: Training Loss: 0.029657406732439995, Validation Loss: 0.026275552809238434\n",
      "Epoch 5, Batch: 2: Training Loss: 0.02902311272919178, Validation Loss: 0.02910270169377327\n",
      "Epoch 5, Batch: 3: Training Loss: 0.02725001610815525, Validation Loss: 0.02477896399796009\n",
      "Epoch 5, Batch: 4: Training Loss: 0.02503965236246586, Validation Loss: 0.027550742030143738\n",
      "Epoch 5, Batch: 5: Training Loss: 0.0251074880361557, Validation Loss: 0.0257936492562294\n",
      "Epoch 5, Batch: 6: Training Loss: 0.031231580302119255, Validation Loss: 0.025212913751602173\n",
      "Epoch 5, Batch: 7: Training Loss: 0.02337781898677349, Validation Loss: 0.02935878001153469\n",
      "Epoch 5, Batch: 8: Training Loss: 0.028999101370573044, Validation Loss: 0.028392210602760315\n",
      "Epoch 5, Batch: 9: Training Loss: 0.026610922068357468, Validation Loss: 0.026897352188825607\n",
      "Epoch 5, Batch: 10: Training Loss: 0.03052082099020481, Validation Loss: 0.029015032574534416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 11: Training Loss: 0.03534498065710068, Validation Loss: 0.02705034613609314\n",
      "Epoch 5, Batch: 12: Training Loss: 0.02880152128636837, Validation Loss: 0.0266132690012455\n",
      "Epoch 5, Batch: 13: Training Loss: 0.03131139278411865, Validation Loss: 0.029411807656288147\n",
      "Epoch 5, Batch: 14: Training Loss: 0.0306899044662714, Validation Loss: 0.027454305440187454\n",
      "Epoch 5, Batch: 15: Training Loss: 0.029443714767694473, Validation Loss: 0.028041543439030647\n",
      "Epoch 5, Batch: 16: Training Loss: 0.02964121289551258, Validation Loss: 0.026977045461535454\n",
      "Epoch 5, Batch: 17: Training Loss: 0.02516012452542782, Validation Loss: 0.02767043747007847\n",
      "Epoch 5, Batch: 18: Training Loss: 0.02892514131963253, Validation Loss: 0.028181608766317368\n",
      "Epoch 5, Batch: 19: Training Loss: 0.025835145264863968, Validation Loss: 0.028364861384034157\n",
      "Epoch 5, Batch: 20: Training Loss: 0.031117716804146767, Validation Loss: 0.029744477942585945\n",
      "Epoch 5, Batch: 21: Training Loss: 0.029284073039889336, Validation Loss: 0.028992151841521263\n",
      "Epoch 5, Batch: 22: Training Loss: 0.031920284032821655, Validation Loss: 0.02995982952415943\n",
      "Epoch 5, Batch: 23: Training Loss: 0.029450856149196625, Validation Loss: 0.029681555926799774\n",
      "Epoch 5, Batch: 24: Training Loss: 0.026233814656734467, Validation Loss: 0.030810771510004997\n",
      "Epoch 5, Batch: 25: Training Loss: 0.026529107242822647, Validation Loss: 0.02924640290439129\n",
      "Epoch 5, Batch: 26: Training Loss: 0.030655227601528168, Validation Loss: 0.027905169874429703\n",
      "Epoch 5, Batch: 27: Training Loss: 0.027628879994153976, Validation Loss: 0.028053831309080124\n",
      "Epoch 5, Batch: 28: Training Loss: 0.029631448909640312, Validation Loss: 0.02834092080593109\n",
      "Epoch 5, Batch: 29: Training Loss: 0.02734561823308468, Validation Loss: 0.02499828301370144\n",
      "Epoch 5, Batch: 30: Training Loss: 0.02291959896683693, Validation Loss: 0.029618393629789352\n",
      "Epoch 5, Batch: 31: Training Loss: 0.02980712614953518, Validation Loss: 0.02845102548599243\n",
      "Epoch 5, Batch: 32: Training Loss: 0.03158904239535332, Validation Loss: 0.028904760256409645\n",
      "Epoch 5, Batch: 33: Training Loss: 0.02825135365128517, Validation Loss: 0.02945047803223133\n",
      "Epoch 5, Batch: 34: Training Loss: 0.02532530575990677, Validation Loss: 0.029089486226439476\n",
      "Epoch 5, Batch: 35: Training Loss: 0.02712935023009777, Validation Loss: 0.029641173779964447\n",
      "Epoch 5, Batch: 36: Training Loss: 0.02639644779264927, Validation Loss: 0.027121517807245255\n",
      "Epoch 5, Batch: 37: Training Loss: 0.025526657700538635, Validation Loss: 0.02839573472738266\n",
      "Epoch 5, Batch: 38: Training Loss: 0.03046930581331253, Validation Loss: 0.028646301478147507\n",
      "Epoch 5, Batch: 39: Training Loss: 0.03171127289533615, Validation Loss: 0.030087079852819443\n",
      "Epoch 5, Batch: 40: Training Loss: 0.03067891299724579, Validation Loss: 0.02496635913848877\n",
      "Epoch 5, Batch: 41: Training Loss: 0.028995933011174202, Validation Loss: 0.027808308601379395\n",
      "Epoch 5, Batch: 42: Training Loss: 0.030836429446935654, Validation Loss: 0.02721727266907692\n",
      "Epoch 5, Batch: 43: Training Loss: 0.02782685123383999, Validation Loss: 0.027058133855462074\n",
      "Epoch 5, Batch: 44: Training Loss: 0.029254844412207603, Validation Loss: 0.029754165560007095\n",
      "Epoch 5, Batch: 45: Training Loss: 0.02736537903547287, Validation Loss: 0.02900034561753273\n",
      "Epoch 5, Batch: 46: Training Loss: 0.02542521432042122, Validation Loss: 0.029370160773396492\n",
      "Epoch 5, Batch: 47: Training Loss: 0.030630553141236305, Validation Loss: 0.027667736634612083\n",
      "Epoch 5, Batch: 48: Training Loss: 0.03240882232785225, Validation Loss: 0.029859574511647224\n",
      "Epoch 5, Batch: 49: Training Loss: 0.030135301873087883, Validation Loss: 0.03153371065855026\n",
      "Epoch 5, Batch: 50: Training Loss: 0.02874877117574215, Validation Loss: 0.02904348075389862\n",
      "Epoch 5, Batch: 51: Training Loss: 0.02800934948027134, Validation Loss: 0.028486819937825203\n",
      "Epoch 5, Batch: 52: Training Loss: 0.0284489244222641, Validation Loss: 0.030509095638990402\n",
      "Epoch 5, Batch: 53: Training Loss: 0.025333933532238007, Validation Loss: 0.032030560076236725\n",
      "Epoch 5, Batch: 54: Training Loss: 0.028741000220179558, Validation Loss: 0.030995508655905724\n",
      "Epoch 5, Batch: 55: Training Loss: 0.02885127253830433, Validation Loss: 0.028672898188233376\n",
      "Epoch 5, Batch: 56: Training Loss: 0.02682649902999401, Validation Loss: 0.032617632299661636\n",
      "Epoch 5, Batch: 57: Training Loss: 0.03060114197432995, Validation Loss: 0.02864193171262741\n",
      "Epoch 5, Batch: 58: Training Loss: 0.027367975562810898, Validation Loss: 0.02972509153187275\n",
      "Epoch 5, Batch: 59: Training Loss: 0.026919091120362282, Validation Loss: 0.02931877225637436\n",
      "Epoch 5, Batch: 60: Training Loss: 0.029233869165182114, Validation Loss: 0.028580252081155777\n",
      "Epoch 5, Batch: 61: Training Loss: 0.03549730405211449, Validation Loss: 0.031905535608530045\n",
      "Epoch 5, Batch: 62: Training Loss: 0.027982914820313454, Validation Loss: 0.030597737058997154\n",
      "Epoch 5, Batch: 63: Training Loss: 0.030390005558729172, Validation Loss: 0.03143981471657753\n",
      "Epoch 5, Batch: 64: Training Loss: 0.02726662904024124, Validation Loss: 0.029752608388662338\n",
      "Epoch 5, Batch: 65: Training Loss: 0.0315127968788147, Validation Loss: 0.029293367639183998\n",
      "Epoch 5, Batch: 66: Training Loss: 0.035156138241291046, Validation Loss: 0.03423326089978218\n",
      "Epoch 5, Batch: 67: Training Loss: 0.03003726713359356, Validation Loss: 0.03326853737235069\n",
      "Epoch 5, Batch: 68: Training Loss: 0.02629680559039116, Validation Loss: 0.031995195895433426\n",
      "Epoch 5, Batch: 69: Training Loss: 0.028129687532782555, Validation Loss: 0.03083900362253189\n",
      "Epoch 5, Batch: 70: Training Loss: 0.028040703386068344, Validation Loss: 0.031455591320991516\n",
      "Epoch 5, Batch: 71: Training Loss: 0.02780991420149803, Validation Loss: 0.029890744015574455\n",
      "Epoch 5, Batch: 72: Training Loss: 0.02756446599960327, Validation Loss: 0.028548652306199074\n",
      "Epoch 5, Batch: 73: Training Loss: 0.030881203711032867, Validation Loss: 0.0282583087682724\n",
      "Epoch 5, Batch: 74: Training Loss: 0.026582051068544388, Validation Loss: 0.029146533459424973\n",
      "Epoch 5, Batch: 75: Training Loss: 0.023463459685444832, Validation Loss: 0.027515355497598648\n",
      "Epoch 5, Batch: 76: Training Loss: 0.028662782162427902, Validation Loss: 0.02474968135356903\n",
      "Epoch 5, Batch: 77: Training Loss: 0.03263844549655914, Validation Loss: 0.02552688680589199\n",
      "Epoch 5, Batch: 78: Training Loss: 0.0308152437210083, Validation Loss: 0.02458648569881916\n",
      "Epoch 5, Batch: 79: Training Loss: 0.026668528094887733, Validation Loss: 0.026670077815651894\n",
      "Epoch 5, Batch: 80: Training Loss: 0.025582440197467804, Validation Loss: 0.027289150282740593\n",
      "Epoch 5, Batch: 81: Training Loss: 0.029646724462509155, Validation Loss: 0.02881680242717266\n",
      "Epoch 5, Batch: 82: Training Loss: 0.031106870621442795, Validation Loss: 0.027724681422114372\n",
      "Epoch 5, Batch: 83: Training Loss: 0.027704467996954918, Validation Loss: 0.029065661132335663\n",
      "Epoch 5, Batch: 84: Training Loss: 0.026208054274320602, Validation Loss: 0.027514053508639336\n",
      "Epoch 5, Batch: 85: Training Loss: 0.025706851854920387, Validation Loss: 0.02862105891108513\n",
      "Epoch 5, Batch: 86: Training Loss: 0.02879071980714798, Validation Loss: 0.027995426207780838\n",
      "Epoch 5, Batch: 87: Training Loss: 0.02894454635679722, Validation Loss: 0.02587738446891308\n",
      "Epoch 5, Batch: 88: Training Loss: 0.029608825221657753, Validation Loss: 0.028821909800171852\n",
      "Epoch 5, Batch: 89: Training Loss: 0.03361940756440163, Validation Loss: 0.02918952889740467\n",
      "Epoch 5, Batch: 90: Training Loss: 0.029874758794903755, Validation Loss: 0.02709430828690529\n",
      "Epoch 5, Batch: 91: Training Loss: 0.02673041820526123, Validation Loss: 0.0302332304418087\n",
      "Epoch 5, Batch: 92: Training Loss: 0.03467825800180435, Validation Loss: 0.027158493176102638\n",
      "Epoch 5, Batch: 93: Training Loss: 0.027994342148303986, Validation Loss: 0.029563745483756065\n",
      "Epoch 5, Batch: 94: Training Loss: 0.031814079731702805, Validation Loss: 0.02772245556116104\n",
      "Epoch 5, Batch: 95: Training Loss: 0.025326648727059364, Validation Loss: 0.02516733668744564\n",
      "Epoch 5, Batch: 96: Training Loss: 0.025406477972865105, Validation Loss: 0.028029948472976685\n",
      "Epoch 5, Batch: 97: Training Loss: 0.028894107788801193, Validation Loss: 0.026895513758063316\n",
      "Epoch 5, Batch: 98: Training Loss: 0.030009645968675613, Validation Loss: 0.0261683389544487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 99: Training Loss: 0.028251899406313896, Validation Loss: 0.028395697474479675\n",
      "Epoch 5, Batch: 100: Training Loss: 0.0263522956520319, Validation Loss: 0.028152329847216606\n",
      "Epoch 5, Batch: 101: Training Loss: 0.027589231729507446, Validation Loss: 0.028086911886930466\n",
      "Epoch 5, Batch: 102: Training Loss: 0.027869906276464462, Validation Loss: 0.027018669992685318\n",
      "Epoch 5, Batch: 103: Training Loss: 0.032849784940481186, Validation Loss: 0.027419406920671463\n",
      "Epoch 5, Batch: 104: Training Loss: 0.023197602480649948, Validation Loss: 0.026335934177041054\n",
      "Epoch 5, Batch: 105: Training Loss: 0.028363008052110672, Validation Loss: 0.02709444984793663\n",
      "Epoch 5, Batch: 106: Training Loss: 0.029841551557183266, Validation Loss: 0.026164397597312927\n",
      "Epoch 5, Batch: 107: Training Loss: 0.029876209795475006, Validation Loss: 0.026096666231751442\n",
      "Epoch 5, Batch: 108: Training Loss: 0.027053164318203926, Validation Loss: 0.025639750063419342\n",
      "Epoch 5, Batch: 109: Training Loss: 0.03214701637625694, Validation Loss: 0.025677278637886047\n",
      "Epoch 5, Batch: 110: Training Loss: 0.03739309310913086, Validation Loss: 0.02579505741596222\n",
      "Epoch 5, Batch: 111: Training Loss: 0.02837979793548584, Validation Loss: 0.028110895305871964\n",
      "Epoch 5, Batch: 112: Training Loss: 0.023444192484021187, Validation Loss: 0.02757244184613228\n",
      "Epoch 5, Batch: 113: Training Loss: 0.026606906205415726, Validation Loss: 0.025632759556174278\n",
      "Epoch 5, Batch: 114: Training Loss: 0.02987651899456978, Validation Loss: 0.02937977947294712\n",
      "Epoch 5, Batch: 115: Training Loss: 0.029102113097906113, Validation Loss: 0.027626866474747658\n",
      "Epoch 5, Batch: 116: Training Loss: 0.025442998856306076, Validation Loss: 0.031142504885792732\n",
      "Epoch 5, Batch: 117: Training Loss: 0.02663060463964939, Validation Loss: 0.027455031871795654\n",
      "Epoch 5, Batch: 118: Training Loss: 0.027054551988840103, Validation Loss: 0.028669113293290138\n",
      "Epoch 5, Batch: 119: Training Loss: 0.02626327984035015, Validation Loss: 0.027627693489193916\n",
      "Epoch 5, Batch: 120: Training Loss: 0.025017116218805313, Validation Loss: 0.03039192035794258\n",
      "Epoch 5, Batch: 121: Training Loss: 0.028977608308196068, Validation Loss: 0.030783673748373985\n",
      "Epoch 5, Batch: 122: Training Loss: 0.030748728662729263, Validation Loss: 0.03148041293025017\n",
      "Epoch 5, Batch: 123: Training Loss: 0.031009890139102936, Validation Loss: 0.028094537556171417\n",
      "Epoch 5, Batch: 124: Training Loss: 0.02838483825325966, Validation Loss: 0.029331717640161514\n",
      "Epoch 5, Batch: 125: Training Loss: 0.02571517787873745, Validation Loss: 0.029652027413249016\n",
      "Epoch 5, Batch: 126: Training Loss: 0.02622983418405056, Validation Loss: 0.029155561700463295\n",
      "Epoch 5, Batch: 127: Training Loss: 0.027940349653363228, Validation Loss: 0.02942495048046112\n",
      "Epoch 5, Batch: 128: Training Loss: 0.026626218110322952, Validation Loss: 0.02971496246755123\n",
      "Epoch 5, Batch: 129: Training Loss: 0.024648724123835564, Validation Loss: 0.029633281752467155\n",
      "Epoch 5, Batch: 130: Training Loss: 0.02535562589764595, Validation Loss: 0.02831951156258583\n",
      "Epoch 5, Batch: 131: Training Loss: 0.028625069186091423, Validation Loss: 0.026445675641298294\n",
      "Epoch 5, Batch: 132: Training Loss: 0.028437023982405663, Validation Loss: 0.02735494263470173\n",
      "Epoch 5, Batch: 133: Training Loss: 0.028718376532197, Validation Loss: 0.028568247333168983\n",
      "Epoch 5, Batch: 134: Training Loss: 0.030378956347703934, Validation Loss: 0.027266914024949074\n",
      "Epoch 5, Batch: 135: Training Loss: 0.028663886711001396, Validation Loss: 0.028238333761692047\n",
      "Epoch 5, Batch: 136: Training Loss: 0.02555944211781025, Validation Loss: 0.029247699305415154\n",
      "Epoch 5, Batch: 137: Training Loss: 0.026053883135318756, Validation Loss: 0.023846367374062538\n",
      "Epoch 5, Batch: 138: Training Loss: 0.026470007374882698, Validation Loss: 0.028188014402985573\n",
      "Epoch 5, Batch: 139: Training Loss: 0.02585344761610031, Validation Loss: 0.026930497959256172\n",
      "Epoch 5, Batch: 140: Training Loss: 0.029271747916936874, Validation Loss: 0.029243910685181618\n",
      "Epoch 5, Batch: 141: Training Loss: 0.027137937024235725, Validation Loss: 0.029904097318649292\n",
      "Epoch 5, Batch: 142: Training Loss: 0.03083745390176773, Validation Loss: 0.026888974010944366\n",
      "Epoch 5, Batch: 143: Training Loss: 0.028234312310814857, Validation Loss: 0.02974475733935833\n",
      "Epoch 5, Batch: 144: Training Loss: 0.02960442192852497, Validation Loss: 0.028532395139336586\n",
      "Epoch 5, Batch: 145: Training Loss: 0.024983447045087814, Validation Loss: 0.027407996356487274\n",
      "Epoch 5, Batch: 146: Training Loss: 0.026542803272604942, Validation Loss: 0.02760978229343891\n",
      "Epoch 5, Batch: 147: Training Loss: 0.028189511969685555, Validation Loss: 0.028463976457715034\n",
      "Epoch 5, Batch: 148: Training Loss: 0.03062318079173565, Validation Loss: 0.02932088077068329\n",
      "Epoch 5, Batch: 149: Training Loss: 0.02274736389517784, Validation Loss: 0.0315023772418499\n",
      "Epoch 5, Batch: 150: Training Loss: 0.028582584112882614, Validation Loss: 0.03145715221762657\n",
      "Epoch 5, Batch: 151: Training Loss: 0.028488079085946083, Validation Loss: 0.02734682708978653\n",
      "Epoch 5, Batch: 152: Training Loss: 0.03186351805925369, Validation Loss: 0.029885046184062958\n",
      "Epoch 5, Batch: 153: Training Loss: 0.02940111607313156, Validation Loss: 0.027822401374578476\n",
      "Epoch 5, Batch: 154: Training Loss: 0.025575019419193268, Validation Loss: 0.028035342693328857\n",
      "Epoch 5, Batch: 155: Training Loss: 0.0354602113366127, Validation Loss: 0.028955131769180298\n",
      "Epoch 5, Batch: 156: Training Loss: 0.029857711866497993, Validation Loss: 0.03019438497722149\n",
      "Epoch 5, Batch: 157: Training Loss: 0.02279668301343918, Validation Loss: 0.03021027334034443\n",
      "Epoch 5, Batch: 158: Training Loss: 0.028287168592214584, Validation Loss: 0.029748741537332535\n",
      "Epoch 5, Batch: 159: Training Loss: 0.029115259647369385, Validation Loss: 0.028528062626719475\n",
      "Epoch 5, Batch: 160: Training Loss: 0.02691972814500332, Validation Loss: 0.026368914172053337\n",
      "Epoch 5, Batch: 161: Training Loss: 0.031531769782304764, Validation Loss: 0.029225170612335205\n",
      "Epoch 5, Batch: 162: Training Loss: 0.027916476130485535, Validation Loss: 0.029675453901290894\n",
      "Epoch 5, Batch: 163: Training Loss: 0.025997407734394073, Validation Loss: 0.03127283230423927\n",
      "Epoch 5, Batch: 164: Training Loss: 0.02736826054751873, Validation Loss: 0.030940739437937737\n",
      "Epoch 5, Batch: 165: Training Loss: 0.029619621112942696, Validation Loss: 0.02989298477768898\n",
      "Epoch 5, Batch: 166: Training Loss: 0.02661246433854103, Validation Loss: 0.02926572412252426\n",
      "Epoch 5, Batch: 167: Training Loss: 0.02624347247183323, Validation Loss: 0.028524132445454597\n",
      "Epoch 5, Batch: 168: Training Loss: 0.02566852793097496, Validation Loss: 0.0266315508633852\n",
      "Epoch 5, Batch: 169: Training Loss: 0.0321652889251709, Validation Loss: 0.028720609843730927\n",
      "Epoch 5, Batch: 170: Training Loss: 0.026855625212192535, Validation Loss: 0.028960494324564934\n",
      "Epoch 5, Batch: 171: Training Loss: 0.02670256979763508, Validation Loss: 0.028576286509633064\n",
      "Epoch 5, Batch: 172: Training Loss: 0.025366121903061867, Validation Loss: 0.02815133146941662\n",
      "Epoch 5, Batch: 173: Training Loss: 0.02713876962661743, Validation Loss: 0.030522963032126427\n",
      "Epoch 5, Batch: 174: Training Loss: 0.028251197189092636, Validation Loss: 0.03149056062102318\n",
      "Epoch 5, Batch: 175: Training Loss: 0.028442298993468285, Validation Loss: 0.029470182955265045\n",
      "Epoch 5, Batch: 176: Training Loss: 0.03340304270386696, Validation Loss: 0.032696157693862915\n",
      "Epoch 5, Batch: 177: Training Loss: 0.027956344187259674, Validation Loss: 0.029633505269885063\n",
      "Epoch 5, Batch: 178: Training Loss: 0.033034853637218475, Validation Loss: 0.03304086998105049\n",
      "Epoch 5, Batch: 179: Training Loss: 0.030570825561881065, Validation Loss: 0.03024829551577568\n",
      "Epoch 5, Batch: 180: Training Loss: 0.031192749738693237, Validation Loss: 0.030656930059194565\n",
      "Epoch 5, Batch: 181: Training Loss: 0.027819799259305, Validation Loss: 0.03219323232769966\n",
      "Epoch 5, Batch: 182: Training Loss: 0.03037603199481964, Validation Loss: 0.030029332265257835\n",
      "Epoch 5, Batch: 183: Training Loss: 0.028272537514567375, Validation Loss: 0.0316198356449604\n",
      "Epoch 5, Batch: 184: Training Loss: 0.03118881583213806, Validation Loss: 0.032372914254665375\n",
      "Epoch 5, Batch: 185: Training Loss: 0.03235837444663048, Validation Loss: 0.030673421919345856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 186: Training Loss: 0.02445320226252079, Validation Loss: 0.03330329433083534\n",
      "Epoch 5, Batch: 187: Training Loss: 0.025179585441946983, Validation Loss: 0.03235189616680145\n",
      "Epoch 5, Batch: 188: Training Loss: 0.03390521556138992, Validation Loss: 0.03088224306702614\n",
      "Epoch 5, Batch: 189: Training Loss: 0.025060925632715225, Validation Loss: 0.031299468129873276\n",
      "Epoch 5, Batch: 190: Training Loss: 0.028776757419109344, Validation Loss: 0.030744826421141624\n",
      "Epoch 5, Batch: 191: Training Loss: 0.03318660333752632, Validation Loss: 0.03392794728279114\n",
      "Epoch 5, Batch: 192: Training Loss: 0.02715448848903179, Validation Loss: 0.03160897642374039\n",
      "Epoch 5, Batch: 193: Training Loss: 0.03248227387666702, Validation Loss: 0.0328957624733448\n",
      "Epoch 5, Batch: 194: Training Loss: 0.03430432826280594, Validation Loss: 0.029124844819307327\n",
      "Epoch 5, Batch: 195: Training Loss: 0.030860116705298424, Validation Loss: 0.03030184842646122\n",
      "Epoch 5, Batch: 196: Training Loss: 0.028986284509301186, Validation Loss: 0.02912178635597229\n",
      "Epoch 5, Batch: 197: Training Loss: 0.026230299845337868, Validation Loss: 0.02858644165098667\n",
      "Epoch 5, Batch: 198: Training Loss: 0.027135618031024933, Validation Loss: 0.02995927818119526\n",
      "Epoch 5, Batch: 199: Training Loss: 0.027654647827148438, Validation Loss: 0.029268484562635422\n",
      "Epoch 5, Batch: 200: Training Loss: 0.025388743728399277, Validation Loss: 0.029866529628634453\n",
      "Epoch 5, Batch: 201: Training Loss: 0.03488817438483238, Validation Loss: 0.027610480785369873\n",
      "Epoch 5, Batch: 202: Training Loss: 0.037085454910993576, Validation Loss: 0.029671480879187584\n",
      "Epoch 5, Batch: 203: Training Loss: 0.03314441815018654, Validation Loss: 0.029257090762257576\n",
      "Epoch 5, Batch: 204: Training Loss: 0.032406728714704514, Validation Loss: 0.029599111527204514\n",
      "Epoch 5, Batch: 205: Training Loss: 0.035274896770715714, Validation Loss: 0.0319700762629509\n",
      "Epoch 5, Batch: 206: Training Loss: 0.028945671394467354, Validation Loss: 0.02865934744477272\n",
      "Epoch 5, Batch: 207: Training Loss: 0.033907536417245865, Validation Loss: 0.030542496591806412\n",
      "Epoch 5, Batch: 208: Training Loss: 0.028705215081572533, Validation Loss: 0.027729082852602005\n",
      "Epoch 5, Batch: 209: Training Loss: 0.03332307189702988, Validation Loss: 0.029076337814331055\n",
      "Epoch 5, Batch: 210: Training Loss: 0.03484971448779106, Validation Loss: 0.0318458117544651\n",
      "Epoch 5, Batch: 211: Training Loss: 0.03355195373296738, Validation Loss: 0.030037542805075645\n",
      "Epoch 5, Batch: 212: Training Loss: 0.02870909497141838, Validation Loss: 0.029388632625341415\n",
      "Epoch 5, Batch: 213: Training Loss: 0.03335293382406235, Validation Loss: 0.030643180012702942\n",
      "Epoch 5, Batch: 214: Training Loss: 0.026831474155187607, Validation Loss: 0.031105929985642433\n",
      "Epoch 5, Batch: 215: Training Loss: 0.030101193115115166, Validation Loss: 0.028840597718954086\n",
      "Epoch 5, Batch: 216: Training Loss: 0.030601803213357925, Validation Loss: 0.029535293579101562\n",
      "Epoch 5, Batch: 217: Training Loss: 0.02999071776866913, Validation Loss: 0.02636532299220562\n",
      "Epoch 5, Batch: 218: Training Loss: 0.026108354330062866, Validation Loss: 0.027973314747214317\n",
      "Epoch 5, Batch: 219: Training Loss: 0.028274428099393845, Validation Loss: 0.028105873614549637\n",
      "Epoch 5, Batch: 220: Training Loss: 0.028474701568484306, Validation Loss: 0.028041444718837738\n",
      "Epoch 5, Batch: 221: Training Loss: 0.026400642469525337, Validation Loss: 0.030502762645483017\n",
      "Epoch 5, Batch: 222: Training Loss: 0.02848265878856182, Validation Loss: 0.031646400690078735\n",
      "Epoch 5, Batch: 223: Training Loss: 0.029926996678113937, Validation Loss: 0.02830411307513714\n",
      "Epoch 5, Batch: 224: Training Loss: 0.03263894096016884, Validation Loss: 0.02855997532606125\n",
      "Epoch 5, Batch: 225: Training Loss: 0.03190602734684944, Validation Loss: 0.027005743235349655\n",
      "Epoch 5, Batch: 226: Training Loss: 0.0313117653131485, Validation Loss: 0.029537703841924667\n",
      "Epoch 5, Batch: 227: Training Loss: 0.026238590478897095, Validation Loss: 0.028361666947603226\n",
      "Epoch 5, Batch: 228: Training Loss: 0.030185429379343987, Validation Loss: 0.031401026993989944\n",
      "Epoch 5, Batch: 229: Training Loss: 0.02975056879222393, Validation Loss: 0.02542777732014656\n",
      "Epoch 5, Batch: 230: Training Loss: 0.028020981699228287, Validation Loss: 0.03055710159242153\n",
      "Epoch 5, Batch: 231: Training Loss: 0.03451123833656311, Validation Loss: 0.0299674179404974\n",
      "Epoch 5, Batch: 232: Training Loss: 0.031821999698877335, Validation Loss: 0.02831990085542202\n",
      "Epoch 5, Batch: 233: Training Loss: 0.024602798745036125, Validation Loss: 0.030112119391560555\n",
      "Epoch 5, Batch: 234: Training Loss: 0.031194044277071953, Validation Loss: 0.027967816218733788\n",
      "Epoch 5, Batch: 235: Training Loss: 0.03733725845813751, Validation Loss: 0.024315588176250458\n",
      "Epoch 5, Batch: 236: Training Loss: 0.03290686756372452, Validation Loss: 0.028808051720261574\n",
      "Epoch 5, Batch: 237: Training Loss: 0.02834833599627018, Validation Loss: 0.02677304670214653\n",
      "Epoch 5, Batch: 238: Training Loss: 0.033086344599723816, Validation Loss: 0.02644718997180462\n",
      "Epoch 5, Batch: 239: Training Loss: 0.030163515359163284, Validation Loss: 0.030987059697508812\n",
      "Epoch 5, Batch: 240: Training Loss: 0.03106451779603958, Validation Loss: 0.029360543936491013\n",
      "Epoch 5, Batch: 241: Training Loss: 0.030291728675365448, Validation Loss: 0.028560515493154526\n",
      "Epoch 5, Batch: 242: Training Loss: 0.027578257024288177, Validation Loss: 0.02997281216084957\n",
      "Epoch 5, Batch: 243: Training Loss: 0.036009471863508224, Validation Loss: 0.02687891572713852\n",
      "Epoch 5, Batch: 244: Training Loss: 0.03164203464984894, Validation Loss: 0.029889095574617386\n",
      "Epoch 5, Batch: 245: Training Loss: 0.03432294353842735, Validation Loss: 0.029254315420985222\n",
      "Epoch 5, Batch: 246: Training Loss: 0.02772456966340542, Validation Loss: 0.03056623972952366\n",
      "Epoch 5, Batch: 247: Training Loss: 0.028672516345977783, Validation Loss: 0.02773519977927208\n",
      "Epoch 5, Batch: 248: Training Loss: 0.030412454158067703, Validation Loss: 0.029405033215880394\n",
      "Epoch 5, Batch: 249: Training Loss: 0.02907373383641243, Validation Loss: 0.024996472522616386\n",
      "Epoch 5, Batch: 250: Training Loss: 0.02587718330323696, Validation Loss: 0.027360014617443085\n",
      "Epoch 5, Batch: 251: Training Loss: 0.02807367406785488, Validation Loss: 0.02892177179455757\n",
      "Epoch 5, Batch: 252: Training Loss: 0.028693892061710358, Validation Loss: 0.028759511187672615\n",
      "Epoch 5, Batch: 253: Training Loss: 0.032704468816518784, Validation Loss: 0.028382502496242523\n",
      "Epoch 5, Batch: 254: Training Loss: 0.025311389937996864, Validation Loss: 0.030767709016799927\n",
      "Epoch 5, Batch: 255: Training Loss: 0.025138311088085175, Validation Loss: 0.030981991440057755\n",
      "Epoch 5, Batch: 256: Training Loss: 0.030322158709168434, Validation Loss: 0.028292473405599594\n",
      "Epoch 5, Batch: 257: Training Loss: 0.03492289409041405, Validation Loss: 0.028220029547810555\n",
      "Epoch 5, Batch: 258: Training Loss: 0.028757942840456963, Validation Loss: 0.02792791649699211\n",
      "Epoch 5, Batch: 259: Training Loss: 0.02926740236580372, Validation Loss: 0.029450226575136185\n",
      "Epoch 5, Batch: 260: Training Loss: 0.03364425525069237, Validation Loss: 0.030014174059033394\n",
      "Epoch 5, Batch: 261: Training Loss: 0.029388999566435814, Validation Loss: 0.0295689906924963\n",
      "Epoch 5, Batch: 262: Training Loss: 0.03141024708747864, Validation Loss: 0.03121407888829708\n",
      "Epoch 5, Batch: 263: Training Loss: 0.03107391856610775, Validation Loss: 0.030796395614743233\n",
      "Epoch 5, Batch: 264: Training Loss: 0.027488689869642258, Validation Loss: 0.03256679326295853\n",
      "Epoch 5, Batch: 265: Training Loss: 0.03281765431165695, Validation Loss: 0.02837025187909603\n",
      "Epoch 5, Batch: 266: Training Loss: 0.030075930058956146, Validation Loss: 0.031369227916002274\n",
      "Epoch 5, Batch: 267: Training Loss: 0.027229592204093933, Validation Loss: 0.025924744084477425\n",
      "Epoch 5, Batch: 268: Training Loss: 0.02961045503616333, Validation Loss: 0.029741989448666573\n",
      "Epoch 5, Batch: 269: Training Loss: 0.03224517032504082, Validation Loss: 0.02825852856040001\n",
      "Epoch 5, Batch: 270: Training Loss: 0.027000587433576584, Validation Loss: 0.0287674218416214\n",
      "Epoch 5, Batch: 271: Training Loss: 0.025412365794181824, Validation Loss: 0.02974083088338375\n",
      "Epoch 5, Batch: 272: Training Loss: 0.028116049244999886, Validation Loss: 0.028756611049175262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 273: Training Loss: 0.025181859731674194, Validation Loss: 0.02801961451768875\n",
      "Epoch 5, Batch: 274: Training Loss: 0.030067915096879005, Validation Loss: 0.027529194951057434\n",
      "Epoch 5, Batch: 275: Training Loss: 0.025524063035845757, Validation Loss: 0.029771221801638603\n",
      "Epoch 5, Batch: 276: Training Loss: 0.029676983132958412, Validation Loss: 0.028440946713089943\n",
      "Epoch 5, Batch: 277: Training Loss: 0.025890816003084183, Validation Loss: 0.028703637421131134\n",
      "Epoch 5, Batch: 278: Training Loss: 0.02913387306034565, Validation Loss: 0.02881891280412674\n",
      "Epoch 5, Batch: 279: Training Loss: 0.027443693950772285, Validation Loss: 0.030028941109776497\n",
      "Epoch 5, Batch: 280: Training Loss: 0.02599334716796875, Validation Loss: 0.03067939355969429\n",
      "Epoch 5, Batch: 281: Training Loss: 0.029154539108276367, Validation Loss: 0.03158700466156006\n",
      "Epoch 5, Batch: 282: Training Loss: 0.027928147464990616, Validation Loss: 0.031304530799388885\n",
      "Epoch 5, Batch: 283: Training Loss: 0.026823924854397774, Validation Loss: 0.02900247648358345\n",
      "Epoch 5, Batch: 284: Training Loss: 0.0294809527695179, Validation Loss: 0.031040403991937637\n",
      "Epoch 5, Batch: 285: Training Loss: 0.03400319442152977, Validation Loss: 0.030178917571902275\n",
      "Epoch 5, Batch: 286: Training Loss: 0.02609516866505146, Validation Loss: 0.030892690643668175\n",
      "Epoch 5, Batch: 287: Training Loss: 0.028546616435050964, Validation Loss: 0.028384730219841003\n",
      "Epoch 5, Batch: 288: Training Loss: 0.029644925147294998, Validation Loss: 0.028636017814278603\n",
      "Epoch 5, Batch: 289: Training Loss: 0.033246058970689774, Validation Loss: 0.029342541471123695\n",
      "Epoch 5, Batch: 290: Training Loss: 0.027460983023047447, Validation Loss: 0.02979874610900879\n",
      "Epoch 5, Batch: 291: Training Loss: 0.030732059851288795, Validation Loss: 0.025853315368294716\n",
      "Epoch 5, Batch: 292: Training Loss: 0.028093624860048294, Validation Loss: 0.03099137172102928\n",
      "Epoch 5, Batch: 293: Training Loss: 0.028478870168328285, Validation Loss: 0.03374447673559189\n",
      "Epoch 5, Batch: 294: Training Loss: 0.02818853035569191, Validation Loss: 0.028808891773223877\n",
      "Epoch 5, Batch: 295: Training Loss: 0.027499085292220116, Validation Loss: 0.02826612815260887\n",
      "Epoch 5, Batch: 296: Training Loss: 0.02854066528379917, Validation Loss: 0.02986397035419941\n",
      "Epoch 5, Batch: 297: Training Loss: 0.029049666598439217, Validation Loss: 0.027276434004306793\n",
      "Epoch 5, Batch: 298: Training Loss: 0.028509998694062233, Validation Loss: 0.027622805908322334\n",
      "Epoch 5, Batch: 299: Training Loss: 0.026410412043333054, Validation Loss: 0.02747982367873192\n",
      "Epoch 5, Batch: 300: Training Loss: 0.02831397019326687, Validation Loss: 0.0261661559343338\n",
      "Epoch 5, Batch: 301: Training Loss: 0.030634799972176552, Validation Loss: 0.026568636298179626\n",
      "Epoch 5, Batch: 302: Training Loss: 0.024727948009967804, Validation Loss: 0.02732119709253311\n",
      "Epoch 5, Batch: 303: Training Loss: 0.02662552334368229, Validation Loss: 0.02599368803203106\n",
      "Epoch 5, Batch: 304: Training Loss: 0.03026248700916767, Validation Loss: 0.02809181436896324\n",
      "Epoch 5, Batch: 305: Training Loss: 0.02195674180984497, Validation Loss: 0.027002504095435143\n",
      "Epoch 5, Batch: 306: Training Loss: 0.0301589984446764, Validation Loss: 0.02537211962044239\n",
      "Epoch 5, Batch: 307: Training Loss: 0.028834141790866852, Validation Loss: 0.02717634104192257\n",
      "Epoch 5, Batch: 308: Training Loss: 0.027457136660814285, Validation Loss: 0.02850755676627159\n",
      "Epoch 5, Batch: 309: Training Loss: 0.029934396967291832, Validation Loss: 0.028508340939879417\n",
      "Epoch 5, Batch: 310: Training Loss: 0.023960579186677933, Validation Loss: 0.027806466445326805\n",
      "Epoch 5, Batch: 311: Training Loss: 0.029506806284189224, Validation Loss: 0.02875988557934761\n",
      "Epoch 5, Batch: 312: Training Loss: 0.026982469484210014, Validation Loss: 0.02751508727669716\n",
      "Epoch 5, Batch: 313: Training Loss: 0.02545413374900818, Validation Loss: 0.02905574068427086\n",
      "Epoch 5, Batch: 314: Training Loss: 0.027845162898302078, Validation Loss: 0.02706865780055523\n",
      "Epoch 5, Batch: 315: Training Loss: 0.028778076171875, Validation Loss: 0.027484044432640076\n",
      "Epoch 5, Batch: 316: Training Loss: 0.027721723541617393, Validation Loss: 0.029722752049565315\n",
      "Epoch 5, Batch: 317: Training Loss: 0.026759596541523933, Validation Loss: 0.02783287689089775\n",
      "Epoch 5, Batch: 318: Training Loss: 0.029988400638103485, Validation Loss: 0.026719342917203903\n",
      "Epoch 5, Batch: 319: Training Loss: 0.032503578811883926, Validation Loss: 0.029537755995988846\n",
      "Epoch 5, Batch: 320: Training Loss: 0.028649695217609406, Validation Loss: 0.03161882609128952\n",
      "Epoch 5, Batch: 321: Training Loss: 0.026417698711156845, Validation Loss: 0.02841743640601635\n",
      "Epoch 5, Batch: 322: Training Loss: 0.03052401728928089, Validation Loss: 0.030692320317029953\n",
      "Epoch 5, Batch: 323: Training Loss: 0.025950996205210686, Validation Loss: 0.031217342242598534\n",
      "Epoch 5, Batch: 324: Training Loss: 0.028094667941331863, Validation Loss: 0.029772406443953514\n",
      "Epoch 5, Batch: 325: Training Loss: 0.026630770415067673, Validation Loss: 0.027737844735383987\n",
      "Epoch 5, Batch: 326: Training Loss: 0.030148034915328026, Validation Loss: 0.03144829347729683\n",
      "Epoch 5, Batch: 327: Training Loss: 0.02381816878914833, Validation Loss: 0.0295111034065485\n",
      "Epoch 5, Batch: 328: Training Loss: 0.0297933891415596, Validation Loss: 0.02930706925690174\n",
      "Epoch 5, Batch: 329: Training Loss: 0.02507617324590683, Validation Loss: 0.029801398515701294\n",
      "Epoch 5, Batch: 330: Training Loss: 0.03048708103597164, Validation Loss: 0.026092568412423134\n",
      "Epoch 5, Batch: 331: Training Loss: 0.027401212602853775, Validation Loss: 0.02779906615614891\n",
      "Epoch 5, Batch: 332: Training Loss: 0.027333619073033333, Validation Loss: 0.02960859425365925\n",
      "Epoch 5, Batch: 333: Training Loss: 0.030469823628664017, Validation Loss: 0.02756059169769287\n",
      "Epoch 5, Batch: 334: Training Loss: 0.02567996457219124, Validation Loss: 0.029790472239255905\n",
      "Epoch 5, Batch: 335: Training Loss: 0.026862256228923798, Validation Loss: 0.03084282949566841\n",
      "Epoch 5, Batch: 336: Training Loss: 0.02896268293261528, Validation Loss: 0.029935771599411964\n",
      "Epoch 5, Batch: 337: Training Loss: 0.023766839876770973, Validation Loss: 0.028268273919820786\n",
      "Epoch 5, Batch: 338: Training Loss: 0.029748424887657166, Validation Loss: 0.02903541922569275\n",
      "Epoch 5, Batch: 339: Training Loss: 0.028217120096087456, Validation Loss: 0.028781184926629066\n",
      "Epoch 5, Batch: 340: Training Loss: 0.02641652710735798, Validation Loss: 0.030534708872437477\n",
      "Epoch 5, Batch: 341: Training Loss: 0.0254221074283123, Validation Loss: 0.028333736583590508\n",
      "Epoch 5, Batch: 342: Training Loss: 0.028743058443069458, Validation Loss: 0.02700498141348362\n",
      "Epoch 5, Batch: 343: Training Loss: 0.02501850202679634, Validation Loss: 0.029400579631328583\n",
      "Epoch 5, Batch: 344: Training Loss: 0.026478735730051994, Validation Loss: 0.02926386520266533\n",
      "Epoch 5, Batch: 345: Training Loss: 0.023952580988407135, Validation Loss: 0.02776254527270794\n",
      "Epoch 5, Batch: 346: Training Loss: 0.027186641469597816, Validation Loss: 0.027226299047470093\n",
      "Epoch 5, Batch: 347: Training Loss: 0.024196764454245567, Validation Loss: 0.027179192751646042\n",
      "Epoch 5, Batch: 348: Training Loss: 0.026236070320010185, Validation Loss: 0.02930574305355549\n",
      "Epoch 5, Batch: 349: Training Loss: 0.029356326907873154, Validation Loss: 0.026257026940584183\n",
      "Epoch 5, Batch: 350: Training Loss: 0.028077024966478348, Validation Loss: 0.027684582397341728\n",
      "Epoch 5, Batch: 351: Training Loss: 0.03305240347981453, Validation Loss: 0.029017597436904907\n",
      "Epoch 5, Batch: 352: Training Loss: 0.028004443272948265, Validation Loss: 0.028742004185914993\n",
      "Epoch 5, Batch: 353: Training Loss: 0.026017745956778526, Validation Loss: 0.0278561282902956\n",
      "Epoch 5, Batch: 354: Training Loss: 0.032434750348329544, Validation Loss: 0.0284536462277174\n",
      "Epoch 5, Batch: 355: Training Loss: 0.024101026356220245, Validation Loss: 0.029377860948443413\n",
      "Epoch 5, Batch: 356: Training Loss: 0.030928853899240494, Validation Loss: 0.028814470395445824\n",
      "Epoch 5, Batch: 357: Training Loss: 0.022861305624246597, Validation Loss: 0.027872273698449135\n",
      "Epoch 5, Batch: 358: Training Loss: 0.031878627836704254, Validation Loss: 0.03126036748290062\n",
      "Epoch 5, Batch: 359: Training Loss: 0.026839744299650192, Validation Loss: 0.030281540006399155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 360: Training Loss: 0.02537827380001545, Validation Loss: 0.029977945610880852\n",
      "Epoch 5, Batch: 361: Training Loss: 0.02857154980301857, Validation Loss: 0.028558092191815376\n",
      "Epoch 5, Batch: 362: Training Loss: 0.026233192533254623, Validation Loss: 0.030656911432743073\n",
      "Epoch 5, Batch: 363: Training Loss: 0.03116821125149727, Validation Loss: 0.02615046128630638\n",
      "Epoch 5, Batch: 364: Training Loss: 0.027060072869062424, Validation Loss: 0.028583750128746033\n",
      "Epoch 5, Batch: 365: Training Loss: 0.030389897525310516, Validation Loss: 0.029040038585662842\n",
      "Epoch 5, Batch: 366: Training Loss: 0.028824497014284134, Validation Loss: 0.029245972633361816\n",
      "Epoch 5, Batch: 367: Training Loss: 0.02732636034488678, Validation Loss: 0.03232249617576599\n",
      "Epoch 5, Batch: 368: Training Loss: 0.026683824136853218, Validation Loss: 0.02866092137992382\n",
      "Epoch 5, Batch: 369: Training Loss: 0.029842939227819443, Validation Loss: 0.031747959554195404\n",
      "Epoch 5, Batch: 370: Training Loss: 0.03163116052746773, Validation Loss: 0.03158903121948242\n",
      "Epoch 5, Batch: 371: Training Loss: 0.027343671768903732, Validation Loss: 0.030024578794836998\n",
      "Epoch 5, Batch: 372: Training Loss: 0.029421502724289894, Validation Loss: 0.029278825968503952\n",
      "Epoch 5, Batch: 373: Training Loss: 0.03455033153295517, Validation Loss: 0.02887994609773159\n",
      "Epoch 5, Batch: 374: Training Loss: 0.02796419896185398, Validation Loss: 0.030362479388713837\n",
      "Epoch 5, Batch: 375: Training Loss: 0.030459096655249596, Validation Loss: 0.0296777430921793\n",
      "Epoch 5, Batch: 376: Training Loss: 0.024678172543644905, Validation Loss: 0.03394628316164017\n",
      "Epoch 5, Batch: 377: Training Loss: 0.026958221569657326, Validation Loss: 0.028226809576153755\n",
      "Epoch 5, Batch: 378: Training Loss: 0.02546825259923935, Validation Loss: 0.02834116853773594\n",
      "Epoch 5, Batch: 379: Training Loss: 0.028275761753320694, Validation Loss: 0.02938954159617424\n",
      "Epoch 5, Batch: 380: Training Loss: 0.03280313313007355, Validation Loss: 0.029125532135367393\n",
      "Epoch 5, Batch: 381: Training Loss: 0.029175549745559692, Validation Loss: 0.03146173059940338\n",
      "Epoch 5, Batch: 382: Training Loss: 0.02673156186938286, Validation Loss: 0.029051294550299644\n",
      "Epoch 5, Batch: 383: Training Loss: 0.023509711027145386, Validation Loss: 0.03152705729007721\n",
      "Epoch 5, Batch: 384: Training Loss: 0.02959389053285122, Validation Loss: 0.03252551704645157\n",
      "Epoch 5, Batch: 385: Training Loss: 0.024185553193092346, Validation Loss: 0.03065461292862892\n",
      "Epoch 5, Batch: 386: Training Loss: 0.028602974489331245, Validation Loss: 0.033349521458148956\n",
      "Epoch 5, Batch: 387: Training Loss: 0.030201271176338196, Validation Loss: 0.030618857592344284\n",
      "Epoch 5, Batch: 388: Training Loss: 0.026465462520718575, Validation Loss: 0.031094025820493698\n",
      "Epoch 5, Batch: 389: Training Loss: 0.03190665319561958, Validation Loss: 0.029768487438559532\n",
      "Epoch 5, Batch: 390: Training Loss: 0.028003821149468422, Validation Loss: 0.025727035477757454\n",
      "Epoch 5, Batch: 391: Training Loss: 0.03327319025993347, Validation Loss: 0.031210219487547874\n",
      "Epoch 5, Batch: 392: Training Loss: 0.027739346027374268, Validation Loss: 0.02903824672102928\n",
      "Epoch 5, Batch: 393: Training Loss: 0.02953099086880684, Validation Loss: 0.03292049095034599\n",
      "Epoch 5, Batch: 394: Training Loss: 0.03008214384317398, Validation Loss: 0.03279377147555351\n",
      "Epoch 5, Batch: 395: Training Loss: 0.02875691093504429, Validation Loss: 0.0346575528383255\n",
      "Epoch 5, Batch: 396: Training Loss: 0.029112117365002632, Validation Loss: 0.033573441207408905\n",
      "Epoch 5, Batch: 397: Training Loss: 0.03287409991025925, Validation Loss: 0.031144745647907257\n",
      "Epoch 5, Batch: 398: Training Loss: 0.029409904032945633, Validation Loss: 0.03201916068792343\n",
      "Epoch 5, Batch: 399: Training Loss: 0.03167252242565155, Validation Loss: 0.029123689979314804\n",
      "Epoch 5, Batch: 400: Training Loss: 0.028092844411730766, Validation Loss: 0.028035765513777733\n",
      "Epoch 5, Batch: 401: Training Loss: 0.02520577609539032, Validation Loss: 0.030169012024998665\n",
      "Epoch 5, Batch: 402: Training Loss: 0.02626940980553627, Validation Loss: 0.026726003736257553\n",
      "Epoch 5, Batch: 403: Training Loss: 0.02894274704158306, Validation Loss: 0.028125736862421036\n",
      "Epoch 5, Batch: 404: Training Loss: 0.026115743443369865, Validation Loss: 0.02730349265038967\n",
      "Epoch 5, Batch: 405: Training Loss: 0.026484426110982895, Validation Loss: 0.027227269485592842\n",
      "Epoch 5, Batch: 406: Training Loss: 0.027341129258275032, Validation Loss: 0.028710737824440002\n",
      "Epoch 5, Batch: 407: Training Loss: 0.02718956582248211, Validation Loss: 0.029851259663701057\n",
      "Epoch 5, Batch: 408: Training Loss: 0.024437369778752327, Validation Loss: 0.026272090151906013\n",
      "Epoch 5, Batch: 409: Training Loss: 0.029077604413032532, Validation Loss: 0.0304868221282959\n",
      "Epoch 5, Batch: 410: Training Loss: 0.029324175789952278, Validation Loss: 0.027615223079919815\n",
      "Epoch 5, Batch: 411: Training Loss: 0.02715790458023548, Validation Loss: 0.02741161175072193\n",
      "Epoch 5, Batch: 412: Training Loss: 0.034693874418735504, Validation Loss: 0.02619529329240322\n",
      "Epoch 5, Batch: 413: Training Loss: 0.029586147516965866, Validation Loss: 0.027137408033013344\n",
      "Epoch 5, Batch: 414: Training Loss: 0.03165297582745552, Validation Loss: 0.02597556635737419\n",
      "Epoch 5, Batch: 415: Training Loss: 0.028034955263137817, Validation Loss: 0.0265710037201643\n",
      "Epoch 5, Batch: 416: Training Loss: 0.02633547969162464, Validation Loss: 0.026634659618139267\n",
      "Epoch 5, Batch: 417: Training Loss: 0.0262660700827837, Validation Loss: 0.02704612724483013\n",
      "Epoch 5, Batch: 418: Training Loss: 0.02252158150076866, Validation Loss: 0.024167753756046295\n",
      "Epoch 5, Batch: 419: Training Loss: 0.02672625705599785, Validation Loss: 0.027914101257920265\n",
      "Epoch 5, Batch: 420: Training Loss: 0.030456000939011574, Validation Loss: 0.024818917736411095\n",
      "Epoch 5, Batch: 421: Training Loss: 0.03362099081277847, Validation Loss: 0.026232177391648293\n",
      "Epoch 5, Batch: 422: Training Loss: 0.030679266899824142, Validation Loss: 0.026434848085045815\n",
      "Epoch 5, Batch: 423: Training Loss: 0.028766777366399765, Validation Loss: 0.02630891278386116\n",
      "Epoch 5, Batch: 424: Training Loss: 0.030205829069018364, Validation Loss: 0.024594387039542198\n",
      "Epoch 5, Batch: 425: Training Loss: 0.025002824142575264, Validation Loss: 0.026908213272690773\n",
      "Epoch 5, Batch: 426: Training Loss: 0.020639928057789803, Validation Loss: 0.026333970949053764\n",
      "Epoch 5, Batch: 427: Training Loss: 0.027665000408887863, Validation Loss: 0.027645127847790718\n",
      "Epoch 5, Batch: 428: Training Loss: 0.024595551192760468, Validation Loss: 0.025408107787370682\n",
      "Epoch 5, Batch: 429: Training Loss: 0.0286413487046957, Validation Loss: 0.028064971789717674\n",
      "Epoch 5, Batch: 430: Training Loss: 0.02954094670712948, Validation Loss: 0.028201047331094742\n",
      "Epoch 5, Batch: 431: Training Loss: 0.02552924118936062, Validation Loss: 0.027820026502013206\n",
      "Epoch 5, Batch: 432: Training Loss: 0.03312745317816734, Validation Loss: 0.028171993792057037\n",
      "Epoch 5, Batch: 433: Training Loss: 0.02861851081252098, Validation Loss: 0.02876325510442257\n",
      "Epoch 5, Batch: 434: Training Loss: 0.027817219495773315, Validation Loss: 0.025307072326540947\n",
      "Epoch 5, Batch: 435: Training Loss: 0.029445433989167213, Validation Loss: 0.02476213499903679\n",
      "Epoch 5, Batch: 436: Training Loss: 0.027306148782372475, Validation Loss: 0.02521718665957451\n",
      "Epoch 5, Batch: 437: Training Loss: 0.024136800318956375, Validation Loss: 0.02614838443696499\n",
      "Epoch 5, Batch: 438: Training Loss: 0.02917570248246193, Validation Loss: 0.028347933664917946\n",
      "Epoch 5, Batch: 439: Training Loss: 0.025633681565523148, Validation Loss: 0.025360815227031708\n",
      "Epoch 5, Batch: 440: Training Loss: 0.02981646917760372, Validation Loss: 0.025549650192260742\n",
      "Epoch 5, Batch: 441: Training Loss: 0.024402249604463577, Validation Loss: 0.025862321257591248\n",
      "Epoch 5, Batch: 442: Training Loss: 0.03003702126443386, Validation Loss: 0.026078928261995316\n",
      "Epoch 5, Batch: 443: Training Loss: 0.030041784048080444, Validation Loss: 0.02624383568763733\n",
      "Epoch 5, Batch: 444: Training Loss: 0.024416260421276093, Validation Loss: 0.025918476283550262\n",
      "Epoch 5, Batch: 445: Training Loss: 0.025151200592517853, Validation Loss: 0.025502730160951614\n",
      "Epoch 5, Batch: 446: Training Loss: 0.030377991497516632, Validation Loss: 0.02655050903558731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch: 447: Training Loss: 0.0292854942381382, Validation Loss: 0.02560552768409252\n",
      "Epoch 5, Batch: 448: Training Loss: 0.022931423038244247, Validation Loss: 0.025352763012051582\n",
      "Epoch 5, Batch: 449: Training Loss: 0.02682666853070259, Validation Loss: 0.024495039135217667\n",
      "Epoch 5, Batch: 450: Training Loss: 0.02615487203001976, Validation Loss: 0.02444685436785221\n",
      "Epoch 5, Batch: 451: Training Loss: 0.02989969216287136, Validation Loss: 0.0248417928814888\n",
      "Epoch 5, Batch: 452: Training Loss: 0.0274443868547678, Validation Loss: 0.027912922203540802\n",
      "Epoch 5, Batch: 453: Training Loss: 0.028139570727944374, Validation Loss: 0.024508865550160408\n",
      "Epoch 5, Batch: 454: Training Loss: 0.027125410735607147, Validation Loss: 0.027236415073275566\n",
      "Epoch 5, Batch: 455: Training Loss: 0.02399636059999466, Validation Loss: 0.026204727590084076\n",
      "Epoch 5, Batch: 456: Training Loss: 0.026596365496516228, Validation Loss: 0.028157906606793404\n",
      "Epoch 5, Batch: 457: Training Loss: 0.02617240697145462, Validation Loss: 0.02821972407400608\n",
      "Epoch 5, Batch: 458: Training Loss: 0.02222750335931778, Validation Loss: 0.028848785907030106\n",
      "Epoch 5, Batch: 459: Training Loss: 0.027330560609698296, Validation Loss: 0.029287954792380333\n",
      "Epoch 5, Batch: 460: Training Loss: 0.031412869691848755, Validation Loss: 0.03082374669611454\n",
      "Epoch 5, Batch: 461: Training Loss: 0.027160638943314552, Validation Loss: 0.028912536799907684\n",
      "Epoch 5, Batch: 462: Training Loss: 0.02889147400856018, Validation Loss: 0.028910653665661812\n",
      "Epoch 5, Batch: 463: Training Loss: 0.029179513454437256, Validation Loss: 0.03233397379517555\n",
      "Epoch 5, Batch: 464: Training Loss: 0.028415875509381294, Validation Loss: 0.03169174864888191\n",
      "Epoch 5, Batch: 465: Training Loss: 0.03061472252011299, Validation Loss: 0.03009979799389839\n",
      "Epoch 5, Batch: 466: Training Loss: 0.024244096130132675, Validation Loss: 0.030369186773896217\n",
      "Epoch 5, Batch: 467: Training Loss: 0.03270348906517029, Validation Loss: 0.02731715515255928\n",
      "Epoch 5, Batch: 468: Training Loss: 0.030203083530068398, Validation Loss: 0.02712031826376915\n",
      "Epoch 5, Batch: 469: Training Loss: 0.030149679630994797, Validation Loss: 0.026129456236958504\n",
      "Epoch 5, Batch: 470: Training Loss: 0.025641435757279396, Validation Loss: 0.027062898501753807\n",
      "Epoch 5, Batch: 471: Training Loss: 0.03072170540690422, Validation Loss: 0.026548251509666443\n",
      "Epoch 5, Batch: 472: Training Loss: 0.029134098440408707, Validation Loss: 0.024216437712311745\n",
      "Epoch 5, Batch: 473: Training Loss: 0.026757489889860153, Validation Loss: 0.028107058256864548\n",
      "Epoch 5, Batch: 474: Training Loss: 0.024045070633292198, Validation Loss: 0.025949735194444656\n",
      "Epoch 5, Batch: 475: Training Loss: 0.02937500737607479, Validation Loss: 0.02749880962073803\n",
      "Epoch 5, Batch: 476: Training Loss: 0.026831012219190598, Validation Loss: 0.02586975321173668\n",
      "Epoch 5, Batch: 477: Training Loss: 0.030406521633267403, Validation Loss: 0.030200475826859474\n",
      "Epoch 5, Batch: 478: Training Loss: 0.025862544775009155, Validation Loss: 0.026912713423371315\n",
      "Epoch 5, Batch: 479: Training Loss: 0.027767563238739967, Validation Loss: 0.026557426899671555\n",
      "Epoch 5, Batch: 480: Training Loss: 0.02582959085702896, Validation Loss: 0.025757484138011932\n",
      "Epoch 5, Batch: 481: Training Loss: 0.028510503470897675, Validation Loss: 0.025938352569937706\n",
      "Epoch 5, Batch: 482: Training Loss: 0.026508120819926262, Validation Loss: 0.028099985793232918\n",
      "Epoch 5, Batch: 483: Training Loss: 0.02738731913268566, Validation Loss: 0.028323829174041748\n",
      "Epoch 5, Batch: 484: Training Loss: 0.022459540516138077, Validation Loss: 0.025091037154197693\n",
      "Epoch 5, Batch: 485: Training Loss: 0.026706401258707047, Validation Loss: 0.02624310739338398\n",
      "Epoch 5, Batch: 486: Training Loss: 0.02609981596469879, Validation Loss: 0.025895437225699425\n",
      "Epoch 5, Batch: 487: Training Loss: 0.028190674260258675, Validation Loss: 0.025235438719391823\n",
      "Epoch 5, Batch: 488: Training Loss: 0.026899637654423714, Validation Loss: 0.028059357777237892\n",
      "Epoch 5, Batch: 489: Training Loss: 0.030385008081793785, Validation Loss: 0.026640763506293297\n",
      "Epoch 5, Batch: 490: Training Loss: 0.030351096764206886, Validation Loss: 0.027692748233675957\n",
      "Epoch 5, Batch: 491: Training Loss: 0.02621651440858841, Validation Loss: 0.0247168131172657\n",
      "Epoch 5, Batch: 492: Training Loss: 0.028350936248898506, Validation Loss: 0.026407459750771523\n",
      "Epoch 5, Batch: 493: Training Loss: 0.029693175107240677, Validation Loss: 0.02896278165280819\n",
      "Epoch 5, Batch: 494: Training Loss: 0.0300297811627388, Validation Loss: 0.027600275352597237\n",
      "Epoch 5, Batch: 495: Training Loss: 0.0252086091786623, Validation Loss: 0.028239578008651733\n",
      "Epoch 5, Batch: 496: Training Loss: 0.027425304055213928, Validation Loss: 0.026130661368370056\n",
      "Epoch 5, Batch: 497: Training Loss: 0.025522887706756592, Validation Loss: 0.026387780904769897\n",
      "Epoch 5, Batch: 498: Training Loss: 0.027300819754600525, Validation Loss: 0.025416480377316475\n",
      "Epoch 5, Batch: 499: Training Loss: 0.029826825484633446, Validation Loss: 0.027700118720531464\n",
      "Epoch 6, Batch: 0: Training Loss: 0.027683908119797707, Validation Loss: 0.0255332849919796\n",
      "Epoch 6, Batch: 1: Training Loss: 0.0244772769510746, Validation Loss: 0.02918168716132641\n",
      "Epoch 6, Batch: 2: Training Loss: 0.028349295258522034, Validation Loss: 0.02612840197980404\n",
      "Epoch 6, Batch: 3: Training Loss: 0.025698300451040268, Validation Loss: 0.02644534781575203\n",
      "Epoch 6, Batch: 4: Training Loss: 0.024607494473457336, Validation Loss: 0.027707243338227272\n",
      "Epoch 6, Batch: 5: Training Loss: 0.02694646641612053, Validation Loss: 0.03006085753440857\n",
      "Epoch 6, Batch: 6: Training Loss: 0.02724023163318634, Validation Loss: 0.027405492961406708\n",
      "Epoch 6, Batch: 7: Training Loss: 0.025136105716228485, Validation Loss: 0.02958579920232296\n",
      "Epoch 6, Batch: 8: Training Loss: 0.0270463265478611, Validation Loss: 0.028812909498810768\n",
      "Epoch 6, Batch: 9: Training Loss: 0.025173908099532127, Validation Loss: 0.028287678956985474\n",
      "Epoch 6, Batch: 10: Training Loss: 0.028268320485949516, Validation Loss: 0.026968827471137047\n",
      "Epoch 6, Batch: 11: Training Loss: 0.03089865855872631, Validation Loss: 0.02992631308734417\n",
      "Epoch 6, Batch: 12: Training Loss: 0.030117684975266457, Validation Loss: 0.025995278730988503\n",
      "Epoch 6, Batch: 13: Training Loss: 0.03050238825380802, Validation Loss: 0.029200203716754913\n",
      "Epoch 6, Batch: 14: Training Loss: 0.030858973041176796, Validation Loss: 0.028501110151410103\n",
      "Epoch 6, Batch: 15: Training Loss: 0.027481136843562126, Validation Loss: 0.029033003374934196\n",
      "Epoch 6, Batch: 16: Training Loss: 0.028184927999973297, Validation Loss: 0.028698712587356567\n",
      "Epoch 6, Batch: 17: Training Loss: 0.02422356605529785, Validation Loss: 0.028112882748246193\n",
      "Epoch 6, Batch: 18: Training Loss: 0.025576910004019737, Validation Loss: 0.03018013760447502\n",
      "Epoch 6, Batch: 19: Training Loss: 0.024999067187309265, Validation Loss: 0.027491174638271332\n",
      "Epoch 6, Batch: 20: Training Loss: 0.031110335141420364, Validation Loss: 0.028207577764987946\n",
      "Epoch 6, Batch: 21: Training Loss: 0.031064774841070175, Validation Loss: 0.02939821034669876\n",
      "Epoch 6, Batch: 22: Training Loss: 0.030064351856708527, Validation Loss: 0.02668182924389839\n",
      "Epoch 6, Batch: 23: Training Loss: 0.027220411226153374, Validation Loss: 0.02748700976371765\n",
      "Epoch 6, Batch: 24: Training Loss: 0.02439192868769169, Validation Loss: 0.030815236270427704\n",
      "Epoch 6, Batch: 25: Training Loss: 0.02893068641424179, Validation Loss: 0.028634333983063698\n",
      "Epoch 6, Batch: 26: Training Loss: 0.030147403478622437, Validation Loss: 0.02689947560429573\n",
      "Epoch 6, Batch: 27: Training Loss: 0.03188711777329445, Validation Loss: 0.02386348322033882\n",
      "Epoch 6, Batch: 28: Training Loss: 0.02926795743405819, Validation Loss: 0.026634473353624344\n",
      "Epoch 6, Batch: 29: Training Loss: 0.028358105570077896, Validation Loss: 0.027238890528678894\n",
      "Epoch 6, Batch: 30: Training Loss: 0.028247496113181114, Validation Loss: 0.026613151654601097\n",
      "Epoch 6, Batch: 31: Training Loss: 0.029654335230588913, Validation Loss: 0.026686647906899452\n",
      "Epoch 6, Batch: 32: Training Loss: 0.026629891246557236, Validation Loss: 0.030285891145467758\n",
      "Epoch 6, Batch: 33: Training Loss: 0.02550916001200676, Validation Loss: 0.03086542896926403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 34: Training Loss: 0.024029329419136047, Validation Loss: 0.02977217547595501\n",
      "Epoch 6, Batch: 35: Training Loss: 0.030238734558224678, Validation Loss: 0.02935226820409298\n",
      "Epoch 6, Batch: 36: Training Loss: 0.02585994079709053, Validation Loss: 0.02972748875617981\n",
      "Epoch 6, Batch: 37: Training Loss: 0.022537218406796455, Validation Loss: 0.0266976747661829\n",
      "Epoch 6, Batch: 38: Training Loss: 0.03108277916908264, Validation Loss: 0.0284015741199255\n",
      "Epoch 6, Batch: 39: Training Loss: 0.03000476583838463, Validation Loss: 0.030036108568310738\n",
      "Epoch 6, Batch: 40: Training Loss: 0.029061222448945045, Validation Loss: 0.02949422039091587\n",
      "Epoch 6, Batch: 41: Training Loss: 0.026061663404107094, Validation Loss: 0.030288362875580788\n",
      "Epoch 6, Batch: 42: Training Loss: 0.028079217299818993, Validation Loss: 0.02777772955596447\n",
      "Epoch 6, Batch: 43: Training Loss: 0.02568906731903553, Validation Loss: 0.028560694307088852\n",
      "Epoch 6, Batch: 44: Training Loss: 0.02812907099723816, Validation Loss: 0.030206680297851562\n",
      "Epoch 6, Batch: 45: Training Loss: 0.02863369509577751, Validation Loss: 0.02868269942700863\n",
      "Epoch 6, Batch: 46: Training Loss: 0.02587619423866272, Validation Loss: 0.02833937294781208\n",
      "Epoch 6, Batch: 47: Training Loss: 0.030132418498396873, Validation Loss: 0.030318211764097214\n",
      "Epoch 6, Batch: 48: Training Loss: 0.02889372780919075, Validation Loss: 0.026712005957961082\n",
      "Epoch 6, Batch: 49: Training Loss: 0.029780421406030655, Validation Loss: 0.026826759800314903\n",
      "Epoch 6, Batch: 50: Training Loss: 0.026113886386156082, Validation Loss: 0.028212841600179672\n",
      "Epoch 6, Batch: 51: Training Loss: 0.026111481711268425, Validation Loss: 0.027429308742284775\n",
      "Epoch 6, Batch: 52: Training Loss: 0.03123387135565281, Validation Loss: 0.029060358181595802\n",
      "Epoch 6, Batch: 53: Training Loss: 0.023359252139925957, Validation Loss: 0.02896430715918541\n",
      "Epoch 6, Batch: 54: Training Loss: 0.026637421920895576, Validation Loss: 0.027531137689948082\n",
      "Epoch 6, Batch: 55: Training Loss: 0.02662697061896324, Validation Loss: 0.028352199122309685\n",
      "Epoch 6, Batch: 56: Training Loss: 0.02853606641292572, Validation Loss: 0.02872796356678009\n",
      "Epoch 6, Batch: 57: Training Loss: 0.02707710675895214, Validation Loss: 0.026780033484101295\n",
      "Epoch 6, Batch: 58: Training Loss: 0.02851942926645279, Validation Loss: 0.025713389739394188\n",
      "Epoch 6, Batch: 59: Training Loss: 0.025577016174793243, Validation Loss: 0.026386680081486702\n",
      "Epoch 6, Batch: 60: Training Loss: 0.02942739799618721, Validation Loss: 0.025532672181725502\n",
      "Epoch 6, Batch: 61: Training Loss: 0.030163776129484177, Validation Loss: 0.025286218151450157\n",
      "Epoch 6, Batch: 62: Training Loss: 0.02793140895664692, Validation Loss: 0.02599593810737133\n",
      "Epoch 6, Batch: 63: Training Loss: 0.027886709198355675, Validation Loss: 0.02989078313112259\n",
      "Epoch 6, Batch: 64: Training Loss: 0.027001837268471718, Validation Loss: 0.02850373275578022\n",
      "Epoch 6, Batch: 65: Training Loss: 0.02964572049677372, Validation Loss: 0.02801717258989811\n",
      "Epoch 6, Batch: 66: Training Loss: 0.026684464886784554, Validation Loss: 0.026784051209688187\n",
      "Epoch 6, Batch: 67: Training Loss: 0.024265367537736893, Validation Loss: 0.027090875431895256\n",
      "Epoch 6, Batch: 68: Training Loss: 0.025111433118581772, Validation Loss: 0.030924266204237938\n",
      "Epoch 6, Batch: 69: Training Loss: 0.02878345176577568, Validation Loss: 0.02738853357732296\n",
      "Epoch 6, Batch: 70: Training Loss: 0.02874544821679592, Validation Loss: 0.028057459741830826\n",
      "Epoch 6, Batch: 71: Training Loss: 0.025537541136145592, Validation Loss: 0.029400425031781197\n",
      "Epoch 6, Batch: 72: Training Loss: 0.027732590213418007, Validation Loss: 0.027860743924975395\n",
      "Epoch 6, Batch: 73: Training Loss: 0.027482805773615837, Validation Loss: 0.03264981508255005\n",
      "Epoch 6, Batch: 74: Training Loss: 0.02587219886481762, Validation Loss: 0.028868895024061203\n",
      "Epoch 6, Batch: 75: Training Loss: 0.02313876524567604, Validation Loss: 0.03077833540737629\n",
      "Epoch 6, Batch: 76: Training Loss: 0.023535411804914474, Validation Loss: 0.029023874551057816\n",
      "Epoch 6, Batch: 77: Training Loss: 0.03244587033987045, Validation Loss: 0.029844310134649277\n",
      "Epoch 6, Batch: 78: Training Loss: 0.028539972379803658, Validation Loss: 0.030951280146837234\n",
      "Epoch 6, Batch: 79: Training Loss: 0.027908125892281532, Validation Loss: 0.02927117608487606\n",
      "Epoch 6, Batch: 80: Training Loss: 0.023630578070878983, Validation Loss: 0.027828289195895195\n",
      "Epoch 6, Batch: 81: Training Loss: 0.02489771507680416, Validation Loss: 0.03054245002567768\n",
      "Epoch 6, Batch: 82: Training Loss: 0.030678221955895424, Validation Loss: 0.029203694313764572\n",
      "Epoch 6, Batch: 83: Training Loss: 0.026720963418483734, Validation Loss: 0.031881872564554214\n",
      "Epoch 6, Batch: 84: Training Loss: 0.030104393139481544, Validation Loss: 0.03128309175372124\n",
      "Epoch 6, Batch: 85: Training Loss: 0.02475452795624733, Validation Loss: 0.030670830979943275\n",
      "Epoch 6, Batch: 86: Training Loss: 0.029008977115154266, Validation Loss: 0.02872895635664463\n",
      "Epoch 6, Batch: 87: Training Loss: 0.02742987498641014, Validation Loss: 0.028313029557466507\n",
      "Epoch 6, Batch: 88: Training Loss: 0.028787091374397278, Validation Loss: 0.030352286994457245\n",
      "Epoch 6, Batch: 89: Training Loss: 0.028169581666588783, Validation Loss: 0.029418081045150757\n",
      "Epoch 6, Batch: 90: Training Loss: 0.02384832687675953, Validation Loss: 0.02924332208931446\n",
      "Epoch 6, Batch: 91: Training Loss: 0.02773330919444561, Validation Loss: 0.029595492407679558\n",
      "Epoch 6, Batch: 92: Training Loss: 0.029048984870314598, Validation Loss: 0.030133413150906563\n",
      "Epoch 6, Batch: 93: Training Loss: 0.029035275802016258, Validation Loss: 0.031661588698625565\n",
      "Epoch 6, Batch: 94: Training Loss: 0.030695578083395958, Validation Loss: 0.030195433646440506\n",
      "Epoch 6, Batch: 95: Training Loss: 0.027523966506123543, Validation Loss: 0.0314481295645237\n",
      "Epoch 6, Batch: 96: Training Loss: 0.027794474735856056, Validation Loss: 0.028330590575933456\n",
      "Epoch 6, Batch: 97: Training Loss: 0.02971477247774601, Validation Loss: 0.029807008802890778\n",
      "Epoch 6, Batch: 98: Training Loss: 0.02795165218412876, Validation Loss: 0.02696535922586918\n",
      "Epoch 6, Batch: 99: Training Loss: 0.027765851467847824, Validation Loss: 0.028764136135578156\n",
      "Epoch 6, Batch: 100: Training Loss: 0.030368810519576073, Validation Loss: 0.02694062888622284\n",
      "Epoch 6, Batch: 101: Training Loss: 0.024371258914470673, Validation Loss: 0.028888147324323654\n",
      "Epoch 6, Batch: 102: Training Loss: 0.027286216616630554, Validation Loss: 0.026821307837963104\n",
      "Epoch 6, Batch: 103: Training Loss: 0.02704809606075287, Validation Loss: 0.030259374529123306\n",
      "Epoch 6, Batch: 104: Training Loss: 0.0237306896597147, Validation Loss: 0.027965877205133438\n",
      "Epoch 6, Batch: 105: Training Loss: 0.026704028248786926, Validation Loss: 0.029086466878652573\n",
      "Epoch 6, Batch: 106: Training Loss: 0.026293164119124413, Validation Loss: 0.029049372300505638\n",
      "Epoch 6, Batch: 107: Training Loss: 0.027743833139538765, Validation Loss: 0.028111331164836884\n",
      "Epoch 6, Batch: 108: Training Loss: 0.025465378537774086, Validation Loss: 0.02883712761104107\n",
      "Epoch 6, Batch: 109: Training Loss: 0.02913573943078518, Validation Loss: 0.028747433796525\n",
      "Epoch 6, Batch: 110: Training Loss: 0.03209160268306732, Validation Loss: 0.025254681706428528\n",
      "Epoch 6, Batch: 111: Training Loss: 0.02650286629796028, Validation Loss: 0.027808014303445816\n",
      "Epoch 6, Batch: 112: Training Loss: 0.02411421574652195, Validation Loss: 0.028246326372027397\n",
      "Epoch 6, Batch: 113: Training Loss: 0.023005414754152298, Validation Loss: 0.028998827561736107\n",
      "Epoch 6, Batch: 114: Training Loss: 0.028452197089791298, Validation Loss: 0.025236772373318672\n",
      "Epoch 6, Batch: 115: Training Loss: 0.029969999566674232, Validation Loss: 0.028421903029084206\n",
      "Epoch 6, Batch: 116: Training Loss: 0.02827037125825882, Validation Loss: 0.02665637992322445\n",
      "Epoch 6, Batch: 117: Training Loss: 0.028243837878108025, Validation Loss: 0.028361143544316292\n",
      "Epoch 6, Batch: 118: Training Loss: 0.02455248311161995, Validation Loss: 0.02872587740421295\n",
      "Epoch 6, Batch: 119: Training Loss: 0.02770991250872612, Validation Loss: 0.026133326813578606\n",
      "Epoch 6, Batch: 120: Training Loss: 0.025224754586815834, Validation Loss: 0.026077887043356895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 121: Training Loss: 0.02563377469778061, Validation Loss: 0.02564137801527977\n",
      "Epoch 6, Batch: 122: Training Loss: 0.026243068277835846, Validation Loss: 0.030481910333037376\n",
      "Epoch 6, Batch: 123: Training Loss: 0.02699887938797474, Validation Loss: 0.025091396644711494\n",
      "Epoch 6, Batch: 124: Training Loss: 0.027252664789557457, Validation Loss: 0.027780814096331596\n",
      "Epoch 6, Batch: 125: Training Loss: 0.026184888556599617, Validation Loss: 0.02585543319582939\n",
      "Epoch 6, Batch: 126: Training Loss: 0.02614840492606163, Validation Loss: 0.02783607505261898\n",
      "Epoch 6, Batch: 127: Training Loss: 0.02816079556941986, Validation Loss: 0.027546923607587814\n",
      "Epoch 6, Batch: 128: Training Loss: 0.029760414734482765, Validation Loss: 0.026195263490080833\n",
      "Epoch 6, Batch: 129: Training Loss: 0.02748130075633526, Validation Loss: 0.02604685351252556\n",
      "Epoch 6, Batch: 130: Training Loss: 0.027018316090106964, Validation Loss: 0.02625008299946785\n",
      "Epoch 6, Batch: 131: Training Loss: 0.02789590321481228, Validation Loss: 0.02434845082461834\n",
      "Epoch 6, Batch: 132: Training Loss: 0.026802245527505875, Validation Loss: 0.024638740345835686\n",
      "Epoch 6, Batch: 133: Training Loss: 0.027208002284169197, Validation Loss: 0.0263350959867239\n",
      "Epoch 6, Batch: 134: Training Loss: 0.033410344272851944, Validation Loss: 0.02590010315179825\n",
      "Epoch 6, Batch: 135: Training Loss: 0.02310994826257229, Validation Loss: 0.02582583948969841\n",
      "Epoch 6, Batch: 136: Training Loss: 0.027901137247681618, Validation Loss: 0.026999296620488167\n",
      "Epoch 6, Batch: 137: Training Loss: 0.025340605527162552, Validation Loss: 0.02625769004225731\n",
      "Epoch 6, Batch: 138: Training Loss: 0.02697967365384102, Validation Loss: 0.025555552914738655\n",
      "Epoch 6, Batch: 139: Training Loss: 0.020779982209205627, Validation Loss: 0.02665441296994686\n",
      "Epoch 6, Batch: 140: Training Loss: 0.029264509677886963, Validation Loss: 0.025694958865642548\n",
      "Epoch 6, Batch: 141: Training Loss: 0.02958708442747593, Validation Loss: 0.02800929546356201\n",
      "Epoch 6, Batch: 142: Training Loss: 0.02969532646238804, Validation Loss: 0.02809401974081993\n",
      "Epoch 6, Batch: 143: Training Loss: 0.02596026286482811, Validation Loss: 0.027892161160707474\n",
      "Epoch 6, Batch: 144: Training Loss: 0.02698427066206932, Validation Loss: 0.025419190526008606\n",
      "Epoch 6, Batch: 145: Training Loss: 0.02475775219500065, Validation Loss: 0.027658341452479362\n",
      "Epoch 6, Batch: 146: Training Loss: 0.028304751962423325, Validation Loss: 0.028306694701313972\n",
      "Epoch 6, Batch: 147: Training Loss: 0.026787932962179184, Validation Loss: 0.024614060297608376\n",
      "Epoch 6, Batch: 148: Training Loss: 0.029205389320850372, Validation Loss: 0.025979967787861824\n",
      "Epoch 6, Batch: 149: Training Loss: 0.02657398395240307, Validation Loss: 0.026561753824353218\n",
      "Epoch 6, Batch: 150: Training Loss: 0.0289728082716465, Validation Loss: 0.026927588507533073\n",
      "Epoch 6, Batch: 151: Training Loss: 0.027486145496368408, Validation Loss: 0.027348080649971962\n",
      "Epoch 6, Batch: 152: Training Loss: 0.031908243894577026, Validation Loss: 0.028887860476970673\n",
      "Epoch 6, Batch: 153: Training Loss: 0.025640102103352547, Validation Loss: 0.027245547622442245\n",
      "Epoch 6, Batch: 154: Training Loss: 0.026680730283260345, Validation Loss: 0.027143003419041634\n",
      "Epoch 6, Batch: 155: Training Loss: 0.03461480513215065, Validation Loss: 0.028024181723594666\n",
      "Epoch 6, Batch: 156: Training Loss: 0.029242999851703644, Validation Loss: 0.02800937369465828\n",
      "Epoch 6, Batch: 157: Training Loss: 0.027373680844902992, Validation Loss: 0.02685307152569294\n",
      "Epoch 6, Batch: 158: Training Loss: 0.02836083620786667, Validation Loss: 0.02828040160238743\n",
      "Epoch 6, Batch: 159: Training Loss: 0.03211410716176033, Validation Loss: 0.026257814839482307\n",
      "Epoch 6, Batch: 160: Training Loss: 0.02528419718146324, Validation Loss: 0.02896120399236679\n",
      "Epoch 6, Batch: 161: Training Loss: 0.031169982627034187, Validation Loss: 0.02991885505616665\n",
      "Epoch 6, Batch: 162: Training Loss: 0.029020261019468307, Validation Loss: 0.027954550459980965\n",
      "Epoch 6, Batch: 163: Training Loss: 0.024437760934233665, Validation Loss: 0.027541985735297203\n",
      "Epoch 6, Batch: 164: Training Loss: 0.026841070502996445, Validation Loss: 0.027713684365153313\n",
      "Epoch 6, Batch: 165: Training Loss: 0.02925216406583786, Validation Loss: 0.03265262395143509\n",
      "Epoch 6, Batch: 166: Training Loss: 0.026565786451101303, Validation Loss: 0.030365144833922386\n",
      "Epoch 6, Batch: 167: Training Loss: 0.02490117773413658, Validation Loss: 0.030714817345142365\n",
      "Epoch 6, Batch: 168: Training Loss: 0.028313886374235153, Validation Loss: 0.03101959452033043\n",
      "Epoch 6, Batch: 169: Training Loss: 0.029187696054577827, Validation Loss: 0.026992905884981155\n",
      "Epoch 6, Batch: 170: Training Loss: 0.027690885588526726, Validation Loss: 0.030340367928147316\n",
      "Epoch 6, Batch: 171: Training Loss: 0.027237683534622192, Validation Loss: 0.02910071425139904\n",
      "Epoch 6, Batch: 172: Training Loss: 0.03162956237792969, Validation Loss: 0.028047868981957436\n",
      "Epoch 6, Batch: 173: Training Loss: 0.026745527982711792, Validation Loss: 0.029450271278619766\n",
      "Epoch 6, Batch: 174: Training Loss: 0.02760257199406624, Validation Loss: 0.028743837028741837\n",
      "Epoch 6, Batch: 175: Training Loss: 0.028412234038114548, Validation Loss: 0.028616076335310936\n",
      "Epoch 6, Batch: 176: Training Loss: 0.03299036994576454, Validation Loss: 0.02823391929268837\n",
      "Epoch 6, Batch: 177: Training Loss: 0.02727445214986801, Validation Loss: 0.02669435553252697\n",
      "Epoch 6, Batch: 178: Training Loss: 0.032872334122657776, Validation Loss: 0.02819809503853321\n",
      "Epoch 6, Batch: 179: Training Loss: 0.029448464512825012, Validation Loss: 0.02748655341565609\n",
      "Epoch 6, Batch: 180: Training Loss: 0.029193805530667305, Validation Loss: 0.02796347625553608\n",
      "Epoch 6, Batch: 181: Training Loss: 0.028241876512765884, Validation Loss: 0.02680511772632599\n",
      "Epoch 6, Batch: 182: Training Loss: 0.03235926479101181, Validation Loss: 0.026400495320558548\n",
      "Epoch 6, Batch: 183: Training Loss: 0.028635136783123016, Validation Loss: 0.026919160038232803\n",
      "Epoch 6, Batch: 184: Training Loss: 0.02675124816596508, Validation Loss: 0.027807381004095078\n",
      "Epoch 6, Batch: 185: Training Loss: 0.03274401277303696, Validation Loss: 0.026209546253085136\n",
      "Epoch 6, Batch: 186: Training Loss: 0.02722368948161602, Validation Loss: 0.027004459872841835\n",
      "Epoch 6, Batch: 187: Training Loss: 0.029544558376073837, Validation Loss: 0.028143545612692833\n",
      "Epoch 6, Batch: 188: Training Loss: 0.028515351936221123, Validation Loss: 0.027768393978476524\n",
      "Epoch 6, Batch: 189: Training Loss: 0.024759693071246147, Validation Loss: 0.02932664379477501\n",
      "Epoch 6, Batch: 190: Training Loss: 0.023229213431477547, Validation Loss: 0.02604898065328598\n",
      "Epoch 6, Batch: 191: Training Loss: 0.03022371418774128, Validation Loss: 0.02673451602458954\n",
      "Epoch 6, Batch: 192: Training Loss: 0.02826467528939247, Validation Loss: 0.028865475207567215\n",
      "Epoch 6, Batch: 193: Training Loss: 0.02829529531300068, Validation Loss: 0.028022855520248413\n",
      "Epoch 6, Batch: 194: Training Loss: 0.030561139807105064, Validation Loss: 0.02780747041106224\n",
      "Epoch 6, Batch: 195: Training Loss: 0.024754555895924568, Validation Loss: 0.02898329682648182\n",
      "Epoch 6, Batch: 196: Training Loss: 0.0304290521889925, Validation Loss: 0.03021574392914772\n",
      "Epoch 6, Batch: 197: Training Loss: 0.0256966520100832, Validation Loss: 0.0309696476906538\n",
      "Epoch 6, Batch: 198: Training Loss: 0.026481149718165398, Validation Loss: 0.028544249013066292\n",
      "Epoch 6, Batch: 199: Training Loss: 0.027648866176605225, Validation Loss: 0.03086628019809723\n",
      "Epoch 6, Batch: 200: Training Loss: 0.02321244217455387, Validation Loss: 0.030816979706287384\n",
      "Epoch 6, Batch: 201: Training Loss: 0.03476310893893242, Validation Loss: 0.029993558302521706\n",
      "Epoch 6, Batch: 202: Training Loss: 0.03039550967514515, Validation Loss: 0.0282167699187994\n",
      "Epoch 6, Batch: 203: Training Loss: 0.03037390299141407, Validation Loss: 0.029446637257933617\n",
      "Epoch 6, Batch: 204: Training Loss: 0.029970597475767136, Validation Loss: 0.02887037768959999\n",
      "Epoch 6, Batch: 205: Training Loss: 0.03365481644868851, Validation Loss: 0.027604108676314354\n",
      "Epoch 6, Batch: 206: Training Loss: 0.025406882166862488, Validation Loss: 0.02795126661658287\n",
      "Epoch 6, Batch: 207: Training Loss: 0.029916835948824883, Validation Loss: 0.028457771986722946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 208: Training Loss: 0.02671295404434204, Validation Loss: 0.0266619510948658\n",
      "Epoch 6, Batch: 209: Training Loss: 0.028193913400173187, Validation Loss: 0.029701193794608116\n",
      "Epoch 6, Batch: 210: Training Loss: 0.03147495165467262, Validation Loss: 0.027516135945916176\n",
      "Epoch 6, Batch: 211: Training Loss: 0.029186749830842018, Validation Loss: 0.023579027503728867\n",
      "Epoch 6, Batch: 212: Training Loss: 0.029069289565086365, Validation Loss: 0.028522716835141182\n",
      "Epoch 6, Batch: 213: Training Loss: 0.02725474163889885, Validation Loss: 0.026582587510347366\n",
      "Epoch 6, Batch: 214: Training Loss: 0.025442082434892654, Validation Loss: 0.029112858697772026\n",
      "Epoch 6, Batch: 215: Training Loss: 0.029412422329187393, Validation Loss: 0.026859663426876068\n",
      "Epoch 6, Batch: 216: Training Loss: 0.025942929089069366, Validation Loss: 0.027702899649739265\n",
      "Epoch 6, Batch: 217: Training Loss: 0.031166406348347664, Validation Loss: 0.03099443018436432\n",
      "Epoch 6, Batch: 218: Training Loss: 0.027013219892978668, Validation Loss: 0.0308696236461401\n",
      "Epoch 6, Batch: 219: Training Loss: 0.028396008536219597, Validation Loss: 0.029555829241871834\n",
      "Epoch 6, Batch: 220: Training Loss: 0.030494695529341698, Validation Loss: 0.03067171759903431\n",
      "Epoch 6, Batch: 221: Training Loss: 0.026449257507920265, Validation Loss: 0.02968659996986389\n",
      "Epoch 6, Batch: 222: Training Loss: 0.030516138300299644, Validation Loss: 0.029069533571600914\n",
      "Epoch 6, Batch: 223: Training Loss: 0.024545952677726746, Validation Loss: 0.030429350212216377\n",
      "Epoch 6, Batch: 224: Training Loss: 0.029508152976632118, Validation Loss: 0.02732781134545803\n",
      "Epoch 6, Batch: 225: Training Loss: 0.02686644345521927, Validation Loss: 0.028246359899640083\n",
      "Epoch 6, Batch: 226: Training Loss: 0.028404269367456436, Validation Loss: 0.02867438644170761\n",
      "Epoch 6, Batch: 227: Training Loss: 0.0270596444606781, Validation Loss: 0.023851053789258003\n",
      "Epoch 6, Batch: 228: Training Loss: 0.026447489857673645, Validation Loss: 0.027651818469166756\n",
      "Epoch 6, Batch: 229: Training Loss: 0.028484918177127838, Validation Loss: 0.028512468561530113\n",
      "Epoch 6, Batch: 230: Training Loss: 0.027023347094655037, Validation Loss: 0.02623339556157589\n",
      "Epoch 6, Batch: 231: Training Loss: 0.03656625375151634, Validation Loss: 0.03126602992415428\n",
      "Epoch 6, Batch: 232: Training Loss: 0.03147459030151367, Validation Loss: 0.028129316866397858\n",
      "Epoch 6, Batch: 233: Training Loss: 0.027573203667998314, Validation Loss: 0.02948511578142643\n",
      "Epoch 6, Batch: 234: Training Loss: 0.02661205641925335, Validation Loss: 0.029318884015083313\n",
      "Epoch 6, Batch: 235: Training Loss: 0.03322656452655792, Validation Loss: 0.028633765876293182\n",
      "Epoch 6, Batch: 236: Training Loss: 0.03302524983882904, Validation Loss: 0.027908727526664734\n",
      "Epoch 6, Batch: 237: Training Loss: 0.031053733080625534, Validation Loss: 0.02969767339527607\n",
      "Epoch 6, Batch: 238: Training Loss: 0.030981259420514107, Validation Loss: 0.03115813620388508\n",
      "Epoch 6, Batch: 239: Training Loss: 0.026408640667796135, Validation Loss: 0.03163439407944679\n",
      "Epoch 6, Batch: 240: Training Loss: 0.026154931634664536, Validation Loss: 0.029082797467708588\n",
      "Epoch 6, Batch: 241: Training Loss: 0.03373974189162254, Validation Loss: 0.033677008002996445\n",
      "Epoch 6, Batch: 242: Training Loss: 0.03090718574821949, Validation Loss: 0.03141433373093605\n",
      "Epoch 6, Batch: 243: Training Loss: 0.034661248326301575, Validation Loss: 0.031540412455797195\n",
      "Epoch 6, Batch: 244: Training Loss: 0.03148295357823372, Validation Loss: 0.031688377261161804\n",
      "Epoch 6, Batch: 245: Training Loss: 0.031275346875190735, Validation Loss: 0.032386861741542816\n",
      "Epoch 6, Batch: 246: Training Loss: 0.02749170921742916, Validation Loss: 0.03249571844935417\n",
      "Epoch 6, Batch: 247: Training Loss: 0.02921738103032112, Validation Loss: 0.0325341634452343\n",
      "Epoch 6, Batch: 248: Training Loss: 0.028563080355525017, Validation Loss: 0.030323831364512444\n",
      "Epoch 6, Batch: 249: Training Loss: 0.025540070608258247, Validation Loss: 0.027746230363845825\n",
      "Epoch 6, Batch: 250: Training Loss: 0.032927125692367554, Validation Loss: 0.028273560106754303\n",
      "Epoch 6, Batch: 251: Training Loss: 0.025383025407791138, Validation Loss: 0.027443885803222656\n",
      "Epoch 6, Batch: 252: Training Loss: 0.031118422746658325, Validation Loss: 0.02798287197947502\n",
      "Epoch 6, Batch: 253: Training Loss: 0.0311285387724638, Validation Loss: 0.029973909258842468\n",
      "Epoch 6, Batch: 254: Training Loss: 0.02632957510650158, Validation Loss: 0.028877325356006622\n",
      "Epoch 6, Batch: 255: Training Loss: 0.02433117851614952, Validation Loss: 0.029662104323506355\n",
      "Epoch 6, Batch: 256: Training Loss: 0.030482862144708633, Validation Loss: 0.027570748701691628\n",
      "Epoch 6, Batch: 257: Training Loss: 0.03331751376390457, Validation Loss: 0.026093048974871635\n",
      "Epoch 6, Batch: 258: Training Loss: 0.03071863390505314, Validation Loss: 0.027219321578741074\n",
      "Epoch 6, Batch: 259: Training Loss: 0.02634406089782715, Validation Loss: 0.0289507657289505\n",
      "Epoch 6, Batch: 260: Training Loss: 0.026408178731799126, Validation Loss: 0.028198616579174995\n",
      "Epoch 6, Batch: 261: Training Loss: 0.027684036642313004, Validation Loss: 0.026701701804995537\n",
      "Epoch 6, Batch: 262: Training Loss: 0.028844274580478668, Validation Loss: 0.027598226442933083\n",
      "Epoch 6, Batch: 263: Training Loss: 0.02781490422785282, Validation Loss: 0.027930203825235367\n",
      "Epoch 6, Batch: 264: Training Loss: 0.026600971817970276, Validation Loss: 0.026805778965353966\n",
      "Epoch 6, Batch: 265: Training Loss: 0.028496434912085533, Validation Loss: 0.027957195416092873\n",
      "Epoch 6, Batch: 266: Training Loss: 0.025493357330560684, Validation Loss: 0.024655232205986977\n",
      "Epoch 6, Batch: 267: Training Loss: 0.025218814611434937, Validation Loss: 0.026986965909600258\n",
      "Epoch 6, Batch: 268: Training Loss: 0.02535267174243927, Validation Loss: 0.02394734136760235\n",
      "Epoch 6, Batch: 269: Training Loss: 0.0312420055270195, Validation Loss: 0.027085892856121063\n",
      "Epoch 6, Batch: 270: Training Loss: 0.025358328595757484, Validation Loss: 0.026805369183421135\n",
      "Epoch 6, Batch: 271: Training Loss: 0.0225669052451849, Validation Loss: 0.028881123289465904\n",
      "Epoch 6, Batch: 272: Training Loss: 0.02521788328886032, Validation Loss: 0.026501042768359184\n",
      "Epoch 6, Batch: 273: Training Loss: 0.025265520438551903, Validation Loss: 0.029645193368196487\n",
      "Epoch 6, Batch: 274: Training Loss: 0.026120567694306374, Validation Loss: 0.02744164504110813\n",
      "Epoch 6, Batch: 275: Training Loss: 0.028280582278966904, Validation Loss: 0.025879457592964172\n",
      "Epoch 6, Batch: 276: Training Loss: 0.030820108950138092, Validation Loss: 0.026339266449213028\n",
      "Epoch 6, Batch: 277: Training Loss: 0.02439725212752819, Validation Loss: 0.02716316655278206\n",
      "Epoch 6, Batch: 278: Training Loss: 0.029115289449691772, Validation Loss: 0.025106552988290787\n",
      "Epoch 6, Batch: 279: Training Loss: 0.030780591070652008, Validation Loss: 0.023977935314178467\n",
      "Epoch 6, Batch: 280: Training Loss: 0.028813743963837624, Validation Loss: 0.024239882826805115\n",
      "Epoch 6, Batch: 281: Training Loss: 0.026846371591091156, Validation Loss: 0.027017321437597275\n",
      "Epoch 6, Batch: 282: Training Loss: 0.027448264881968498, Validation Loss: 0.025540536269545555\n",
      "Epoch 6, Batch: 283: Training Loss: 0.027088116854429245, Validation Loss: 0.02535928599536419\n",
      "Epoch 6, Batch: 284: Training Loss: 0.026657085865736008, Validation Loss: 0.027048319578170776\n",
      "Epoch 6, Batch: 285: Training Loss: 0.026870930567383766, Validation Loss: 0.023600634187459946\n",
      "Epoch 6, Batch: 286: Training Loss: 0.025160254910588264, Validation Loss: 0.023926222696900368\n",
      "Epoch 6, Batch: 287: Training Loss: 0.02926246076822281, Validation Loss: 0.02736366167664528\n",
      "Epoch 6, Batch: 288: Training Loss: 0.028069214895367622, Validation Loss: 0.02766740880906582\n",
      "Epoch 6, Batch: 289: Training Loss: 0.03074810840189457, Validation Loss: 0.025755465030670166\n",
      "Epoch 6, Batch: 290: Training Loss: 0.0289800763130188, Validation Loss: 0.026964016258716583\n",
      "Epoch 6, Batch: 291: Training Loss: 0.025533247739076614, Validation Loss: 0.02836594544351101\n",
      "Epoch 6, Batch: 292: Training Loss: 0.027885697782039642, Validation Loss: 0.026797909289598465\n",
      "Epoch 6, Batch: 293: Training Loss: 0.028591813519597054, Validation Loss: 0.02747275121510029\n",
      "Epoch 6, Batch: 294: Training Loss: 0.027177860960364342, Validation Loss: 0.026090499013662338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 295: Training Loss: 0.024260997772216797, Validation Loss: 0.02711731381714344\n",
      "Epoch 6, Batch: 296: Training Loss: 0.028489485383033752, Validation Loss: 0.027825331315398216\n",
      "Epoch 6, Batch: 297: Training Loss: 0.027720995247364044, Validation Loss: 0.026724357157945633\n",
      "Epoch 6, Batch: 298: Training Loss: 0.03164251148700714, Validation Loss: 0.02514246664941311\n",
      "Epoch 6, Batch: 299: Training Loss: 0.027315223589539528, Validation Loss: 0.02851833589375019\n",
      "Epoch 6, Batch: 300: Training Loss: 0.026944056153297424, Validation Loss: 0.025360846891999245\n",
      "Epoch 6, Batch: 301: Training Loss: 0.026601517572999, Validation Loss: 0.029786212369799614\n",
      "Epoch 6, Batch: 302: Training Loss: 0.02566087618470192, Validation Loss: 0.026431003585457802\n",
      "Epoch 6, Batch: 303: Training Loss: 0.02728092670440674, Validation Loss: 0.026872672140598297\n",
      "Epoch 6, Batch: 304: Training Loss: 0.02507738396525383, Validation Loss: 0.02880558930337429\n",
      "Epoch 6, Batch: 305: Training Loss: 0.02133425697684288, Validation Loss: 0.02583666704595089\n",
      "Epoch 6, Batch: 306: Training Loss: 0.02978205308318138, Validation Loss: 0.026484964415431023\n",
      "Epoch 6, Batch: 307: Training Loss: 0.027882305905222893, Validation Loss: 0.025448214262723923\n",
      "Epoch 6, Batch: 308: Training Loss: 0.026000766083598137, Validation Loss: 0.02704503946006298\n",
      "Epoch 6, Batch: 309: Training Loss: 0.02716427855193615, Validation Loss: 0.026349104940891266\n",
      "Epoch 6, Batch: 310: Training Loss: 0.02408510632812977, Validation Loss: 0.023608047515153885\n",
      "Epoch 6, Batch: 311: Training Loss: 0.024737881496548653, Validation Loss: 0.027678659185767174\n",
      "Epoch 6, Batch: 312: Training Loss: 0.024681074544787407, Validation Loss: 0.025289250537753105\n",
      "Epoch 6, Batch: 313: Training Loss: 0.028143318369984627, Validation Loss: 0.027666112408041954\n",
      "Epoch 6, Batch: 314: Training Loss: 0.027726974338293076, Validation Loss: 0.025462880730628967\n",
      "Epoch 6, Batch: 315: Training Loss: 0.029220134019851685, Validation Loss: 0.02858767658472061\n",
      "Epoch 6, Batch: 316: Training Loss: 0.027638161554932594, Validation Loss: 0.028080064803361893\n",
      "Epoch 6, Batch: 317: Training Loss: 0.02716892957687378, Validation Loss: 0.02680322527885437\n",
      "Epoch 6, Batch: 318: Training Loss: 0.025205867365002632, Validation Loss: 0.02596869319677353\n",
      "Epoch 6, Batch: 319: Training Loss: 0.02536473050713539, Validation Loss: 0.025690671056509018\n",
      "Epoch 6, Batch: 320: Training Loss: 0.028600212186574936, Validation Loss: 0.026090964674949646\n",
      "Epoch 6, Batch: 321: Training Loss: 0.022638898342847824, Validation Loss: 0.026933301240205765\n",
      "Epoch 6, Batch: 322: Training Loss: 0.02707911655306816, Validation Loss: 0.027946190908551216\n",
      "Epoch 6, Batch: 323: Training Loss: 0.02504853345453739, Validation Loss: 0.0292736254632473\n",
      "Epoch 6, Batch: 324: Training Loss: 0.031787991523742676, Validation Loss: 0.029484812170267105\n",
      "Epoch 6, Batch: 325: Training Loss: 0.025154123082756996, Validation Loss: 0.026458028703927994\n",
      "Epoch 6, Batch: 326: Training Loss: 0.03257245197892189, Validation Loss: 0.02742016687989235\n",
      "Epoch 6, Batch: 327: Training Loss: 0.025966405868530273, Validation Loss: 0.025596383959054947\n",
      "Epoch 6, Batch: 328: Training Loss: 0.028984053060412407, Validation Loss: 0.028029656037688255\n",
      "Epoch 6, Batch: 329: Training Loss: 0.027798067778348923, Validation Loss: 0.026951290667057037\n",
      "Epoch 6, Batch: 330: Training Loss: 0.02865087427198887, Validation Loss: 0.026780173182487488\n",
      "Epoch 6, Batch: 331: Training Loss: 0.028353584930300713, Validation Loss: 0.027694907039403915\n",
      "Epoch 6, Batch: 332: Training Loss: 0.03043965809047222, Validation Loss: 0.02552679181098938\n",
      "Epoch 6, Batch: 333: Training Loss: 0.028410181403160095, Validation Loss: 0.026769444346427917\n",
      "Epoch 6, Batch: 334: Training Loss: 0.03137029707431793, Validation Loss: 0.02821904420852661\n",
      "Epoch 6, Batch: 335: Training Loss: 0.027162758633494377, Validation Loss: 0.02699902094900608\n",
      "Epoch 6, Batch: 336: Training Loss: 0.027951298281550407, Validation Loss: 0.025629088282585144\n",
      "Epoch 6, Batch: 337: Training Loss: 0.023189859464764595, Validation Loss: 0.025353025645017624\n",
      "Epoch 6, Batch: 338: Training Loss: 0.029364431276917458, Validation Loss: 0.024218203499913216\n",
      "Epoch 6, Batch: 339: Training Loss: 0.025575578212738037, Validation Loss: 0.02599308453500271\n",
      "Epoch 6, Batch: 340: Training Loss: 0.029746247455477715, Validation Loss: 0.028540868312120438\n",
      "Epoch 6, Batch: 341: Training Loss: 0.026468368247151375, Validation Loss: 0.024964770302176476\n",
      "Epoch 6, Batch: 342: Training Loss: 0.029127255082130432, Validation Loss: 0.026594365015625954\n",
      "Epoch 6, Batch: 343: Training Loss: 0.027452051639556885, Validation Loss: 0.027269307523965836\n",
      "Epoch 6, Batch: 344: Training Loss: 0.029098153114318848, Validation Loss: 0.028831666335463524\n",
      "Epoch 6, Batch: 345: Training Loss: 0.028963133692741394, Validation Loss: 0.026402099058032036\n",
      "Epoch 6, Batch: 346: Training Loss: 0.025473400950431824, Validation Loss: 0.028617197647690773\n",
      "Epoch 6, Batch: 347: Training Loss: 0.02786516211926937, Validation Loss: 0.02831350825726986\n",
      "Epoch 6, Batch: 348: Training Loss: 0.026959940791130066, Validation Loss: 0.028653522953391075\n",
      "Epoch 6, Batch: 349: Training Loss: 0.029420118778944016, Validation Loss: 0.027724489569664\n",
      "Epoch 6, Batch: 350: Training Loss: 0.024787873029708862, Validation Loss: 0.027446990832686424\n",
      "Epoch 6, Batch: 351: Training Loss: 0.027029454708099365, Validation Loss: 0.028336672112345695\n",
      "Epoch 6, Batch: 352: Training Loss: 0.029095260426402092, Validation Loss: 0.03016369789838791\n",
      "Epoch 6, Batch: 353: Training Loss: 0.026818081736564636, Validation Loss: 0.027823178097605705\n",
      "Epoch 6, Batch: 354: Training Loss: 0.027920398861169815, Validation Loss: 0.027738023549318314\n",
      "Epoch 6, Batch: 355: Training Loss: 0.023471591994166374, Validation Loss: 0.03054448962211609\n",
      "Epoch 6, Batch: 356: Training Loss: 0.0307903029024601, Validation Loss: 0.02906719222664833\n",
      "Epoch 6, Batch: 357: Training Loss: 0.024144699797034264, Validation Loss: 0.027792057022452354\n",
      "Epoch 6, Batch: 358: Training Loss: 0.03181484714150429, Validation Loss: 0.02715536579489708\n",
      "Epoch 6, Batch: 359: Training Loss: 0.029048528522253036, Validation Loss: 0.025943905115127563\n",
      "Epoch 6, Batch: 360: Training Loss: 0.02927875705063343, Validation Loss: 0.026624957099556923\n",
      "Epoch 6, Batch: 361: Training Loss: 0.027942920103669167, Validation Loss: 0.027320802211761475\n",
      "Epoch 6, Batch: 362: Training Loss: 0.023792875930666924, Validation Loss: 0.0251413993537426\n",
      "Epoch 6, Batch: 363: Training Loss: 0.027542416006326675, Validation Loss: 0.02468707226216793\n",
      "Epoch 6, Batch: 364: Training Loss: 0.030982308089733124, Validation Loss: 0.025937501341104507\n",
      "Epoch 6, Batch: 365: Training Loss: 0.029151229187846184, Validation Loss: 0.026354065164923668\n",
      "Epoch 6, Batch: 366: Training Loss: 0.026416275650262833, Validation Loss: 0.025268739089369774\n",
      "Epoch 6, Batch: 367: Training Loss: 0.023454392328858376, Validation Loss: 0.02830570749938488\n",
      "Epoch 6, Batch: 368: Training Loss: 0.027225906029343605, Validation Loss: 0.024475472047924995\n",
      "Epoch 6, Batch: 369: Training Loss: 0.0273818951100111, Validation Loss: 0.025477878749370575\n",
      "Epoch 6, Batch: 370: Training Loss: 0.03093225508928299, Validation Loss: 0.026353346183896065\n",
      "Epoch 6, Batch: 371: Training Loss: 0.027389366179704666, Validation Loss: 0.02590501308441162\n",
      "Epoch 6, Batch: 372: Training Loss: 0.029592646285891533, Validation Loss: 0.02671092376112938\n",
      "Epoch 6, Batch: 373: Training Loss: 0.03100847825407982, Validation Loss: 0.028012441471219063\n",
      "Epoch 6, Batch: 374: Training Loss: 0.0278134997934103, Validation Loss: 0.02595589868724346\n",
      "Epoch 6, Batch: 375: Training Loss: 0.026691874489188194, Validation Loss: 0.027385255321860313\n",
      "Epoch 6, Batch: 376: Training Loss: 0.025967391207814217, Validation Loss: 0.025651326403021812\n",
      "Epoch 6, Batch: 377: Training Loss: 0.027774622663855553, Validation Loss: 0.026249073445796967\n",
      "Epoch 6, Batch: 378: Training Loss: 0.02290036343038082, Validation Loss: 0.02685648575425148\n",
      "Epoch 6, Batch: 379: Training Loss: 0.028348997235298157, Validation Loss: 0.030591361224651337\n",
      "Epoch 6, Batch: 380: Training Loss: 0.03340110927820206, Validation Loss: 0.024451779201626778\n",
      "Epoch 6, Batch: 381: Training Loss: 0.031680308282375336, Validation Loss: 0.026917992159724236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 382: Training Loss: 0.0313500352203846, Validation Loss: 0.027190497145056725\n",
      "Epoch 6, Batch: 383: Training Loss: 0.029013164341449738, Validation Loss: 0.027231376618146896\n",
      "Epoch 6, Batch: 384: Training Loss: 0.028091229498386383, Validation Loss: 0.0285731703042984\n",
      "Epoch 6, Batch: 385: Training Loss: 0.027260953560471535, Validation Loss: 0.02789975143969059\n",
      "Epoch 6, Batch: 386: Training Loss: 0.026499852538108826, Validation Loss: 0.02565010078251362\n",
      "Epoch 6, Batch: 387: Training Loss: 0.03154602274298668, Validation Loss: 0.027841323986649513\n",
      "Epoch 6, Batch: 388: Training Loss: 0.02749759517610073, Validation Loss: 0.02697480283677578\n",
      "Epoch 6, Batch: 389: Training Loss: 0.0321691669523716, Validation Loss: 0.02609420381486416\n",
      "Epoch 6, Batch: 390: Training Loss: 0.024619944393634796, Validation Loss: 0.026029475033283234\n",
      "Epoch 6, Batch: 391: Training Loss: 0.027777232229709625, Validation Loss: 0.025096651166677475\n",
      "Epoch 6, Batch: 392: Training Loss: 0.02645895816385746, Validation Loss: 0.02669772133231163\n",
      "Saving new best model w/ loss: 0.02285848557949066\n",
      "Epoch 6, Batch: 393: Training Loss: 0.030156247317790985, Validation Loss: 0.02285848557949066\n",
      "Epoch 6, Batch: 394: Training Loss: 0.030138015747070312, Validation Loss: 0.02430732361972332\n",
      "Epoch 6, Batch: 395: Training Loss: 0.027215976268053055, Validation Loss: 0.026917750015854836\n",
      "Epoch 6, Batch: 396: Training Loss: 0.02665611170232296, Validation Loss: 0.025959143415093422\n",
      "Epoch 6, Batch: 397: Training Loss: 0.030116597190499306, Validation Loss: 0.02698391117155552\n",
      "Epoch 6, Batch: 398: Training Loss: 0.02775624766945839, Validation Loss: 0.025063592940568924\n",
      "Epoch 6, Batch: 399: Training Loss: 0.028419073671102524, Validation Loss: 0.02625054121017456\n",
      "Epoch 6, Batch: 400: Training Loss: 0.029702981933951378, Validation Loss: 0.024933699518442154\n",
      "Epoch 6, Batch: 401: Training Loss: 0.027657996863126755, Validation Loss: 0.024970000609755516\n",
      "Epoch 6, Batch: 402: Training Loss: 0.026946650817990303, Validation Loss: 0.024648239836096764\n",
      "Epoch 6, Batch: 403: Training Loss: 0.030437206849455833, Validation Loss: 0.028234943747520447\n",
      "Epoch 6, Batch: 404: Training Loss: 0.02806766889989376, Validation Loss: 0.027457239106297493\n",
      "Epoch 6, Batch: 405: Training Loss: 0.028516288846731186, Validation Loss: 0.029177019372582436\n",
      "Epoch 6, Batch: 406: Training Loss: 0.027146581560373306, Validation Loss: 0.028931085020303726\n",
      "Epoch 6, Batch: 407: Training Loss: 0.026367561891674995, Validation Loss: 0.02832961454987526\n",
      "Epoch 6, Batch: 408: Training Loss: 0.02894115075469017, Validation Loss: 0.029271768406033516\n",
      "Epoch 6, Batch: 409: Training Loss: 0.028182106092572212, Validation Loss: 0.030217867344617844\n",
      "Epoch 6, Batch: 410: Training Loss: 0.02673335373401642, Validation Loss: 0.026893267408013344\n",
      "Epoch 6, Batch: 411: Training Loss: 0.026361167430877686, Validation Loss: 0.025906242430210114\n",
      "Epoch 6, Batch: 412: Training Loss: 0.02992802858352661, Validation Loss: 0.026526227593421936\n",
      "Epoch 6, Batch: 413: Training Loss: 0.026171691715717316, Validation Loss: 0.026238512247800827\n",
      "Epoch 6, Batch: 414: Training Loss: 0.025214921683073044, Validation Loss: 0.030909821391105652\n",
      "Epoch 6, Batch: 415: Training Loss: 0.027797367423772812, Validation Loss: 0.02649708092212677\n",
      "Epoch 6, Batch: 416: Training Loss: 0.027978938072919846, Validation Loss: 0.028113756328821182\n",
      "Epoch 6, Batch: 417: Training Loss: 0.031107839196920395, Validation Loss: 0.02865460142493248\n",
      "Epoch 6, Batch: 418: Training Loss: 0.02432774193584919, Validation Loss: 0.026832502335309982\n",
      "Epoch 6, Batch: 419: Training Loss: 0.029399270191788673, Validation Loss: 0.028533725067973137\n",
      "Epoch 6, Batch: 420: Training Loss: 0.02539287507534027, Validation Loss: 0.029433291405439377\n",
      "Epoch 6, Batch: 421: Training Loss: 0.03286077454686165, Validation Loss: 0.028090063482522964\n",
      "Epoch 6, Batch: 422: Training Loss: 0.028675079345703125, Validation Loss: 0.02777247130870819\n",
      "Epoch 6, Batch: 423: Training Loss: 0.033186350017786026, Validation Loss: 0.02682865411043167\n",
      "Epoch 6, Batch: 424: Training Loss: 0.02740386500954628, Validation Loss: 0.025885071605443954\n",
      "Epoch 6, Batch: 425: Training Loss: 0.028227193281054497, Validation Loss: 0.02647862397134304\n",
      "Epoch 6, Batch: 426: Training Loss: 0.02652222476899624, Validation Loss: 0.027293726801872253\n",
      "Epoch 6, Batch: 427: Training Loss: 0.03176838904619217, Validation Loss: 0.027506431564688683\n",
      "Epoch 6, Batch: 428: Training Loss: 0.030148206278681755, Validation Loss: 0.029437938705086708\n",
      "Epoch 6, Batch: 429: Training Loss: 0.030187683179974556, Validation Loss: 0.02902618795633316\n",
      "Epoch 6, Batch: 430: Training Loss: 0.026601167395710945, Validation Loss: 0.029484348371624947\n",
      "Epoch 6, Batch: 431: Training Loss: 0.031195348128676414, Validation Loss: 0.027689438313245773\n",
      "Epoch 6, Batch: 432: Training Loss: 0.034379515796899796, Validation Loss: 0.02513481117784977\n",
      "Epoch 6, Batch: 433: Training Loss: 0.02966322936117649, Validation Loss: 0.027447745203971863\n",
      "Epoch 6, Batch: 434: Training Loss: 0.02576925978064537, Validation Loss: 0.027913860976696014\n",
      "Epoch 6, Batch: 435: Training Loss: 0.0251875389367342, Validation Loss: 0.026916900649666786\n",
      "Epoch 6, Batch: 436: Training Loss: 0.02559947408735752, Validation Loss: 0.026414915919303894\n",
      "Epoch 6, Batch: 437: Training Loss: 0.02268820069730282, Validation Loss: 0.027834240347146988\n",
      "Epoch 6, Batch: 438: Training Loss: 0.022608524188399315, Validation Loss: 0.025897618383169174\n",
      "Epoch 6, Batch: 439: Training Loss: 0.027871713042259216, Validation Loss: 0.02826799638569355\n",
      "Epoch 6, Batch: 440: Training Loss: 0.030091309919953346, Validation Loss: 0.02798115834593773\n",
      "Epoch 6, Batch: 441: Training Loss: 0.026473689824342728, Validation Loss: 0.028424866497516632\n",
      "Epoch 6, Batch: 442: Training Loss: 0.027044469490647316, Validation Loss: 0.028599373996257782\n",
      "Epoch 6, Batch: 443: Training Loss: 0.02313881367444992, Validation Loss: 0.027419645339250565\n",
      "Epoch 6, Batch: 444: Training Loss: 0.02361701801419258, Validation Loss: 0.029554737731814384\n",
      "Epoch 6, Batch: 445: Training Loss: 0.028096847236156464, Validation Loss: 0.02981259860098362\n",
      "Epoch 6, Batch: 446: Training Loss: 0.03282478451728821, Validation Loss: 0.02576078660786152\n",
      "Epoch 6, Batch: 447: Training Loss: 0.02577969804406166, Validation Loss: 0.026898233219981194\n",
      "Epoch 6, Batch: 448: Training Loss: 0.028556043282151222, Validation Loss: 0.028049197047948837\n",
      "Epoch 6, Batch: 449: Training Loss: 0.02704905904829502, Validation Loss: 0.02776372991502285\n",
      "Epoch 6, Batch: 450: Training Loss: 0.024774575605988503, Validation Loss: 0.02705027349293232\n",
      "Epoch 6, Batch: 451: Training Loss: 0.030461812391877174, Validation Loss: 0.02771991491317749\n",
      "Epoch 6, Batch: 452: Training Loss: 0.027829909697175026, Validation Loss: 0.027013100683689117\n",
      "Epoch 6, Batch: 453: Training Loss: 0.027842091396450996, Validation Loss: 0.026721417903900146\n",
      "Epoch 6, Batch: 454: Training Loss: 0.025963131338357925, Validation Loss: 0.03040838986635208\n",
      "Epoch 6, Batch: 455: Training Loss: 0.020903125405311584, Validation Loss: 0.024274691939353943\n",
      "Epoch 6, Batch: 456: Training Loss: 0.02527264691889286, Validation Loss: 0.02587314322590828\n",
      "Epoch 6, Batch: 457: Training Loss: 0.028458304703235626, Validation Loss: 0.027335915714502335\n",
      "Epoch 6, Batch: 458: Training Loss: 0.02292811870574951, Validation Loss: 0.02676105685532093\n",
      "Epoch 6, Batch: 459: Training Loss: 0.028229884803295135, Validation Loss: 0.02627645805478096\n",
      "Epoch 6, Batch: 460: Training Loss: 0.026825373992323875, Validation Loss: 0.026272764429450035\n",
      "Epoch 6, Batch: 461: Training Loss: 0.02304423600435257, Validation Loss: 0.02522283047437668\n",
      "Epoch 6, Batch: 462: Training Loss: 0.021224431693553925, Validation Loss: 0.026763398200273514\n",
      "Epoch 6, Batch: 463: Training Loss: 0.026151088997721672, Validation Loss: 0.028387466445565224\n",
      "Epoch 6, Batch: 464: Training Loss: 0.024520186707377434, Validation Loss: 0.02713528648018837\n",
      "Epoch 6, Batch: 465: Training Loss: 0.02940690703690052, Validation Loss: 0.027102915570139885\n",
      "Epoch 6, Batch: 466: Training Loss: 0.02350650355219841, Validation Loss: 0.026487722992897034\n",
      "Epoch 6, Batch: 467: Training Loss: 0.03499620407819748, Validation Loss: 0.02630012296140194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch: 468: Training Loss: 0.029324352741241455, Validation Loss: 0.025567250326275826\n",
      "Epoch 6, Batch: 469: Training Loss: 0.02561993896961212, Validation Loss: 0.0266959797590971\n",
      "Epoch 6, Batch: 470: Training Loss: 0.027116037905216217, Validation Loss: 0.028335707262158394\n",
      "Epoch 6, Batch: 471: Training Loss: 0.028954077512025833, Validation Loss: 0.027552025392651558\n",
      "Epoch 6, Batch: 472: Training Loss: 0.030859576538205147, Validation Loss: 0.029051460325717926\n",
      "Epoch 6, Batch: 473: Training Loss: 0.02287072315812111, Validation Loss: 0.02700081467628479\n",
      "Epoch 6, Batch: 474: Training Loss: 0.02643364481627941, Validation Loss: 0.028053410351276398\n",
      "Epoch 6, Batch: 475: Training Loss: 0.02826118841767311, Validation Loss: 0.02818717435002327\n",
      "Epoch 6, Batch: 476: Training Loss: 0.025984516367316246, Validation Loss: 0.027594109997153282\n",
      "Epoch 6, Batch: 477: Training Loss: 0.030593810603022575, Validation Loss: 0.02993694320321083\n",
      "Epoch 6, Batch: 478: Training Loss: 0.026731368154287338, Validation Loss: 0.02508527599275112\n",
      "Epoch 6, Batch: 479: Training Loss: 0.02990431897342205, Validation Loss: 0.030154814943671227\n",
      "Epoch 6, Batch: 480: Training Loss: 0.029935242608189583, Validation Loss: 0.027712542563676834\n",
      "Epoch 6, Batch: 481: Training Loss: 0.02812567725777626, Validation Loss: 0.028490733355283737\n",
      "Epoch 6, Batch: 482: Training Loss: 0.03164900839328766, Validation Loss: 0.028478626161813736\n",
      "Epoch 6, Batch: 483: Training Loss: 0.026227619498968124, Validation Loss: 0.029858915135264397\n",
      "Epoch 6, Batch: 484: Training Loss: 0.026393692940473557, Validation Loss: 0.027884434908628464\n",
      "Epoch 6, Batch: 485: Training Loss: 0.027900639921426773, Validation Loss: 0.026281872764229774\n",
      "Epoch 6, Batch: 486: Training Loss: 0.031046800315380096, Validation Loss: 0.030038166791200638\n",
      "Epoch 6, Batch: 487: Training Loss: 0.025432443246245384, Validation Loss: 0.029027622193098068\n",
      "Epoch 6, Batch: 488: Training Loss: 0.024889281019568443, Validation Loss: 0.028708944097161293\n",
      "Epoch 6, Batch: 489: Training Loss: 0.03107474185526371, Validation Loss: 0.03067939169704914\n",
      "Epoch 6, Batch: 490: Training Loss: 0.027936330065131187, Validation Loss: 0.02751988172531128\n",
      "Epoch 6, Batch: 491: Training Loss: 0.02692233957350254, Validation Loss: 0.025429705157876015\n",
      "Epoch 6, Batch: 492: Training Loss: 0.027223456650972366, Validation Loss: 0.029653698205947876\n",
      "Epoch 6, Batch: 493: Training Loss: 0.02965131215751171, Validation Loss: 0.02728951722383499\n",
      "Epoch 6, Batch: 494: Training Loss: 0.03595636412501335, Validation Loss: 0.0281162578612566\n",
      "Epoch 6, Batch: 495: Training Loss: 0.025218941271305084, Validation Loss: 0.027897804975509644\n",
      "Epoch 6, Batch: 496: Training Loss: 0.028667328879237175, Validation Loss: 0.028006672859191895\n",
      "Epoch 6, Batch: 497: Training Loss: 0.023026734590530396, Validation Loss: 0.02737540751695633\n",
      "Epoch 6, Batch: 498: Training Loss: 0.02325928956270218, Validation Loss: 0.024825097993016243\n",
      "Epoch 6, Batch: 499: Training Loss: 0.023475406691432, Validation Loss: 0.02693915367126465\n",
      "Epoch 7, Batch: 0: Training Loss: 0.02719978801906109, Validation Loss: 0.023817559704184532\n",
      "Epoch 7, Batch: 1: Training Loss: 0.02357253059744835, Validation Loss: 0.02476205863058567\n",
      "Epoch 7, Batch: 2: Training Loss: 0.0259255301207304, Validation Loss: 0.023573091253638268\n",
      "Epoch 7, Batch: 3: Training Loss: 0.024975674226880074, Validation Loss: 0.025387456640601158\n",
      "Epoch 7, Batch: 4: Training Loss: 0.023328987881541252, Validation Loss: 0.026658663526177406\n",
      "Epoch 7, Batch: 5: Training Loss: 0.02635813131928444, Validation Loss: 0.028949439525604248\n",
      "Epoch 7, Batch: 6: Training Loss: 0.027719395235180855, Validation Loss: 0.02385864406824112\n",
      "Epoch 7, Batch: 7: Training Loss: 0.025513548403978348, Validation Loss: 0.025095704942941666\n",
      "Epoch 7, Batch: 8: Training Loss: 0.027951225638389587, Validation Loss: 0.027394341304898262\n",
      "Epoch 7, Batch: 9: Training Loss: 0.024115361273288727, Validation Loss: 0.028246186673641205\n",
      "Epoch 7, Batch: 10: Training Loss: 0.027032917365431786, Validation Loss: 0.030013084411621094\n",
      "Epoch 7, Batch: 11: Training Loss: 0.030936116352677345, Validation Loss: 0.025289855897426605\n",
      "Epoch 7, Batch: 12: Training Loss: 0.026966538280248642, Validation Loss: 0.029363328590989113\n",
      "Epoch 7, Batch: 13: Training Loss: 0.032631389796733856, Validation Loss: 0.026719003915786743\n",
      "Epoch 7, Batch: 14: Training Loss: 0.029504675418138504, Validation Loss: 0.02765260636806488\n",
      "Epoch 7, Batch: 15: Training Loss: 0.0297717172652483, Validation Loss: 0.028835395351052284\n",
      "Epoch 7, Batch: 16: Training Loss: 0.02683265507221222, Validation Loss: 0.02686495892703533\n",
      "Epoch 7, Batch: 17: Training Loss: 0.0276536513119936, Validation Loss: 0.027723321691155434\n",
      "Epoch 7, Batch: 18: Training Loss: 0.023867465555667877, Validation Loss: 0.02419459819793701\n",
      "Epoch 7, Batch: 19: Training Loss: 0.025162244215607643, Validation Loss: 0.026945212855935097\n",
      "Epoch 7, Batch: 20: Training Loss: 0.030086051672697067, Validation Loss: 0.025950279086828232\n",
      "Epoch 7, Batch: 21: Training Loss: 0.024407681077718735, Validation Loss: 0.025790810585021973\n",
      "Epoch 7, Batch: 22: Training Loss: 0.030451271682977676, Validation Loss: 0.02656923606991768\n",
      "Epoch 7, Batch: 23: Training Loss: 0.026788324117660522, Validation Loss: 0.024144699797034264\n",
      "Epoch 7, Batch: 24: Training Loss: 0.0274293702095747, Validation Loss: 0.028024397790431976\n",
      "Epoch 7, Batch: 25: Training Loss: 0.0255163311958313, Validation Loss: 0.027254316955804825\n",
      "Epoch 7, Batch: 26: Training Loss: 0.027568910270929337, Validation Loss: 0.024971814826130867\n",
      "Epoch 7, Batch: 27: Training Loss: 0.02346697263419628, Validation Loss: 0.026027634739875793\n",
      "Epoch 7, Batch: 28: Training Loss: 0.028898850083351135, Validation Loss: 0.0268727857619524\n",
      "Epoch 7, Batch: 29: Training Loss: 0.02481590211391449, Validation Loss: 0.0275132916867733\n",
      "Epoch 7, Batch: 30: Training Loss: 0.023390915244817734, Validation Loss: 0.027498386800289154\n",
      "Epoch 7, Batch: 31: Training Loss: 0.028523316606879234, Validation Loss: 0.0279550701379776\n",
      "Epoch 7, Batch: 32: Training Loss: 0.028362028300762177, Validation Loss: 0.028743363916873932\n",
      "Epoch 7, Batch: 33: Training Loss: 0.025770869106054306, Validation Loss: 0.02757301740348339\n",
      "Epoch 7, Batch: 34: Training Loss: 0.025134313851594925, Validation Loss: 0.028330175206065178\n",
      "Epoch 7, Batch: 35: Training Loss: 0.025616085156798363, Validation Loss: 0.02617166005074978\n",
      "Epoch 7, Batch: 36: Training Loss: 0.02464321441948414, Validation Loss: 0.028227943927049637\n",
      "Epoch 7, Batch: 37: Training Loss: 0.023966506123542786, Validation Loss: 0.02692628838121891\n",
      "Epoch 7, Batch: 38: Training Loss: 0.025871550664305687, Validation Loss: 0.027821462601423264\n",
      "Epoch 7, Batch: 39: Training Loss: 0.028701404109597206, Validation Loss: 0.025936182588338852\n",
      "Epoch 7, Batch: 40: Training Loss: 0.03170934319496155, Validation Loss: 0.02675340510904789\n",
      "Epoch 7, Batch: 41: Training Loss: 0.025290029123425484, Validation Loss: 0.02720063365995884\n",
      "Epoch 7, Batch: 42: Training Loss: 0.02322652004659176, Validation Loss: 0.02744125947356224\n",
      "Epoch 7, Batch: 43: Training Loss: 0.023878313601017, Validation Loss: 0.029104769229888916\n",
      "Epoch 7, Batch: 44: Training Loss: 0.02456243522465229, Validation Loss: 0.028577104210853577\n",
      "Epoch 7, Batch: 45: Training Loss: 0.027229376137256622, Validation Loss: 0.029445705935359\n",
      "Epoch 7, Batch: 46: Training Loss: 0.02395256981253624, Validation Loss: 0.02786698192358017\n",
      "Epoch 7, Batch: 47: Training Loss: 0.027490830048918724, Validation Loss: 0.02504832297563553\n",
      "Epoch 7, Batch: 48: Training Loss: 0.030382080003619194, Validation Loss: 0.026714500039815903\n",
      "Epoch 7, Batch: 49: Training Loss: 0.029780004173517227, Validation Loss: 0.02812172658741474\n",
      "Epoch 7, Batch: 50: Training Loss: 0.027204962447285652, Validation Loss: 0.02826990932226181\n",
      "Epoch 7, Batch: 51: Training Loss: 0.02942010574042797, Validation Loss: 0.026689676567912102\n",
      "Epoch 7, Batch: 52: Training Loss: 0.024325409904122353, Validation Loss: 0.02878125198185444\n",
      "Epoch 7, Batch: 53: Training Loss: 0.026246799156069756, Validation Loss: 0.02961658500134945\n",
      "Epoch 7, Batch: 54: Training Loss: 0.027795493602752686, Validation Loss: 0.030914530158042908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 55: Training Loss: 0.027323350310325623, Validation Loss: 0.02932359091937542\n",
      "Epoch 7, Batch: 56: Training Loss: 0.024941079318523407, Validation Loss: 0.027687951922416687\n",
      "Epoch 7, Batch: 57: Training Loss: 0.026911940425634384, Validation Loss: 0.027198055759072304\n",
      "Epoch 7, Batch: 58: Training Loss: 0.031120650470256805, Validation Loss: 0.026833832263946533\n",
      "Epoch 7, Batch: 59: Training Loss: 0.026425758376717567, Validation Loss: 0.026885725557804108\n",
      "Epoch 7, Batch: 60: Training Loss: 0.025448497384786606, Validation Loss: 0.025931553915143013\n",
      "Epoch 7, Batch: 61: Training Loss: 0.031312741339206696, Validation Loss: 0.026501255109906197\n",
      "Epoch 7, Batch: 62: Training Loss: 0.027740759775042534, Validation Loss: 0.0287364199757576\n",
      "Epoch 7, Batch: 63: Training Loss: 0.031829945743083954, Validation Loss: 0.027640454471111298\n",
      "Epoch 7, Batch: 64: Training Loss: 0.02578430064022541, Validation Loss: 0.029635529965162277\n",
      "Epoch 7, Batch: 65: Training Loss: 0.03141101077198982, Validation Loss: 0.02931952476501465\n",
      "Epoch 7, Batch: 66: Training Loss: 0.028705410659313202, Validation Loss: 0.028407035395503044\n",
      "Epoch 7, Batch: 67: Training Loss: 0.02384793385863304, Validation Loss: 0.028765050694346428\n",
      "Epoch 7, Batch: 68: Training Loss: 0.027873756363987923, Validation Loss: 0.029587116092443466\n",
      "Epoch 7, Batch: 69: Training Loss: 0.03018803894519806, Validation Loss: 0.03125540912151337\n",
      "Epoch 7, Batch: 70: Training Loss: 0.02991938218474388, Validation Loss: 0.030913498252630234\n",
      "Epoch 7, Batch: 71: Training Loss: 0.0240199975669384, Validation Loss: 0.027361465618014336\n",
      "Epoch 7, Batch: 72: Training Loss: 0.02846590429544449, Validation Loss: 0.029818857088685036\n",
      "Epoch 7, Batch: 73: Training Loss: 0.024255797266960144, Validation Loss: 0.02869504876434803\n",
      "Epoch 7, Batch: 74: Training Loss: 0.024049146100878716, Validation Loss: 0.02869916707277298\n",
      "Epoch 7, Batch: 75: Training Loss: 0.023308755829930305, Validation Loss: 0.03016071207821369\n",
      "Epoch 7, Batch: 76: Training Loss: 0.026034753769636154, Validation Loss: 0.025900693610310555\n",
      "Epoch 7, Batch: 77: Training Loss: 0.03173580765724182, Validation Loss: 0.02601112797856331\n",
      "Epoch 7, Batch: 78: Training Loss: 0.03408648073673248, Validation Loss: 0.02798023447394371\n",
      "Epoch 7, Batch: 79: Training Loss: 0.023641422390937805, Validation Loss: 0.027911439538002014\n",
      "Epoch 7, Batch: 80: Training Loss: 0.027499733492732048, Validation Loss: 0.028013022616505623\n",
      "Epoch 7, Batch: 81: Training Loss: 0.025033477693796158, Validation Loss: 0.028789857402443886\n",
      "Epoch 7, Batch: 82: Training Loss: 0.028642687946558, Validation Loss: 0.026654528453946114\n",
      "Epoch 7, Batch: 83: Training Loss: 0.026006976142525673, Validation Loss: 0.028572678565979004\n",
      "Epoch 7, Batch: 84: Training Loss: 0.02880305051803589, Validation Loss: 0.028603490442037582\n",
      "Epoch 7, Batch: 85: Training Loss: 0.02657526545226574, Validation Loss: 0.026806361973285675\n",
      "Epoch 7, Batch: 86: Training Loss: 0.033919405192136765, Validation Loss: 0.028099600225687027\n",
      "Epoch 7, Batch: 87: Training Loss: 0.03158194199204445, Validation Loss: 0.02855711616575718\n",
      "Epoch 7, Batch: 88: Training Loss: 0.02965915948152542, Validation Loss: 0.027498390525579453\n",
      "Epoch 7, Batch: 89: Training Loss: 0.033653948456048965, Validation Loss: 0.027123864740133286\n",
      "Epoch 7, Batch: 90: Training Loss: 0.029427941888570786, Validation Loss: 0.026998311281204224\n",
      "Epoch 7, Batch: 91: Training Loss: 0.03303693234920502, Validation Loss: 0.02827928215265274\n",
      "Epoch 7, Batch: 92: Training Loss: 0.032782379537820816, Validation Loss: 0.027630042284727097\n",
      "Epoch 7, Batch: 93: Training Loss: 0.031182870268821716, Validation Loss: 0.028019575402140617\n",
      "Epoch 7, Batch: 94: Training Loss: 0.03152073919773102, Validation Loss: 0.030398884788155556\n",
      "Epoch 7, Batch: 95: Training Loss: 0.029212383553385735, Validation Loss: 0.031506266444921494\n",
      "Epoch 7, Batch: 96: Training Loss: 0.029908791184425354, Validation Loss: 0.02961651422083378\n",
      "Epoch 7, Batch: 97: Training Loss: 0.03386807069182396, Validation Loss: 0.03330686688423157\n",
      "Epoch 7, Batch: 98: Training Loss: 0.033446088433265686, Validation Loss: 0.03237578272819519\n",
      "Epoch 7, Batch: 99: Training Loss: 0.027950750663876534, Validation Loss: 0.03107815980911255\n",
      "Epoch 7, Batch: 100: Training Loss: 0.03171907365322113, Validation Loss: 0.030977357178926468\n",
      "Epoch 7, Batch: 101: Training Loss: 0.027487633749842644, Validation Loss: 0.03268851339817047\n",
      "Epoch 7, Batch: 102: Training Loss: 0.028566304594278336, Validation Loss: 0.03339388221502304\n",
      "Epoch 7, Batch: 103: Training Loss: 0.03419467434287071, Validation Loss: 0.03202269226312637\n",
      "Epoch 7, Batch: 104: Training Loss: 0.02701088972389698, Validation Loss: 0.029604755342006683\n",
      "Epoch 7, Batch: 105: Training Loss: 0.02674848586320877, Validation Loss: 0.03329888731241226\n",
      "Epoch 7, Batch: 106: Training Loss: 0.03145672753453255, Validation Loss: 0.028361521661281586\n",
      "Epoch 7, Batch: 107: Training Loss: 0.028207046911120415, Validation Loss: 0.03208183869719505\n",
      "Epoch 7, Batch: 108: Training Loss: 0.0272967629134655, Validation Loss: 0.03247881308197975\n",
      "Epoch 7, Batch: 109: Training Loss: 0.032510098069906235, Validation Loss: 0.03306535258889198\n",
      "Epoch 7, Batch: 110: Training Loss: 0.030138110741972923, Validation Loss: 0.031325049698352814\n",
      "Epoch 7, Batch: 111: Training Loss: 0.026833925396203995, Validation Loss: 0.03161071985960007\n",
      "Epoch 7, Batch: 112: Training Loss: 0.02223145216703415, Validation Loss: 0.031168300658464432\n",
      "Epoch 7, Batch: 113: Training Loss: 0.028608016669750214, Validation Loss: 0.027504941448569298\n",
      "Epoch 7, Batch: 114: Training Loss: 0.026102952659130096, Validation Loss: 0.02919437550008297\n",
      "Epoch 7, Batch: 115: Training Loss: 0.026384200900793076, Validation Loss: 0.027682585641741753\n",
      "Epoch 7, Batch: 116: Training Loss: 0.026302795857191086, Validation Loss: 0.028086654841899872\n",
      "Epoch 7, Batch: 117: Training Loss: 0.028270915150642395, Validation Loss: 0.03062109462916851\n",
      "Epoch 7, Batch: 118: Training Loss: 0.027382628992199898, Validation Loss: 0.02846778929233551\n",
      "Epoch 7, Batch: 119: Training Loss: 0.030033068731427193, Validation Loss: 0.027528809383511543\n",
      "Epoch 7, Batch: 120: Training Loss: 0.022410685196518898, Validation Loss: 0.026593757793307304\n",
      "Epoch 7, Batch: 121: Training Loss: 0.029433662071824074, Validation Loss: 0.0280501339584589\n",
      "Epoch 7, Batch: 122: Training Loss: 0.028219005092978477, Validation Loss: 0.02646024152636528\n",
      "Epoch 7, Batch: 123: Training Loss: 0.02740238793194294, Validation Loss: 0.026960143819451332\n",
      "Epoch 7, Batch: 124: Training Loss: 0.02356959506869316, Validation Loss: 0.026534464210271835\n",
      "Epoch 7, Batch: 125: Training Loss: 0.022627154365181923, Validation Loss: 0.02613511122763157\n",
      "Epoch 7, Batch: 126: Training Loss: 0.027350490912795067, Validation Loss: 0.02896217070519924\n",
      "Epoch 7, Batch: 127: Training Loss: 0.022725164890289307, Validation Loss: 0.02829280123114586\n",
      "Epoch 7, Batch: 128: Training Loss: 0.025098320096731186, Validation Loss: 0.02728933095932007\n",
      "Epoch 7, Batch: 129: Training Loss: 0.02483021281659603, Validation Loss: 0.0255800299346447\n",
      "Epoch 7, Batch: 130: Training Loss: 0.02735801227390766, Validation Loss: 0.028598790988326073\n",
      "Epoch 7, Batch: 131: Training Loss: 0.0239532683044672, Validation Loss: 0.02898910641670227\n",
      "Epoch 7, Batch: 132: Training Loss: 0.029138771817088127, Validation Loss: 0.025950364768505096\n",
      "Epoch 7, Batch: 133: Training Loss: 0.028669679537415504, Validation Loss: 0.02784561552107334\n",
      "Epoch 7, Batch: 134: Training Loss: 0.02686215564608574, Validation Loss: 0.02791408821940422\n",
      "Epoch 7, Batch: 135: Training Loss: 0.024592943489551544, Validation Loss: 0.029174497351050377\n",
      "Epoch 7, Batch: 136: Training Loss: 0.028218833729624748, Validation Loss: 0.02895808033645153\n",
      "Epoch 7, Batch: 137: Training Loss: 0.026477808132767677, Validation Loss: 0.02810865454375744\n",
      "Epoch 7, Batch: 138: Training Loss: 0.027226503938436508, Validation Loss: 0.030107509344816208\n",
      "Epoch 7, Batch: 139: Training Loss: 0.025374362245202065, Validation Loss: 0.028103141114115715\n",
      "Epoch 7, Batch: 140: Training Loss: 0.032877158373594284, Validation Loss: 0.029357677325606346\n",
      "Epoch 7, Batch: 141: Training Loss: 0.024121632799506187, Validation Loss: 0.026183491572737694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 142: Training Loss: 0.024839917197823524, Validation Loss: 0.0276167094707489\n",
      "Epoch 7, Batch: 143: Training Loss: 0.029847485944628716, Validation Loss: 0.02590535208582878\n",
      "Epoch 7, Batch: 144: Training Loss: 0.026762815192341805, Validation Loss: 0.026101129129529\n",
      "Epoch 7, Batch: 145: Training Loss: 0.023776507005095482, Validation Loss: 0.028339320793747902\n",
      "Epoch 7, Batch: 146: Training Loss: 0.024547263979911804, Validation Loss: 0.029673021286725998\n",
      "Epoch 7, Batch: 147: Training Loss: 0.02773708663880825, Validation Loss: 0.027940360829234123\n",
      "Epoch 7, Batch: 148: Training Loss: 0.02564220502972603, Validation Loss: 0.02821703627705574\n",
      "Epoch 7, Batch: 149: Training Loss: 0.02850564382970333, Validation Loss: 0.028350558131933212\n",
      "Epoch 7, Batch: 150: Training Loss: 0.02656899020075798, Validation Loss: 0.027489207684993744\n",
      "Epoch 7, Batch: 151: Training Loss: 0.028089730069041252, Validation Loss: 0.025869084522128105\n",
      "Epoch 7, Batch: 152: Training Loss: 0.027529727667570114, Validation Loss: 0.02593012899160385\n",
      "Epoch 7, Batch: 153: Training Loss: 0.02629816345870495, Validation Loss: 0.025250600650906563\n",
      "Epoch 7, Batch: 154: Training Loss: 0.027288133278489113, Validation Loss: 0.02744913659989834\n",
      "Epoch 7, Batch: 155: Training Loss: 0.0331462100148201, Validation Loss: 0.025568289682269096\n",
      "Epoch 7, Batch: 156: Training Loss: 0.028661873191595078, Validation Loss: 0.030564332380890846\n",
      "Epoch 7, Batch: 157: Training Loss: 0.02038397639989853, Validation Loss: 0.02652677893638611\n",
      "Epoch 7, Batch: 158: Training Loss: 0.027907688170671463, Validation Loss: 0.02775416150689125\n",
      "Epoch 7, Batch: 159: Training Loss: 0.024470984935760498, Validation Loss: 0.027181372046470642\n",
      "Epoch 7, Batch: 160: Training Loss: 0.02556765452027321, Validation Loss: 0.02694864757359028\n",
      "Epoch 7, Batch: 161: Training Loss: 0.02522130496799946, Validation Loss: 0.025863975286483765\n",
      "Epoch 7, Batch: 162: Training Loss: 0.026854880154132843, Validation Loss: 0.028667138889431953\n",
      "Epoch 7, Batch: 163: Training Loss: 0.025669027119874954, Validation Loss: 0.024848559871315956\n",
      "Epoch 7, Batch: 164: Training Loss: 0.025344042107462883, Validation Loss: 0.024459704756736755\n",
      "Epoch 7, Batch: 165: Training Loss: 0.02655807137489319, Validation Loss: 0.025067538022994995\n",
      "Epoch 7, Batch: 166: Training Loss: 0.023900851607322693, Validation Loss: 0.0257039126008749\n",
      "Epoch 7, Batch: 167: Training Loss: 0.024964457377791405, Validation Loss: 0.027250397950410843\n",
      "Epoch 7, Batch: 168: Training Loss: 0.02448228746652603, Validation Loss: 0.02639276720583439\n",
      "Epoch 7, Batch: 169: Training Loss: 0.02778160199522972, Validation Loss: 0.028119627386331558\n",
      "Epoch 7, Batch: 170: Training Loss: 0.025052385404706, Validation Loss: 0.02501259185373783\n",
      "Epoch 7, Batch: 171: Training Loss: 0.024384548887610435, Validation Loss: 0.02591695450246334\n",
      "Epoch 7, Batch: 172: Training Loss: 0.024228429421782494, Validation Loss: 0.03042968176305294\n",
      "Epoch 7, Batch: 173: Training Loss: 0.023636965081095695, Validation Loss: 0.028888845816254616\n",
      "Epoch 7, Batch: 174: Training Loss: 0.027643898501992226, Validation Loss: 0.027878712862730026\n",
      "Epoch 7, Batch: 175: Training Loss: 0.024255724623799324, Validation Loss: 0.025155242532491684\n",
      "Epoch 7, Batch: 176: Training Loss: 0.027621567249298096, Validation Loss: 0.025478722527623177\n",
      "Epoch 7, Batch: 177: Training Loss: 0.024693109095096588, Validation Loss: 0.02926057018339634\n",
      "Epoch 7, Batch: 178: Training Loss: 0.032631855458021164, Validation Loss: 0.03027820587158203\n",
      "Epoch 7, Batch: 179: Training Loss: 0.02929595485329628, Validation Loss: 0.028871748596429825\n",
      "Epoch 7, Batch: 180: Training Loss: 0.027235524728894234, Validation Loss: 0.030844828113913536\n",
      "Epoch 7, Batch: 181: Training Loss: 0.02645236626267433, Validation Loss: 0.028968041762709618\n",
      "Epoch 7, Batch: 182: Training Loss: 0.030448583886027336, Validation Loss: 0.028475606814026833\n",
      "Epoch 7, Batch: 183: Training Loss: 0.030790673568844795, Validation Loss: 0.028828373178839684\n",
      "Epoch 7, Batch: 184: Training Loss: 0.0259648859500885, Validation Loss: 0.028273608535528183\n",
      "Epoch 7, Batch: 185: Training Loss: 0.03260120749473572, Validation Loss: 0.0265964288264513\n",
      "Epoch 7, Batch: 186: Training Loss: 0.029118148609995842, Validation Loss: 0.030321048572659492\n",
      "Epoch 7, Batch: 187: Training Loss: 0.025328250601887703, Validation Loss: 0.026848161593079567\n",
      "Epoch 7, Batch: 188: Training Loss: 0.028285853564739227, Validation Loss: 0.027444135397672653\n",
      "Epoch 7, Batch: 189: Training Loss: 0.026062455028295517, Validation Loss: 0.028831906616687775\n",
      "Epoch 7, Batch: 190: Training Loss: 0.026771921664476395, Validation Loss: 0.026729173958301544\n",
      "Epoch 7, Batch: 191: Training Loss: 0.03108651004731655, Validation Loss: 0.028780784457921982\n",
      "Epoch 7, Batch: 192: Training Loss: 0.024277951568365097, Validation Loss: 0.02789231762290001\n",
      "Epoch 7, Batch: 193: Training Loss: 0.024941712617874146, Validation Loss: 0.026785029098391533\n",
      "Epoch 7, Batch: 194: Training Loss: 0.029062220826745033, Validation Loss: 0.026303691789507866\n",
      "Epoch 7, Batch: 195: Training Loss: 0.026790687814354897, Validation Loss: 0.02765863575041294\n",
      "Epoch 7, Batch: 196: Training Loss: 0.02633107639849186, Validation Loss: 0.02883242443203926\n",
      "Epoch 7, Batch: 197: Training Loss: 0.02894362434744835, Validation Loss: 0.028845632448792458\n",
      "Epoch 7, Batch: 198: Training Loss: 0.027328327298164368, Validation Loss: 0.02908383309841156\n",
      "Epoch 7, Batch: 199: Training Loss: 0.028221487998962402, Validation Loss: 0.027981437742710114\n",
      "Epoch 7, Batch: 200: Training Loss: 0.02467649057507515, Validation Loss: 0.030322397127747536\n",
      "Epoch 7, Batch: 201: Training Loss: 0.031098656356334686, Validation Loss: 0.028956156224012375\n",
      "Epoch 7, Batch: 202: Training Loss: 0.026337232440710068, Validation Loss: 0.029263539239764214\n",
      "Epoch 7, Batch: 203: Training Loss: 0.02741282433271408, Validation Loss: 0.0278022363781929\n",
      "Epoch 7, Batch: 204: Training Loss: 0.024333352223038673, Validation Loss: 0.029459305107593536\n",
      "Epoch 7, Batch: 205: Training Loss: 0.027312707155942917, Validation Loss: 0.030421307310461998\n",
      "Epoch 7, Batch: 206: Training Loss: 0.02726024016737938, Validation Loss: 0.027146348729729652\n",
      "Epoch 7, Batch: 207: Training Loss: 0.028997179120779037, Validation Loss: 0.027518296614289284\n",
      "Epoch 7, Batch: 208: Training Loss: 0.02552163600921631, Validation Loss: 0.029757149517536163\n",
      "Epoch 7, Batch: 209: Training Loss: 0.02590327523648739, Validation Loss: 0.02510027587413788\n",
      "Epoch 7, Batch: 210: Training Loss: 0.02985360287129879, Validation Loss: 0.02636413276195526\n",
      "Epoch 7, Batch: 211: Training Loss: 0.02534370869398117, Validation Loss: 0.025917286053299904\n",
      "Epoch 7, Batch: 212: Training Loss: 0.02925952337682247, Validation Loss: 0.0249790009111166\n",
      "Epoch 7, Batch: 213: Training Loss: 0.027354078367352486, Validation Loss: 0.024824174121022224\n",
      "Epoch 7, Batch: 214: Training Loss: 0.02641322836279869, Validation Loss: 0.025906022638082504\n",
      "Epoch 7, Batch: 215: Training Loss: 0.02751605026423931, Validation Loss: 0.026513632386922836\n",
      "Epoch 7, Batch: 216: Training Loss: 0.026239922270178795, Validation Loss: 0.025268003344535828\n",
      "Epoch 7, Batch: 217: Training Loss: 0.026081448420882225, Validation Loss: 0.028375715017318726\n",
      "Epoch 7, Batch: 218: Training Loss: 0.027932535856962204, Validation Loss: 0.027636554092168808\n",
      "Epoch 7, Batch: 219: Training Loss: 0.027896085754036903, Validation Loss: 0.02662837691605091\n",
      "Epoch 7, Batch: 220: Training Loss: 0.02801666222512722, Validation Loss: 0.0264283437281847\n",
      "Epoch 7, Batch: 221: Training Loss: 0.02778669074177742, Validation Loss: 0.026926957070827484\n",
      "Epoch 7, Batch: 222: Training Loss: 0.02717355079948902, Validation Loss: 0.027125535532832146\n",
      "Epoch 7, Batch: 223: Training Loss: 0.02803245559334755, Validation Loss: 0.029545307159423828\n",
      "Epoch 7, Batch: 224: Training Loss: 0.027888912707567215, Validation Loss: 0.02744731865823269\n",
      "Epoch 7, Batch: 225: Training Loss: 0.03117593377828598, Validation Loss: 0.027762439101934433\n",
      "Epoch 7, Batch: 226: Training Loss: 0.02780105173587799, Validation Loss: 0.02791774645447731\n",
      "Epoch 7, Batch: 227: Training Loss: 0.023450562730431557, Validation Loss: 0.0270928256213665\n",
      "Epoch 7, Batch: 228: Training Loss: 0.02881302684545517, Validation Loss: 0.028260143473744392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 229: Training Loss: 0.026358621194958687, Validation Loss: 0.026366855949163437\n",
      "Epoch 7, Batch: 230: Training Loss: 0.02471492812037468, Validation Loss: 0.028921466320753098\n",
      "Epoch 7, Batch: 231: Training Loss: 0.03654191642999649, Validation Loss: 0.028108663856983185\n",
      "Epoch 7, Batch: 232: Training Loss: 0.030818572267889977, Validation Loss: 0.02828548289835453\n",
      "Epoch 7, Batch: 233: Training Loss: 0.02443641982972622, Validation Loss: 0.03016943484544754\n",
      "Epoch 7, Batch: 234: Training Loss: 0.030733665451407433, Validation Loss: 0.027122512459754944\n",
      "Epoch 7, Batch: 235: Training Loss: 0.033409614115953445, Validation Loss: 0.029137708246707916\n",
      "Epoch 7, Batch: 236: Training Loss: 0.029566502198576927, Validation Loss: 0.028835687786340714\n",
      "Epoch 7, Batch: 237: Training Loss: 0.027753598988056183, Validation Loss: 0.027652500197291374\n",
      "Epoch 7, Batch: 238: Training Loss: 0.026856765151023865, Validation Loss: 0.026287604123353958\n",
      "Epoch 7, Batch: 239: Training Loss: 0.02900402620434761, Validation Loss: 0.026121439412236214\n",
      "Epoch 7, Batch: 240: Training Loss: 0.02733708545565605, Validation Loss: 0.027226267382502556\n",
      "Epoch 7, Batch: 241: Training Loss: 0.027854664251208305, Validation Loss: 0.026911620050668716\n",
      "Epoch 7, Batch: 242: Training Loss: 0.02825308032333851, Validation Loss: 0.024793431162834167\n",
      "Epoch 7, Batch: 243: Training Loss: 0.029846761375665665, Validation Loss: 0.025501219555735588\n",
      "Epoch 7, Batch: 244: Training Loss: 0.029350677505135536, Validation Loss: 0.026300199329853058\n",
      "Epoch 7, Batch: 245: Training Loss: 0.02946295775473118, Validation Loss: 0.025595897808670998\n",
      "Epoch 7, Batch: 246: Training Loss: 0.02983890101313591, Validation Loss: 0.029190005734562874\n",
      "Epoch 7, Batch: 247: Training Loss: 0.0317443311214447, Validation Loss: 0.025233453139662743\n",
      "Epoch 7, Batch: 248: Training Loss: 0.025114646181464195, Validation Loss: 0.028328312560915947\n",
      "Epoch 7, Batch: 249: Training Loss: 0.02513577789068222, Validation Loss: 0.028728488832712173\n",
      "Epoch 7, Batch: 250: Training Loss: 0.026500696316361427, Validation Loss: 0.027121691033244133\n",
      "Epoch 7, Batch: 251: Training Loss: 0.023534860461950302, Validation Loss: 0.026070594787597656\n",
      "Epoch 7, Batch: 252: Training Loss: 0.026410479098558426, Validation Loss: 0.02695264108479023\n",
      "Epoch 7, Batch: 253: Training Loss: 0.030769193544983864, Validation Loss: 0.027132827788591385\n",
      "Epoch 7, Batch: 254: Training Loss: 0.024444717913866043, Validation Loss: 0.02868851274251938\n",
      "Epoch 7, Batch: 255: Training Loss: 0.02612161636352539, Validation Loss: 0.028290171176195145\n",
      "Epoch 7, Batch: 256: Training Loss: 0.0281441118568182, Validation Loss: 0.029416248202323914\n",
      "Epoch 7, Batch: 257: Training Loss: 0.029600422829389572, Validation Loss: 0.030111299827694893\n",
      "Epoch 7, Batch: 258: Training Loss: 0.03012513369321823, Validation Loss: 0.028899751603603363\n",
      "Epoch 7, Batch: 259: Training Loss: 0.027533849701285362, Validation Loss: 0.02764364331960678\n",
      "Epoch 7, Batch: 260: Training Loss: 0.029957447201013565, Validation Loss: 0.02826426737010479\n",
      "Epoch 7, Batch: 261: Training Loss: 0.026565587148070335, Validation Loss: 0.027275826781988144\n",
      "Epoch 7, Batch: 262: Training Loss: 0.028956325724720955, Validation Loss: 0.029542064294219017\n",
      "Epoch 7, Batch: 263: Training Loss: 0.024124590680003166, Validation Loss: 0.029220739379525185\n",
      "Epoch 7, Batch: 264: Training Loss: 0.02560863085091114, Validation Loss: 0.02736804634332657\n",
      "Epoch 7, Batch: 265: Training Loss: 0.028788873925805092, Validation Loss: 0.028090521693229675\n",
      "Epoch 7, Batch: 266: Training Loss: 0.028188154101371765, Validation Loss: 0.026239711791276932\n",
      "Epoch 7, Batch: 267: Training Loss: 0.025230756029486656, Validation Loss: 0.026354823261499405\n",
      "Epoch 7, Batch: 268: Training Loss: 0.02777981013059616, Validation Loss: 0.028118567541241646\n",
      "Epoch 7, Batch: 269: Training Loss: 0.02569168619811535, Validation Loss: 0.026417875662446022\n",
      "Epoch 7, Batch: 270: Training Loss: 0.024446232244372368, Validation Loss: 0.027246927842497826\n",
      "Epoch 7, Batch: 271: Training Loss: 0.026180077344179153, Validation Loss: 0.028387239202857018\n",
      "Epoch 7, Batch: 272: Training Loss: 0.02671341598033905, Validation Loss: 0.02960805781185627\n",
      "Epoch 7, Batch: 273: Training Loss: 0.027628550305962563, Validation Loss: 0.02945476770401001\n",
      "Epoch 7, Batch: 274: Training Loss: 0.026600850746035576, Validation Loss: 0.0297952089458704\n",
      "Epoch 7, Batch: 275: Training Loss: 0.024290235713124275, Validation Loss: 0.02705143764615059\n",
      "Epoch 7, Batch: 276: Training Loss: 0.024842239916324615, Validation Loss: 0.029954517260193825\n",
      "Epoch 7, Batch: 277: Training Loss: 0.024322090670466423, Validation Loss: 0.02870430238544941\n",
      "Epoch 7, Batch: 278: Training Loss: 0.0257277749478817, Validation Loss: 0.0290643572807312\n",
      "Epoch 7, Batch: 279: Training Loss: 0.027836259454488754, Validation Loss: 0.027336658909916878\n",
      "Epoch 7, Batch: 280: Training Loss: 0.028119385242462158, Validation Loss: 0.029467947781085968\n",
      "Epoch 7, Batch: 281: Training Loss: 0.029736638069152832, Validation Loss: 0.02918759360909462\n",
      "Epoch 7, Batch: 282: Training Loss: 0.02500593662261963, Validation Loss: 0.030678147450089455\n",
      "Epoch 7, Batch: 283: Training Loss: 0.025794263929128647, Validation Loss: 0.029854407534003258\n",
      "Epoch 7, Batch: 284: Training Loss: 0.028368163853883743, Validation Loss: 0.02824149653315544\n",
      "Epoch 7, Batch: 285: Training Loss: 0.028072895482182503, Validation Loss: 0.026538830250501633\n",
      "Epoch 7, Batch: 286: Training Loss: 0.027742864564061165, Validation Loss: 0.030552562326192856\n",
      "Epoch 7, Batch: 287: Training Loss: 0.028839539736509323, Validation Loss: 0.029366903007030487\n",
      "Epoch 7, Batch: 288: Training Loss: 0.026025211438536644, Validation Loss: 0.02830629050731659\n",
      "Epoch 7, Batch: 289: Training Loss: 0.03177007660269737, Validation Loss: 0.026080282405018806\n",
      "Epoch 7, Batch: 290: Training Loss: 0.027812622487545013, Validation Loss: 0.02831801027059555\n",
      "Epoch 7, Batch: 291: Training Loss: 0.027814580127596855, Validation Loss: 0.028493134304881096\n",
      "Epoch 7, Batch: 292: Training Loss: 0.028175588697195053, Validation Loss: 0.028953511267900467\n",
      "Epoch 7, Batch: 293: Training Loss: 0.02961037866771221, Validation Loss: 0.026755020022392273\n",
      "Epoch 7, Batch: 294: Training Loss: 0.029961643740534782, Validation Loss: 0.031016625463962555\n",
      "Epoch 7, Batch: 295: Training Loss: 0.027971262112259865, Validation Loss: 0.0278667863458395\n",
      "Epoch 7, Batch: 296: Training Loss: 0.029574591666460037, Validation Loss: 0.029043426737189293\n",
      "Epoch 7, Batch: 297: Training Loss: 0.029825935140252113, Validation Loss: 0.028321627527475357\n",
      "Epoch 7, Batch: 298: Training Loss: 0.026029858738183975, Validation Loss: 0.02644357644021511\n",
      "Epoch 7, Batch: 299: Training Loss: 0.026301244273781776, Validation Loss: 0.027905166149139404\n",
      "Epoch 7, Batch: 300: Training Loss: 0.02777952514588833, Validation Loss: 0.027472922578454018\n",
      "Epoch 7, Batch: 301: Training Loss: 0.02748524397611618, Validation Loss: 0.02842685580253601\n",
      "Epoch 7, Batch: 302: Training Loss: 0.0248795785009861, Validation Loss: 0.029121486470103264\n",
      "Epoch 7, Batch: 303: Training Loss: 0.025055529549717903, Validation Loss: 0.026816008612513542\n",
      "Epoch 7, Batch: 304: Training Loss: 0.02605631574988365, Validation Loss: 0.03005487658083439\n",
      "Epoch 7, Batch: 305: Training Loss: 0.02488008327782154, Validation Loss: 0.027405045926570892\n",
      "Epoch 7, Batch: 306: Training Loss: 0.029508333653211594, Validation Loss: 0.026038341224193573\n",
      "Epoch 7, Batch: 307: Training Loss: 0.02724294178187847, Validation Loss: 0.027486683800816536\n",
      "Epoch 7, Batch: 308: Training Loss: 0.024502169340848923, Validation Loss: 0.028480222448706627\n",
      "Epoch 7, Batch: 309: Training Loss: 0.023981640115380287, Validation Loss: 0.02540605142712593\n",
      "Epoch 7, Batch: 310: Training Loss: 0.025091972202062607, Validation Loss: 0.02812854014337063\n",
      "Epoch 7, Batch: 311: Training Loss: 0.02527916431427002, Validation Loss: 0.02890676259994507\n",
      "Epoch 7, Batch: 312: Training Loss: 0.028494497761130333, Validation Loss: 0.028619401156902313\n",
      "Epoch 7, Batch: 313: Training Loss: 0.02408202365040779, Validation Loss: 0.02749772183597088\n",
      "Epoch 7, Batch: 314: Training Loss: 0.023786557838320732, Validation Loss: 0.02762420102953911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 315: Training Loss: 0.024909475818276405, Validation Loss: 0.027863118797540665\n",
      "Epoch 7, Batch: 316: Training Loss: 0.024204730987548828, Validation Loss: 0.03001968190073967\n",
      "Epoch 7, Batch: 317: Training Loss: 0.02467544563114643, Validation Loss: 0.02894796058535576\n",
      "Epoch 7, Batch: 318: Training Loss: 0.024802671745419502, Validation Loss: 0.026758678257465363\n",
      "Epoch 7, Batch: 319: Training Loss: 0.027024565264582634, Validation Loss: 0.028328321874141693\n",
      "Epoch 7, Batch: 320: Training Loss: 0.026996392756700516, Validation Loss: 0.02733905240893364\n",
      "Epoch 7, Batch: 321: Training Loss: 0.022586163133382797, Validation Loss: 0.02976125478744507\n",
      "Epoch 7, Batch: 322: Training Loss: 0.02734009176492691, Validation Loss: 0.031331103295087814\n",
      "Epoch 7, Batch: 323: Training Loss: 0.028666973114013672, Validation Loss: 0.030148444697260857\n",
      "Epoch 7, Batch: 324: Training Loss: 0.02913486957550049, Validation Loss: 0.029085418209433556\n",
      "Epoch 7, Batch: 325: Training Loss: 0.025003591552376747, Validation Loss: 0.030964888632297516\n",
      "Epoch 7, Batch: 326: Training Loss: 0.03119598887860775, Validation Loss: 0.030111148953437805\n",
      "Epoch 7, Batch: 327: Training Loss: 0.024168742820620537, Validation Loss: 0.028382714837789536\n",
      "Epoch 7, Batch: 328: Training Loss: 0.02882002294063568, Validation Loss: 0.031118053942918777\n",
      "Epoch 7, Batch: 329: Training Loss: 0.027129290625452995, Validation Loss: 0.03114771470427513\n",
      "Epoch 7, Batch: 330: Training Loss: 0.02759811282157898, Validation Loss: 0.03198651969432831\n",
      "Epoch 7, Batch: 331: Training Loss: 0.02676936611533165, Validation Loss: 0.030749894678592682\n",
      "Epoch 7, Batch: 332: Training Loss: 0.0253413338214159, Validation Loss: 0.03098621591925621\n",
      "Epoch 7, Batch: 333: Training Loss: 0.028836114332079887, Validation Loss: 0.030518265441060066\n",
      "Epoch 7, Batch: 334: Training Loss: 0.028766700997948647, Validation Loss: 0.029873110353946686\n",
      "Epoch 7, Batch: 335: Training Loss: 0.027102163061499596, Validation Loss: 0.028815442696213722\n",
      "Epoch 7, Batch: 336: Training Loss: 0.024919137358665466, Validation Loss: 0.028158821165561676\n",
      "Epoch 7, Batch: 337: Training Loss: 0.025734039023518562, Validation Loss: 0.03278231993317604\n",
      "Epoch 7, Batch: 338: Training Loss: 0.027540108188986778, Validation Loss: 0.03081720881164074\n",
      "Epoch 7, Batch: 339: Training Loss: 0.025325486436486244, Validation Loss: 0.027687951922416687\n",
      "Epoch 7, Batch: 340: Training Loss: 0.029330510646104813, Validation Loss: 0.030995117500424385\n",
      "Epoch 7, Batch: 341: Training Loss: 0.025211425498127937, Validation Loss: 0.02927454560995102\n",
      "Epoch 7, Batch: 342: Training Loss: 0.029354805126786232, Validation Loss: 0.025533199310302734\n",
      "Epoch 7, Batch: 343: Training Loss: 0.02576960064470768, Validation Loss: 0.026901939883828163\n",
      "Epoch 7, Batch: 344: Training Loss: 0.027595750987529755, Validation Loss: 0.024668430909514427\n",
      "Epoch 7, Batch: 345: Training Loss: 0.02740946039557457, Validation Loss: 0.025779860094189644\n",
      "Epoch 7, Batch: 346: Training Loss: 0.02861243300139904, Validation Loss: 0.02807055041193962\n",
      "Epoch 7, Batch: 347: Training Loss: 0.024438980966806412, Validation Loss: 0.02623024396598339\n",
      "Epoch 7, Batch: 348: Training Loss: 0.026406390592455864, Validation Loss: 0.026832053437829018\n",
      "Epoch 7, Batch: 349: Training Loss: 0.027093060314655304, Validation Loss: 0.029119858518242836\n",
      "Epoch 7, Batch: 350: Training Loss: 0.023486603051424026, Validation Loss: 0.027822328731417656\n",
      "Epoch 7, Batch: 351: Training Loss: 0.03179844841361046, Validation Loss: 0.027723141014575958\n",
      "Epoch 7, Batch: 352: Training Loss: 0.028050333261489868, Validation Loss: 0.028877396136522293\n",
      "Epoch 7, Batch: 353: Training Loss: 0.02605614997446537, Validation Loss: 0.027348794043064117\n",
      "Epoch 7, Batch: 354: Training Loss: 0.027643030509352684, Validation Loss: 0.026243334636092186\n",
      "Epoch 7, Batch: 355: Training Loss: 0.02095099166035652, Validation Loss: 0.031364887952804565\n",
      "Epoch 7, Batch: 356: Training Loss: 0.02679448015987873, Validation Loss: 0.027596553787589073\n",
      "Epoch 7, Batch: 357: Training Loss: 0.022795727476477623, Validation Loss: 0.028180990368127823\n",
      "Epoch 7, Batch: 358: Training Loss: 0.029902134090662003, Validation Loss: 0.02913488820195198\n",
      "Epoch 7, Batch: 359: Training Loss: 0.02468009479343891, Validation Loss: 0.027403714135289192\n",
      "Epoch 7, Batch: 360: Training Loss: 0.027339480817317963, Validation Loss: 0.02788427285850048\n",
      "Epoch 7, Batch: 361: Training Loss: 0.02820027433335781, Validation Loss: 0.027974456548690796\n",
      "Epoch 7, Batch: 362: Training Loss: 0.027306536212563515, Validation Loss: 0.02590828761458397\n",
      "Epoch 7, Batch: 363: Training Loss: 0.033467017114162445, Validation Loss: 0.029073204845190048\n",
      "Epoch 7, Batch: 364: Training Loss: 0.02999529428780079, Validation Loss: 0.028119148686528206\n",
      "Epoch 7, Batch: 365: Training Loss: 0.030885525047779083, Validation Loss: 0.028292790055274963\n",
      "Epoch 7, Batch: 366: Training Loss: 0.02870427444577217, Validation Loss: 0.027424108237028122\n",
      "Epoch 7, Batch: 367: Training Loss: 0.027413364499807358, Validation Loss: 0.030074797570705414\n",
      "Epoch 7, Batch: 368: Training Loss: 0.029217436909675598, Validation Loss: 0.027507711201906204\n",
      "Epoch 7, Batch: 369: Training Loss: 0.026354681700468063, Validation Loss: 0.025475546717643738\n",
      "Epoch 7, Batch: 370: Training Loss: 0.02983659878373146, Validation Loss: 0.02722523733973503\n",
      "Epoch 7, Batch: 371: Training Loss: 0.027963217347860336, Validation Loss: 0.02883847989141941\n",
      "Epoch 7, Batch: 372: Training Loss: 0.026363275945186615, Validation Loss: 0.027894053608179092\n",
      "Epoch 7, Batch: 373: Training Loss: 0.02976253256201744, Validation Loss: 0.027902917936444283\n",
      "Epoch 7, Batch: 374: Training Loss: 0.02413598820567131, Validation Loss: 0.029008539393544197\n",
      "Epoch 7, Batch: 375: Training Loss: 0.028100784868001938, Validation Loss: 0.028828728944063187\n",
      "Epoch 7, Batch: 376: Training Loss: 0.02425951138138771, Validation Loss: 0.028480323031544685\n",
      "Epoch 7, Batch: 377: Training Loss: 0.029832232743501663, Validation Loss: 0.026556944474577904\n",
      "Epoch 7, Batch: 378: Training Loss: 0.022658251225948334, Validation Loss: 0.02877994067966938\n",
      "Epoch 7, Batch: 379: Training Loss: 0.0257706455886364, Validation Loss: 0.028841231018304825\n",
      "Epoch 7, Batch: 380: Training Loss: 0.03249999135732651, Validation Loss: 0.026957295835018158\n",
      "Epoch 7, Batch: 381: Training Loss: 0.032183315604925156, Validation Loss: 0.02814236842095852\n",
      "Epoch 7, Batch: 382: Training Loss: 0.02859719656407833, Validation Loss: 0.029119351878762245\n",
      "Epoch 7, Batch: 383: Training Loss: 0.02537430077791214, Validation Loss: 0.027652766555547714\n",
      "Epoch 7, Batch: 384: Training Loss: 0.02829386107623577, Validation Loss: 0.029108235612511635\n",
      "Epoch 7, Batch: 385: Training Loss: 0.027952272444963455, Validation Loss: 0.02532108500599861\n",
      "Epoch 7, Batch: 386: Training Loss: 0.023772791028022766, Validation Loss: 0.027671394869685173\n",
      "Epoch 7, Batch: 387: Training Loss: 0.02755803056061268, Validation Loss: 0.027089834213256836\n",
      "Epoch 7, Batch: 388: Training Loss: 0.028418930247426033, Validation Loss: 0.025556951761245728\n",
      "Epoch 7, Batch: 389: Training Loss: 0.03150593116879463, Validation Loss: 0.029017319902777672\n",
      "Epoch 7, Batch: 390: Training Loss: 0.027160193771123886, Validation Loss: 0.026287393644452095\n",
      "Epoch 7, Batch: 391: Training Loss: 0.027914542704820633, Validation Loss: 0.02874261885881424\n",
      "Epoch 7, Batch: 392: Training Loss: 0.025487953796982765, Validation Loss: 0.02738412469625473\n",
      "Epoch 7, Batch: 393: Training Loss: 0.026556314900517464, Validation Loss: 0.027817733585834503\n",
      "Epoch 7, Batch: 394: Training Loss: 0.029624108225107193, Validation Loss: 0.02906147949397564\n",
      "Epoch 7, Batch: 395: Training Loss: 0.026651965454220772, Validation Loss: 0.029476342722773552\n",
      "Epoch 7, Batch: 396: Training Loss: 0.030423644930124283, Validation Loss: 0.026875430718064308\n",
      "Epoch 7, Batch: 397: Training Loss: 0.026971200481057167, Validation Loss: 0.025500871241092682\n",
      "Epoch 7, Batch: 398: Training Loss: 0.02648763358592987, Validation Loss: 0.028159793466329575\n",
      "Epoch 7, Batch: 399: Training Loss: 0.026809412986040115, Validation Loss: 0.025117849931120872\n",
      "Epoch 7, Batch: 400: Training Loss: 0.02932453714311123, Validation Loss: 0.025442933663725853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 401: Training Loss: 0.025760259479284286, Validation Loss: 0.0276047233492136\n",
      "Epoch 7, Batch: 402: Training Loss: 0.024420544505119324, Validation Loss: 0.026674531400203705\n",
      "Epoch 7, Batch: 403: Training Loss: 0.0338936522603035, Validation Loss: 0.03053843043744564\n",
      "Epoch 7, Batch: 404: Training Loss: 0.024814002215862274, Validation Loss: 0.029050715267658234\n",
      "Epoch 7, Batch: 405: Training Loss: 0.030575424432754517, Validation Loss: 0.027189387008547783\n",
      "Epoch 7, Batch: 406: Training Loss: 0.02429022826254368, Validation Loss: 0.028649168089032173\n",
      "Epoch 7, Batch: 407: Training Loss: 0.025517908856272697, Validation Loss: 0.024145210161805153\n",
      "Epoch 7, Batch: 408: Training Loss: 0.025908436626195908, Validation Loss: 0.02633238397538662\n",
      "Epoch 7, Batch: 409: Training Loss: 0.025166509672999382, Validation Loss: 0.027615899220108986\n",
      "Epoch 7, Batch: 410: Training Loss: 0.02522294968366623, Validation Loss: 0.026834342628717422\n",
      "Epoch 7, Batch: 411: Training Loss: 0.025801081210374832, Validation Loss: 0.028296859934926033\n",
      "Epoch 7, Batch: 412: Training Loss: 0.030217453837394714, Validation Loss: 0.0280128363519907\n",
      "Epoch 7, Batch: 413: Training Loss: 0.02919084206223488, Validation Loss: 0.02870091237127781\n",
      "Epoch 7, Batch: 414: Training Loss: 0.03061479516327381, Validation Loss: 0.027907010167837143\n",
      "Epoch 7, Batch: 415: Training Loss: 0.029917938634753227, Validation Loss: 0.0316317081451416\n",
      "Epoch 7, Batch: 416: Training Loss: 0.0270377229899168, Validation Loss: 0.025463219732046127\n",
      "Epoch 7, Batch: 417: Training Loss: 0.027491487562656403, Validation Loss: 0.02350549027323723\n",
      "Epoch 7, Batch: 418: Training Loss: 0.024255890399217606, Validation Loss: 0.02611464262008667\n",
      "Epoch 7, Batch: 419: Training Loss: 0.026721064001321793, Validation Loss: 0.02808127924799919\n",
      "Epoch 7, Batch: 420: Training Loss: 0.026332087814807892, Validation Loss: 0.028240974992513657\n",
      "Epoch 7, Batch: 421: Training Loss: 0.0317571647465229, Validation Loss: 0.026614949107170105\n",
      "Epoch 7, Batch: 422: Training Loss: 0.027897490188479424, Validation Loss: 0.02543792314827442\n",
      "Epoch 7, Batch: 423: Training Loss: 0.03237204626202583, Validation Loss: 0.02454284019768238\n",
      "Epoch 7, Batch: 424: Training Loss: 0.02756323665380478, Validation Loss: 0.024685462936758995\n",
      "Epoch 7, Batch: 425: Training Loss: 0.02702389284968376, Validation Loss: 0.024647623300552368\n",
      "Epoch 7, Batch: 426: Training Loss: 0.02817589044570923, Validation Loss: 0.023660801351070404\n",
      "Epoch 7, Batch: 427: Training Loss: 0.0258081816136837, Validation Loss: 0.02717452123761177\n",
      "Epoch 7, Batch: 428: Training Loss: 0.02644565887749195, Validation Loss: 0.02540448307991028\n",
      "Epoch 7, Batch: 429: Training Loss: 0.03047843649983406, Validation Loss: 0.02529217302799225\n",
      "Epoch 7, Batch: 430: Training Loss: 0.026954662054777145, Validation Loss: 0.026093242689967155\n",
      "Epoch 7, Batch: 431: Training Loss: 0.030098633840680122, Validation Loss: 0.026500796899199486\n",
      "Epoch 7, Batch: 432: Training Loss: 0.02541094645857811, Validation Loss: 0.0246681310236454\n",
      "Epoch 7, Batch: 433: Training Loss: 0.02654173970222473, Validation Loss: 0.026765936985611916\n",
      "Epoch 7, Batch: 434: Training Loss: 0.024745943024754524, Validation Loss: 0.025292227044701576\n",
      "Epoch 7, Batch: 435: Training Loss: 0.028283551335334778, Validation Loss: 0.025430679321289062\n",
      "Epoch 7, Batch: 436: Training Loss: 0.02437715046107769, Validation Loss: 0.02517913654446602\n",
      "Epoch 7, Batch: 437: Training Loss: 0.025596443563699722, Validation Loss: 0.02446199767291546\n",
      "Epoch 7, Batch: 438: Training Loss: 0.0257925633341074, Validation Loss: 0.023255730047822\n",
      "Epoch 7, Batch: 439: Training Loss: 0.024376990273594856, Validation Loss: 0.025007784366607666\n",
      "Epoch 7, Batch: 440: Training Loss: 0.02973351813852787, Validation Loss: 0.025033462792634964\n",
      "Epoch 7, Batch: 441: Training Loss: 0.028452180325984955, Validation Loss: 0.02462281845510006\n",
      "Epoch 7, Batch: 442: Training Loss: 0.025789402425289154, Validation Loss: 0.027295326814055443\n",
      "Epoch 7, Batch: 443: Training Loss: 0.026491710916161537, Validation Loss: 0.02307385765016079\n",
      "Epoch 7, Batch: 444: Training Loss: 0.02642381191253662, Validation Loss: 0.025014175102114677\n",
      "Epoch 7, Batch: 445: Training Loss: 0.026862401515245438, Validation Loss: 0.024803774431347847\n",
      "Epoch 7, Batch: 446: Training Loss: 0.025937316939234734, Validation Loss: 0.02714036963880062\n",
      "Epoch 7, Batch: 447: Training Loss: 0.026995636522769928, Validation Loss: 0.023944687098264694\n",
      "Epoch 7, Batch: 448: Training Loss: 0.027049187570810318, Validation Loss: 0.02797342836856842\n",
      "Epoch 7, Batch: 449: Training Loss: 0.02454403229057789, Validation Loss: 0.02695644646883011\n",
      "Epoch 7, Batch: 450: Training Loss: 0.025622393935918808, Validation Loss: 0.024992873892188072\n",
      "Epoch 7, Batch: 451: Training Loss: 0.028657592833042145, Validation Loss: 0.025155220180749893\n",
      "Epoch 7, Batch: 452: Training Loss: 0.030741460621356964, Validation Loss: 0.026874559000134468\n",
      "Epoch 7, Batch: 453: Training Loss: 0.027615245431661606, Validation Loss: 0.02646094374358654\n",
      "Epoch 7, Batch: 454: Training Loss: 0.02524358220398426, Validation Loss: 0.02439589984714985\n",
      "Epoch 7, Batch: 455: Training Loss: 0.023063940927386284, Validation Loss: 0.02656441740691662\n",
      "Epoch 7, Batch: 456: Training Loss: 0.025107180699706078, Validation Loss: 0.02555087022483349\n",
      "Epoch 7, Batch: 457: Training Loss: 0.028022587299346924, Validation Loss: 0.023185517638921738\n",
      "Epoch 7, Batch: 458: Training Loss: 0.023717261850833893, Validation Loss: 0.025180622935295105\n",
      "Saving new best model w/ loss: 0.02215910516679287\n",
      "Epoch 7, Batch: 459: Training Loss: 0.027229731902480125, Validation Loss: 0.02215910516679287\n",
      "Epoch 7, Batch: 460: Training Loss: 0.023687293753027916, Validation Loss: 0.02383975312113762\n",
      "Epoch 7, Batch: 461: Training Loss: 0.022054454311728477, Validation Loss: 0.023427750915288925\n",
      "Epoch 7, Batch: 462: Training Loss: 0.021493416279554367, Validation Loss: 0.024257631972432137\n",
      "Epoch 7, Batch: 463: Training Loss: 0.028965642675757408, Validation Loss: 0.02361382730305195\n",
      "Epoch 7, Batch: 464: Training Loss: 0.024547046050429344, Validation Loss: 0.024455511942505836\n",
      "Epoch 7, Batch: 465: Training Loss: 0.025218814611434937, Validation Loss: 0.02540857531130314\n",
      "Epoch 7, Batch: 466: Training Loss: 0.02526015043258667, Validation Loss: 0.028135845437645912\n",
      "Epoch 7, Batch: 467: Training Loss: 0.033770497888326645, Validation Loss: 0.022162465378642082\n",
      "Epoch 7, Batch: 468: Training Loss: 0.030581234022974968, Validation Loss: 0.024295080453157425\n",
      "Epoch 7, Batch: 469: Training Loss: 0.02471463568508625, Validation Loss: 0.02463369630277157\n",
      "Epoch 7, Batch: 470: Training Loss: 0.027392828837037086, Validation Loss: 0.02675532177090645\n",
      "Epoch 7, Batch: 471: Training Loss: 0.025641515851020813, Validation Loss: 0.025767473503947258\n",
      "Epoch 7, Batch: 472: Training Loss: 0.02481924369931221, Validation Loss: 0.0230391975492239\n",
      "Epoch 7, Batch: 473: Training Loss: 0.023198053240776062, Validation Loss: 0.026722662150859833\n",
      "Epoch 7, Batch: 474: Training Loss: 0.02619965188205242, Validation Loss: 0.024877863004803658\n",
      "Epoch 7, Batch: 475: Training Loss: 0.028979027643799782, Validation Loss: 0.026539724320173264\n",
      "Epoch 7, Batch: 476: Training Loss: 0.025994515046477318, Validation Loss: 0.02657395228743553\n",
      "Epoch 7, Batch: 477: Training Loss: 0.030778761953115463, Validation Loss: 0.025778410956263542\n",
      "Epoch 7, Batch: 478: Training Loss: 0.027256041765213013, Validation Loss: 0.027571652084589005\n",
      "Epoch 7, Batch: 479: Training Loss: 0.02491612918674946, Validation Loss: 0.025353390723466873\n",
      "Epoch 7, Batch: 480: Training Loss: 0.02892564982175827, Validation Loss: 0.02501104399561882\n",
      "Epoch 7, Batch: 481: Training Loss: 0.026764577254652977, Validation Loss: 0.02633555792272091\n",
      "Epoch 7, Batch: 482: Training Loss: 0.028545891866087914, Validation Loss: 0.025132857263088226\n",
      "Epoch 7, Batch: 483: Training Loss: 0.02637821063399315, Validation Loss: 0.026004312559962273\n",
      "Epoch 7, Batch: 484: Training Loss: 0.025962887331843376, Validation Loss: 0.026449421420693398\n",
      "Epoch 7, Batch: 485: Training Loss: 0.025728868320584297, Validation Loss: 0.025606254115700722\n",
      "Epoch 7, Batch: 486: Training Loss: 0.027311859652400017, Validation Loss: 0.023859206587076187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch: 487: Training Loss: 0.026352111250162125, Validation Loss: 0.02438150905072689\n",
      "Epoch 7, Batch: 488: Training Loss: 0.028193216770887375, Validation Loss: 0.02410244196653366\n",
      "Epoch 7, Batch: 489: Training Loss: 0.029512066394090652, Validation Loss: 0.024221271276474\n",
      "Epoch 7, Batch: 490: Training Loss: 0.026150455698370934, Validation Loss: 0.023365510627627373\n",
      "Epoch 7, Batch: 491: Training Loss: 0.02537456341087818, Validation Loss: 0.022313321009278297\n",
      "Epoch 7, Batch: 492: Training Loss: 0.028960194438695908, Validation Loss: 0.024914147332310677\n",
      "Epoch 7, Batch: 493: Training Loss: 0.02692083828151226, Validation Loss: 0.02374057099223137\n",
      "Epoch 7, Batch: 494: Training Loss: 0.029436398297548294, Validation Loss: 0.02445594221353531\n",
      "Epoch 7, Batch: 495: Training Loss: 0.024979135021567345, Validation Loss: 0.024962168186903\n",
      "Epoch 7, Batch: 496: Training Loss: 0.028041120618581772, Validation Loss: 0.02560814656317234\n",
      "Epoch 7, Batch: 497: Training Loss: 0.027681460604071617, Validation Loss: 0.027895214036107063\n",
      "Epoch 7, Batch: 498: Training Loss: 0.02708069235086441, Validation Loss: 0.025862619280815125\n",
      "Epoch 7, Batch: 499: Training Loss: 0.02432505413889885, Validation Loss: 0.026041598990559578\n",
      "Epoch 8, Batch: 0: Training Loss: 0.02745503932237625, Validation Loss: 0.028736067935824394\n",
      "Epoch 8, Batch: 1: Training Loss: 0.02591443620622158, Validation Loss: 0.02628292329609394\n",
      "Epoch 8, Batch: 2: Training Loss: 0.032667286694049835, Validation Loss: 0.026119237765669823\n",
      "Epoch 8, Batch: 3: Training Loss: 0.02541220188140869, Validation Loss: 0.027621332556009293\n",
      "Epoch 8, Batch: 4: Training Loss: 0.02191973477602005, Validation Loss: 0.02696608565747738\n",
      "Epoch 8, Batch: 5: Training Loss: 0.02652810327708721, Validation Loss: 0.02478654310107231\n",
      "Epoch 8, Batch: 6: Training Loss: 0.02770291268825531, Validation Loss: 0.03035815618932247\n",
      "Epoch 8, Batch: 7: Training Loss: 0.027197811752557755, Validation Loss: 0.02617403119802475\n",
      "Epoch 8, Batch: 8: Training Loss: 0.025970645248889923, Validation Loss: 0.026170479133725166\n",
      "Epoch 8, Batch: 9: Training Loss: 0.028057977557182312, Validation Loss: 0.024328747764229774\n",
      "Epoch 8, Batch: 10: Training Loss: 0.028994765132665634, Validation Loss: 0.0269600972533226\n",
      "Epoch 8, Batch: 11: Training Loss: 0.02952989749610424, Validation Loss: 0.026948243379592896\n",
      "Epoch 8, Batch: 12: Training Loss: 0.028977395966649055, Validation Loss: 0.023021481931209564\n",
      "Epoch 8, Batch: 13: Training Loss: 0.028305398300290108, Validation Loss: 0.02478088065981865\n",
      "Epoch 8, Batch: 14: Training Loss: 0.030894169583916664, Validation Loss: 0.02470068447291851\n",
      "Epoch 8, Batch: 15: Training Loss: 0.0305635966360569, Validation Loss: 0.025852428749203682\n",
      "Epoch 8, Batch: 16: Training Loss: 0.030112698674201965, Validation Loss: 0.02833833545446396\n",
      "Epoch 8, Batch: 17: Training Loss: 0.025411121547222137, Validation Loss: 0.0264492928981781\n",
      "Epoch 8, Batch: 18: Training Loss: 0.025924785062670708, Validation Loss: 0.030594922602176666\n",
      "Epoch 8, Batch: 19: Training Loss: 0.026504049077630043, Validation Loss: 0.028721844777464867\n",
      "Epoch 8, Batch: 20: Training Loss: 0.02620265632867813, Validation Loss: 0.027615565806627274\n",
      "Epoch 8, Batch: 21: Training Loss: 0.029002880677580833, Validation Loss: 0.028152570128440857\n",
      "Epoch 8, Batch: 22: Training Loss: 0.032453350722789764, Validation Loss: 0.029839742928743362\n",
      "Epoch 8, Batch: 23: Training Loss: 0.02670373022556305, Validation Loss: 0.03147347643971443\n",
      "Epoch 8, Batch: 24: Training Loss: 0.029116734862327576, Validation Loss: 0.0299557875841856\n",
      "Epoch 8, Batch: 25: Training Loss: 0.025961920619010925, Validation Loss: 0.02929769828915596\n",
      "Epoch 8, Batch: 26: Training Loss: 0.02793809026479721, Validation Loss: 0.03208543360233307\n",
      "Epoch 8, Batch: 27: Training Loss: 0.029192879796028137, Validation Loss: 0.029361054301261902\n",
      "Epoch 8, Batch: 28: Training Loss: 0.03270300105214119, Validation Loss: 0.02854953147470951\n",
      "Epoch 8, Batch: 29: Training Loss: 0.028897086158394814, Validation Loss: 0.030275564640760422\n",
      "Epoch 8, Batch: 30: Training Loss: 0.02732071839272976, Validation Loss: 0.02703431248664856\n",
      "Epoch 8, Batch: 31: Training Loss: 0.03291364386677742, Validation Loss: 0.03098427690565586\n",
      "Epoch 8, Batch: 32: Training Loss: 0.02839645743370056, Validation Loss: 0.02610565721988678\n",
      "Epoch 8, Batch: 33: Training Loss: 0.023878594860434532, Validation Loss: 0.026322627440094948\n",
      "Epoch 8, Batch: 34: Training Loss: 0.02846394293010235, Validation Loss: 0.028418222442269325\n",
      "Epoch 8, Batch: 35: Training Loss: 0.02769775316119194, Validation Loss: 0.028304556384682655\n",
      "Epoch 8, Batch: 36: Training Loss: 0.025091800838708878, Validation Loss: 0.028635704889893532\n",
      "Epoch 8, Batch: 37: Training Loss: 0.02542881853878498, Validation Loss: 0.028851129114627838\n",
      "Epoch 8, Batch: 38: Training Loss: 0.02844195067882538, Validation Loss: 0.028126629069447517\n",
      "Epoch 8, Batch: 39: Training Loss: 0.03212026134133339, Validation Loss: 0.02864663302898407\n",
      "Epoch 8, Batch: 40: Training Loss: 0.030005281791090965, Validation Loss: 0.029857808724045753\n",
      "Epoch 8, Batch: 41: Training Loss: 0.029189832508563995, Validation Loss: 0.03099617175757885\n",
      "Epoch 8, Batch: 42: Training Loss: 0.02513873763382435, Validation Loss: 0.029380911961197853\n",
      "Epoch 8, Batch: 43: Training Loss: 0.027638079598546028, Validation Loss: 0.029380839318037033\n",
      "Epoch 8, Batch: 44: Training Loss: 0.027835799381136894, Validation Loss: 0.029631800949573517\n",
      "Epoch 8, Batch: 45: Training Loss: 0.02864387258887291, Validation Loss: 0.027893204241991043\n",
      "Epoch 8, Batch: 46: Training Loss: 0.02668583393096924, Validation Loss: 0.030456218868494034\n",
      "Epoch 8, Batch: 47: Training Loss: 0.026308933272957802, Validation Loss: 0.02971940115094185\n",
      "Epoch 8, Batch: 48: Training Loss: 0.03312883526086807, Validation Loss: 0.027534576132893562\n",
      "Epoch 8, Batch: 49: Training Loss: 0.027429142966866493, Validation Loss: 0.02776671200990677\n",
      "Epoch 8, Batch: 50: Training Loss: 0.026874355971813202, Validation Loss: 0.027836620807647705\n",
      "Epoch 8, Batch: 51: Training Loss: 0.027938716113567352, Validation Loss: 0.029939910396933556\n",
      "Epoch 8, Batch: 52: Training Loss: 0.028865313157439232, Validation Loss: 0.027999375015497208\n",
      "Epoch 8, Batch: 53: Training Loss: 0.02560124173760414, Validation Loss: 0.02573200687766075\n",
      "Epoch 8, Batch: 54: Training Loss: 0.025554662570357323, Validation Loss: 0.028207093477249146\n",
      "Epoch 8, Batch: 55: Training Loss: 0.027258392423391342, Validation Loss: 0.02836104854941368\n",
      "Epoch 8, Batch: 56: Training Loss: 0.029177768155932426, Validation Loss: 0.029196111485362053\n",
      "Epoch 8, Batch: 57: Training Loss: 0.027732767164707184, Validation Loss: 0.029007472097873688\n",
      "Epoch 8, Batch: 58: Training Loss: 0.025545088574290276, Validation Loss: 0.02602626010775566\n",
      "Epoch 8, Batch: 59: Training Loss: 0.02451425977051258, Validation Loss: 0.02645096741616726\n",
      "Epoch 8, Batch: 60: Training Loss: 0.025072088465094566, Validation Loss: 0.031570859253406525\n",
      "Epoch 8, Batch: 61: Training Loss: 0.027905968949198723, Validation Loss: 0.02598574012517929\n",
      "Epoch 8, Batch: 62: Training Loss: 0.031069837510585785, Validation Loss: 0.025157054886221886\n",
      "Epoch 8, Batch: 63: Training Loss: 0.0276207085698843, Validation Loss: 0.026725884526968002\n",
      "Epoch 8, Batch: 64: Training Loss: 0.02601544000208378, Validation Loss: 0.026706727221608162\n",
      "Epoch 8, Batch: 65: Training Loss: 0.03157036006450653, Validation Loss: 0.02850102260708809\n",
      "Epoch 8, Batch: 66: Training Loss: 0.029069626703858376, Validation Loss: 0.024469846859574318\n",
      "Epoch 8, Batch: 67: Training Loss: 0.023248862475156784, Validation Loss: 0.02897653728723526\n",
      "Epoch 8, Batch: 68: Training Loss: 0.029084566980600357, Validation Loss: 0.028558090329170227\n",
      "Epoch 8, Batch: 69: Training Loss: 0.0294555202126503, Validation Loss: 0.027684848755598068\n",
      "Epoch 8, Batch: 70: Training Loss: 0.030695751309394836, Validation Loss: 0.028763538226485252\n",
      "Epoch 8, Batch: 71: Training Loss: 0.0258368831127882, Validation Loss: 0.02571350894868374\n",
      "Epoch 8, Batch: 72: Training Loss: 0.02328142710030079, Validation Loss: 0.026808302849531174\n",
      "Epoch 8, Batch: 73: Training Loss: 0.027346597984433174, Validation Loss: 0.02412600629031658\n",
      "Epoch 8, Batch: 74: Training Loss: 0.0238229688256979, Validation Loss: 0.02696777880191803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch: 75: Training Loss: 0.022006914019584656, Validation Loss: 0.026478726416826248\n",
      "Epoch 8, Batch: 76: Training Loss: 0.023655153810977936, Validation Loss: 0.024768561124801636\n",
      "Epoch 8, Batch: 77: Training Loss: 0.03241850808262825, Validation Loss: 0.02726575918495655\n",
      "Epoch 8, Batch: 78: Training Loss: 0.034241024404764175, Validation Loss: 0.02620554529130459\n",
      "Epoch 8, Batch: 79: Training Loss: 0.024853022769093513, Validation Loss: 0.026386400684714317\n",
      "Epoch 8, Batch: 80: Training Loss: 0.026822244748473167, Validation Loss: 0.026240162551403046\n",
      "Epoch 8, Batch: 81: Training Loss: 0.03281162306666374, Validation Loss: 0.030156176537275314\n",
      "Epoch 8, Batch: 82: Training Loss: 0.02855110727250576, Validation Loss: 0.02660634182393551\n",
      "Epoch 8, Batch: 83: Training Loss: 0.03236408159136772, Validation Loss: 0.026159968227148056\n",
      "Epoch 8, Batch: 84: Training Loss: 0.02677970379590988, Validation Loss: 0.025340737774968147\n",
      "Epoch 8, Batch: 85: Training Loss: 0.026653818786144257, Validation Loss: 0.02805088460445404\n",
      "Epoch 8, Batch: 86: Training Loss: 0.03234095126390457, Validation Loss: 0.03105451725423336\n",
      "Epoch 8, Batch: 87: Training Loss: 0.03347645327448845, Validation Loss: 0.02714129537343979\n",
      "Epoch 8, Batch: 88: Training Loss: 0.03561544045805931, Validation Loss: 0.02663753181695938\n",
      "Epoch 8, Batch: 89: Training Loss: 0.03265037760138512, Validation Loss: 0.027030758559703827\n",
      "Epoch 8, Batch: 90: Training Loss: 0.026733675971627235, Validation Loss: 0.025282111018896103\n",
      "Epoch 8, Batch: 91: Training Loss: 0.031382836401462555, Validation Loss: 0.027950970456004143\n",
      "Epoch 8, Batch: 92: Training Loss: 0.03302067890763283, Validation Loss: 0.02571185678243637\n",
      "Epoch 8, Batch: 93: Training Loss: 0.03016366809606552, Validation Loss: 0.02659142017364502\n",
      "Epoch 8, Batch: 94: Training Loss: 0.032232724130153656, Validation Loss: 0.02620808593928814\n",
      "Epoch 8, Batch: 95: Training Loss: 0.028740424662828445, Validation Loss: 0.025541415438055992\n",
      "Epoch 8, Batch: 96: Training Loss: 0.02806560881435871, Validation Loss: 0.024369463324546814\n",
      "Epoch 8, Batch: 97: Training Loss: 0.028600336983799934, Validation Loss: 0.027025269344449043\n",
      "Epoch 8, Batch: 98: Training Loss: 0.026983831077814102, Validation Loss: 0.02674393355846405\n",
      "Epoch 8, Batch: 99: Training Loss: 0.030965665355324745, Validation Loss: 0.02431105636060238\n",
      "Epoch 8, Batch: 100: Training Loss: 0.026505660265684128, Validation Loss: 0.02693811058998108\n",
      "Epoch 8, Batch: 101: Training Loss: 0.027141142636537552, Validation Loss: 0.02648652344942093\n",
      "Epoch 8, Batch: 102: Training Loss: 0.024587953463196754, Validation Loss: 0.025429872795939445\n",
      "Epoch 8, Batch: 103: Training Loss: 0.029838023707270622, Validation Loss: 0.02543829008936882\n",
      "Epoch 8, Batch: 104: Training Loss: 0.023034682497382164, Validation Loss: 0.02773880958557129\n",
      "Epoch 8, Batch: 105: Training Loss: 0.02533625438809395, Validation Loss: 0.025553377345204353\n",
      "Epoch 8, Batch: 106: Training Loss: 0.029062043875455856, Validation Loss: 0.027284687384963036\n",
      "Epoch 8, Batch: 107: Training Loss: 0.026849891990423203, Validation Loss: 0.027189986780285835\n",
      "Epoch 8, Batch: 108: Training Loss: 0.026625346392393112, Validation Loss: 0.026565415784716606\n",
      "Epoch 8, Batch: 109: Training Loss: 0.02715715952217579, Validation Loss: 0.02795926295220852\n",
      "Epoch 8, Batch: 110: Training Loss: 0.02855822443962097, Validation Loss: 0.02851194702088833\n",
      "Epoch 8, Batch: 111: Training Loss: 0.028849266469478607, Validation Loss: 0.02912096120417118\n",
      "Epoch 8, Batch: 112: Training Loss: 0.02414604090154171, Validation Loss: 0.030718253925442696\n",
      "Epoch 8, Batch: 113: Training Loss: 0.029894689098000526, Validation Loss: 0.02885742112994194\n",
      "Epoch 8, Batch: 114: Training Loss: 0.026251258328557014, Validation Loss: 0.02946876734495163\n",
      "Epoch 8, Batch: 115: Training Loss: 0.029350200667977333, Validation Loss: 0.029441164806485176\n",
      "Epoch 8, Batch: 116: Training Loss: 0.030439062044024467, Validation Loss: 0.028901059180498123\n",
      "Epoch 8, Batch: 117: Training Loss: 0.03154220059514046, Validation Loss: 0.027087172493338585\n",
      "Epoch 8, Batch: 118: Training Loss: 0.027209347113966942, Validation Loss: 0.028188379481434822\n",
      "Epoch 8, Batch: 119: Training Loss: 0.02766602486371994, Validation Loss: 0.025916144251823425\n",
      "Epoch 8, Batch: 120: Training Loss: 0.024919258430600166, Validation Loss: 0.025832591578364372\n",
      "Epoch 8, Batch: 121: Training Loss: 0.02615148201584816, Validation Loss: 0.028499752283096313\n",
      "Epoch 8, Batch: 122: Training Loss: 0.029073338955640793, Validation Loss: 0.02530786581337452\n",
      "Epoch 8, Batch: 123: Training Loss: 0.025481365621089935, Validation Loss: 0.02937532216310501\n",
      "Epoch 8, Batch: 124: Training Loss: 0.02864612080156803, Validation Loss: 0.025081105530261993\n",
      "Epoch 8, Batch: 125: Training Loss: 0.02740894816815853, Validation Loss: 0.028696222230792046\n",
      "Epoch 8, Batch: 126: Training Loss: 0.027347784489393234, Validation Loss: 0.02429242618381977\n",
      "Epoch 8, Batch: 127: Training Loss: 0.025421949103474617, Validation Loss: 0.026793338358402252\n",
      "Epoch 8, Batch: 128: Training Loss: 0.0279844980686903, Validation Loss: 0.02390390820801258\n",
      "Epoch 8, Batch: 129: Training Loss: 0.025107406079769135, Validation Loss: 0.02405969426035881\n",
      "Epoch 8, Batch: 130: Training Loss: 0.026853756979107857, Validation Loss: 0.02520456165075302\n",
      "Epoch 8, Batch: 131: Training Loss: 0.024813121184706688, Validation Loss: 0.025767894461750984\n",
      "Epoch 8, Batch: 132: Training Loss: 0.025004690513014793, Validation Loss: 0.024539781734347343\n",
      "Epoch 8, Batch: 133: Training Loss: 0.02262031100690365, Validation Loss: 0.02500115893781185\n",
      "Epoch 8, Batch: 134: Training Loss: 0.02822139672935009, Validation Loss: 0.02788427099585533\n",
      "Epoch 8, Batch: 135: Training Loss: 0.02706725150346756, Validation Loss: 0.024848561733961105\n",
      "Epoch 8, Batch: 136: Training Loss: 0.02524607442319393, Validation Loss: 0.022607894614338875\n",
      "Epoch 8, Batch: 137: Training Loss: 0.022468695417046547, Validation Loss: 0.025383640080690384\n",
      "Epoch 8, Batch: 138: Training Loss: 0.02435530535876751, Validation Loss: 0.024581756442785263\n",
      "Epoch 8, Batch: 139: Training Loss: 0.02567000314593315, Validation Loss: 0.027670010924339294\n",
      "Epoch 8, Batch: 140: Training Loss: 0.03371741995215416, Validation Loss: 0.0257983710616827\n",
      "Epoch 8, Batch: 141: Training Loss: 0.02599065937101841, Validation Loss: 0.025291217491030693\n",
      "Epoch 8, Batch: 142: Training Loss: 0.028356393799185753, Validation Loss: 0.025007855147123337\n",
      "Epoch 8, Batch: 143: Training Loss: 0.02749655582010746, Validation Loss: 0.027976080775260925\n",
      "Epoch 8, Batch: 144: Training Loss: 0.024507323279976845, Validation Loss: 0.02459697797894478\n",
      "Epoch 8, Batch: 145: Training Loss: 0.024220295250415802, Validation Loss: 0.025083456188440323\n",
      "Epoch 8, Batch: 146: Training Loss: 0.02329067885875702, Validation Loss: 0.02550034411251545\n",
      "Epoch 8, Batch: 147: Training Loss: 0.026584213599562645, Validation Loss: 0.028905874118208885\n",
      "Epoch 8, Batch: 148: Training Loss: 0.024690182879567146, Validation Loss: 0.03008849546313286\n",
      "Epoch 8, Batch: 149: Training Loss: 0.025683071464300156, Validation Loss: 0.030445057898759842\n",
      "Epoch 8, Batch: 150: Training Loss: 0.03046812117099762, Validation Loss: 0.02790801413357258\n",
      "Epoch 8, Batch: 151: Training Loss: 0.030291246250271797, Validation Loss: 0.030517278239130974\n",
      "Epoch 8, Batch: 152: Training Loss: 0.02829030714929104, Validation Loss: 0.029542844742536545\n",
      "Epoch 8, Batch: 153: Training Loss: 0.029201559722423553, Validation Loss: 0.02745950222015381\n",
      "Epoch 8, Batch: 154: Training Loss: 0.026417361572384834, Validation Loss: 0.026850827038288116\n",
      "Epoch 8, Batch: 155: Training Loss: 0.03402724489569664, Validation Loss: 0.02734493464231491\n",
      "Epoch 8, Batch: 156: Training Loss: 0.028027264401316643, Validation Loss: 0.026508139446377754\n",
      "Epoch 8, Batch: 157: Training Loss: 0.020599979907274246, Validation Loss: 0.029883382841944695\n",
      "Epoch 8, Batch: 158: Training Loss: 0.02852536179125309, Validation Loss: 0.028128482401371002\n",
      "Epoch 8, Batch: 159: Training Loss: 0.024531815201044083, Validation Loss: 0.02764408476650715\n",
      "Epoch 8, Batch: 160: Training Loss: 0.026070425286889076, Validation Loss: 0.030236579477787018\n",
      "Epoch 8, Batch: 161: Training Loss: 0.025418654084205627, Validation Loss: 0.0289434976875782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch: 162: Training Loss: 0.029201773926615715, Validation Loss: 0.030775083228945732\n",
      "Epoch 8, Batch: 163: Training Loss: 0.025324596092104912, Validation Loss: 0.029432354494929314\n",
      "Epoch 8, Batch: 164: Training Loss: 0.022911107167601585, Validation Loss: 0.026670441031455994\n",
      "Epoch 8, Batch: 165: Training Loss: 0.02704182080924511, Validation Loss: 0.03037089854478836\n",
      "Epoch 8, Batch: 166: Training Loss: 0.025493554770946503, Validation Loss: 0.028111008927226067\n",
      "Epoch 8, Batch: 167: Training Loss: 0.027825672179460526, Validation Loss: 0.02733682282269001\n",
      "Epoch 8, Batch: 168: Training Loss: 0.02644766867160797, Validation Loss: 0.026973869651556015\n",
      "Epoch 8, Batch: 169: Training Loss: 0.0268552266061306, Validation Loss: 0.02805407904088497\n",
      "Epoch 8, Batch: 170: Training Loss: 0.02492004632949829, Validation Loss: 0.027958501130342484\n",
      "Epoch 8, Batch: 171: Training Loss: 0.02616223134100437, Validation Loss: 0.027903156355023384\n",
      "Epoch 8, Batch: 172: Training Loss: 0.02361467108130455, Validation Loss: 0.02979096956551075\n",
      "Epoch 8, Batch: 173: Training Loss: 0.025519641116261482, Validation Loss: 0.028161801397800446\n",
      "Epoch 8, Batch: 174: Training Loss: 0.026784811168909073, Validation Loss: 0.028385387733578682\n",
      "Epoch 8, Batch: 175: Training Loss: 0.027246246114373207, Validation Loss: 0.02899445965886116\n",
      "Epoch 8, Batch: 176: Training Loss: 0.027505438774824142, Validation Loss: 0.02719089388847351\n",
      "Epoch 8, Batch: 177: Training Loss: 0.0264168418943882, Validation Loss: 0.028514239937067032\n",
      "Epoch 8, Batch: 178: Training Loss: 0.02725781686604023, Validation Loss: 0.029918277636170387\n",
      "Epoch 8, Batch: 179: Training Loss: 0.02652706578373909, Validation Loss: 0.028389669954776764\n",
      "Epoch 8, Batch: 180: Training Loss: 0.03109978511929512, Validation Loss: 0.031696587800979614\n",
      "Epoch 8, Batch: 181: Training Loss: 0.029630860313773155, Validation Loss: 0.03052588738501072\n",
      "Epoch 8, Batch: 182: Training Loss: 0.03244812786579132, Validation Loss: 0.030596785247325897\n",
      "Epoch 8, Batch: 183: Training Loss: 0.033413950353860855, Validation Loss: 0.0303378626704216\n",
      "Epoch 8, Batch: 184: Training Loss: 0.029125390574336052, Validation Loss: 0.029795723035931587\n",
      "Epoch 8, Batch: 185: Training Loss: 0.032016996294260025, Validation Loss: 0.02865150198340416\n",
      "Epoch 8, Batch: 186: Training Loss: 0.030232030898332596, Validation Loss: 0.02836182527244091\n",
      "Epoch 8, Batch: 187: Training Loss: 0.025429559871554375, Validation Loss: 0.02768905833363533\n",
      "Epoch 8, Batch: 188: Training Loss: 0.026379521936178207, Validation Loss: 0.027280179783701897\n",
      "Epoch 8, Batch: 189: Training Loss: 0.027462240308523178, Validation Loss: 0.027833567932248116\n",
      "Epoch 8, Batch: 190: Training Loss: 0.028646299615502357, Validation Loss: 0.026325419545173645\n",
      "Epoch 8, Batch: 191: Training Loss: 0.028137095272541046, Validation Loss: 0.024942152202129364\n",
      "Epoch 8, Batch: 192: Training Loss: 0.025740116834640503, Validation Loss: 0.024603508412837982\n",
      "Epoch 8, Batch: 193: Training Loss: 0.026539497077465057, Validation Loss: 0.0246964693069458\n",
      "Epoch 8, Batch: 194: Training Loss: 0.03127095103263855, Validation Loss: 0.027789002284407616\n",
      "Epoch 8, Batch: 195: Training Loss: 0.02432263270020485, Validation Loss: 0.026002245023846626\n",
      "Epoch 8, Batch: 196: Training Loss: 0.027280239388346672, Validation Loss: 0.028833212330937386\n",
      "Epoch 8, Batch: 197: Training Loss: 0.025792842730879784, Validation Loss: 0.028914889320731163\n",
      "Epoch 8, Batch: 198: Training Loss: 0.02372691035270691, Validation Loss: 0.026217401027679443\n",
      "Epoch 8, Batch: 199: Training Loss: 0.02352168597280979, Validation Loss: 0.028027504682540894\n",
      "Epoch 8, Batch: 200: Training Loss: 0.023160584270954132, Validation Loss: 0.027354154735803604\n",
      "Epoch 8, Batch: 201: Training Loss: 0.02889443375170231, Validation Loss: 0.029660018160939217\n",
      "Epoch 8, Batch: 202: Training Loss: 0.03046993538737297, Validation Loss: 0.02584223821759224\n",
      "Epoch 8, Batch: 203: Training Loss: 0.028954321518540382, Validation Loss: 0.027521541342139244\n",
      "Epoch 8, Batch: 204: Training Loss: 0.03000347688794136, Validation Loss: 0.030328474938869476\n",
      "Epoch 8, Batch: 205: Training Loss: 0.031221697106957436, Validation Loss: 0.028487851843237877\n",
      "Epoch 8, Batch: 206: Training Loss: 0.027004454284906387, Validation Loss: 0.029649725183844566\n",
      "Epoch 8, Batch: 207: Training Loss: 0.0327601283788681, Validation Loss: 0.029183847829699516\n",
      "Epoch 8, Batch: 208: Training Loss: 0.02696988359093666, Validation Loss: 0.028273429721593857\n",
      "Epoch 8, Batch: 209: Training Loss: 0.031831033527851105, Validation Loss: 0.031153621152043343\n",
      "Epoch 8, Batch: 210: Training Loss: 0.0342598594725132, Validation Loss: 0.030030764639377594\n",
      "Epoch 8, Batch: 211: Training Loss: 0.02990746684372425, Validation Loss: 0.02493724785745144\n",
      "Epoch 8, Batch: 212: Training Loss: 0.03047289326786995, Validation Loss: 0.032711684703826904\n",
      "Epoch 8, Batch: 213: Training Loss: 0.03213861212134361, Validation Loss: 0.02635655365884304\n",
      "Epoch 8, Batch: 214: Training Loss: 0.027101345360279083, Validation Loss: 0.02789350040256977\n",
      "Epoch 8, Batch: 215: Training Loss: 0.030992407351732254, Validation Loss: 0.02660861611366272\n",
      "Epoch 8, Batch: 216: Training Loss: 0.025661101564764977, Validation Loss: 0.02709837071597576\n",
      "Epoch 8, Batch: 217: Training Loss: 0.02756364271044731, Validation Loss: 0.027405958622694016\n",
      "Epoch 8, Batch: 218: Training Loss: 0.028965570032596588, Validation Loss: 0.024931855499744415\n",
      "Epoch 8, Batch: 219: Training Loss: 0.027979999780654907, Validation Loss: 0.023756200447678566\n",
      "Epoch 8, Batch: 220: Training Loss: 0.028665753081440926, Validation Loss: 0.026634180918335915\n",
      "Epoch 8, Batch: 221: Training Loss: 0.02834642305970192, Validation Loss: 0.029070187360048294\n",
      "Epoch 8, Batch: 222: Training Loss: 0.026285294443368912, Validation Loss: 0.02886849455535412\n",
      "Epoch 8, Batch: 223: Training Loss: 0.027874836698174477, Validation Loss: 0.024369485676288605\n",
      "Epoch 8, Batch: 224: Training Loss: 0.030458208173513412, Validation Loss: 0.024304352700710297\n",
      "Epoch 8, Batch: 225: Training Loss: 0.029223231598734856, Validation Loss: 0.026901455596089363\n",
      "Epoch 8, Batch: 226: Training Loss: 0.030740126967430115, Validation Loss: 0.025390487164258957\n",
      "Epoch 8, Batch: 227: Training Loss: 0.025759533047676086, Validation Loss: 0.026271648705005646\n",
      "Epoch 8, Batch: 228: Training Loss: 0.02938859350979328, Validation Loss: 0.02777271531522274\n",
      "Epoch 8, Batch: 229: Training Loss: 0.02927589975297451, Validation Loss: 0.02624642476439476\n",
      "Epoch 8, Batch: 230: Training Loss: 0.025235338136553764, Validation Loss: 0.027142656967043877\n",
      "Epoch 8, Batch: 231: Training Loss: 0.03039221465587616, Validation Loss: 0.027531569823622704\n",
      "Epoch 8, Batch: 232: Training Loss: 0.031230777502059937, Validation Loss: 0.026449041441082954\n",
      "Epoch 8, Batch: 233: Training Loss: 0.028434352949261665, Validation Loss: 0.026503682136535645\n",
      "Epoch 8, Batch: 234: Training Loss: 0.029535269364714622, Validation Loss: 0.027172144502401352\n",
      "Epoch 8, Batch: 235: Training Loss: 0.03107384778559208, Validation Loss: 0.026958078145980835\n",
      "Epoch 8, Batch: 236: Training Loss: 0.028469650074839592, Validation Loss: 0.026624424383044243\n",
      "Epoch 8, Batch: 237: Training Loss: 0.023878291249275208, Validation Loss: 0.02821141481399536\n",
      "Epoch 8, Batch: 238: Training Loss: 0.02650914527475834, Validation Loss: 0.02795509621500969\n",
      "Epoch 8, Batch: 239: Training Loss: 0.023962950333952904, Validation Loss: 0.026475179940462112\n",
      "Epoch 8, Batch: 240: Training Loss: 0.0249424260109663, Validation Loss: 0.028497541323304176\n",
      "Epoch 8, Batch: 241: Training Loss: 0.026810940355062485, Validation Loss: 0.02700095809996128\n",
      "Epoch 8, Batch: 242: Training Loss: 0.02522675134241581, Validation Loss: 0.026777436956763268\n",
      "Epoch 8, Batch: 243: Training Loss: 0.03155994415283203, Validation Loss: 0.02840641513466835\n",
      "Epoch 8, Batch: 244: Training Loss: 0.029619872570037842, Validation Loss: 0.028731955215334892\n",
      "Epoch 8, Batch: 245: Training Loss: 0.030728351324796677, Validation Loss: 0.026982393115758896\n",
      "Epoch 8, Batch: 246: Training Loss: 0.02613152004778385, Validation Loss: 0.02778671495616436\n",
      "Epoch 8, Batch: 247: Training Loss: 0.030244717374444008, Validation Loss: 0.02847905270755291\n",
      "Epoch 8, Batch: 248: Training Loss: 0.02424229495227337, Validation Loss: 0.027255259454250336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch: 249: Training Loss: 0.027273191139101982, Validation Loss: 0.027400821447372437\n",
      "Epoch 8, Batch: 250: Training Loss: 0.025924935936927795, Validation Loss: 0.029523303732275963\n",
      "Epoch 8, Batch: 251: Training Loss: 0.023298164829611778, Validation Loss: 0.031235216185450554\n",
      "Epoch 8, Batch: 252: Training Loss: 0.02596060372889042, Validation Loss: 0.02741323783993721\n",
      "Epoch 8, Batch: 253: Training Loss: 0.030896125361323357, Validation Loss: 0.026588499546051025\n",
      "Epoch 8, Batch: 254: Training Loss: 0.024517469108104706, Validation Loss: 0.030207214877009392\n",
      "Epoch 8, Batch: 255: Training Loss: 0.028905581682920456, Validation Loss: 0.028610870242118835\n",
      "Epoch 8, Batch: 256: Training Loss: 0.0325455404818058, Validation Loss: 0.030068807303905487\n",
      "Epoch 8, Batch: 257: Training Loss: 0.027712974697351456, Validation Loss: 0.027301572263240814\n",
      "Epoch 8, Batch: 258: Training Loss: 0.029919246211647987, Validation Loss: 0.027242174372076988\n",
      "Epoch 8, Batch: 259: Training Loss: 0.02828780561685562, Validation Loss: 0.027606559917330742\n",
      "Epoch 8, Batch: 260: Training Loss: 0.031954169273376465, Validation Loss: 0.02920791506767273\n",
      "Epoch 8, Batch: 261: Training Loss: 0.02714863419532776, Validation Loss: 0.030633803457021713\n",
      "Epoch 8, Batch: 262: Training Loss: 0.0275898315012455, Validation Loss: 0.030439579859375954\n",
      "Epoch 8, Batch: 263: Training Loss: 0.026755914092063904, Validation Loss: 0.0305768009275198\n",
      "Epoch 8, Batch: 264: Training Loss: 0.022718951106071472, Validation Loss: 0.028741145506501198\n",
      "Epoch 8, Batch: 265: Training Loss: 0.029092218726873398, Validation Loss: 0.029949095100164413\n",
      "Epoch 8, Batch: 266: Training Loss: 0.02716165967285633, Validation Loss: 0.02885795757174492\n",
      "Epoch 8, Batch: 267: Training Loss: 0.028989171609282494, Validation Loss: 0.028292588889598846\n",
      "Epoch 8, Batch: 268: Training Loss: 0.026970967650413513, Validation Loss: 0.027868885546922684\n",
      "Epoch 8, Batch: 269: Training Loss: 0.02456335537135601, Validation Loss: 0.028530249372124672\n",
      "Epoch 8, Batch: 270: Training Loss: 0.025840267539024353, Validation Loss: 0.02942001260817051\n",
      "Epoch 8, Batch: 271: Training Loss: 0.026265272870659828, Validation Loss: 0.029143285006284714\n",
      "Epoch 8, Batch: 272: Training Loss: 0.028326377272605896, Validation Loss: 0.029718216508626938\n",
      "Epoch 8, Batch: 273: Training Loss: 0.027626531198620796, Validation Loss: 0.027009984478354454\n",
      "Epoch 8, Batch: 274: Training Loss: 0.02816804312169552, Validation Loss: 0.028113678097724915\n",
      "Epoch 8, Batch: 275: Training Loss: 0.028685249388217926, Validation Loss: 0.026976723223924637\n",
      "Epoch 8, Batch: 276: Training Loss: 0.027128711342811584, Validation Loss: 0.02598259039223194\n",
      "Epoch 8, Batch: 277: Training Loss: 0.02612951025366783, Validation Loss: 0.031047629192471504\n",
      "Epoch 8, Batch: 278: Training Loss: 0.02977980487048626, Validation Loss: 0.027006343007087708\n",
      "Epoch 8, Batch: 279: Training Loss: 0.025790691375732422, Validation Loss: 0.029021544381976128\n",
      "Epoch 8, Batch: 280: Training Loss: 0.023517990484833717, Validation Loss: 0.027605636045336723\n",
      "Epoch 8, Batch: 281: Training Loss: 0.02594728395342827, Validation Loss: 0.0296009574085474\n",
      "Epoch 8, Batch: 282: Training Loss: 0.024748221039772034, Validation Loss: 0.03286031633615494\n",
      "Epoch 8, Batch: 283: Training Loss: 0.02700996771454811, Validation Loss: 0.028708934783935547\n",
      "Epoch 8, Batch: 284: Training Loss: 0.03019874542951584, Validation Loss: 0.02929779142141342\n",
      "Epoch 8, Batch: 285: Training Loss: 0.027585027739405632, Validation Loss: 0.03184494748711586\n",
      "Epoch 8, Batch: 286: Training Loss: 0.026422947645187378, Validation Loss: 0.027207547798752785\n",
      "Epoch 8, Batch: 287: Training Loss: 0.03187099099159241, Validation Loss: 0.030363338068127632\n",
      "Epoch 8, Batch: 288: Training Loss: 0.024029485881328583, Validation Loss: 0.027746623381972313\n",
      "Epoch 8, Batch: 289: Training Loss: 0.03220074251294136, Validation Loss: 0.026378503069281578\n",
      "Epoch 8, Batch: 290: Training Loss: 0.026702623814344406, Validation Loss: 0.02774188108742237\n",
      "Epoch 8, Batch: 291: Training Loss: 0.02674502693116665, Validation Loss: 0.026739610359072685\n",
      "Epoch 8, Batch: 292: Training Loss: 0.031017551198601723, Validation Loss: 0.030122756958007812\n",
      "Epoch 8, Batch: 293: Training Loss: 0.03360944613814354, Validation Loss: 0.028777403756976128\n",
      "Epoch 8, Batch: 294: Training Loss: 0.029462993144989014, Validation Loss: 0.02583090215921402\n",
      "Epoch 8, Batch: 295: Training Loss: 0.028297817334532738, Validation Loss: 0.026568638160824776\n",
      "Epoch 8, Batch: 296: Training Loss: 0.025519724935293198, Validation Loss: 0.026043862104415894\n",
      "Epoch 8, Batch: 297: Training Loss: 0.028362968936562538, Validation Loss: 0.02645949460566044\n",
      "Epoch 8, Batch: 298: Training Loss: 0.022561736404895782, Validation Loss: 0.025224024429917336\n",
      "Epoch 8, Batch: 299: Training Loss: 0.024624014273285866, Validation Loss: 0.027387825772166252\n",
      "Epoch 8, Batch: 300: Training Loss: 0.026805181056261063, Validation Loss: 0.027788829058408737\n",
      "Epoch 8, Batch: 301: Training Loss: 0.02557474933564663, Validation Loss: 0.03045075573027134\n",
      "Epoch 8, Batch: 302: Training Loss: 0.02204119600355625, Validation Loss: 0.028225049376487732\n",
      "Epoch 8, Batch: 303: Training Loss: 0.024503517895936966, Validation Loss: 0.0280888881534338\n",
      "Epoch 8, Batch: 304: Training Loss: 0.026337143033742905, Validation Loss: 0.026801373809576035\n",
      "Epoch 8, Batch: 305: Training Loss: 0.021420283243060112, Validation Loss: 0.025526417419314384\n",
      "Epoch 8, Batch: 306: Training Loss: 0.025429055094718933, Validation Loss: 0.028227534145116806\n",
      "Epoch 8, Batch: 307: Training Loss: 0.02416217513382435, Validation Loss: 0.027964934706687927\n",
      "Epoch 8, Batch: 308: Training Loss: 0.024784088134765625, Validation Loss: 0.02627088688313961\n",
      "Epoch 8, Batch: 309: Training Loss: 0.02558792196214199, Validation Loss: 0.027566276490688324\n",
      "Epoch 8, Batch: 310: Training Loss: 0.023327087983489037, Validation Loss: 0.024929264560341835\n",
      "Epoch 8, Batch: 311: Training Loss: 0.024347510188817978, Validation Loss: 0.02364264614880085\n",
      "Epoch 8, Batch: 312: Training Loss: 0.028493650257587433, Validation Loss: 0.0260157510638237\n",
      "Epoch 8, Batch: 313: Training Loss: 0.023399535566568375, Validation Loss: 0.028450563549995422\n",
      "Epoch 8, Batch: 314: Training Loss: 0.027451036497950554, Validation Loss: 0.025269338861107826\n",
      "Epoch 8, Batch: 315: Training Loss: 0.028362711891531944, Validation Loss: 0.02818114496767521\n",
      "Epoch 8, Batch: 316: Training Loss: 0.024563636630773544, Validation Loss: 0.025551361963152885\n",
      "Epoch 8, Batch: 317: Training Loss: 0.02352151647210121, Validation Loss: 0.027135776355862617\n",
      "Epoch 8, Batch: 318: Training Loss: 0.02395663596689701, Validation Loss: 0.028439801186323166\n",
      "Epoch 8, Batch: 319: Training Loss: 0.025160424411296844, Validation Loss: 0.03228267282247543\n",
      "Epoch 8, Batch: 320: Training Loss: 0.028201622888445854, Validation Loss: 0.028280500322580338\n",
      "Epoch 8, Batch: 321: Training Loss: 0.02415669523179531, Validation Loss: 0.028201738372445107\n",
      "Epoch 8, Batch: 322: Training Loss: 0.02374875918030739, Validation Loss: 0.028756560757756233\n",
      "Epoch 8, Batch: 323: Training Loss: 0.026253361254930496, Validation Loss: 0.028441792353987694\n",
      "Epoch 8, Batch: 324: Training Loss: 0.02622421458363533, Validation Loss: 0.031265951693058014\n",
      "Epoch 8, Batch: 325: Training Loss: 0.025045296177268028, Validation Loss: 0.029721323400735855\n",
      "Epoch 8, Batch: 326: Training Loss: 0.02643936313688755, Validation Loss: 0.029156949371099472\n",
      "Epoch 8, Batch: 327: Training Loss: 0.02474340982735157, Validation Loss: 0.029272031038999557\n",
      "Epoch 8, Batch: 328: Training Loss: 0.03140263631939888, Validation Loss: 0.028725948184728622\n",
      "Epoch 8, Batch: 329: Training Loss: 0.025583365932106972, Validation Loss: 0.030240565538406372\n",
      "Epoch 8, Batch: 330: Training Loss: 0.027577580884099007, Validation Loss: 0.028908805921673775\n",
      "Epoch 8, Batch: 331: Training Loss: 0.024551192298531532, Validation Loss: 0.027379505336284637\n",
      "Epoch 8, Batch: 332: Training Loss: 0.02887989953160286, Validation Loss: 0.030820511281490326\n",
      "Epoch 8, Batch: 333: Training Loss: 0.03089108131825924, Validation Loss: 0.029120998457074165\n",
      "Epoch 8, Batch: 334: Training Loss: 0.029186449944972992, Validation Loss: 0.03208770602941513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch: 335: Training Loss: 0.02708202414214611, Validation Loss: 0.030038246884942055\n",
      "Epoch 8, Batch: 336: Training Loss: 0.028918689116835594, Validation Loss: 0.03142896667122841\n",
      "Epoch 8, Batch: 337: Training Loss: 0.021414605900645256, Validation Loss: 0.02914283610880375\n",
      "Epoch 8, Batch: 338: Training Loss: 0.027428124099969864, Validation Loss: 0.030457569286227226\n",
      "Epoch 8, Batch: 339: Training Loss: 0.02496081218123436, Validation Loss: 0.03177424892783165\n",
      "Epoch 8, Batch: 340: Training Loss: 0.026833096519112587, Validation Loss: 0.029884524643421173\n",
      "Epoch 8, Batch: 341: Training Loss: 0.02549009397625923, Validation Loss: 0.029889296740293503\n",
      "Epoch 8, Batch: 342: Training Loss: 0.027301611378788948, Validation Loss: 0.0296187624335289\n",
      "Epoch 8, Batch: 343: Training Loss: 0.02125011943280697, Validation Loss: 0.03135202080011368\n",
      "Epoch 8, Batch: 344: Training Loss: 0.028343353420495987, Validation Loss: 0.029209909960627556\n",
      "Epoch 8, Batch: 345: Training Loss: 0.027160415425896645, Validation Loss: 0.027673354372382164\n",
      "Epoch 8, Batch: 346: Training Loss: 0.026614729315042496, Validation Loss: 0.03075540065765381\n",
      "Epoch 8, Batch: 347: Training Loss: 0.026147635653614998, Validation Loss: 0.03015330620110035\n",
      "Epoch 8, Batch: 348: Training Loss: 0.0234848540276289, Validation Loss: 0.029330650344491005\n",
      "Epoch 8, Batch: 349: Training Loss: 0.027737976983189583, Validation Loss: 0.029522297903895378\n",
      "Epoch 8, Batch: 350: Training Loss: 0.02644295059144497, Validation Loss: 0.03304101526737213\n",
      "Epoch 8, Batch: 351: Training Loss: 0.03458184376358986, Validation Loss: 0.030981682240962982\n",
      "Epoch 8, Batch: 352: Training Loss: 0.02097274921834469, Validation Loss: 0.030474262312054634\n",
      "Epoch 8, Batch: 353: Training Loss: 0.02445862628519535, Validation Loss: 0.030710997059941292\n",
      "Epoch 8, Batch: 354: Training Loss: 0.031103959307074547, Validation Loss: 0.028812242671847343\n",
      "Epoch 8, Batch: 355: Training Loss: 0.021972564980387688, Validation Loss: 0.029036909341812134\n",
      "Epoch 8, Batch: 356: Training Loss: 0.030039038509130478, Validation Loss: 0.02804861031472683\n",
      "Epoch 8, Batch: 357: Training Loss: 0.023197690024971962, Validation Loss: 0.026471486315131187\n",
      "Epoch 8, Batch: 358: Training Loss: 0.027640514075756073, Validation Loss: 0.027977295219898224\n",
      "Epoch 8, Batch: 359: Training Loss: 0.024411164224147797, Validation Loss: 0.030376501381397247\n",
      "Epoch 8, Batch: 360: Training Loss: 0.025595717132091522, Validation Loss: 0.028744643554091454\n",
      "Epoch 8, Batch: 361: Training Loss: 0.024558305740356445, Validation Loss: 0.027006596326828003\n",
      "Epoch 8, Batch: 362: Training Loss: 0.025997132062911987, Validation Loss: 0.03130913898348808\n",
      "Epoch 8, Batch: 363: Training Loss: 0.02893717586994171, Validation Loss: 0.029376693069934845\n",
      "Epoch 8, Batch: 364: Training Loss: 0.024805981665849686, Validation Loss: 0.02767323888838291\n",
      "Epoch 8, Batch: 365: Training Loss: 0.026684526354074478, Validation Loss: 0.02681824192404747\n",
      "Epoch 8, Batch: 366: Training Loss: 0.028207814320921898, Validation Loss: 0.02894461713731289\n",
      "Epoch 8, Batch: 367: Training Loss: 0.025871559977531433, Validation Loss: 0.02828374132514\n",
      "Epoch 8, Batch: 368: Training Loss: 0.024724695831537247, Validation Loss: 0.029591605067253113\n",
      "Epoch 8, Batch: 369: Training Loss: 0.024123113602399826, Validation Loss: 0.032001346349716187\n",
      "Epoch 8, Batch: 370: Training Loss: 0.025250118225812912, Validation Loss: 0.03197275474667549\n",
      "Epoch 8, Batch: 371: Training Loss: 0.02501041814684868, Validation Loss: 0.027193984016776085\n",
      "Epoch 8, Batch: 372: Training Loss: 0.0255135428160429, Validation Loss: 0.028832511976361275\n",
      "Epoch 8, Batch: 373: Training Loss: 0.030756577849388123, Validation Loss: 0.027610469609498978\n",
      "Epoch 8, Batch: 374: Training Loss: 0.02592560276389122, Validation Loss: 0.025583606213331223\n",
      "Epoch 8, Batch: 375: Training Loss: 0.029439754784107208, Validation Loss: 0.026478836312890053\n",
      "Epoch 8, Batch: 376: Training Loss: 0.028924088925123215, Validation Loss: 0.027591902762651443\n",
      "Epoch 8, Batch: 377: Training Loss: 0.03027409315109253, Validation Loss: 0.02744271606206894\n",
      "Epoch 8, Batch: 378: Training Loss: 0.02735089510679245, Validation Loss: 0.02578107640147209\n",
      "Epoch 8, Batch: 379: Training Loss: 0.02672572433948517, Validation Loss: 0.026021083816885948\n",
      "Epoch 8, Batch: 380: Training Loss: 0.0315951444208622, Validation Loss: 0.02630685269832611\n",
      "Epoch 8, Batch: 381: Training Loss: 0.02571725659072399, Validation Loss: 0.023669913411140442\n",
      "Epoch 8, Batch: 382: Training Loss: 0.026358217000961304, Validation Loss: 0.025528624653816223\n",
      "Epoch 8, Batch: 383: Training Loss: 0.027119230479002, Validation Loss: 0.026065364480018616\n",
      "Epoch 8, Batch: 384: Training Loss: 0.028483588248491287, Validation Loss: 0.02696284092962742\n",
      "Epoch 8, Batch: 385: Training Loss: 0.026089061051607132, Validation Loss: 0.026123985648155212\n",
      "Epoch 8, Batch: 386: Training Loss: 0.02419411763548851, Validation Loss: 0.02838507667183876\n",
      "Epoch 8, Batch: 387: Training Loss: 0.026297971606254578, Validation Loss: 0.027665801346302032\n",
      "Epoch 8, Batch: 388: Training Loss: 0.027917269617319107, Validation Loss: 0.026824073866009712\n",
      "Epoch 8, Batch: 389: Training Loss: 0.02741745114326477, Validation Loss: 0.027821874246001244\n",
      "Epoch 8, Batch: 390: Training Loss: 0.027540849521756172, Validation Loss: 0.02540886029601097\n",
      "Epoch 8, Batch: 391: Training Loss: 0.02673283778131008, Validation Loss: 0.02655685506761074\n",
      "Epoch 8, Batch: 392: Training Loss: 0.029637852683663368, Validation Loss: 0.027427827939391136\n",
      "Epoch 8, Batch: 393: Training Loss: 0.026577701792120934, Validation Loss: 0.026320727542042732\n",
      "Epoch 8, Batch: 394: Training Loss: 0.02731035277247429, Validation Loss: 0.02725895121693611\n",
      "Epoch 8, Batch: 395: Training Loss: 0.02403068169951439, Validation Loss: 0.024036748334765434\n",
      "Epoch 8, Batch: 396: Training Loss: 0.02731550857424736, Validation Loss: 0.02874624915421009\n",
      "Epoch 8, Batch: 397: Training Loss: 0.03090468980371952, Validation Loss: 0.023983363062143326\n",
      "Epoch 8, Batch: 398: Training Loss: 0.024691147729754448, Validation Loss: 0.022950544953346252\n",
      "Epoch 8, Batch: 399: Training Loss: 0.02592763863503933, Validation Loss: 0.025737475603818893\n",
      "Epoch 8, Batch: 400: Training Loss: 0.027085673063993454, Validation Loss: 0.02803395874798298\n",
      "Epoch 8, Batch: 401: Training Loss: 0.02584378980100155, Validation Loss: 0.029212461784482002\n",
      "Epoch 8, Batch: 402: Training Loss: 0.02518414333462715, Validation Loss: 0.028005380183458328\n",
      "Epoch 8, Batch: 403: Training Loss: 0.028064580634236336, Validation Loss: 0.02729102596640587\n",
      "Epoch 8, Batch: 404: Training Loss: 0.02883739210665226, Validation Loss: 0.030662115663290024\n",
      "Epoch 8, Batch: 405: Training Loss: 0.026667652651667595, Validation Loss: 0.02696257270872593\n",
      "Epoch 8, Batch: 406: Training Loss: 0.028679216280579567, Validation Loss: 0.028497004881501198\n",
      "Epoch 8, Batch: 407: Training Loss: 0.02779514342546463, Validation Loss: 0.028959862887859344\n",
      "Epoch 8, Batch: 408: Training Loss: 0.023025942966341972, Validation Loss: 0.02975262515246868\n",
      "Epoch 8, Batch: 409: Training Loss: 0.02769666165113449, Validation Loss: 0.02878214232623577\n",
      "Epoch 8, Batch: 410: Training Loss: 0.025992533192038536, Validation Loss: 0.026283981278538704\n",
      "Epoch 8, Batch: 411: Training Loss: 0.024678610265254974, Validation Loss: 0.02994234673678875\n",
      "Epoch 8, Batch: 412: Training Loss: 0.031943053007125854, Validation Loss: 0.027516914531588554\n",
      "Epoch 8, Batch: 413: Training Loss: 0.029793502762913704, Validation Loss: 0.029711991548538208\n",
      "Epoch 8, Batch: 414: Training Loss: 0.026420753449201584, Validation Loss: 0.028951449319720268\n",
      "Epoch 8, Batch: 415: Training Loss: 0.028427688404917717, Validation Loss: 0.02693171799182892\n",
      "Epoch 8, Batch: 416: Training Loss: 0.0279702078551054, Validation Loss: 0.028187241405248642\n",
      "Epoch 8, Batch: 417: Training Loss: 0.027603916823863983, Validation Loss: 0.02873370237648487\n",
      "Epoch 8, Batch: 418: Training Loss: 0.026225639507174492, Validation Loss: 0.027958940714597702\n",
      "Epoch 8, Batch: 419: Training Loss: 0.028026597574353218, Validation Loss: 0.028758594766259193\n",
      "Epoch 8, Batch: 420: Training Loss: 0.03019355610013008, Validation Loss: 0.030405331403017044\n",
      "Epoch 8, Batch: 421: Training Loss: 0.031524788588285446, Validation Loss: 0.030125124379992485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch: 422: Training Loss: 0.030469201505184174, Validation Loss: 0.028442541137337685\n",
      "Epoch 8, Batch: 423: Training Loss: 0.026438415050506592, Validation Loss: 0.02971830777823925\n",
      "Epoch 8, Batch: 424: Training Loss: 0.02732609212398529, Validation Loss: 0.03263380005955696\n",
      "Epoch 8, Batch: 425: Training Loss: 0.029272865504026413, Validation Loss: 0.029471825808286667\n",
      "Epoch 8, Batch: 426: Training Loss: 0.02833540365099907, Validation Loss: 0.028880231082439423\n",
      "Epoch 8, Batch: 427: Training Loss: 0.028180649504065514, Validation Loss: 0.02904919721186161\n",
      "Epoch 8, Batch: 428: Training Loss: 0.024318544194102287, Validation Loss: 0.030510714277625084\n",
      "Epoch 8, Batch: 429: Training Loss: 0.025305362418293953, Validation Loss: 0.030474886298179626\n",
      "Epoch 8, Batch: 430: Training Loss: 0.02414628304541111, Validation Loss: 0.029043110087513924\n",
      "Epoch 8, Batch: 431: Training Loss: 0.029132995754480362, Validation Loss: 0.03142537176609039\n",
      "Epoch 8, Batch: 432: Training Loss: 0.03173651918768883, Validation Loss: 0.03151516988873482\n",
      "Epoch 8, Batch: 433: Training Loss: 0.028340965509414673, Validation Loss: 0.03254806995391846\n",
      "Epoch 8, Batch: 434: Training Loss: 0.027430687099695206, Validation Loss: 0.030598586425185204\n",
      "Epoch 8, Batch: 435: Training Loss: 0.027356358245015144, Validation Loss: 0.03430379554629326\n",
      "Epoch 8, Batch: 436: Training Loss: 0.026706617325544357, Validation Loss: 0.03052663616836071\n",
      "Epoch 8, Batch: 437: Training Loss: 0.026304669678211212, Validation Loss: 0.02967156283557415\n",
      "Epoch 8, Batch: 438: Training Loss: 0.028457913547754288, Validation Loss: 0.029248349368572235\n",
      "Epoch 8, Batch: 439: Training Loss: 0.028042098507285118, Validation Loss: 0.029060445725917816\n",
      "Epoch 8, Batch: 440: Training Loss: 0.0298417080193758, Validation Loss: 0.02975757047533989\n",
      "Epoch 8, Batch: 441: Training Loss: 0.025096707046031952, Validation Loss: 0.03145316615700722\n",
      "Epoch 8, Batch: 442: Training Loss: 0.027239937335252762, Validation Loss: 0.0283887330442667\n",
      "Epoch 8, Batch: 443: Training Loss: 0.026446213945746422, Validation Loss: 0.027663588523864746\n",
      "Epoch 8, Batch: 444: Training Loss: 0.0241093672811985, Validation Loss: 0.02627570368349552\n",
      "Epoch 8, Batch: 445: Training Loss: 0.02502443827688694, Validation Loss: 0.025942448526620865\n",
      "Epoch 8, Batch: 446: Training Loss: 0.027389533817768097, Validation Loss: 0.025760410353541374\n",
      "Epoch 8, Batch: 447: Training Loss: 0.025214767083525658, Validation Loss: 0.025859178975224495\n",
      "Epoch 8, Batch: 448: Training Loss: 0.023429926484823227, Validation Loss: 0.027807606384158134\n",
      "Epoch 8, Batch: 449: Training Loss: 0.029527870938181877, Validation Loss: 0.027789900079369545\n",
      "Epoch 8, Batch: 450: Training Loss: 0.026988308876752853, Validation Loss: 0.025150686502456665\n",
      "Epoch 8, Batch: 451: Training Loss: 0.027025530114769936, Validation Loss: 0.025965552777051926\n",
      "Epoch 8, Batch: 452: Training Loss: 0.02864251472055912, Validation Loss: 0.025422753766179085\n",
      "Epoch 8, Batch: 453: Training Loss: 0.02796722576022148, Validation Loss: 0.02490401640534401\n",
      "Epoch 8, Batch: 454: Training Loss: 0.027758052572607994, Validation Loss: 0.024988865479826927\n",
      "Epoch 8, Batch: 455: Training Loss: 0.027352280914783478, Validation Loss: 0.028039107099175453\n",
      "Epoch 8, Batch: 456: Training Loss: 0.028257867321372032, Validation Loss: 0.028465550392866135\n",
      "Epoch 8, Batch: 457: Training Loss: 0.030482076108455658, Validation Loss: 0.030873825773596764\n",
      "Epoch 8, Batch: 458: Training Loss: 0.02644350193440914, Validation Loss: 0.02704600617289543\n",
      "Epoch 8, Batch: 459: Training Loss: 0.031321022659540176, Validation Loss: 0.029112106189131737\n",
      "Epoch 8, Batch: 460: Training Loss: 0.03071586601436138, Validation Loss: 0.028244705870747566\n",
      "Epoch 8, Batch: 461: Training Loss: 0.02686144784092903, Validation Loss: 0.02813313528895378\n",
      "Epoch 8, Batch: 462: Training Loss: 0.024983789771795273, Validation Loss: 0.028937876224517822\n",
      "Epoch 8, Batch: 463: Training Loss: 0.031164614483714104, Validation Loss: 0.025993796065449715\n",
      "Epoch 8, Batch: 464: Training Loss: 0.025780973955988884, Validation Loss: 0.028727926313877106\n",
      "Epoch 8, Batch: 465: Training Loss: 0.029696572571992874, Validation Loss: 0.027398139238357544\n",
      "Epoch 8, Batch: 466: Training Loss: 0.025978684425354004, Validation Loss: 0.025968287140130997\n",
      "Epoch 8, Batch: 467: Training Loss: 0.03309275582432747, Validation Loss: 0.026835540309548378\n",
      "Epoch 8, Batch: 468: Training Loss: 0.03356331214308739, Validation Loss: 0.026086430996656418\n",
      "Epoch 8, Batch: 469: Training Loss: 0.028886940330266953, Validation Loss: 0.02765289694070816\n",
      "Epoch 8, Batch: 470: Training Loss: 0.03046037256717682, Validation Loss: 0.025597160682082176\n",
      "Epoch 8, Batch: 471: Training Loss: 0.030067821964621544, Validation Loss: 0.026041226461529732\n",
      "Epoch 8, Batch: 472: Training Loss: 0.02716703899204731, Validation Loss: 0.026339486241340637\n",
      "Epoch 8, Batch: 473: Training Loss: 0.028230121359229088, Validation Loss: 0.023590493947267532\n",
      "Epoch 8, Batch: 474: Training Loss: 0.024795228615403175, Validation Loss: 0.02774948626756668\n",
      "Epoch 8, Batch: 475: Training Loss: 0.0312880277633667, Validation Loss: 0.02695288509130478\n",
      "Epoch 8, Batch: 476: Training Loss: 0.028954599052667618, Validation Loss: 0.02445306070148945\n",
      "Epoch 8, Batch: 477: Training Loss: 0.029828770086169243, Validation Loss: 0.025923317298293114\n",
      "Epoch 8, Batch: 478: Training Loss: 0.02499714121222496, Validation Loss: 0.03125732019543648\n",
      "Epoch 8, Batch: 479: Training Loss: 0.028696829453110695, Validation Loss: 0.02714657410979271\n",
      "Epoch 8, Batch: 480: Training Loss: 0.02921796403825283, Validation Loss: 0.028273293748497963\n",
      "Epoch 8, Batch: 481: Training Loss: 0.029016895219683647, Validation Loss: 0.028552379459142685\n",
      "Epoch 8, Batch: 482: Training Loss: 0.03192790225148201, Validation Loss: 0.03057168796658516\n",
      "Epoch 8, Batch: 483: Training Loss: 0.02414107508957386, Validation Loss: 0.028560901060700417\n",
      "Epoch 8, Batch: 484: Training Loss: 0.02567555382847786, Validation Loss: 0.028990689665079117\n",
      "Epoch 8, Batch: 485: Training Loss: 0.02763986960053444, Validation Loss: 0.03028586320579052\n",
      "Epoch 8, Batch: 486: Training Loss: 0.02784232795238495, Validation Loss: 0.026607606559991837\n",
      "Epoch 8, Batch: 487: Training Loss: 0.025602571666240692, Validation Loss: 0.028573788702487946\n",
      "Epoch 8, Batch: 488: Training Loss: 0.02819684147834778, Validation Loss: 0.027209928259253502\n",
      "Epoch 8, Batch: 489: Training Loss: 0.032355282455682755, Validation Loss: 0.028528651222586632\n",
      "Epoch 8, Batch: 490: Training Loss: 0.029243938624858856, Validation Loss: 0.02799379825592041\n",
      "Epoch 8, Batch: 491: Training Loss: 0.025241181254386902, Validation Loss: 0.026752501726150513\n",
      "Epoch 8, Batch: 492: Training Loss: 0.026440750807523727, Validation Loss: 0.029451865702867508\n",
      "Epoch 8, Batch: 493: Training Loss: 0.028271706774830818, Validation Loss: 0.029677195474505424\n",
      "Epoch 8, Batch: 494: Training Loss: 0.028005683794617653, Validation Loss: 0.0295695923268795\n",
      "Epoch 8, Batch: 495: Training Loss: 0.026348110288381577, Validation Loss: 0.027876749634742737\n",
      "Epoch 8, Batch: 496: Training Loss: 0.024960162118077278, Validation Loss: 0.02595514804124832\n",
      "Epoch 8, Batch: 497: Training Loss: 0.0240914449095726, Validation Loss: 0.02709893323481083\n",
      "Epoch 8, Batch: 498: Training Loss: 0.02730601094663143, Validation Loss: 0.027674293145537376\n",
      "Epoch 8, Batch: 499: Training Loss: 0.026381004601716995, Validation Loss: 0.02678997814655304\n",
      "Epoch 9, Batch: 0: Training Loss: 0.027413515374064445, Validation Loss: 0.029380755499005318\n",
      "Epoch 9, Batch: 1: Training Loss: 0.025164566934108734, Validation Loss: 0.026278385892510414\n",
      "Epoch 9, Batch: 2: Training Loss: 0.027338890358805656, Validation Loss: 0.02886079251766205\n",
      "Epoch 9, Batch: 3: Training Loss: 0.027340656146407127, Validation Loss: 0.027636827901005745\n",
      "Epoch 9, Batch: 4: Training Loss: 0.026868911460042, Validation Loss: 0.029237601906061172\n",
      "Epoch 9, Batch: 5: Training Loss: 0.025993796065449715, Validation Loss: 0.025619110092520714\n",
      "Epoch 9, Batch: 6: Training Loss: 0.028626520186662674, Validation Loss: 0.029921088367700577\n",
      "Epoch 9, Batch: 7: Training Loss: 0.02814100868999958, Validation Loss: 0.025909744203090668\n",
      "Epoch 9, Batch: 8: Training Loss: 0.026319220662117004, Validation Loss: 0.027639450505375862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 9: Training Loss: 0.02760372683405876, Validation Loss: 0.02603812702000141\n",
      "Epoch 9, Batch: 10: Training Loss: 0.027983510866761208, Validation Loss: 0.025585731491446495\n",
      "Epoch 9, Batch: 11: Training Loss: 0.02954893186688423, Validation Loss: 0.02590654231607914\n",
      "Epoch 9, Batch: 12: Training Loss: 0.02835245616734028, Validation Loss: 0.0259123295545578\n",
      "Epoch 9, Batch: 13: Training Loss: 0.0279455054551363, Validation Loss: 0.026312366127967834\n",
      "Epoch 9, Batch: 14: Training Loss: 0.02820405550301075, Validation Loss: 0.026815999299287796\n",
      "Epoch 9, Batch: 15: Training Loss: 0.02594558522105217, Validation Loss: 0.02562173828482628\n",
      "Epoch 9, Batch: 16: Training Loss: 0.02783849835395813, Validation Loss: 0.026589425280690193\n",
      "Epoch 9, Batch: 17: Training Loss: 0.026719320565462112, Validation Loss: 0.02726995199918747\n",
      "Epoch 9, Batch: 18: Training Loss: 0.027639688923954964, Validation Loss: 0.026362380012869835\n",
      "Epoch 9, Batch: 19: Training Loss: 0.027209674939513206, Validation Loss: 0.02778342179954052\n",
      "Epoch 9, Batch: 20: Training Loss: 0.025932328775525093, Validation Loss: 0.027077006176114082\n",
      "Epoch 9, Batch: 21: Training Loss: 0.02644840069115162, Validation Loss: 0.026595352217555046\n",
      "Epoch 9, Batch: 22: Training Loss: 0.03392902389168739, Validation Loss: 0.029172983020544052\n",
      "Epoch 9, Batch: 23: Training Loss: 0.027513088658452034, Validation Loss: 0.02874288335442543\n",
      "Epoch 9, Batch: 24: Training Loss: 0.028028041124343872, Validation Loss: 0.02857540361583233\n",
      "Epoch 9, Batch: 25: Training Loss: 0.023907745257019997, Validation Loss: 0.02698654867708683\n",
      "Epoch 9, Batch: 26: Training Loss: 0.027116622775793076, Validation Loss: 0.03043859824538231\n",
      "Epoch 9, Batch: 27: Training Loss: 0.026912720873951912, Validation Loss: 0.02578454464673996\n",
      "Epoch 9, Batch: 28: Training Loss: 0.03068619780242443, Validation Loss: 0.027943721041083336\n",
      "Epoch 9, Batch: 29: Training Loss: 0.028025804087519646, Validation Loss: 0.02697029896080494\n",
      "Epoch 9, Batch: 30: Training Loss: 0.026065820828080177, Validation Loss: 0.02651257812976837\n",
      "Epoch 9, Batch: 31: Training Loss: 0.03447145223617554, Validation Loss: 0.02466502971947193\n",
      "Epoch 9, Batch: 32: Training Loss: 0.0265726987272501, Validation Loss: 0.027291184291243553\n",
      "Epoch 9, Batch: 33: Training Loss: 0.02607940137386322, Validation Loss: 0.029722386971116066\n",
      "Epoch 9, Batch: 34: Training Loss: 0.023175984621047974, Validation Loss: 0.028072530403733253\n",
      "Epoch 9, Batch: 35: Training Loss: 0.0250080656260252, Validation Loss: 0.0255816001445055\n",
      "Epoch 9, Batch: 36: Training Loss: 0.022969486191868782, Validation Loss: 0.02513873390853405\n",
      "Epoch 9, Batch: 37: Training Loss: 0.023013655096292496, Validation Loss: 0.02694983221590519\n",
      "Epoch 9, Batch: 38: Training Loss: 0.030025973916053772, Validation Loss: 0.025674857199192047\n",
      "Epoch 9, Batch: 39: Training Loss: 0.027687259018421173, Validation Loss: 0.024321889504790306\n",
      "Epoch 9, Batch: 40: Training Loss: 0.02881297655403614, Validation Loss: 0.028193427249789238\n",
      "Epoch 9, Batch: 41: Training Loss: 0.026241255924105644, Validation Loss: 0.02738569676876068\n",
      "Epoch 9, Batch: 42: Training Loss: 0.024760400876402855, Validation Loss: 0.026654517278075218\n",
      "Epoch 9, Batch: 43: Training Loss: 0.02583192102611065, Validation Loss: 0.028551360592246056\n",
      "Epoch 9, Batch: 44: Training Loss: 0.029450224712491035, Validation Loss: 0.02662881836295128\n",
      "Epoch 9, Batch: 45: Training Loss: 0.024235138669610023, Validation Loss: 0.026425456628203392\n",
      "Epoch 9, Batch: 46: Training Loss: 0.028009524568915367, Validation Loss: 0.025131836533546448\n",
      "Epoch 9, Batch: 47: Training Loss: 0.027434734627604485, Validation Loss: 0.026832181960344315\n",
      "Epoch 9, Batch: 48: Training Loss: 0.031918756663799286, Validation Loss: 0.026926320046186447\n",
      "Epoch 9, Batch: 49: Training Loss: 0.028452811762690544, Validation Loss: 0.02659616246819496\n",
      "Epoch 9, Batch: 50: Training Loss: 0.02361256629228592, Validation Loss: 0.025318138301372528\n",
      "Epoch 9, Batch: 51: Training Loss: 0.025825224816799164, Validation Loss: 0.026602478697896004\n",
      "Epoch 9, Batch: 52: Training Loss: 0.028814340010285378, Validation Loss: 0.029747921973466873\n",
      "Epoch 9, Batch: 53: Training Loss: 0.028235919773578644, Validation Loss: 0.024994147941470146\n",
      "Epoch 9, Batch: 54: Training Loss: 0.02906109392642975, Validation Loss: 0.027205366641283035\n",
      "Epoch 9, Batch: 55: Training Loss: 0.02943793497979641, Validation Loss: 0.02608555555343628\n",
      "Epoch 9, Batch: 56: Training Loss: 0.031040258705615997, Validation Loss: 0.027019420638680458\n",
      "Epoch 9, Batch: 57: Training Loss: 0.02592754177749157, Validation Loss: 0.025975484400987625\n",
      "Epoch 9, Batch: 58: Training Loss: 0.03069828636944294, Validation Loss: 0.024995533749461174\n",
      "Epoch 9, Batch: 59: Training Loss: 0.026379019021987915, Validation Loss: 0.02552553080022335\n",
      "Epoch 9, Batch: 60: Training Loss: 0.025885963812470436, Validation Loss: 0.024597473442554474\n",
      "Epoch 9, Batch: 61: Training Loss: 0.029973763972520828, Validation Loss: 0.028240162879228592\n",
      "Epoch 9, Batch: 62: Training Loss: 0.02699674293398857, Validation Loss: 0.02451145462691784\n",
      "Epoch 9, Batch: 63: Training Loss: 0.02842114306986332, Validation Loss: 0.025071749463677406\n",
      "Epoch 9, Batch: 64: Training Loss: 0.024983687326312065, Validation Loss: 0.025526797398924828\n",
      "Epoch 9, Batch: 65: Training Loss: 0.029661020264029503, Validation Loss: 0.026471024379134178\n",
      "Epoch 9, Batch: 66: Training Loss: 0.026764558628201485, Validation Loss: 0.027051951736211777\n",
      "Epoch 9, Batch: 67: Training Loss: 0.022645696997642517, Validation Loss: 0.023285720497369766\n",
      "Epoch 9, Batch: 68: Training Loss: 0.028136592358350754, Validation Loss: 0.025982888415455818\n",
      "Epoch 9, Batch: 69: Training Loss: 0.02708474174141884, Validation Loss: 0.026385333389043808\n",
      "Epoch 9, Batch: 70: Training Loss: 0.02745949849486351, Validation Loss: 0.026255296543240547\n",
      "Epoch 9, Batch: 71: Training Loss: 0.025773312896490097, Validation Loss: 0.0265812948346138\n",
      "Epoch 9, Batch: 72: Training Loss: 0.026355894282460213, Validation Loss: 0.026180576533079147\n",
      "Epoch 9, Batch: 73: Training Loss: 0.026107637211680412, Validation Loss: 0.022963209077715874\n",
      "Epoch 9, Batch: 74: Training Loss: 0.02537611499428749, Validation Loss: 0.026066979393363\n",
      "Epoch 9, Batch: 75: Training Loss: 0.0220758356153965, Validation Loss: 0.02339525707066059\n",
      "Epoch 9, Batch: 76: Training Loss: 0.025329962372779846, Validation Loss: 0.02744387835264206\n",
      "Epoch 9, Batch: 77: Training Loss: 0.031007103621959686, Validation Loss: 0.023883704096078873\n",
      "Epoch 9, Batch: 78: Training Loss: 0.027488097548484802, Validation Loss: 0.025335626676678658\n",
      "Epoch 9, Batch: 79: Training Loss: 0.023587100207805634, Validation Loss: 0.027103979140520096\n",
      "Epoch 9, Batch: 80: Training Loss: 0.02424987033009529, Validation Loss: 0.02482251077890396\n",
      "Epoch 9, Batch: 81: Training Loss: 0.02762838639318943, Validation Loss: 0.02537637948989868\n",
      "Epoch 9, Batch: 82: Training Loss: 0.027268260717391968, Validation Loss: 0.024090120568871498\n",
      "Epoch 9, Batch: 83: Training Loss: 0.026460153982043266, Validation Loss: 0.025590796023607254\n",
      "Epoch 9, Batch: 84: Training Loss: 0.024093613028526306, Validation Loss: 0.02298726700246334\n",
      "Epoch 9, Batch: 85: Training Loss: 0.02421010099351406, Validation Loss: 0.02362586185336113\n",
      "Epoch 9, Batch: 86: Training Loss: 0.026624973863363266, Validation Loss: 0.02507413737475872\n",
      "Epoch 9, Batch: 87: Training Loss: 0.028383217751979828, Validation Loss: 0.02410096488893032\n",
      "Epoch 9, Batch: 88: Training Loss: 0.027174843475222588, Validation Loss: 0.025424180552363396\n",
      "Epoch 9, Batch: 89: Training Loss: 0.03170621022582054, Validation Loss: 0.02547594904899597\n",
      "Epoch 9, Batch: 90: Training Loss: 0.027516694739460945, Validation Loss: 0.02453584223985672\n",
      "Epoch 9, Batch: 91: Training Loss: 0.029614806175231934, Validation Loss: 0.02497812919318676\n",
      "Epoch 9, Batch: 92: Training Loss: 0.03153888136148453, Validation Loss: 0.02512720227241516\n",
      "Epoch 9, Batch: 93: Training Loss: 0.027194073423743248, Validation Loss: 0.02464686892926693\n",
      "Epoch 9, Batch: 94: Training Loss: 0.027771897614002228, Validation Loss: 0.027056096121668816\n",
      "Epoch 9, Batch: 95: Training Loss: 0.026027310639619827, Validation Loss: 0.028252797201275826\n",
      "Epoch 9, Batch: 96: Training Loss: 0.02965843863785267, Validation Loss: 0.02536308392882347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 97: Training Loss: 0.02851450815796852, Validation Loss: 0.02566014602780342\n",
      "Epoch 9, Batch: 98: Training Loss: 0.02709384821355343, Validation Loss: 0.026228751987218857\n",
      "Epoch 9, Batch: 99: Training Loss: 0.027198348194360733, Validation Loss: 0.027971386909484863\n",
      "Epoch 9, Batch: 100: Training Loss: 0.026667026802897453, Validation Loss: 0.026828505098819733\n",
      "Epoch 9, Batch: 101: Training Loss: 0.025888800621032715, Validation Loss: 0.026531724259257317\n",
      "Epoch 9, Batch: 102: Training Loss: 0.024296078830957413, Validation Loss: 0.024690208956599236\n",
      "Epoch 9, Batch: 103: Training Loss: 0.029320036992430687, Validation Loss: 0.02584073878824711\n",
      "Epoch 9, Batch: 104: Training Loss: 0.027168773114681244, Validation Loss: 0.025256818160414696\n",
      "Epoch 9, Batch: 105: Training Loss: 0.02988811396062374, Validation Loss: 0.025388672947883606\n",
      "Epoch 9, Batch: 106: Training Loss: 0.02991161122918129, Validation Loss: 0.02596159651875496\n",
      "Epoch 9, Batch: 107: Training Loss: 0.03161657229065895, Validation Loss: 0.025952821597456932\n",
      "Epoch 9, Batch: 108: Training Loss: 0.026486555114388466, Validation Loss: 0.026923447847366333\n",
      "Epoch 9, Batch: 109: Training Loss: 0.029151277616620064, Validation Loss: 0.02518324926495552\n",
      "Epoch 9, Batch: 110: Training Loss: 0.03169231489300728, Validation Loss: 0.024379853159189224\n",
      "Epoch 9, Batch: 111: Training Loss: 0.025568772107362747, Validation Loss: 0.02675238624215126\n",
      "Epoch 9, Batch: 112: Training Loss: 0.0238498542457819, Validation Loss: 0.026260804384946823\n",
      "Epoch 9, Batch: 113: Training Loss: 0.026776336133480072, Validation Loss: 0.029679536819458008\n",
      "Epoch 9, Batch: 114: Training Loss: 0.023541593924164772, Validation Loss: 0.028593888506293297\n",
      "Epoch 9, Batch: 115: Training Loss: 0.03148332238197327, Validation Loss: 0.026798730716109276\n",
      "Epoch 9, Batch: 116: Training Loss: 0.026019848883152008, Validation Loss: 0.027194112539291382\n",
      "Epoch 9, Batch: 117: Training Loss: 0.029705816879868507, Validation Loss: 0.025936201214790344\n",
      "Epoch 9, Batch: 118: Training Loss: 0.0269852913916111, Validation Loss: 0.026102647185325623\n",
      "Epoch 9, Batch: 119: Training Loss: 0.028123747557401657, Validation Loss: 0.026707861572504044\n",
      "Epoch 9, Batch: 120: Training Loss: 0.024987950921058655, Validation Loss: 0.024427860975265503\n",
      "Epoch 9, Batch: 121: Training Loss: 0.029901891946792603, Validation Loss: 0.024052007123827934\n",
      "Epoch 9, Batch: 122: Training Loss: 0.025364702567458153, Validation Loss: 0.024847285822033882\n",
      "Epoch 9, Batch: 123: Training Loss: 0.023956093937158585, Validation Loss: 0.027290038764476776\n",
      "Epoch 9, Batch: 124: Training Loss: 0.025491854175925255, Validation Loss: 0.027662081643939018\n",
      "Epoch 9, Batch: 125: Training Loss: 0.022542623803019524, Validation Loss: 0.026076149195432663\n",
      "Epoch 9, Batch: 126: Training Loss: 0.024560224264860153, Validation Loss: 0.025900427252054214\n",
      "Epoch 9, Batch: 127: Training Loss: 0.026471277698874474, Validation Loss: 0.02631588652729988\n",
      "Epoch 9, Batch: 128: Training Loss: 0.026374762877821922, Validation Loss: 0.027608314529061317\n",
      "Epoch 9, Batch: 129: Training Loss: 0.024901878088712692, Validation Loss: 0.02607390098273754\n",
      "Epoch 9, Batch: 130: Training Loss: 0.024683654308319092, Validation Loss: 0.0288121085613966\n",
      "Epoch 9, Batch: 131: Training Loss: 0.0260165985673666, Validation Loss: 0.026204366236925125\n",
      "Epoch 9, Batch: 132: Training Loss: 0.03073897212743759, Validation Loss: 0.0243280827999115\n",
      "Epoch 9, Batch: 133: Training Loss: 0.028389548882842064, Validation Loss: 0.025909578427672386\n",
      "Epoch 9, Batch: 134: Training Loss: 0.026943860575556755, Validation Loss: 0.02726822718977928\n",
      "Epoch 9, Batch: 135: Training Loss: 0.027276495471596718, Validation Loss: 0.02545037865638733\n",
      "Epoch 9, Batch: 136: Training Loss: 0.02424538880586624, Validation Loss: 0.02566841058433056\n",
      "Epoch 9, Batch: 137: Training Loss: 0.02555524744093418, Validation Loss: 0.0255846306681633\n",
      "Epoch 9, Batch: 138: Training Loss: 0.025339189916849136, Validation Loss: 0.02602347731590271\n",
      "Epoch 9, Batch: 139: Training Loss: 0.02333230711519718, Validation Loss: 0.025199078023433685\n",
      "Epoch 9, Batch: 140: Training Loss: 0.029025889933109283, Validation Loss: 0.027354400604963303\n",
      "Epoch 9, Batch: 141: Training Loss: 0.027448805049061775, Validation Loss: 0.027192840352654457\n",
      "Epoch 9, Batch: 142: Training Loss: 0.023411037400364876, Validation Loss: 0.02563289739191532\n",
      "Epoch 9, Batch: 143: Training Loss: 0.02606741338968277, Validation Loss: 0.026476219296455383\n",
      "Epoch 9, Batch: 144: Training Loss: 0.027951523661613464, Validation Loss: 0.029441002756357193\n",
      "Epoch 9, Batch: 145: Training Loss: 0.027143042534589767, Validation Loss: 0.027317209169268608\n",
      "Epoch 9, Batch: 146: Training Loss: 0.029009027406573296, Validation Loss: 0.029017193242907524\n",
      "Epoch 9, Batch: 147: Training Loss: 0.026657994836568832, Validation Loss: 0.03011038526892662\n",
      "Epoch 9, Batch: 148: Training Loss: 0.028661420568823814, Validation Loss: 0.03029172495007515\n",
      "Epoch 9, Batch: 149: Training Loss: 0.026415593922138214, Validation Loss: 0.028548531234264374\n",
      "Epoch 9, Batch: 150: Training Loss: 0.03202858939766884, Validation Loss: 0.02982470393180847\n",
      "Epoch 9, Batch: 151: Training Loss: 0.029965084046125412, Validation Loss: 0.028417861089110374\n",
      "Epoch 9, Batch: 152: Training Loss: 0.03145979344844818, Validation Loss: 0.030944572761654854\n",
      "Epoch 9, Batch: 153: Training Loss: 0.03178258240222931, Validation Loss: 0.02891348861157894\n",
      "Epoch 9, Batch: 154: Training Loss: 0.026089515537023544, Validation Loss: 0.02759743109345436\n",
      "Epoch 9, Batch: 155: Training Loss: 0.030732562765479088, Validation Loss: 0.028872806578874588\n",
      "Epoch 9, Batch: 156: Training Loss: 0.02671217732131481, Validation Loss: 0.02827886864542961\n",
      "Epoch 9, Batch: 157: Training Loss: 0.023227037861943245, Validation Loss: 0.02609136700630188\n",
      "Epoch 9, Batch: 158: Training Loss: 0.029207875952124596, Validation Loss: 0.029391003772616386\n",
      "Epoch 9, Batch: 159: Training Loss: 0.027248045429587364, Validation Loss: 0.026394367218017578\n",
      "Epoch 9, Batch: 160: Training Loss: 0.02464006096124649, Validation Loss: 0.0270635187625885\n",
      "Epoch 9, Batch: 161: Training Loss: 0.027950948104262352, Validation Loss: 0.030010048300027847\n",
      "Epoch 9, Batch: 162: Training Loss: 0.03277492895722389, Validation Loss: 0.0298333540558815\n",
      "Epoch 9, Batch: 163: Training Loss: 0.029116136953234673, Validation Loss: 0.029260536655783653\n",
      "Epoch 9, Batch: 164: Training Loss: 0.027347147464752197, Validation Loss: 0.028013858944177628\n",
      "Epoch 9, Batch: 165: Training Loss: 0.02716759778559208, Validation Loss: 0.02593155764043331\n",
      "Epoch 9, Batch: 166: Training Loss: 0.02537778578698635, Validation Loss: 0.029732471331954002\n",
      "Epoch 9, Batch: 167: Training Loss: 0.025092236697673798, Validation Loss: 0.02936660498380661\n",
      "Epoch 9, Batch: 168: Training Loss: 0.026170609518885612, Validation Loss: 0.02567438967525959\n",
      "Epoch 9, Batch: 169: Training Loss: 0.030555620789527893, Validation Loss: 0.025600984692573547\n",
      "Epoch 9, Batch: 170: Training Loss: 0.027291912585496902, Validation Loss: 0.03020072914659977\n",
      "Epoch 9, Batch: 171: Training Loss: 0.02589496597647667, Validation Loss: 0.028999660164117813\n",
      "Epoch 9, Batch: 172: Training Loss: 0.026163283735513687, Validation Loss: 0.027165908366441727\n",
      "Epoch 9, Batch: 173: Training Loss: 0.028920205309987068, Validation Loss: 0.0267921332269907\n",
      "Epoch 9, Batch: 174: Training Loss: 0.027084555476903915, Validation Loss: 0.02718372456729412\n",
      "Epoch 9, Batch: 175: Training Loss: 0.02685604803264141, Validation Loss: 0.028439508751034737\n",
      "Epoch 9, Batch: 176: Training Loss: 0.0337245836853981, Validation Loss: 0.027138827368617058\n",
      "Epoch 9, Batch: 177: Training Loss: 0.026219980791211128, Validation Loss: 0.028462640941143036\n",
      "Epoch 9, Batch: 178: Training Loss: 0.029739484190940857, Validation Loss: 0.027866067364811897\n",
      "Epoch 9, Batch: 179: Training Loss: 0.025002866983413696, Validation Loss: 0.02941250056028366\n",
      "Epoch 9, Batch: 180: Training Loss: 0.027762245386838913, Validation Loss: 0.027456609532237053\n",
      "Epoch 9, Batch: 181: Training Loss: 0.02539333887398243, Validation Loss: 0.028457755222916603\n",
      "Epoch 9, Batch: 182: Training Loss: 0.02726190723478794, Validation Loss: 0.028269479051232338\n",
      "Epoch 9, Batch: 183: Training Loss: 0.030679889023303986, Validation Loss: 0.02722986415028572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 184: Training Loss: 0.02617737650871277, Validation Loss: 0.027948373928666115\n",
      "Epoch 9, Batch: 185: Training Loss: 0.03017754666507244, Validation Loss: 0.029965810477733612\n",
      "Epoch 9, Batch: 186: Training Loss: 0.0269822645932436, Validation Loss: 0.027905263006687164\n",
      "Epoch 9, Batch: 187: Training Loss: 0.024076703935861588, Validation Loss: 0.0279034785926342\n",
      "Epoch 9, Batch: 188: Training Loss: 0.028375115245580673, Validation Loss: 0.029880022630095482\n",
      "Epoch 9, Batch: 189: Training Loss: 0.02733631059527397, Validation Loss: 0.029211919754743576\n",
      "Epoch 9, Batch: 190: Training Loss: 0.023840470239520073, Validation Loss: 0.027407070621848106\n",
      "Epoch 9, Batch: 191: Training Loss: 0.027559811249375343, Validation Loss: 0.026422366499900818\n",
      "Epoch 9, Batch: 192: Training Loss: 0.026578182354569435, Validation Loss: 0.027999192476272583\n",
      "Epoch 9, Batch: 193: Training Loss: 0.029503537341952324, Validation Loss: 0.02859613485634327\n",
      "Epoch 9, Batch: 194: Training Loss: 0.02840971015393734, Validation Loss: 0.02728605642914772\n",
      "Epoch 9, Batch: 195: Training Loss: 0.02693079598248005, Validation Loss: 0.027939505875110626\n",
      "Epoch 9, Batch: 196: Training Loss: 0.02649727463722229, Validation Loss: 0.0276047233492136\n",
      "Epoch 9, Batch: 197: Training Loss: 0.02670714072883129, Validation Loss: 0.025947315618395805\n",
      "Epoch 9, Batch: 198: Training Loss: 0.025093821808695793, Validation Loss: 0.025420265272259712\n",
      "Epoch 9, Batch: 199: Training Loss: 0.02449817955493927, Validation Loss: 0.027358174324035645\n",
      "Epoch 9, Batch: 200: Training Loss: 0.024677440524101257, Validation Loss: 0.027506887912750244\n",
      "Epoch 9, Batch: 201: Training Loss: 0.030040334910154343, Validation Loss: 0.03005243092775345\n",
      "Epoch 9, Batch: 202: Training Loss: 0.025903573259711266, Validation Loss: 0.027712790295481682\n",
      "Epoch 9, Batch: 203: Training Loss: 0.030648496001958847, Validation Loss: 0.02552017755806446\n",
      "Epoch 9, Batch: 204: Training Loss: 0.02802649512887001, Validation Loss: 0.02622389607131481\n",
      "Epoch 9, Batch: 205: Training Loss: 0.027861326932907104, Validation Loss: 0.02902737818658352\n",
      "Epoch 9, Batch: 206: Training Loss: 0.025286754593253136, Validation Loss: 0.027296124026179314\n",
      "Epoch 9, Batch: 207: Training Loss: 0.02847902476787567, Validation Loss: 0.026855330914258957\n",
      "Epoch 9, Batch: 208: Training Loss: 0.025202840566635132, Validation Loss: 0.026240767911076546\n",
      "Epoch 9, Batch: 209: Training Loss: 0.025192314758896828, Validation Loss: 0.027375582605600357\n",
      "Epoch 9, Batch: 210: Training Loss: 0.02714245766401291, Validation Loss: 0.027846699580550194\n",
      "Epoch 9, Batch: 211: Training Loss: 0.02989133819937706, Validation Loss: 0.024899886921048164\n",
      "Epoch 9, Batch: 212: Training Loss: 0.025551768019795418, Validation Loss: 0.026813693344593048\n",
      "Epoch 9, Batch: 213: Training Loss: 0.029914485290646553, Validation Loss: 0.026487842202186584\n",
      "Epoch 9, Batch: 214: Training Loss: 0.025692325085401535, Validation Loss: 0.02795768715441227\n",
      "Epoch 9, Batch: 215: Training Loss: 0.025993268936872482, Validation Loss: 0.0257327388972044\n",
      "Epoch 9, Batch: 216: Training Loss: 0.023999424651265144, Validation Loss: 0.027667224407196045\n",
      "Epoch 9, Batch: 217: Training Loss: 0.027721667662262917, Validation Loss: 0.025823405012488365\n",
      "Epoch 9, Batch: 218: Training Loss: 0.025928113609552383, Validation Loss: 0.02699245512485504\n",
      "Epoch 9, Batch: 219: Training Loss: 0.026611503213644028, Validation Loss: 0.02584953047335148\n",
      "Epoch 9, Batch: 220: Training Loss: 0.030080953612923622, Validation Loss: 0.024815082550048828\n",
      "Epoch 9, Batch: 221: Training Loss: 0.025996530428528786, Validation Loss: 0.02549493871629238\n",
      "Epoch 9, Batch: 222: Training Loss: 0.030260540544986725, Validation Loss: 0.025142226368188858\n",
      "Epoch 9, Batch: 223: Training Loss: 0.02450447902083397, Validation Loss: 0.024402135983109474\n",
      "Epoch 9, Batch: 224: Training Loss: 0.02908286266028881, Validation Loss: 0.02550540678203106\n",
      "Epoch 9, Batch: 225: Training Loss: 0.025696994736790657, Validation Loss: 0.025776982307434082\n",
      "Epoch 9, Batch: 226: Training Loss: 0.028663376346230507, Validation Loss: 0.023464228957891464\n",
      "Epoch 9, Batch: 227: Training Loss: 0.024329323321580887, Validation Loss: 0.024906925857067108\n",
      "Epoch 9, Batch: 228: Training Loss: 0.025023723021149635, Validation Loss: 0.02776745706796646\n",
      "Epoch 9, Batch: 229: Training Loss: 0.027188606560230255, Validation Loss: 0.025973709300160408\n",
      "Epoch 9, Batch: 230: Training Loss: 0.02424214594066143, Validation Loss: 0.02394205890595913\n",
      "Epoch 9, Batch: 231: Training Loss: 0.029577190056443214, Validation Loss: 0.025980396196246147\n",
      "Epoch 9, Batch: 232: Training Loss: 0.025405902415513992, Validation Loss: 0.025340726599097252\n",
      "Epoch 9, Batch: 233: Training Loss: 0.02888311631977558, Validation Loss: 0.027567856013774872\n",
      "Epoch 9, Batch: 234: Training Loss: 0.030584566295146942, Validation Loss: 0.02606433629989624\n",
      "Epoch 9, Batch: 235: Training Loss: 0.031009405851364136, Validation Loss: 0.02852807380259037\n",
      "Epoch 9, Batch: 236: Training Loss: 0.034102216362953186, Validation Loss: 0.02711939811706543\n",
      "Epoch 9, Batch: 237: Training Loss: 0.02702849544584751, Validation Loss: 0.025456713512539864\n",
      "Epoch 9, Batch: 238: Training Loss: 0.0262004341930151, Validation Loss: 0.024850429967045784\n",
      "Epoch 9, Batch: 239: Training Loss: 0.02641228400170803, Validation Loss: 0.026852363720536232\n",
      "Epoch 9, Batch: 240: Training Loss: 0.023847797885537148, Validation Loss: 0.03024921752512455\n",
      "Epoch 9, Batch: 241: Training Loss: 0.02403353527188301, Validation Loss: 0.02650618553161621\n",
      "Epoch 9, Batch: 242: Training Loss: 0.025980640202760696, Validation Loss: 0.02712518908083439\n",
      "Epoch 9, Batch: 243: Training Loss: 0.028415504842996597, Validation Loss: 0.025901995599269867\n",
      "Epoch 9, Batch: 244: Training Loss: 0.02808745577931404, Validation Loss: 0.02436128444969654\n",
      "Epoch 9, Batch: 245: Training Loss: 0.0296927522867918, Validation Loss: 0.024651015177369118\n",
      "Epoch 9, Batch: 246: Training Loss: 0.02364765666425228, Validation Loss: 0.02625587210059166\n",
      "Epoch 9, Batch: 247: Training Loss: 0.02917977049946785, Validation Loss: 0.02524135448038578\n",
      "Epoch 9, Batch: 248: Training Loss: 0.02651362121105194, Validation Loss: 0.025926943868398666\n",
      "Epoch 9, Batch: 249: Training Loss: 0.02419239655137062, Validation Loss: 0.02812666818499565\n",
      "Epoch 9, Batch: 250: Training Loss: 0.029782956466078758, Validation Loss: 0.026301667094230652\n",
      "Epoch 9, Batch: 251: Training Loss: 0.021626152098178864, Validation Loss: 0.024716585874557495\n",
      "Epoch 9, Batch: 252: Training Loss: 0.026970041915774345, Validation Loss: 0.027946101501584053\n",
      "Epoch 9, Batch: 253: Training Loss: 0.02844989486038685, Validation Loss: 0.027704497799277306\n",
      "Epoch 9, Batch: 254: Training Loss: 0.026191135868430138, Validation Loss: 0.026747846975922585\n",
      "Epoch 9, Batch: 255: Training Loss: 0.025178322568535805, Validation Loss: 0.028864923864603043\n",
      "Epoch 9, Batch: 256: Training Loss: 0.027544934302568436, Validation Loss: 0.027493350207805634\n",
      "Epoch 9, Batch: 257: Training Loss: 0.030187878757715225, Validation Loss: 0.029631808400154114\n",
      "Epoch 9, Batch: 258: Training Loss: 0.03077150322496891, Validation Loss: 0.02880982682108879\n",
      "Epoch 9, Batch: 259: Training Loss: 0.030751381069421768, Validation Loss: 0.028111882507801056\n",
      "Epoch 9, Batch: 260: Training Loss: 0.031019283458590508, Validation Loss: 0.030193805694580078\n",
      "Epoch 9, Batch: 261: Training Loss: 0.026462219655513763, Validation Loss: 0.027021469548344612\n",
      "Epoch 9, Batch: 262: Training Loss: 0.024367714300751686, Validation Loss: 0.026128273457288742\n",
      "Epoch 9, Batch: 263: Training Loss: 0.027817079797387123, Validation Loss: 0.028056083247065544\n",
      "Epoch 9, Batch: 264: Training Loss: 0.02236103266477585, Validation Loss: 0.029152899980545044\n",
      "Epoch 9, Batch: 265: Training Loss: 0.027637669816613197, Validation Loss: 0.028473127633333206\n",
      "Epoch 9, Batch: 266: Training Loss: 0.02612835355103016, Validation Loss: 0.026869162917137146\n",
      "Epoch 9, Batch: 267: Training Loss: 0.02454705350100994, Validation Loss: 0.02535385452210903\n",
      "Epoch 9, Batch: 268: Training Loss: 0.0259842611849308, Validation Loss: 0.025918180122971535\n",
      "Epoch 9, Batch: 269: Training Loss: 0.027431253343820572, Validation Loss: 0.028804881498217583\n",
      "Epoch 9, Batch: 270: Training Loss: 0.025823356583714485, Validation Loss: 0.025817902758717537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 271: Training Loss: 0.024001311510801315, Validation Loss: 0.025535304099321365\n",
      "Epoch 9, Batch: 272: Training Loss: 0.027376705780625343, Validation Loss: 0.028096169233322144\n",
      "Epoch 9, Batch: 273: Training Loss: 0.026452405378222466, Validation Loss: 0.02580222487449646\n",
      "Epoch 9, Batch: 274: Training Loss: 0.027177514508366585, Validation Loss: 0.02810675837099552\n",
      "Epoch 9, Batch: 275: Training Loss: 0.027721626684069633, Validation Loss: 0.028055008500814438\n",
      "Epoch 9, Batch: 276: Training Loss: 0.026200173422694206, Validation Loss: 0.027138983830809593\n",
      "Epoch 9, Batch: 277: Training Loss: 0.0208459310233593, Validation Loss: 0.027936501428484917\n",
      "Epoch 9, Batch: 278: Training Loss: 0.024178672581911087, Validation Loss: 0.026390597224235535\n",
      "Epoch 9, Batch: 279: Training Loss: 0.027306195348501205, Validation Loss: 0.02441505156457424\n",
      "Epoch 9, Batch: 280: Training Loss: 0.0268404483795166, Validation Loss: 0.027864156290888786\n",
      "Epoch 9, Batch: 281: Training Loss: 0.025909120216965675, Validation Loss: 0.02862834557890892\n",
      "Epoch 9, Batch: 282: Training Loss: 0.02469644881784916, Validation Loss: 0.029226534068584442\n",
      "Epoch 9, Batch: 283: Training Loss: 0.026507316157221794, Validation Loss: 0.027791578322649002\n",
      "Epoch 9, Batch: 284: Training Loss: 0.02572079747915268, Validation Loss: 0.028548626229166985\n",
      "Epoch 9, Batch: 285: Training Loss: 0.028194693848490715, Validation Loss: 0.02778153494000435\n",
      "Epoch 9, Batch: 286: Training Loss: 0.024701077491044998, Validation Loss: 0.029838135465979576\n",
      "Epoch 9, Batch: 287: Training Loss: 0.027247775346040726, Validation Loss: 0.026726938784122467\n",
      "Epoch 9, Batch: 288: Training Loss: 0.02549138106405735, Validation Loss: 0.027366597205400467\n",
      "Epoch 9, Batch: 289: Training Loss: 0.029597481712698936, Validation Loss: 0.029319114983081818\n",
      "Epoch 9, Batch: 290: Training Loss: 0.025455081835389137, Validation Loss: 0.026734303683042526\n",
      "Epoch 9, Batch: 291: Training Loss: 0.02692917175590992, Validation Loss: 0.025525199249386787\n",
      "Epoch 9, Batch: 292: Training Loss: 0.027704233303666115, Validation Loss: 0.028022538870573044\n",
      "Epoch 9, Batch: 293: Training Loss: 0.02819148637354374, Validation Loss: 0.026903444901108742\n",
      "Epoch 9, Batch: 294: Training Loss: 0.032717444002628326, Validation Loss: 0.028155716136097908\n",
      "Epoch 9, Batch: 295: Training Loss: 0.029554380103945732, Validation Loss: 0.024859070777893066\n",
      "Epoch 9, Batch: 296: Training Loss: 0.023950885981321335, Validation Loss: 0.02638159692287445\n",
      "Epoch 9, Batch: 297: Training Loss: 0.028838226571679115, Validation Loss: 0.02574823796749115\n",
      "Epoch 9, Batch: 298: Training Loss: 0.025690602138638496, Validation Loss: 0.024691814556717873\n",
      "Epoch 9, Batch: 299: Training Loss: 0.022855548188090324, Validation Loss: 0.024705899879336357\n",
      "Epoch 9, Batch: 300: Training Loss: 0.025575049221515656, Validation Loss: 0.025251442566514015\n",
      "Epoch 9, Batch: 301: Training Loss: 0.026420259848237038, Validation Loss: 0.02338978461921215\n",
      "Epoch 9, Batch: 302: Training Loss: 0.02543911710381508, Validation Loss: 0.02507944218814373\n",
      "Epoch 9, Batch: 303: Training Loss: 0.025893129408359528, Validation Loss: 0.026031063869595528\n",
      "Epoch 9, Batch: 304: Training Loss: 0.029976483434438705, Validation Loss: 0.026065072044730186\n",
      "Epoch 9, Batch: 305: Training Loss: 0.020148787647485733, Validation Loss: 0.02659117430448532\n",
      "Epoch 9, Batch: 306: Training Loss: 0.02783512882888317, Validation Loss: 0.025622287765145302\n",
      "Epoch 9, Batch: 307: Training Loss: 0.024139463901519775, Validation Loss: 0.02518054097890854\n",
      "Epoch 9, Batch: 308: Training Loss: 0.02683364413678646, Validation Loss: 0.026006219908595085\n",
      "Epoch 9, Batch: 309: Training Loss: 0.02443627640604973, Validation Loss: 0.024451015517115593\n",
      "Epoch 9, Batch: 310: Training Loss: 0.02607826702296734, Validation Loss: 0.023422397673130035\n",
      "Epoch 9, Batch: 311: Training Loss: 0.023617519065737724, Validation Loss: 0.025510499253869057\n",
      "Epoch 9, Batch: 312: Training Loss: 0.026594573631882668, Validation Loss: 0.026215752586722374\n",
      "Epoch 9, Batch: 313: Training Loss: 0.02389928326010704, Validation Loss: 0.026284901425242424\n",
      "Epoch 9, Batch: 314: Training Loss: 0.02658132277429104, Validation Loss: 0.026409273967146873\n",
      "Epoch 9, Batch: 315: Training Loss: 0.02719780243933201, Validation Loss: 0.026486720889806747\n",
      "Epoch 9, Batch: 316: Training Loss: 0.022862695157527924, Validation Loss: 0.02588569186627865\n",
      "Epoch 9, Batch: 317: Training Loss: 0.02490176260471344, Validation Loss: 0.026578253135085106\n",
      "Epoch 9, Batch: 318: Training Loss: 0.027406791225075722, Validation Loss: 0.023855578154325485\n",
      "Epoch 9, Batch: 319: Training Loss: 0.026868458837270737, Validation Loss: 0.025588804855942726\n",
      "Epoch 9, Batch: 320: Training Loss: 0.026325322687625885, Validation Loss: 0.025170130655169487\n",
      "Epoch 9, Batch: 321: Training Loss: 0.025147777050733566, Validation Loss: 0.027041206136345863\n",
      "Epoch 9, Batch: 322: Training Loss: 0.03068646602332592, Validation Loss: 0.02518513612449169\n",
      "Epoch 9, Batch: 323: Training Loss: 0.029577644541859627, Validation Loss: 0.02587912604212761\n",
      "Epoch 9, Batch: 324: Training Loss: 0.027404863387346268, Validation Loss: 0.025408213958144188\n",
      "Epoch 9, Batch: 325: Training Loss: 0.025996269658207893, Validation Loss: 0.025545096024870872\n",
      "Epoch 9, Batch: 326: Training Loss: 0.02696344070136547, Validation Loss: 0.02401743456721306\n",
      "Epoch 9, Batch: 327: Training Loss: 0.024879949167370796, Validation Loss: 0.023753687739372253\n",
      "Epoch 9, Batch: 328: Training Loss: 0.02879985235631466, Validation Loss: 0.02409989945590496\n",
      "Epoch 9, Batch: 329: Training Loss: 0.025480329990386963, Validation Loss: 0.02351203002035618\n",
      "Epoch 9, Batch: 330: Training Loss: 0.027829937636852264, Validation Loss: 0.023552263155579567\n",
      "Epoch 9, Batch: 331: Training Loss: 0.031079616397619247, Validation Loss: 0.025682881474494934\n",
      "Epoch 9, Batch: 332: Training Loss: 0.02721184864640236, Validation Loss: 0.02391233667731285\n",
      "Epoch 9, Batch: 333: Training Loss: 0.02624005265533924, Validation Loss: 0.025681650266051292\n",
      "Epoch 9, Batch: 334: Training Loss: 0.024949824437499046, Validation Loss: 0.026678957045078278\n",
      "Epoch 9, Batch: 335: Training Loss: 0.023211468011140823, Validation Loss: 0.028235357254743576\n",
      "Epoch 9, Batch: 336: Training Loss: 0.02561861462891102, Validation Loss: 0.026318199932575226\n",
      "Epoch 9, Batch: 337: Training Loss: 0.02470908686518669, Validation Loss: 0.02729658968746662\n",
      "Epoch 9, Batch: 338: Training Loss: 0.026801133528351784, Validation Loss: 0.027513595297932625\n",
      "Epoch 9, Batch: 339: Training Loss: 0.028714971616864204, Validation Loss: 0.027529900893568993\n",
      "Epoch 9, Batch: 340: Training Loss: 0.026924658566713333, Validation Loss: 0.02900048717856407\n",
      "Epoch 9, Batch: 341: Training Loss: 0.025372615084052086, Validation Loss: 0.02863169088959694\n",
      "Epoch 9, Batch: 342: Training Loss: 0.025219297036528587, Validation Loss: 0.029257241636514664\n",
      "Epoch 9, Batch: 343: Training Loss: 0.02368614822626114, Validation Loss: 0.027708210051059723\n",
      "Epoch 9, Batch: 344: Training Loss: 0.029019795358181, Validation Loss: 0.028434179723262787\n",
      "Epoch 9, Batch: 345: Training Loss: 0.02420458197593689, Validation Loss: 0.029318327084183693\n",
      "Epoch 9, Batch: 346: Training Loss: 0.025172846391797066, Validation Loss: 0.030069679021835327\n",
      "Epoch 9, Batch: 347: Training Loss: 0.025645069777965546, Validation Loss: 0.028664182871580124\n",
      "Epoch 9, Batch: 348: Training Loss: 0.024302521720528603, Validation Loss: 0.02891954407095909\n",
      "Epoch 9, Batch: 349: Training Loss: 0.028289269655942917, Validation Loss: 0.027470331639051437\n",
      "Epoch 9, Batch: 350: Training Loss: 0.02516212686896324, Validation Loss: 0.026872189715504646\n",
      "Epoch 9, Batch: 351: Training Loss: 0.03294266015291214, Validation Loss: 0.02640034817159176\n",
      "Epoch 9, Batch: 352: Training Loss: 0.025980902835726738, Validation Loss: 0.031075868755578995\n",
      "Epoch 9, Batch: 353: Training Loss: 0.027290502563118935, Validation Loss: 0.02802000194787979\n",
      "Epoch 9, Batch: 354: Training Loss: 0.026541559025645256, Validation Loss: 0.029613720253109932\n",
      "Epoch 9, Batch: 355: Training Loss: 0.023913776502013206, Validation Loss: 0.03152281418442726\n",
      "Epoch 9, Batch: 356: Training Loss: 0.028270216658711433, Validation Loss: 0.027167314663529396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 357: Training Loss: 0.0235290490090847, Validation Loss: 0.02999071218073368\n",
      "Epoch 9, Batch: 358: Training Loss: 0.03281403332948685, Validation Loss: 0.02782033383846283\n",
      "Epoch 9, Batch: 359: Training Loss: 0.02720818668603897, Validation Loss: 0.0299159437417984\n",
      "Epoch 9, Batch: 360: Training Loss: 0.029429003596305847, Validation Loss: 0.02782977558672428\n",
      "Epoch 9, Batch: 361: Training Loss: 0.025811070576310158, Validation Loss: 0.028789818286895752\n",
      "Epoch 9, Batch: 362: Training Loss: 0.027972090989351273, Validation Loss: 0.029623912647366524\n",
      "Epoch 9, Batch: 363: Training Loss: 0.03422698751091957, Validation Loss: 0.03224290907382965\n",
      "Epoch 9, Batch: 364: Training Loss: 0.028943922370672226, Validation Loss: 0.027598023414611816\n",
      "Epoch 9, Batch: 365: Training Loss: 0.02809622511267662, Validation Loss: 0.029965952038764954\n",
      "Epoch 9, Batch: 366: Training Loss: 0.030668573454022408, Validation Loss: 0.0295492485165596\n",
      "Epoch 9, Batch: 367: Training Loss: 0.024891650304198265, Validation Loss: 0.029560282826423645\n",
      "Epoch 9, Batch: 368: Training Loss: 0.031148338690400124, Validation Loss: 0.027148442342877388\n",
      "Epoch 9, Batch: 369: Training Loss: 0.028258634731173515, Validation Loss: 0.02728894166648388\n",
      "Epoch 9, Batch: 370: Training Loss: 0.027232017368078232, Validation Loss: 0.029706837609410286\n",
      "Epoch 9, Batch: 371: Training Loss: 0.024698767811059952, Validation Loss: 0.027933361008763313\n",
      "Epoch 9, Batch: 372: Training Loss: 0.02699309214949608, Validation Loss: 0.028676798567175865\n",
      "Epoch 9, Batch: 373: Training Loss: 0.03344462439417839, Validation Loss: 0.028400905430316925\n",
      "Epoch 9, Batch: 374: Training Loss: 0.025406943634152412, Validation Loss: 0.029892679303884506\n",
      "Epoch 9, Batch: 375: Training Loss: 0.03193395212292671, Validation Loss: 0.030293334275484085\n",
      "Epoch 9, Batch: 376: Training Loss: 0.025276558473706245, Validation Loss: 0.02988615073263645\n",
      "Epoch 9, Batch: 377: Training Loss: 0.026087550446391106, Validation Loss: 0.027846679091453552\n",
      "Epoch 9, Batch: 378: Training Loss: 0.023833787068724632, Validation Loss: 0.02774241752922535\n",
      "Epoch 9, Batch: 379: Training Loss: 0.024893928319215775, Validation Loss: 0.02489715814590454\n",
      "Epoch 9, Batch: 380: Training Loss: 0.028359102085232735, Validation Loss: 0.026598908007144928\n",
      "Epoch 9, Batch: 381: Training Loss: 0.02387387491762638, Validation Loss: 0.025941425934433937\n",
      "Epoch 9, Batch: 382: Training Loss: 0.025838324800133705, Validation Loss: 0.0268534105271101\n",
      "Epoch 9, Batch: 383: Training Loss: 0.0286493431776762, Validation Loss: 0.026320554316043854\n",
      "Epoch 9, Batch: 384: Training Loss: 0.029088791459798813, Validation Loss: 0.028283318504691124\n",
      "Epoch 9, Batch: 385: Training Loss: 0.025505078956484795, Validation Loss: 0.02778811566531658\n",
      "Epoch 9, Batch: 386: Training Loss: 0.02473808079957962, Validation Loss: 0.02786177396774292\n",
      "Epoch 9, Batch: 387: Training Loss: 0.02773127891123295, Validation Loss: 0.028529517352581024\n",
      "Epoch 9, Batch: 388: Training Loss: 0.027307886630296707, Validation Loss: 0.026993991807103157\n",
      "Epoch 9, Batch: 389: Training Loss: 0.03130380064249039, Validation Loss: 0.0264098159968853\n",
      "Epoch 9, Batch: 390: Training Loss: 0.02947777695953846, Validation Loss: 0.025690417736768723\n",
      "Epoch 9, Batch: 391: Training Loss: 0.02954721450805664, Validation Loss: 0.02421761304140091\n",
      "Epoch 9, Batch: 392: Training Loss: 0.024923672899603844, Validation Loss: 0.02429792284965515\n",
      "Epoch 9, Batch: 393: Training Loss: 0.02667991630733013, Validation Loss: 0.027197353541851044\n",
      "Epoch 9, Batch: 394: Training Loss: 0.026299968361854553, Validation Loss: 0.024129444733262062\n",
      "Epoch 9, Batch: 395: Training Loss: 0.022566651925444603, Validation Loss: 0.023974521085619926\n",
      "Epoch 9, Batch: 396: Training Loss: 0.02547820843756199, Validation Loss: 0.02417777292430401\n",
      "Epoch 9, Batch: 397: Training Loss: 0.030512480065226555, Validation Loss: 0.02503920905292034\n",
      "Epoch 9, Batch: 398: Training Loss: 0.0259631909430027, Validation Loss: 0.023512834683060646\n",
      "Epoch 9, Batch: 399: Training Loss: 0.028453094884753227, Validation Loss: 0.0240127332508564\n",
      "Epoch 9, Batch: 400: Training Loss: 0.024045884609222412, Validation Loss: 0.024378765374422073\n",
      "Epoch 9, Batch: 401: Training Loss: 0.02699468284845352, Validation Loss: 0.023842699825763702\n",
      "Epoch 9, Batch: 402: Training Loss: 0.024598687887191772, Validation Loss: 0.022634152323007584\n",
      "Epoch 9, Batch: 403: Training Loss: 0.026771511882543564, Validation Loss: 0.02624567225575447\n",
      "Epoch 9, Batch: 404: Training Loss: 0.024663085117936134, Validation Loss: 0.026519179344177246\n",
      "Epoch 9, Batch: 405: Training Loss: 0.026481227949261665, Validation Loss: 0.025590458884835243\n",
      "Epoch 9, Batch: 406: Training Loss: 0.029489735141396523, Validation Loss: 0.024167999625205994\n",
      "Epoch 9, Batch: 407: Training Loss: 0.02723102457821369, Validation Loss: 0.025685641914606094\n",
      "Epoch 9, Batch: 408: Training Loss: 0.02347327023744583, Validation Loss: 0.024663031101226807\n",
      "Epoch 9, Batch: 409: Training Loss: 0.02642998658120632, Validation Loss: 0.02478424832224846\n",
      "Epoch 9, Batch: 410: Training Loss: 0.02768595702946186, Validation Loss: 0.023868238553404808\n",
      "Epoch 9, Batch: 411: Training Loss: 0.025293895974755287, Validation Loss: 0.02365410141646862\n",
      "Epoch 9, Batch: 412: Training Loss: 0.028767110779881477, Validation Loss: 0.023266740143299103\n",
      "Epoch 9, Batch: 413: Training Loss: 0.024206696078181267, Validation Loss: 0.023967670276761055\n",
      "Epoch 9, Batch: 414: Training Loss: 0.023926034569740295, Validation Loss: 0.024145619943737984\n",
      "Epoch 9, Batch: 415: Training Loss: 0.025010840967297554, Validation Loss: 0.024995453655719757\n",
      "Epoch 9, Batch: 416: Training Loss: 0.026798918843269348, Validation Loss: 0.022529881447553635\n",
      "Epoch 9, Batch: 417: Training Loss: 0.025251347571611404, Validation Loss: 0.024197915568947792\n",
      "Epoch 9, Batch: 418: Training Loss: 0.02234533429145813, Validation Loss: 0.024077225476503372\n",
      "Epoch 9, Batch: 419: Training Loss: 0.024653369560837746, Validation Loss: 0.022602738812565804\n",
      "Epoch 9, Batch: 420: Training Loss: 0.023865826427936554, Validation Loss: 0.02491113543510437\n",
      "Epoch 9, Batch: 421: Training Loss: 0.02870294265449047, Validation Loss: 0.025795046240091324\n",
      "Epoch 9, Batch: 422: Training Loss: 0.02854747697710991, Validation Loss: 0.02639329433441162\n",
      "Epoch 9, Batch: 423: Training Loss: 0.03125941380858421, Validation Loss: 0.02458706870675087\n",
      "Epoch 9, Batch: 424: Training Loss: 0.030884813517332077, Validation Loss: 0.02415301278233528\n",
      "Epoch 9, Batch: 425: Training Loss: 0.025647515431046486, Validation Loss: 0.026007339358329773\n",
      "Epoch 9, Batch: 426: Training Loss: 0.02777588553726673, Validation Loss: 0.02520887367427349\n",
      "Epoch 9, Batch: 427: Training Loss: 0.028277011588215828, Validation Loss: 0.02574704959988594\n",
      "Epoch 9, Batch: 428: Training Loss: 0.02114792913198471, Validation Loss: 0.026012279093265533\n",
      "Epoch 9, Batch: 429: Training Loss: 0.023021163418889046, Validation Loss: 0.02662079781293869\n",
      "Epoch 9, Batch: 430: Training Loss: 0.022510400041937828, Validation Loss: 0.02682335674762726\n",
      "Epoch 9, Batch: 431: Training Loss: 0.02596627175807953, Validation Loss: 0.026797352358698845\n",
      "Epoch 9, Batch: 432: Training Loss: 0.02761005610227585, Validation Loss: 0.02710643783211708\n",
      "Epoch 9, Batch: 433: Training Loss: 0.02301323600113392, Validation Loss: 0.02548833005130291\n",
      "Epoch 9, Batch: 434: Training Loss: 0.02301853336393833, Validation Loss: 0.027068473398685455\n",
      "Epoch 9, Batch: 435: Training Loss: 0.026715895161032677, Validation Loss: 0.026224369183182716\n",
      "Epoch 9, Batch: 436: Training Loss: 0.025579869747161865, Validation Loss: 0.027351336553692818\n",
      "Epoch 9, Batch: 437: Training Loss: 0.023742662742733955, Validation Loss: 0.027916068211197853\n",
      "Epoch 9, Batch: 438: Training Loss: 0.02372930571436882, Validation Loss: 0.030485287308692932\n",
      "Epoch 9, Batch: 439: Training Loss: 0.021194573491811752, Validation Loss: 0.028064420446753502\n",
      "Epoch 9, Batch: 440: Training Loss: 0.028491374105215073, Validation Loss: 0.029494209215044975\n",
      "Epoch 9, Batch: 441: Training Loss: 0.02387911081314087, Validation Loss: 0.026856951415538788\n",
      "Epoch 9, Batch: 442: Training Loss: 0.026726992800831795, Validation Loss: 0.028483273461461067\n",
      "Epoch 9, Batch: 443: Training Loss: 0.022812331095337868, Validation Loss: 0.028083667159080505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch: 444: Training Loss: 0.022387832403182983, Validation Loss: 0.029774073511362076\n",
      "Epoch 9, Batch: 445: Training Loss: 0.022786637768149376, Validation Loss: 0.02729465253651142\n",
      "Epoch 9, Batch: 446: Training Loss: 0.03282849118113518, Validation Loss: 0.02694619819521904\n",
      "Epoch 9, Batch: 447: Training Loss: 0.02736295387148857, Validation Loss: 0.027681516483426094\n",
      "Epoch 9, Batch: 448: Training Loss: 0.027342818677425385, Validation Loss: 0.026936283335089684\n",
      "Epoch 9, Batch: 449: Training Loss: 0.024073945358395576, Validation Loss: 0.027152525261044502\n",
      "Epoch 9, Batch: 450: Training Loss: 0.02714531123638153, Validation Loss: 0.027345895767211914\n",
      "Epoch 9, Batch: 451: Training Loss: 0.028945639729499817, Validation Loss: 0.027095159515738487\n",
      "Epoch 9, Batch: 452: Training Loss: 0.029953893274068832, Validation Loss: 0.029419956728816032\n",
      "Epoch 9, Batch: 453: Training Loss: 0.025766761973500252, Validation Loss: 0.02655240148305893\n",
      "Epoch 9, Batch: 454: Training Loss: 0.02800591289997101, Validation Loss: 0.02687070146203041\n",
      "Epoch 9, Batch: 455: Training Loss: 0.01981939934194088, Validation Loss: 0.025684110820293427\n",
      "Epoch 9, Batch: 456: Training Loss: 0.02460230141878128, Validation Loss: 0.0286310613155365\n",
      "Epoch 9, Batch: 457: Training Loss: 0.028827499598264694, Validation Loss: 0.027922770008444786\n",
      "Epoch 9, Batch: 458: Training Loss: 0.022455647587776184, Validation Loss: 0.024429282173514366\n",
      "Epoch 9, Batch: 459: Training Loss: 0.027356702834367752, Validation Loss: 0.02874007634818554\n",
      "Epoch 9, Batch: 460: Training Loss: 0.0259530171751976, Validation Loss: 0.0264580175280571\n",
      "Epoch 9, Batch: 461: Training Loss: 0.022143810987472534, Validation Loss: 0.02842099778354168\n",
      "Epoch 9, Batch: 462: Training Loss: 0.02328409068286419, Validation Loss: 0.026856645941734314\n",
      "Epoch 9, Batch: 463: Training Loss: 0.028339585289359093, Validation Loss: 0.027092160657048225\n",
      "Epoch 9, Batch: 464: Training Loss: 0.02190888486802578, Validation Loss: 0.026088308542966843\n",
      "Epoch 9, Batch: 465: Training Loss: 0.022693540900945663, Validation Loss: 0.02397835999727249\n",
      "Epoch 9, Batch: 466: Training Loss: 0.026126908138394356, Validation Loss: 0.02469574101269245\n",
      "Epoch 9, Batch: 467: Training Loss: 0.029642388224601746, Validation Loss: 0.027116598561406136\n",
      "Epoch 9, Batch: 468: Training Loss: 0.03038392961025238, Validation Loss: 0.023900747299194336\n",
      "Epoch 9, Batch: 469: Training Loss: 0.026022853329777718, Validation Loss: 0.024496596306562424\n",
      "Epoch 9, Batch: 470: Training Loss: 0.02571212872862816, Validation Loss: 0.026443947106599808\n",
      "Epoch 9, Batch: 471: Training Loss: 0.02946949191391468, Validation Loss: 0.023630255833268166\n",
      "Epoch 9, Batch: 472: Training Loss: 0.031843941658735275, Validation Loss: 0.02708565630018711\n",
      "Epoch 9, Batch: 473: Training Loss: 0.022963304072618484, Validation Loss: 0.025872349739074707\n",
      "Epoch 9, Batch: 474: Training Loss: 0.024584298953413963, Validation Loss: 0.028385203331708908\n",
      "Epoch 9, Batch: 475: Training Loss: 0.027065493166446686, Validation Loss: 0.02705768495798111\n",
      "Epoch 9, Batch: 476: Training Loss: 0.02505514957010746, Validation Loss: 0.02929164282977581\n",
      "Epoch 9, Batch: 477: Training Loss: 0.031862739473581314, Validation Loss: 0.026812346652150154\n",
      "Epoch 9, Batch: 478: Training Loss: 0.02538699470460415, Validation Loss: 0.0279252827167511\n",
      "Epoch 9, Batch: 479: Training Loss: 0.025773100554943085, Validation Loss: 0.024806683883070946\n",
      "Epoch 9, Batch: 480: Training Loss: 0.025122538208961487, Validation Loss: 0.02920345775783062\n",
      "Epoch 9, Batch: 481: Training Loss: 0.027982134371995926, Validation Loss: 0.028130652382969856\n",
      "Epoch 9, Batch: 482: Training Loss: 0.029395265504717827, Validation Loss: 0.030732426792383194\n",
      "Epoch 9, Batch: 483: Training Loss: 0.024547314271330833, Validation Loss: 0.030339345335960388\n",
      "Epoch 9, Batch: 484: Training Loss: 0.022958913818001747, Validation Loss: 0.02933693863451481\n",
      "Epoch 9, Batch: 485: Training Loss: 0.023635150864720345, Validation Loss: 0.027003996074199677\n",
      "Epoch 9, Batch: 486: Training Loss: 0.027795158326625824, Validation Loss: 0.02824830450117588\n",
      "Epoch 9, Batch: 487: Training Loss: 0.026861200109124184, Validation Loss: 0.028118502348661423\n",
      "Epoch 9, Batch: 488: Training Loss: 0.023262914270162582, Validation Loss: 0.02724365144968033\n",
      "Epoch 9, Batch: 489: Training Loss: 0.029226452112197876, Validation Loss: 0.030195781961083412\n",
      "Epoch 9, Batch: 490: Training Loss: 0.02688557468354702, Validation Loss: 0.029297245666384697\n",
      "Epoch 9, Batch: 491: Training Loss: 0.023873157799243927, Validation Loss: 0.027359340339899063\n",
      "Epoch 9, Batch: 492: Training Loss: 0.02791476808488369, Validation Loss: 0.026593806222081184\n",
      "Epoch 9, Batch: 493: Training Loss: 0.023713907226920128, Validation Loss: 0.027056610211730003\n",
      "Epoch 9, Batch: 494: Training Loss: 0.028705136850476265, Validation Loss: 0.027171490713953972\n",
      "Epoch 9, Batch: 495: Training Loss: 0.0244013462215662, Validation Loss: 0.026011941954493523\n",
      "Epoch 9, Batch: 496: Training Loss: 0.022839484736323357, Validation Loss: 0.024623040109872818\n",
      "Epoch 9, Batch: 497: Training Loss: 0.024760525673627853, Validation Loss: 0.023771606385707855\n",
      "Epoch 9, Batch: 498: Training Loss: 0.027181288227438927, Validation Loss: 0.02615266852080822\n",
      "Epoch 9, Batch: 499: Training Loss: 0.0252369437366724, Validation Loss: 0.026010796427726746\n",
      "Epoch 10, Batch: 0: Training Loss: 0.027060717344284058, Validation Loss: 0.02619004435837269\n",
      "Epoch 10, Batch: 1: Training Loss: 0.024594029411673546, Validation Loss: 0.025469111278653145\n",
      "Epoch 10, Batch: 2: Training Loss: 0.029715240001678467, Validation Loss: 0.029480822384357452\n",
      "Epoch 10, Batch: 3: Training Loss: 0.02489457279443741, Validation Loss: 0.027570100501179695\n",
      "Epoch 10, Batch: 4: Training Loss: 0.021555230021476746, Validation Loss: 0.026580294594168663\n",
      "Epoch 10, Batch: 5: Training Loss: 0.027581142261624336, Validation Loss: 0.027103696018457413\n",
      "Epoch 10, Batch: 6: Training Loss: 0.024225017055869102, Validation Loss: 0.026956239715218544\n",
      "Epoch 10, Batch: 7: Training Loss: 0.025091391056776047, Validation Loss: 0.029399028047919273\n",
      "Epoch 10, Batch: 8: Training Loss: 0.028358912095427513, Validation Loss: 0.02764742262661457\n",
      "Epoch 10, Batch: 9: Training Loss: 0.026594111695885658, Validation Loss: 0.02564128488302231\n",
      "Epoch 10, Batch: 10: Training Loss: 0.024473577737808228, Validation Loss: 0.0261103305965662\n",
      "Epoch 10, Batch: 11: Training Loss: 0.02857547625899315, Validation Loss: 0.026756370440125465\n",
      "Epoch 10, Batch: 12: Training Loss: 0.02602635696530342, Validation Loss: 0.027937401086091995\n",
      "Epoch 10, Batch: 13: Training Loss: 0.030432775616645813, Validation Loss: 0.026485754176974297\n",
      "Epoch 10, Batch: 14: Training Loss: 0.02560463361442089, Validation Loss: 0.03050673007965088\n",
      "Epoch 10, Batch: 15: Training Loss: 0.02384914457798004, Validation Loss: 0.023823782801628113\n",
      "Epoch 10, Batch: 16: Training Loss: 0.02851606346666813, Validation Loss: 0.027394691482186317\n",
      "Epoch 10, Batch: 17: Training Loss: 0.02547340653836727, Validation Loss: 0.026430219411849976\n",
      "Epoch 10, Batch: 18: Training Loss: 0.028022345155477524, Validation Loss: 0.027637338265776634\n",
      "Epoch 10, Batch: 19: Training Loss: 0.022476544603705406, Validation Loss: 0.025042695924639702\n",
      "Epoch 10, Batch: 20: Training Loss: 0.028972327709197998, Validation Loss: 0.023855192586779594\n",
      "Epoch 10, Batch: 21: Training Loss: 0.024116413667798042, Validation Loss: 0.02647857554256916\n",
      "Epoch 10, Batch: 22: Training Loss: 0.032418981194496155, Validation Loss: 0.023681391030550003\n",
      "Epoch 10, Batch: 23: Training Loss: 0.023318786174058914, Validation Loss: 0.023397577926516533\n",
      "Epoch 10, Batch: 24: Training Loss: 0.024774210527539253, Validation Loss: 0.02498648688197136\n",
      "Epoch 10, Batch: 25: Training Loss: 0.02783828228712082, Validation Loss: 0.023703785613179207\n",
      "Epoch 10, Batch: 26: Training Loss: 0.030254002660512924, Validation Loss: 0.026401517912745476\n",
      "Epoch 10, Batch: 27: Training Loss: 0.02693571336567402, Validation Loss: 0.026468994095921516\n",
      "Epoch 10, Batch: 28: Training Loss: 0.03219543769955635, Validation Loss: 0.02776334062218666\n",
      "Epoch 10, Batch: 29: Training Loss: 0.031405285000801086, Validation Loss: 0.027274124324321747\n",
      "Epoch 10, Batch: 30: Training Loss: 0.023654881864786148, Validation Loss: 0.029371468350291252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 31: Training Loss: 0.028493542224168777, Validation Loss: 0.02740395814180374\n",
      "Epoch 10, Batch: 32: Training Loss: 0.025149615481495857, Validation Loss: 0.028137682005763054\n",
      "Epoch 10, Batch: 33: Training Loss: 0.02899930626153946, Validation Loss: 0.03170980513095856\n",
      "Epoch 10, Batch: 34: Training Loss: 0.027176136150956154, Validation Loss: 0.02869890257716179\n",
      "Epoch 10, Batch: 35: Training Loss: 0.029660463333129883, Validation Loss: 0.02538587898015976\n",
      "Epoch 10, Batch: 36: Training Loss: 0.02378406748175621, Validation Loss: 0.02685704082250595\n",
      "Epoch 10, Batch: 37: Training Loss: 0.02609875425696373, Validation Loss: 0.025911342352628708\n",
      "Epoch 10, Batch: 38: Training Loss: 0.027419626712799072, Validation Loss: 0.028182437643408775\n",
      "Epoch 10, Batch: 39: Training Loss: 0.026867737993597984, Validation Loss: 0.023401997983455658\n",
      "Epoch 10, Batch: 40: Training Loss: 0.028498414903879166, Validation Loss: 0.02479381114244461\n",
      "Epoch 10, Batch: 41: Training Loss: 0.02796788327395916, Validation Loss: 0.02614002116024494\n",
      "Epoch 10, Batch: 42: Training Loss: 0.02467033825814724, Validation Loss: 0.026778383180499077\n",
      "Epoch 10, Batch: 43: Training Loss: 0.028596563264727592, Validation Loss: 0.026929419487714767\n",
      "Epoch 10, Batch: 44: Training Loss: 0.030281582847237587, Validation Loss: 0.02803010120987892\n",
      "Epoch 10, Batch: 45: Training Loss: 0.026550523936748505, Validation Loss: 0.028285659849643707\n",
      "Epoch 10, Batch: 46: Training Loss: 0.032362401485443115, Validation Loss: 0.02710764855146408\n",
      "Epoch 10, Batch: 47: Training Loss: 0.030606692656874657, Validation Loss: 0.027056556195020676\n",
      "Epoch 10, Batch: 48: Training Loss: 0.03156084194779396, Validation Loss: 0.027331264689564705\n",
      "Epoch 10, Batch: 49: Training Loss: 0.027855869382619858, Validation Loss: 0.02659648098051548\n",
      "Epoch 10, Batch: 50: Training Loss: 0.02857387624680996, Validation Loss: 0.028002843260765076\n",
      "Epoch 10, Batch: 51: Training Loss: 0.03043428249657154, Validation Loss: 0.031056497246026993\n",
      "Epoch 10, Batch: 52: Training Loss: 0.023648753762245178, Validation Loss: 0.029631441459059715\n",
      "Epoch 10, Batch: 53: Training Loss: 0.026928581297397614, Validation Loss: 0.03103143349289894\n",
      "Epoch 10, Batch: 54: Training Loss: 0.026206152513623238, Validation Loss: 0.027067726477980614\n",
      "Epoch 10, Batch: 55: Training Loss: 0.027847709134221077, Validation Loss: 0.025645757094025612\n",
      "Epoch 10, Batch: 56: Training Loss: 0.027887258678674698, Validation Loss: 0.02769007347524166\n",
      "Epoch 10, Batch: 57: Training Loss: 0.024538932368159294, Validation Loss: 0.02716686762869358\n",
      "Epoch 10, Batch: 58: Training Loss: 0.025245871394872665, Validation Loss: 0.028110183775424957\n",
      "Epoch 10, Batch: 59: Training Loss: 0.023141728714108467, Validation Loss: 0.02760622650384903\n",
      "Epoch 10, Batch: 60: Training Loss: 0.029536766931414604, Validation Loss: 0.027617622166872025\n",
      "Epoch 10, Batch: 61: Training Loss: 0.030201751738786697, Validation Loss: 0.026975343003869057\n",
      "Epoch 10, Batch: 62: Training Loss: 0.02480880357325077, Validation Loss: 0.024225158616900444\n",
      "Epoch 10, Batch: 63: Training Loss: 0.02698497846722603, Validation Loss: 0.025108303874731064\n",
      "Epoch 10, Batch: 64: Training Loss: 0.025810446590185165, Validation Loss: 0.0260402113199234\n",
      "Epoch 10, Batch: 65: Training Loss: 0.02600962482392788, Validation Loss: 0.02314150147140026\n",
      "Epoch 10, Batch: 66: Training Loss: 0.024459268897771835, Validation Loss: 0.024783212691545486\n",
      "Epoch 10, Batch: 67: Training Loss: 0.02056056633591652, Validation Loss: 0.0243573896586895\n",
      "Epoch 10, Batch: 68: Training Loss: 0.025912651792168617, Validation Loss: 0.02638285979628563\n",
      "Epoch 10, Batch: 69: Training Loss: 0.029318686574697495, Validation Loss: 0.022995099425315857\n",
      "Epoch 10, Batch: 70: Training Loss: 0.025927595794200897, Validation Loss: 0.0232694149017334\n",
      "Epoch 10, Batch: 71: Training Loss: 0.026113562285900116, Validation Loss: 0.024926815181970596\n",
      "Epoch 10, Batch: 72: Training Loss: 0.026740195229649544, Validation Loss: 0.02448531985282898\n",
      "Epoch 10, Batch: 73: Training Loss: 0.02522151917219162, Validation Loss: 0.023561816662549973\n",
      "Epoch 10, Batch: 74: Training Loss: 0.02474309876561165, Validation Loss: 0.02458139881491661\n",
      "Epoch 10, Batch: 75: Training Loss: 0.022435881197452545, Validation Loss: 0.025162523612380028\n",
      "Epoch 10, Batch: 76: Training Loss: 0.02429143153131008, Validation Loss: 0.023374928161501884\n",
      "Epoch 10, Batch: 77: Training Loss: 0.026423649862408638, Validation Loss: 0.023196380585432053\n",
      "Epoch 10, Batch: 78: Training Loss: 0.02599538117647171, Validation Loss: 0.0267765112221241\n",
      "Epoch 10, Batch: 79: Training Loss: 0.02536768652498722, Validation Loss: 0.025241244584321976\n",
      "Epoch 10, Batch: 80: Training Loss: 0.022092696279287338, Validation Loss: 0.025929629802703857\n",
      "Epoch 10, Batch: 81: Training Loss: 0.024579009041190147, Validation Loss: 0.02510468102991581\n",
      "Epoch 10, Batch: 82: Training Loss: 0.025470564141869545, Validation Loss: 0.026042936369776726\n",
      "Epoch 10, Batch: 83: Training Loss: 0.02400120534002781, Validation Loss: 0.02638872154057026\n",
      "Epoch 10, Batch: 84: Training Loss: 0.025594692677259445, Validation Loss: 0.02494041621685028\n",
      "Epoch 10, Batch: 85: Training Loss: 0.02571982890367508, Validation Loss: 0.024613015353679657\n",
      "Epoch 10, Batch: 86: Training Loss: 0.026858029887080193, Validation Loss: 0.02587212063372135\n",
      "Epoch 10, Batch: 87: Training Loss: 0.02895614132285118, Validation Loss: 0.02688947319984436\n",
      "Epoch 10, Batch: 88: Training Loss: 0.02859063260257244, Validation Loss: 0.02514595165848732\n",
      "Epoch 10, Batch: 89: Training Loss: 0.03312499821186066, Validation Loss: 0.026060663163661957\n",
      "Epoch 10, Batch: 90: Training Loss: 0.023979948833584785, Validation Loss: 0.025281378999352455\n",
      "Epoch 10, Batch: 91: Training Loss: 0.030164774507284164, Validation Loss: 0.02440745197236538\n",
      "Epoch 10, Batch: 92: Training Loss: 0.03009144403040409, Validation Loss: 0.023477882146835327\n",
      "Epoch 10, Batch: 93: Training Loss: 0.023308251053094864, Validation Loss: 0.02623276598751545\n",
      "Epoch 10, Batch: 94: Training Loss: 0.029078958556056023, Validation Loss: 0.02596225030720234\n",
      "Epoch 10, Batch: 95: Training Loss: 0.024647532030940056, Validation Loss: 0.02677273191511631\n",
      "Epoch 10, Batch: 96: Training Loss: 0.021367806941270828, Validation Loss: 0.024236604571342468\n",
      "Epoch 10, Batch: 97: Training Loss: 0.02821793593466282, Validation Loss: 0.028006261214613914\n",
      "Epoch 10, Batch: 98: Training Loss: 0.027871858328580856, Validation Loss: 0.023559829220175743\n",
      "Epoch 10, Batch: 99: Training Loss: 0.026569701731204987, Validation Loss: 0.025894995778799057\n",
      "Epoch 10, Batch: 100: Training Loss: 0.026586836203932762, Validation Loss: 0.02565179020166397\n",
      "Epoch 10, Batch: 101: Training Loss: 0.026143966242671013, Validation Loss: 0.025335567072033882\n",
      "Epoch 10, Batch: 102: Training Loss: 0.022004811093211174, Validation Loss: 0.023289592936635017\n",
      "Epoch 10, Batch: 103: Training Loss: 0.030364200472831726, Validation Loss: 0.02489587850868702\n",
      "Epoch 10, Batch: 104: Training Loss: 0.019403858110308647, Validation Loss: 0.02562246285378933\n",
      "Epoch 10, Batch: 105: Training Loss: 0.025431405752897263, Validation Loss: 0.025428716093301773\n",
      "Epoch 10, Batch: 106: Training Loss: 0.0275200754404068, Validation Loss: 0.027177469804883003\n",
      "Epoch 10, Batch: 107: Training Loss: 0.02815147303044796, Validation Loss: 0.025748562067747116\n",
      "Epoch 10, Batch: 108: Training Loss: 0.025326186791062355, Validation Loss: 0.029021140187978745\n",
      "Epoch 10, Batch: 109: Training Loss: 0.027554314583539963, Validation Loss: 0.02502928115427494\n",
      "Epoch 10, Batch: 110: Training Loss: 0.029489414766430855, Validation Loss: 0.02546871080994606\n",
      "Epoch 10, Batch: 111: Training Loss: 0.025435980409383774, Validation Loss: 0.022658051922917366\n",
      "Epoch 10, Batch: 112: Training Loss: 0.02398212067782879, Validation Loss: 0.0257269199937582\n",
      "Epoch 10, Batch: 113: Training Loss: 0.024636154994368553, Validation Loss: 0.024142080917954445\n",
      "Epoch 10, Batch: 114: Training Loss: 0.025993341580033302, Validation Loss: 0.0254853293299675\n",
      "Epoch 10, Batch: 115: Training Loss: 0.0260545015335083, Validation Loss: 0.024512026458978653\n",
      "Epoch 10, Batch: 116: Training Loss: 0.02635011449456215, Validation Loss: 0.02414296194911003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 117: Training Loss: 0.026594458147883415, Validation Loss: 0.02527209371328354\n",
      "Epoch 10, Batch: 118: Training Loss: 0.023910339921712875, Validation Loss: 0.026534227654337883\n",
      "Epoch 10, Batch: 119: Training Loss: 0.024514494463801384, Validation Loss: 0.025106867775321007\n",
      "Epoch 10, Batch: 120: Training Loss: 0.023121550679206848, Validation Loss: 0.025844359770417213\n",
      "Epoch 10, Batch: 121: Training Loss: 0.02746020257472992, Validation Loss: 0.02559794671833515\n",
      "Epoch 10, Batch: 122: Training Loss: 0.026902055367827415, Validation Loss: 0.025788025930523872\n",
      "Epoch 10, Batch: 123: Training Loss: 0.026472516357898712, Validation Loss: 0.023648813366889954\n",
      "Epoch 10, Batch: 124: Training Loss: 0.02533869259059429, Validation Loss: 0.025076881051063538\n",
      "Epoch 10, Batch: 125: Training Loss: 0.020864691585302353, Validation Loss: 0.024415120482444763\n",
      "Epoch 10, Batch: 126: Training Loss: 0.026602111756801605, Validation Loss: 0.027878517284989357\n",
      "Epoch 10, Batch: 127: Training Loss: 0.024463210254907608, Validation Loss: 0.02682705968618393\n",
      "Epoch 10, Batch: 128: Training Loss: 0.030965516343712807, Validation Loss: 0.026022963225841522\n",
      "Epoch 10, Batch: 129: Training Loss: 0.023724215105175972, Validation Loss: 0.022473739460110664\n",
      "Epoch 10, Batch: 130: Training Loss: 0.02714485116302967, Validation Loss: 0.026329856365919113\n",
      "Epoch 10, Batch: 131: Training Loss: 0.025537630543112755, Validation Loss: 0.0264299213886261\n",
      "Epoch 10, Batch: 132: Training Loss: 0.024896301329135895, Validation Loss: 0.02584937959909439\n",
      "Epoch 10, Batch: 133: Training Loss: 0.024082258343696594, Validation Loss: 0.024259839206933975\n",
      "Epoch 10, Batch: 134: Training Loss: 0.027614323422312737, Validation Loss: 0.029324064031243324\n",
      "Epoch 10, Batch: 135: Training Loss: 0.025886356830596924, Validation Loss: 0.026818908751010895\n",
      "Epoch 10, Batch: 136: Training Loss: 0.02797091007232666, Validation Loss: 0.025772979483008385\n",
      "Epoch 10, Batch: 137: Training Loss: 0.02516811713576317, Validation Loss: 0.028430992737412453\n",
      "Epoch 10, Batch: 138: Training Loss: 0.02387482300400734, Validation Loss: 0.024723825976252556\n",
      "Epoch 10, Batch: 139: Training Loss: 0.023323310539126396, Validation Loss: 0.027568059042096138\n",
      "Epoch 10, Batch: 140: Training Loss: 0.031054364517331123, Validation Loss: 0.029365185648202896\n",
      "Epoch 10, Batch: 141: Training Loss: 0.023512087762355804, Validation Loss: 0.026805240660905838\n",
      "Epoch 10, Batch: 142: Training Loss: 0.02523716911673546, Validation Loss: 0.02644159458577633\n",
      "Epoch 10, Batch: 143: Training Loss: 0.027955783531069756, Validation Loss: 0.025630386546254158\n",
      "Epoch 10, Batch: 144: Training Loss: 0.026928944513201714, Validation Loss: 0.03102908283472061\n",
      "Epoch 10, Batch: 145: Training Loss: 0.021971343085169792, Validation Loss: 0.027126366272568703\n",
      "Epoch 10, Batch: 146: Training Loss: 0.024517888203263283, Validation Loss: 0.025415068492293358\n",
      "Epoch 10, Batch: 147: Training Loss: 0.026758473366498947, Validation Loss: 0.027115115895867348\n",
      "Epoch 10, Batch: 148: Training Loss: 0.02556670643389225, Validation Loss: 0.02614995837211609\n",
      "Epoch 10, Batch: 149: Training Loss: 0.02510295808315277, Validation Loss: 0.02749127708375454\n",
      "Epoch 10, Batch: 150: Training Loss: 0.02769889310002327, Validation Loss: 0.026189299300312996\n",
      "Epoch 10, Batch: 151: Training Loss: 0.026293613016605377, Validation Loss: 0.026588352397084236\n",
      "Epoch 10, Batch: 152: Training Loss: 0.031066125258803368, Validation Loss: 0.026802556589245796\n",
      "Epoch 10, Batch: 153: Training Loss: 0.024711884558200836, Validation Loss: 0.02743224985897541\n",
      "Epoch 10, Batch: 154: Training Loss: 0.026859136298298836, Validation Loss: 0.02691580355167389\n",
      "Epoch 10, Batch: 155: Training Loss: 0.03505329415202141, Validation Loss: 0.0274950098246336\n",
      "Epoch 10, Batch: 156: Training Loss: 0.02201680652797222, Validation Loss: 0.025901127606630325\n",
      "Epoch 10, Batch: 157: Training Loss: 0.022327547892928123, Validation Loss: 0.02504228800535202\n",
      "Epoch 10, Batch: 158: Training Loss: 0.02455652691423893, Validation Loss: 0.02654290199279785\n",
      "Epoch 10, Batch: 159: Training Loss: 0.025455167517066002, Validation Loss: 0.022835692390799522\n",
      "Epoch 10, Batch: 160: Training Loss: 0.02520829252898693, Validation Loss: 0.025229008868336678\n",
      "Epoch 10, Batch: 161: Training Loss: 0.025156261399388313, Validation Loss: 0.027418700978159904\n",
      "Epoch 10, Batch: 162: Training Loss: 0.02721242792904377, Validation Loss: 0.02802172116935253\n",
      "Epoch 10, Batch: 163: Training Loss: 0.023627905175089836, Validation Loss: 0.02658638171851635\n",
      "Epoch 10, Batch: 164: Training Loss: 0.023371530696749687, Validation Loss: 0.026349198073148727\n",
      "Epoch 10, Batch: 165: Training Loss: 0.024920934811234474, Validation Loss: 0.026445752009749413\n",
      "Epoch 10, Batch: 166: Training Loss: 0.027300307527184486, Validation Loss: 0.029015006497502327\n",
      "Epoch 10, Batch: 167: Training Loss: 0.024573322385549545, Validation Loss: 0.02581123262643814\n",
      "Epoch 10, Batch: 168: Training Loss: 0.029214588925242424, Validation Loss: 0.026389703154563904\n",
      "Epoch 10, Batch: 169: Training Loss: 0.026619285345077515, Validation Loss: 0.026026422157883644\n",
      "Epoch 10, Batch: 170: Training Loss: 0.02765367366373539, Validation Loss: 0.026822347193956375\n",
      "Epoch 10, Batch: 171: Training Loss: 0.02138288877904415, Validation Loss: 0.024587558582425117\n",
      "Epoch 10, Batch: 172: Training Loss: 0.024837126955389977, Validation Loss: 0.027285240590572357\n",
      "Epoch 10, Batch: 173: Training Loss: 0.021338915452361107, Validation Loss: 0.024618197232484818\n",
      "Epoch 10, Batch: 174: Training Loss: 0.024546505883336067, Validation Loss: 0.02619491145014763\n",
      "Epoch 10, Batch: 175: Training Loss: 0.026525355875492096, Validation Loss: 0.02694670855998993\n",
      "Epoch 10, Batch: 176: Training Loss: 0.02756425365805626, Validation Loss: 0.024472778663039207\n",
      "Epoch 10, Batch: 177: Training Loss: 0.02563345618546009, Validation Loss: 0.025230776518583298\n",
      "Epoch 10, Batch: 178: Training Loss: 0.028310546651482582, Validation Loss: 0.02768530324101448\n",
      "Epoch 10, Batch: 179: Training Loss: 0.025672871619462967, Validation Loss: 0.023340094834566116\n",
      "Epoch 10, Batch: 180: Training Loss: 0.024090465158224106, Validation Loss: 0.024032970890402794\n",
      "Epoch 10, Batch: 181: Training Loss: 0.024484913796186447, Validation Loss: 0.023383306339383125\n",
      "Epoch 10, Batch: 182: Training Loss: 0.028145361691713333, Validation Loss: 0.02546890825033188\n",
      "Epoch 10, Batch: 183: Training Loss: 0.027836382389068604, Validation Loss: 0.025293877348303795\n",
      "Epoch 10, Batch: 184: Training Loss: 0.024947911500930786, Validation Loss: 0.024042785167694092\n",
      "Epoch 10, Batch: 185: Training Loss: 0.030194249004125595, Validation Loss: 0.02667788229882717\n",
      "Epoch 10, Batch: 186: Training Loss: 0.029157795011997223, Validation Loss: 0.025779908522963524\n",
      "Epoch 10, Batch: 187: Training Loss: 0.022945798933506012, Validation Loss: 0.027271490544080734\n",
      "Epoch 10, Batch: 188: Training Loss: 0.02430793270468712, Validation Loss: 0.027445491403341293\n",
      "Epoch 10, Batch: 189: Training Loss: 0.023862067610025406, Validation Loss: 0.024638835340738297\n",
      "Epoch 10, Batch: 190: Training Loss: 0.03032970428466797, Validation Loss: 0.02429920807480812\n",
      "Epoch 10, Batch: 191: Training Loss: 0.028144681826233864, Validation Loss: 0.02506549097597599\n",
      "Epoch 10, Batch: 192: Training Loss: 0.026284601539373398, Validation Loss: 0.026702003553509712\n",
      "Epoch 10, Batch: 193: Training Loss: 0.02612392045557499, Validation Loss: 0.025640834122896194\n",
      "Saving new best model w/ loss: 0.021820560097694397\n",
      "Epoch 10, Batch: 194: Training Loss: 0.02624494582414627, Validation Loss: 0.021820560097694397\n",
      "Epoch 10, Batch: 195: Training Loss: 0.022519046440720558, Validation Loss: 0.024783551692962646\n",
      "Epoch 10, Batch: 196: Training Loss: 0.02952737733721733, Validation Loss: 0.02602292411029339\n",
      "Epoch 10, Batch: 197: Training Loss: 0.0246165469288826, Validation Loss: 0.023497991263866425\n",
      "Epoch 10, Batch: 198: Training Loss: 0.024186892434954643, Validation Loss: 0.026360874995589256\n",
      "Epoch 10, Batch: 199: Training Loss: 0.022656507790088654, Validation Loss: 0.027165550738573074\n",
      "Epoch 10, Batch: 200: Training Loss: 0.02056746743619442, Validation Loss: 0.025967415422201157\n",
      "Epoch 10, Batch: 201: Training Loss: 0.028165634721517563, Validation Loss: 0.027303114533424377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 202: Training Loss: 0.028753040358424187, Validation Loss: 0.026698710396885872\n",
      "Epoch 10, Batch: 203: Training Loss: 0.027685318142175674, Validation Loss: 0.023934215307235718\n",
      "Epoch 10, Batch: 204: Training Loss: 0.02501433528959751, Validation Loss: 0.022972652688622475\n",
      "Epoch 10, Batch: 205: Training Loss: 0.0309833325445652, Validation Loss: 0.023349957540631294\n",
      "Epoch 10, Batch: 206: Training Loss: 0.02788589708507061, Validation Loss: 0.02292385883629322\n",
      "Epoch 10, Batch: 207: Training Loss: 0.026126660406589508, Validation Loss: 0.02568543516099453\n",
      "Epoch 10, Batch: 208: Training Loss: 0.02600827068090439, Validation Loss: 0.025700489059090614\n",
      "Epoch 10, Batch: 209: Training Loss: 0.022986045107245445, Validation Loss: 0.025212163105607033\n",
      "Epoch 10, Batch: 210: Training Loss: 0.03114594891667366, Validation Loss: 0.026986518874764442\n",
      "Epoch 10, Batch: 211: Training Loss: 0.026801250874996185, Validation Loss: 0.02642481029033661\n",
      "Epoch 10, Batch: 212: Training Loss: 0.027289072051644325, Validation Loss: 0.02904161997139454\n",
      "Epoch 10, Batch: 213: Training Loss: 0.030872683972120285, Validation Loss: 0.02844536118209362\n",
      "Epoch 10, Batch: 214: Training Loss: 0.021436285227537155, Validation Loss: 0.02931204065680504\n",
      "Epoch 10, Batch: 215: Training Loss: 0.02792840637266636, Validation Loss: 0.028247738257050514\n",
      "Epoch 10, Batch: 216: Training Loss: 0.028302215039730072, Validation Loss: 0.02860148623585701\n",
      "Epoch 10, Batch: 217: Training Loss: 0.029187364503741264, Validation Loss: 0.024864738807082176\n",
      "Epoch 10, Batch: 218: Training Loss: 0.028405580669641495, Validation Loss: 0.028448309749364853\n",
      "Epoch 10, Batch: 219: Training Loss: 0.027976665645837784, Validation Loss: 0.023294508457183838\n",
      "Epoch 10, Batch: 220: Training Loss: 0.02589305490255356, Validation Loss: 0.027915004640817642\n",
      "Epoch 10, Batch: 221: Training Loss: 0.024241013452410698, Validation Loss: 0.028425607830286026\n",
      "Epoch 10, Batch: 222: Training Loss: 0.027882302179932594, Validation Loss: 0.027042917907238007\n",
      "Epoch 10, Batch: 223: Training Loss: 0.026268884539604187, Validation Loss: 0.027364881709218025\n",
      "Epoch 10, Batch: 224: Training Loss: 0.02764805220067501, Validation Loss: 0.02851153537631035\n",
      "Epoch 10, Batch: 225: Training Loss: 0.0299884844571352, Validation Loss: 0.029884355142712593\n",
      "Epoch 10, Batch: 226: Training Loss: 0.028967497870326042, Validation Loss: 0.029367219656705856\n",
      "Epoch 10, Batch: 227: Training Loss: 0.025600971654057503, Validation Loss: 0.03013281337916851\n",
      "Epoch 10, Batch: 228: Training Loss: 0.030560318380594254, Validation Loss: 0.03178524225950241\n",
      "Epoch 10, Batch: 229: Training Loss: 0.024590453132987022, Validation Loss: 0.03230002149939537\n",
      "Epoch 10, Batch: 230: Training Loss: 0.0286234263330698, Validation Loss: 0.02958391234278679\n",
      "Epoch 10, Batch: 231: Training Loss: 0.031447622925043106, Validation Loss: 0.02836143784224987\n",
      "Epoch 10, Batch: 232: Training Loss: 0.029318783432245255, Validation Loss: 0.027889994904398918\n",
      "Epoch 10, Batch: 233: Training Loss: 0.024514490738511086, Validation Loss: 0.02915569208562374\n",
      "Epoch 10, Batch: 234: Training Loss: 0.029583489522337914, Validation Loss: 0.027501583099365234\n",
      "Epoch 10, Batch: 235: Training Loss: 0.029830340296030045, Validation Loss: 0.02950902096927166\n",
      "Epoch 10, Batch: 236: Training Loss: 0.02840370126068592, Validation Loss: 0.02705484628677368\n",
      "Epoch 10, Batch: 237: Training Loss: 0.027018830180168152, Validation Loss: 0.02956298738718033\n",
      "Epoch 10, Batch: 238: Training Loss: 0.02854016050696373, Validation Loss: 0.028539950028061867\n",
      "Epoch 10, Batch: 239: Training Loss: 0.026542024686932564, Validation Loss: 0.027784081175923347\n",
      "Epoch 10, Batch: 240: Training Loss: 0.022352222353219986, Validation Loss: 0.02843865007162094\n",
      "Epoch 10, Batch: 241: Training Loss: 0.021517984569072723, Validation Loss: 0.028927315026521683\n",
      "Epoch 10, Batch: 242: Training Loss: 0.026678048074245453, Validation Loss: 0.030030881986021996\n",
      "Epoch 10, Batch: 243: Training Loss: 0.027091428637504578, Validation Loss: 0.028823014348745346\n",
      "Epoch 10, Batch: 244: Training Loss: 0.026761310175061226, Validation Loss: 0.03061489202082157\n",
      "Epoch 10, Batch: 245: Training Loss: 0.026772918179631233, Validation Loss: 0.02961188741028309\n",
      "Epoch 10, Batch: 246: Training Loss: 0.02596350386738777, Validation Loss: 0.029881631955504417\n",
      "Epoch 10, Batch: 247: Training Loss: 0.02758692018687725, Validation Loss: 0.02896750159561634\n",
      "Epoch 10, Batch: 248: Training Loss: 0.026137392967939377, Validation Loss: 0.02878570184111595\n",
      "Epoch 10, Batch: 249: Training Loss: 0.02445559948682785, Validation Loss: 0.027294009923934937\n",
      "Epoch 10, Batch: 250: Training Loss: 0.027681538835167885, Validation Loss: 0.028974246233701706\n",
      "Epoch 10, Batch: 251: Training Loss: 0.02520676888525486, Validation Loss: 0.02980674058198929\n",
      "Epoch 10, Batch: 252: Training Loss: 0.02749381959438324, Validation Loss: 0.031044648960232735\n",
      "Epoch 10, Batch: 253: Training Loss: 0.028840195387601852, Validation Loss: 0.032896652817726135\n",
      "Epoch 10, Batch: 254: Training Loss: 0.02439209260046482, Validation Loss: 0.028873786330223083\n",
      "Epoch 10, Batch: 255: Training Loss: 0.028157567605376244, Validation Loss: 0.02991683967411518\n",
      "Epoch 10, Batch: 256: Training Loss: 0.03060080297291279, Validation Loss: 0.02664019539952278\n",
      "Epoch 10, Batch: 257: Training Loss: 0.028545759618282318, Validation Loss: 0.026527797803282738\n",
      "Epoch 10, Batch: 258: Training Loss: 0.02993975393474102, Validation Loss: 0.02932905964553356\n",
      "Epoch 10, Batch: 259: Training Loss: 0.023849209770560265, Validation Loss: 0.02659107744693756\n",
      "Epoch 10, Batch: 260: Training Loss: 0.02915034629404545, Validation Loss: 0.024779966101050377\n",
      "Epoch 10, Batch: 261: Training Loss: 0.024525556713342667, Validation Loss: 0.02524666301906109\n",
      "Epoch 10, Batch: 262: Training Loss: 0.025011157616972923, Validation Loss: 0.024261170998215675\n",
      "Epoch 10, Batch: 263: Training Loss: 0.02715197764337063, Validation Loss: 0.023749638348817825\n",
      "Epoch 10, Batch: 264: Training Loss: 0.022719979286193848, Validation Loss: 0.024739868938922882\n",
      "Epoch 10, Batch: 265: Training Loss: 0.026004478335380554, Validation Loss: 0.02314119040966034\n",
      "Epoch 10, Batch: 266: Training Loss: 0.025101160630583763, Validation Loss: 0.025843827053904533\n",
      "Epoch 10, Batch: 267: Training Loss: 0.02348661608994007, Validation Loss: 0.023428114131093025\n",
      "Epoch 10, Batch: 268: Training Loss: 0.024756191298365593, Validation Loss: 0.023573916405439377\n",
      "Epoch 10, Batch: 269: Training Loss: 0.024369504302740097, Validation Loss: 0.025717036798596382\n",
      "Epoch 10, Batch: 270: Training Loss: 0.023546461015939713, Validation Loss: 0.026762036606669426\n",
      "Epoch 10, Batch: 271: Training Loss: 0.02587466686964035, Validation Loss: 0.025097358971834183\n",
      "Epoch 10, Batch: 272: Training Loss: 0.024137089028954506, Validation Loss: 0.026721172034740448\n",
      "Epoch 10, Batch: 273: Training Loss: 0.026310164481401443, Validation Loss: 0.026869215071201324\n",
      "Epoch 10, Batch: 274: Training Loss: 0.02882670797407627, Validation Loss: 0.02551088109612465\n",
      "Epoch 10, Batch: 275: Training Loss: 0.026678239926695824, Validation Loss: 0.028987988829612732\n",
      "Epoch 10, Batch: 276: Training Loss: 0.022190604358911514, Validation Loss: 0.027416132390499115\n",
      "Epoch 10, Batch: 277: Training Loss: 0.02234072983264923, Validation Loss: 0.0282023586332798\n",
      "Epoch 10, Batch: 278: Training Loss: 0.02730410173535347, Validation Loss: 0.029306335374712944\n",
      "Epoch 10, Batch: 279: Training Loss: 0.029147760942578316, Validation Loss: 0.02863496169447899\n",
      "Epoch 10, Batch: 280: Training Loss: 0.02368568256497383, Validation Loss: 0.02906636707484722\n",
      "Epoch 10, Batch: 281: Training Loss: 0.025889845564961433, Validation Loss: 0.02959151193499565\n",
      "Epoch 10, Batch: 282: Training Loss: 0.026396840810775757, Validation Loss: 0.02927010878920555\n",
      "Epoch 10, Batch: 283: Training Loss: 0.02604321949183941, Validation Loss: 0.03129051625728607\n",
      "Epoch 10, Batch: 284: Training Loss: 0.026211632415652275, Validation Loss: 0.028947407379746437\n",
      "Epoch 10, Batch: 285: Training Loss: 0.02958807721734047, Validation Loss: 0.027307292446494102\n",
      "Epoch 10, Batch: 286: Training Loss: 0.023830387741327286, Validation Loss: 0.030890153720974922\n",
      "Epoch 10, Batch: 287: Training Loss: 0.02928772382438183, Validation Loss: 0.02698739990592003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 288: Training Loss: 0.023970112204551697, Validation Loss: 0.030447466298937798\n",
      "Epoch 10, Batch: 289: Training Loss: 0.03128775954246521, Validation Loss: 0.025890996679663658\n",
      "Epoch 10, Batch: 290: Training Loss: 0.02742333523929119, Validation Loss: 0.027502965182065964\n",
      "Epoch 10, Batch: 291: Training Loss: 0.025708762928843498, Validation Loss: 0.027888910844922066\n",
      "Epoch 10, Batch: 292: Training Loss: 0.027812646701931953, Validation Loss: 0.029077831655740738\n",
      "Epoch 10, Batch: 293: Training Loss: 0.028653016313910484, Validation Loss: 0.02741323597729206\n",
      "Epoch 10, Batch: 294: Training Loss: 0.027492545545101166, Validation Loss: 0.03025847114622593\n",
      "Epoch 10, Batch: 295: Training Loss: 0.02771807834506035, Validation Loss: 0.02876894362270832\n",
      "Epoch 10, Batch: 296: Training Loss: 0.025087924674153328, Validation Loss: 0.029132628813385963\n",
      "Epoch 10, Batch: 297: Training Loss: 0.029928036034107208, Validation Loss: 0.02778693474829197\n",
      "Epoch 10, Batch: 298: Training Loss: 0.026518451049923897, Validation Loss: 0.027473410591483116\n",
      "Epoch 10, Batch: 299: Training Loss: 0.021985039114952087, Validation Loss: 0.026753375306725502\n",
      "Epoch 10, Batch: 300: Training Loss: 0.02728630229830742, Validation Loss: 0.027317699044942856\n",
      "Epoch 10, Batch: 301: Training Loss: 0.023724624887108803, Validation Loss: 0.028313154354691505\n",
      "Epoch 10, Batch: 302: Training Loss: 0.017479265108704567, Validation Loss: 0.026206279173493385\n",
      "Epoch 10, Batch: 303: Training Loss: 0.027213161811232567, Validation Loss: 0.02462375909090042\n",
      "Epoch 10, Batch: 304: Training Loss: 0.026210347190499306, Validation Loss: 0.025676436722278595\n",
      "Epoch 10, Batch: 305: Training Loss: 0.021678360179066658, Validation Loss: 0.026112934574484825\n",
      "Epoch 10, Batch: 306: Training Loss: 0.023786529898643494, Validation Loss: 0.02607683092355728\n",
      "Epoch 10, Batch: 307: Training Loss: 0.021905943751335144, Validation Loss: 0.02859215997159481\n",
      "Epoch 10, Batch: 308: Training Loss: 0.023911241441965103, Validation Loss: 0.027980215847492218\n",
      "Epoch 10, Batch: 309: Training Loss: 0.026900606229901314, Validation Loss: 0.029609808698296547\n",
      "Epoch 10, Batch: 310: Training Loss: 0.025532079860568047, Validation Loss: 0.03038974478840828\n",
      "Epoch 10, Batch: 311: Training Loss: 0.02227235957980156, Validation Loss: 0.03035852313041687\n",
      "Epoch 10, Batch: 312: Training Loss: 0.024669162929058075, Validation Loss: 0.032593224197626114\n",
      "Epoch 10, Batch: 313: Training Loss: 0.021314946934580803, Validation Loss: 0.03059963509440422\n",
      "Epoch 10, Batch: 314: Training Loss: 0.025755329057574272, Validation Loss: 0.02755136974155903\n",
      "Epoch 10, Batch: 315: Training Loss: 0.02357003651559353, Validation Loss: 0.026489445939660072\n",
      "Epoch 10, Batch: 316: Training Loss: 0.02306937612593174, Validation Loss: 0.028984643518924713\n",
      "Epoch 10, Batch: 317: Training Loss: 0.02337939850986004, Validation Loss: 0.031129395589232445\n",
      "Epoch 10, Batch: 318: Training Loss: 0.02447759546339512, Validation Loss: 0.03157435730099678\n",
      "Epoch 10, Batch: 319: Training Loss: 0.023545928299427032, Validation Loss: 0.030537189915776253\n",
      "Epoch 10, Batch: 320: Training Loss: 0.027617134153842926, Validation Loss: 0.03046794980764389\n",
      "Epoch 10, Batch: 321: Training Loss: 0.024753449484705925, Validation Loss: 0.03132304549217224\n",
      "Epoch 10, Batch: 322: Training Loss: 0.023371553048491478, Validation Loss: 0.02891497127711773\n",
      "Epoch 10, Batch: 323: Training Loss: 0.02702760137617588, Validation Loss: 0.031476940959692\n",
      "Epoch 10, Batch: 324: Training Loss: 0.030419589951634407, Validation Loss: 0.030546674504876137\n",
      "Epoch 10, Batch: 325: Training Loss: 0.0244094580411911, Validation Loss: 0.02992856316268444\n",
      "Epoch 10, Batch: 326: Training Loss: 0.02888304367661476, Validation Loss: 0.030856037512421608\n",
      "Epoch 10, Batch: 327: Training Loss: 0.02425478771328926, Validation Loss: 0.028098104521632195\n",
      "Epoch 10, Batch: 328: Training Loss: 0.02536710537970066, Validation Loss: 0.029756147414445877\n",
      "Epoch 10, Batch: 329: Training Loss: 0.027641240507364273, Validation Loss: 0.02866252325475216\n",
      "Epoch 10, Batch: 330: Training Loss: 0.0254517775028944, Validation Loss: 0.028712283819913864\n",
      "Epoch 10, Batch: 331: Training Loss: 0.026591166853904724, Validation Loss: 0.02988010086119175\n",
      "Epoch 10, Batch: 332: Training Loss: 0.02317105233669281, Validation Loss: 0.02872513234615326\n",
      "Epoch 10, Batch: 333: Training Loss: 0.02461007423698902, Validation Loss: 0.03043653629720211\n",
      "Epoch 10, Batch: 334: Training Loss: 0.02580951154232025, Validation Loss: 0.025930756703019142\n",
      "Epoch 10, Batch: 335: Training Loss: 0.025681473314762115, Validation Loss: 0.03033672831952572\n",
      "Epoch 10, Batch: 336: Training Loss: 0.025127699598670006, Validation Loss: 0.026958607137203217\n",
      "Epoch 10, Batch: 337: Training Loss: 0.021955471485853195, Validation Loss: 0.024821463972330093\n",
      "Epoch 10, Batch: 338: Training Loss: 0.025638414546847343, Validation Loss: 0.028498351573944092\n",
      "Epoch 10, Batch: 339: Training Loss: 0.025415128096938133, Validation Loss: 0.02800525166094303\n",
      "Epoch 10, Batch: 340: Training Loss: 0.027637600898742676, Validation Loss: 0.02595703862607479\n",
      "Epoch 10, Batch: 341: Training Loss: 0.024499235674738884, Validation Loss: 0.02859755977988243\n",
      "Epoch 10, Batch: 342: Training Loss: 0.02488729916512966, Validation Loss: 0.028373345732688904\n",
      "Epoch 10, Batch: 343: Training Loss: 0.023117341101169586, Validation Loss: 0.025639014318585396\n",
      "Epoch 10, Batch: 344: Training Loss: 0.025541987270116806, Validation Loss: 0.02714192494750023\n",
      "Epoch 10, Batch: 345: Training Loss: 0.024855535477399826, Validation Loss: 0.027599060907959938\n",
      "Epoch 10, Batch: 346: Training Loss: 0.025307616218924522, Validation Loss: 0.02890882082283497\n",
      "Epoch 10, Batch: 347: Training Loss: 0.02208339236676693, Validation Loss: 0.0290546752512455\n",
      "Epoch 10, Batch: 348: Training Loss: 0.023008210584521294, Validation Loss: 0.025153959169983864\n",
      "Epoch 10, Batch: 349: Training Loss: 0.0287846177816391, Validation Loss: 0.023327220231294632\n",
      "Epoch 10, Batch: 350: Training Loss: 0.022861385717988014, Validation Loss: 0.027603767812252045\n",
      "Epoch 10, Batch: 351: Training Loss: 0.027861829847097397, Validation Loss: 0.024189118295907974\n",
      "Epoch 10, Batch: 352: Training Loss: 0.02868383564054966, Validation Loss: 0.023780755698680878\n",
      "Epoch 10, Batch: 353: Training Loss: 0.024423638358712196, Validation Loss: 0.02495950274169445\n",
      "Epoch 10, Batch: 354: Training Loss: 0.026332983747124672, Validation Loss: 0.024655021727085114\n",
      "Epoch 10, Batch: 355: Training Loss: 0.02300439588725567, Validation Loss: 0.026558665558695793\n",
      "Epoch 10, Batch: 356: Training Loss: 0.02604067698121071, Validation Loss: 0.022941887378692627\n",
      "Epoch 10, Batch: 357: Training Loss: 0.025705451145768166, Validation Loss: 0.026776250451803207\n",
      "Epoch 10, Batch: 358: Training Loss: 0.0271572545170784, Validation Loss: 0.02651457116007805\n",
      "Epoch 10, Batch: 359: Training Loss: 0.027983171865344048, Validation Loss: 0.024883978068828583\n",
      "Epoch 10, Batch: 360: Training Loss: 0.027779243886470795, Validation Loss: 0.02530580200254917\n",
      "Epoch 10, Batch: 361: Training Loss: 0.02601347304880619, Validation Loss: 0.026192912831902504\n",
      "Epoch 10, Batch: 362: Training Loss: 0.022217288613319397, Validation Loss: 0.024442749097943306\n",
      "Epoch 10, Batch: 363: Training Loss: 0.0297559704631567, Validation Loss: 0.02687031775712967\n",
      "Epoch 10, Batch: 364: Training Loss: 0.024955274537205696, Validation Loss: 0.026724494993686676\n",
      "Epoch 10, Batch: 365: Training Loss: 0.02627870813012123, Validation Loss: 0.027155309915542603\n",
      "Epoch 10, Batch: 366: Training Loss: 0.026674868538975716, Validation Loss: 0.028342077508568764\n",
      "Epoch 10, Batch: 367: Training Loss: 0.024333273991942406, Validation Loss: 0.02581644430756569\n",
      "Epoch 10, Batch: 368: Training Loss: 0.023943383246660233, Validation Loss: 0.02423214726150036\n",
      "Epoch 10, Batch: 369: Training Loss: 0.025896219536662102, Validation Loss: 0.02599530853331089\n",
      "Epoch 10, Batch: 370: Training Loss: 0.024751244112849236, Validation Loss: 0.027175404131412506\n",
      "Epoch 10, Batch: 371: Training Loss: 0.02501102164387703, Validation Loss: 0.027063697576522827\n",
      "Epoch 10, Batch: 372: Training Loss: 0.026665236800909042, Validation Loss: 0.025934241712093353\n",
      "Epoch 10, Batch: 373: Training Loss: 0.028592549264431, Validation Loss: 0.024751650169491768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 374: Training Loss: 0.023697949945926666, Validation Loss: 0.024471916258335114\n",
      "Epoch 10, Batch: 375: Training Loss: 0.03130356967449188, Validation Loss: 0.026781020686030388\n",
      "Epoch 10, Batch: 376: Training Loss: 0.024457797408103943, Validation Loss: 0.02714676968753338\n",
      "Epoch 10, Batch: 377: Training Loss: 0.0253404900431633, Validation Loss: 0.02575831301510334\n",
      "Epoch 10, Batch: 378: Training Loss: 0.024738892912864685, Validation Loss: 0.026633283123373985\n",
      "Epoch 10, Batch: 379: Training Loss: 0.0234674159437418, Validation Loss: 0.02482619322836399\n",
      "Epoch 10, Batch: 380: Training Loss: 0.027919599786400795, Validation Loss: 0.026938023045659065\n",
      "Epoch 10, Batch: 381: Training Loss: 0.02465122938156128, Validation Loss: 0.026228992268443108\n",
      "Epoch 10, Batch: 382: Training Loss: 0.02865694835782051, Validation Loss: 0.025912771001458168\n",
      "Epoch 10, Batch: 383: Training Loss: 0.02688674069941044, Validation Loss: 0.028942180797457695\n",
      "Epoch 10, Batch: 384: Training Loss: 0.026461772620677948, Validation Loss: 0.026053451001644135\n",
      "Epoch 10, Batch: 385: Training Loss: 0.0241679847240448, Validation Loss: 0.028559189289808273\n",
      "Epoch 10, Batch: 386: Training Loss: 0.02648363821208477, Validation Loss: 0.028205987066030502\n",
      "Epoch 10, Batch: 387: Training Loss: 0.026190264150500298, Validation Loss: 0.029753563925623894\n",
      "Epoch 10, Batch: 388: Training Loss: 0.021299999207258224, Validation Loss: 0.027885904535651207\n",
      "Epoch 10, Batch: 389: Training Loss: 0.028745489194989204, Validation Loss: 0.030242644250392914\n",
      "Epoch 10, Batch: 390: Training Loss: 0.026403669267892838, Validation Loss: 0.03082459419965744\n",
      "Epoch 10, Batch: 391: Training Loss: 0.02635291963815689, Validation Loss: 0.028610149398446083\n",
      "Epoch 10, Batch: 392: Training Loss: 0.026644079014658928, Validation Loss: 0.03171603009104729\n",
      "Epoch 10, Batch: 393: Training Loss: 0.022438282147049904, Validation Loss: 0.02730843797326088\n",
      "Epoch 10, Batch: 394: Training Loss: 0.025795109570026398, Validation Loss: 0.02583691105246544\n",
      "Epoch 10, Batch: 395: Training Loss: 0.025103192776441574, Validation Loss: 0.029622793197631836\n",
      "Epoch 10, Batch: 396: Training Loss: 0.024071987718343735, Validation Loss: 0.026698291301727295\n",
      "Epoch 10, Batch: 397: Training Loss: 0.0258910171687603, Validation Loss: 0.02466628886759281\n",
      "Epoch 10, Batch: 398: Training Loss: 0.025892112404108047, Validation Loss: 0.02851864881813526\n",
      "Epoch 10, Batch: 399: Training Loss: 0.02646748721599579, Validation Loss: 0.027751758694648743\n",
      "Epoch 10, Batch: 400: Training Loss: 0.02407808043062687, Validation Loss: 0.029438327997922897\n",
      "Epoch 10, Batch: 401: Training Loss: 0.02633776143193245, Validation Loss: 0.027035947889089584\n",
      "Epoch 10, Batch: 402: Training Loss: 0.026662545278668404, Validation Loss: 0.027413543313741684\n",
      "Epoch 10, Batch: 403: Training Loss: 0.03225688263773918, Validation Loss: 0.029742544516921043\n",
      "Epoch 10, Batch: 404: Training Loss: 0.025283927097916603, Validation Loss: 0.02802516333758831\n",
      "Epoch 10, Batch: 405: Training Loss: 0.028280338272452354, Validation Loss: 0.028915435075759888\n",
      "Epoch 10, Batch: 406: Training Loss: 0.027776919305324554, Validation Loss: 0.02690834552049637\n",
      "Epoch 10, Batch: 407: Training Loss: 0.026536623015999794, Validation Loss: 0.025484459474682808\n",
      "Epoch 10, Batch: 408: Training Loss: 0.024036558344960213, Validation Loss: 0.027664242312312126\n",
      "Epoch 10, Batch: 409: Training Loss: 0.028282202780246735, Validation Loss: 0.030438661575317383\n",
      "Epoch 10, Batch: 410: Training Loss: 0.026968633756041527, Validation Loss: 0.02862538769841194\n",
      "Epoch 10, Batch: 411: Training Loss: 0.025496015325188637, Validation Loss: 0.026336519047617912\n",
      "Epoch 10, Batch: 412: Training Loss: 0.026806393638253212, Validation Loss: 0.02751670964062214\n",
      "Epoch 10, Batch: 413: Training Loss: 0.030110318213701248, Validation Loss: 0.02549128048121929\n",
      "Epoch 10, Batch: 414: Training Loss: 0.026442546397447586, Validation Loss: 0.026144621893763542\n",
      "Epoch 10, Batch: 415: Training Loss: 0.027407396584749222, Validation Loss: 0.026714226230978966\n",
      "Epoch 10, Batch: 416: Training Loss: 0.026575108990073204, Validation Loss: 0.025734368711709976\n",
      "Epoch 10, Batch: 417: Training Loss: 0.028046084567904472, Validation Loss: 0.026704534888267517\n",
      "Epoch 10, Batch: 418: Training Loss: 0.024577217176556587, Validation Loss: 0.023148568347096443\n",
      "Epoch 10, Batch: 419: Training Loss: 0.026039430871605873, Validation Loss: 0.02661045640707016\n",
      "Epoch 10, Batch: 420: Training Loss: 0.03479020297527313, Validation Loss: 0.02582119032740593\n",
      "Epoch 10, Batch: 421: Training Loss: 0.033525217324495316, Validation Loss: 0.026216180995106697\n",
      "Epoch 10, Batch: 422: Training Loss: 0.028147390112280846, Validation Loss: 0.026386940851807594\n",
      "Epoch 10, Batch: 423: Training Loss: 0.034275829792022705, Validation Loss: 0.025963766500353813\n",
      "Epoch 10, Batch: 424: Training Loss: 0.03066445142030716, Validation Loss: 0.026011381298303604\n",
      "Epoch 10, Batch: 425: Training Loss: 0.026881059631705284, Validation Loss: 0.027590841054916382\n",
      "Epoch 10, Batch: 426: Training Loss: 0.026804430410265923, Validation Loss: 0.027423623949289322\n",
      "Epoch 10, Batch: 427: Training Loss: 0.026733627542853355, Validation Loss: 0.02824871614575386\n",
      "Epoch 10, Batch: 428: Training Loss: 0.026035809889435768, Validation Loss: 0.02662111446261406\n",
      "Epoch 10, Batch: 429: Training Loss: 0.02624998800456524, Validation Loss: 0.02623460441827774\n",
      "Epoch 10, Batch: 430: Training Loss: 0.023816773667931557, Validation Loss: 0.0267775971442461\n",
      "Epoch 10, Batch: 431: Training Loss: 0.027355922386050224, Validation Loss: 0.025738153606653214\n",
      "Epoch 10, Batch: 432: Training Loss: 0.033012717962265015, Validation Loss: 0.02636183425784111\n",
      "Epoch 10, Batch: 433: Training Loss: 0.026065535843372345, Validation Loss: 0.027273720130324364\n",
      "Epoch 10, Batch: 434: Training Loss: 0.028482694178819656, Validation Loss: 0.027803555130958557\n",
      "Epoch 10, Batch: 435: Training Loss: 0.027746789157390594, Validation Loss: 0.027319742366671562\n",
      "Epoch 10, Batch: 436: Training Loss: 0.024438153952360153, Validation Loss: 0.02713804692029953\n",
      "Epoch 10, Batch: 437: Training Loss: 0.021499762311577797, Validation Loss: 0.029393957927823067\n",
      "Epoch 10, Batch: 438: Training Loss: 0.024371283128857613, Validation Loss: 0.028142213821411133\n",
      "Epoch 10, Batch: 439: Training Loss: 0.024335695430636406, Validation Loss: 0.02900141477584839\n",
      "Epoch 10, Batch: 440: Training Loss: 0.029326247051358223, Validation Loss: 0.026945212855935097\n",
      "Epoch 10, Batch: 441: Training Loss: 0.025785736739635468, Validation Loss: 0.025317154824733734\n",
      "Epoch 10, Batch: 442: Training Loss: 0.02621011808514595, Validation Loss: 0.025459757074713707\n",
      "Epoch 10, Batch: 443: Training Loss: 0.02666563168168068, Validation Loss: 0.025335166603326797\n",
      "Epoch 10, Batch: 444: Training Loss: 0.02534107305109501, Validation Loss: 0.02513102814555168\n",
      "Epoch 10, Batch: 445: Training Loss: 0.023274537175893784, Validation Loss: 0.024356434121727943\n",
      "Epoch 10, Batch: 446: Training Loss: 0.026094095781445503, Validation Loss: 0.02693105675280094\n",
      "Epoch 10, Batch: 447: Training Loss: 0.024930475279688835, Validation Loss: 0.025343332439661026\n",
      "Epoch 10, Batch: 448: Training Loss: 0.02343226596713066, Validation Loss: 0.026356616988778114\n",
      "Epoch 10, Batch: 449: Training Loss: 0.024062395095825195, Validation Loss: 0.028781762346625328\n",
      "Epoch 10, Batch: 450: Training Loss: 0.024952996522188187, Validation Loss: 0.027282577008008957\n",
      "Epoch 10, Batch: 451: Training Loss: 0.02569354511797428, Validation Loss: 0.023871542885899544\n",
      "Epoch 10, Batch: 452: Training Loss: 0.02793894149363041, Validation Loss: 0.02572704292833805\n",
      "Epoch 10, Batch: 453: Training Loss: 0.0266824122518301, Validation Loss: 0.02822631597518921\n",
      "Epoch 10, Batch: 454: Training Loss: 0.02536328323185444, Validation Loss: 0.02947305515408516\n",
      "Epoch 10, Batch: 455: Training Loss: 0.025821560993790627, Validation Loss: 0.02925237827003002\n",
      "Epoch 10, Batch: 456: Training Loss: 0.02683255635201931, Validation Loss: 0.02578037977218628\n",
      "Epoch 10, Batch: 457: Training Loss: 0.02628052607178688, Validation Loss: 0.027042344212532043\n",
      "Epoch 10, Batch: 458: Training Loss: 0.021503761410713196, Validation Loss: 0.026449555531144142\n",
      "Epoch 10, Batch: 459: Training Loss: 0.027088353410363197, Validation Loss: 0.026423146948218346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch: 460: Training Loss: 0.023415107280015945, Validation Loss: 0.024288559332489967\n",
      "Epoch 10, Batch: 461: Training Loss: 0.025628508999943733, Validation Loss: 0.028100688010454178\n",
      "Epoch 10, Batch: 462: Training Loss: 0.02382192574441433, Validation Loss: 0.026615718379616737\n",
      "Epoch 10, Batch: 463: Training Loss: 0.025585388764739037, Validation Loss: 0.025913890451192856\n",
      "Epoch 10, Batch: 464: Training Loss: 0.02329922281205654, Validation Loss: 0.02557525411248207\n",
      "Epoch 10, Batch: 465: Training Loss: 0.023811981081962585, Validation Loss: 0.02554204873740673\n",
      "Epoch 10, Batch: 466: Training Loss: 0.021381860598921776, Validation Loss: 0.026244573295116425\n",
      "Epoch 10, Batch: 467: Training Loss: 0.03278086706995964, Validation Loss: 0.025606794282794\n",
      "Epoch 10, Batch: 468: Training Loss: 0.02983366698026657, Validation Loss: 0.02576802857220173\n",
      "Epoch 10, Batch: 469: Training Loss: 0.027411090210080147, Validation Loss: 0.026020260527729988\n",
      "Epoch 10, Batch: 470: Training Loss: 0.02317364513874054, Validation Loss: 0.025213781744241714\n",
      "Epoch 10, Batch: 471: Training Loss: 0.02683088928461075, Validation Loss: 0.02430189587175846\n",
      "Epoch 10, Batch: 472: Training Loss: 0.02610456943511963, Validation Loss: 0.024220669642090797\n",
      "Epoch 10, Batch: 473: Training Loss: 0.025108933448791504, Validation Loss: 0.023591812700033188\n",
      "Epoch 10, Batch: 474: Training Loss: 0.024811837822198868, Validation Loss: 0.02544224075973034\n",
      "Epoch 10, Batch: 475: Training Loss: 0.026849376037716866, Validation Loss: 0.026959525421261787\n",
      "Epoch 10, Batch: 476: Training Loss: 0.027542324736714363, Validation Loss: 0.02468148246407509\n",
      "Epoch 10, Batch: 477: Training Loss: 0.03030814416706562, Validation Loss: 0.02603154070675373\n",
      "Epoch 10, Batch: 478: Training Loss: 0.023688355460762978, Validation Loss: 0.02383388765156269\n",
      "Epoch 10, Batch: 479: Training Loss: 0.027087442576885223, Validation Loss: 0.025839736685156822\n",
      "Epoch 10, Batch: 480: Training Loss: 0.024578824639320374, Validation Loss: 0.02554645575582981\n",
      "Epoch 10, Batch: 481: Training Loss: 0.02587602473795414, Validation Loss: 0.02342735417187214\n",
      "Epoch 10, Batch: 482: Training Loss: 0.029873639345169067, Validation Loss: 0.02413124404847622\n",
      "Epoch 10, Batch: 483: Training Loss: 0.025386860594153404, Validation Loss: 0.025631902739405632\n",
      "Epoch 10, Batch: 484: Training Loss: 0.02872464247047901, Validation Loss: 0.027743753045797348\n",
      "Epoch 10, Batch: 485: Training Loss: 0.02452775277197361, Validation Loss: 0.02498314157128334\n",
      "Epoch 10, Batch: 486: Training Loss: 0.030616968870162964, Validation Loss: 0.026950426399707794\n",
      "Epoch 10, Batch: 487: Training Loss: 0.025716405361890793, Validation Loss: 0.02633402682840824\n",
      "Epoch 10, Batch: 488: Training Loss: 0.026142241433262825, Validation Loss: 0.02481345273554325\n",
      "Epoch 10, Batch: 489: Training Loss: 0.033900924026966095, Validation Loss: 0.027019158005714417\n",
      "Epoch 10, Batch: 490: Training Loss: 0.02582383342087269, Validation Loss: 0.025896353647112846\n",
      "Epoch 10, Batch: 491: Training Loss: 0.022582119330763817, Validation Loss: 0.026044795289635658\n",
      "Epoch 10, Batch: 492: Training Loss: 0.025601286441087723, Validation Loss: 0.0239452812820673\n",
      "Epoch 10, Batch: 493: Training Loss: 0.026303105056285858, Validation Loss: 0.023044072091579437\n",
      "Epoch 10, Batch: 494: Training Loss: 0.031924743205308914, Validation Loss: 0.024997850880026817\n",
      "Epoch 10, Batch: 495: Training Loss: 0.02494886890053749, Validation Loss: 0.024647600948810577\n",
      "Epoch 10, Batch: 496: Training Loss: 0.023306820541620255, Validation Loss: 0.02617701143026352\n",
      "Epoch 10, Batch: 497: Training Loss: 0.025424135848879814, Validation Loss: 0.026789113879203796\n",
      "Epoch 10, Batch: 498: Training Loss: 0.02722230553627014, Validation Loss: 0.024980582296848297\n",
      "Epoch 10, Batch: 499: Training Loss: 0.022250071167945862, Validation Loss: 0.023851120844483376\n",
      "Epoch 11, Batch: 0: Training Loss: 0.025769000872969627, Validation Loss: 0.0252499058842659\n",
      "Epoch 11, Batch: 1: Training Loss: 0.02671368606388569, Validation Loss: 0.02728823386132717\n",
      "Epoch 11, Batch: 2: Training Loss: 0.028137730434536934, Validation Loss: 0.028292883187532425\n",
      "Epoch 11, Batch: 3: Training Loss: 0.019140932708978653, Validation Loss: 0.026132509112358093\n",
      "Epoch 11, Batch: 4: Training Loss: 0.022649893537163734, Validation Loss: 0.028558051213622093\n",
      "Epoch 11, Batch: 5: Training Loss: 0.023604121059179306, Validation Loss: 0.026439234614372253\n",
      "Epoch 11, Batch: 6: Training Loss: 0.024418486282229424, Validation Loss: 0.025025350973010063\n",
      "Epoch 11, Batch: 7: Training Loss: 0.023768233135342598, Validation Loss: 0.02628767117857933\n",
      "Epoch 11, Batch: 8: Training Loss: 0.024407589808106422, Validation Loss: 0.02968749962747097\n",
      "Epoch 11, Batch: 9: Training Loss: 0.022294944152235985, Validation Loss: 0.02736765332520008\n",
      "Epoch 11, Batch: 10: Training Loss: 0.022127993404865265, Validation Loss: 0.026564667001366615\n",
      "Epoch 11, Batch: 11: Training Loss: 0.02488759346306324, Validation Loss: 0.02520168572664261\n",
      "Epoch 11, Batch: 12: Training Loss: 0.02965092472732067, Validation Loss: 0.028217695653438568\n",
      "Epoch 11, Batch: 13: Training Loss: 0.02692745253443718, Validation Loss: 0.028971465304493904\n",
      "Epoch 11, Batch: 14: Training Loss: 0.027071651071310043, Validation Loss: 0.026681428775191307\n",
      "Epoch 11, Batch: 15: Training Loss: 0.02569786086678505, Validation Loss: 0.02726505696773529\n",
      "Epoch 11, Batch: 16: Training Loss: 0.026690417900681496, Validation Loss: 0.02768777124583721\n",
      "Epoch 11, Batch: 17: Training Loss: 0.021664751693606377, Validation Loss: 0.025550520047545433\n",
      "Epoch 11, Batch: 18: Training Loss: 0.02433049865067005, Validation Loss: 0.025850653648376465\n",
      "Epoch 11, Batch: 19: Training Loss: 0.022243956103920937, Validation Loss: 0.02584291435778141\n",
      "Epoch 11, Batch: 20: Training Loss: 0.028531478717923164, Validation Loss: 0.025047432631254196\n",
      "Epoch 11, Batch: 21: Training Loss: 0.02935357578098774, Validation Loss: 0.025510510429739952\n",
      "Epoch 11, Batch: 22: Training Loss: 0.02567346766591072, Validation Loss: 0.02782372385263443\n",
      "Epoch 11, Batch: 23: Training Loss: 0.023593803867697716, Validation Loss: 0.025660403072834015\n",
      "Epoch 11, Batch: 24: Training Loss: 0.023900840431451797, Validation Loss: 0.025302674621343613\n",
      "Epoch 11, Batch: 25: Training Loss: 0.022500328719615936, Validation Loss: 0.02384498529136181\n",
      "Epoch 11, Batch: 26: Training Loss: 0.026173293590545654, Validation Loss: 0.024399269372224808\n",
      "Epoch 11, Batch: 27: Training Loss: 0.022767778486013412, Validation Loss: 0.024951037019491196\n",
      "Epoch 11, Batch: 28: Training Loss: 0.029709869995713234, Validation Loss: 0.02674456313252449\n",
      "Epoch 11, Batch: 29: Training Loss: 0.025323057547211647, Validation Loss: 0.025070223957300186\n",
      "Epoch 11, Batch: 30: Training Loss: 0.024836478754878044, Validation Loss: 0.023113863542675972\n",
      "Epoch 11, Batch: 31: Training Loss: 0.028506336733698845, Validation Loss: 0.025268960744142532\n",
      "Epoch 11, Batch: 32: Training Loss: 0.029765203595161438, Validation Loss: 0.02557855285704136\n",
      "Epoch 11, Batch: 33: Training Loss: 0.024355769157409668, Validation Loss: 0.022432275116443634\n",
      "Epoch 11, Batch: 34: Training Loss: 0.024264410138130188, Validation Loss: 0.026356838643550873\n",
      "Epoch 11, Batch: 35: Training Loss: 0.028436992317438126, Validation Loss: 0.02616983838379383\n",
      "Epoch 11, Batch: 36: Training Loss: 0.025084197521209717, Validation Loss: 0.025071829557418823\n",
      "Epoch 11, Batch: 37: Training Loss: 0.023442016914486885, Validation Loss: 0.026308709755539894\n",
      "Epoch 11, Batch: 38: Training Loss: 0.027491897344589233, Validation Loss: 0.027506228536367416\n",
      "Epoch 11, Batch: 39: Training Loss: 0.02808387391269207, Validation Loss: 0.02457824908196926\n",
      "Epoch 11, Batch: 40: Training Loss: 0.025107743218541145, Validation Loss: 0.025355007499456406\n",
      "Epoch 11, Batch: 41: Training Loss: 0.02458236552774906, Validation Loss: 0.025061221793293953\n",
      "Epoch 11, Batch: 42: Training Loss: 0.022309323772788048, Validation Loss: 0.024976519867777824\n",
      "Epoch 11, Batch: 43: Training Loss: 0.02179858274757862, Validation Loss: 0.025247612968087196\n",
      "Epoch 11, Batch: 44: Training Loss: 0.023780712857842445, Validation Loss: 0.0227937251329422\n",
      "Epoch 11, Batch: 45: Training Loss: 0.02391699329018593, Validation Loss: 0.027012260630726814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 46: Training Loss: 0.027244532480835915, Validation Loss: 0.024260500445961952\n",
      "Epoch 11, Batch: 47: Training Loss: 0.025833116844296455, Validation Loss: 0.025166120380163193\n",
      "Epoch 11, Batch: 48: Training Loss: 0.031076332554221153, Validation Loss: 0.02407838962972164\n",
      "Epoch 11, Batch: 49: Training Loss: 0.02556188404560089, Validation Loss: 0.02594674378633499\n",
      "Epoch 11, Batch: 50: Training Loss: 0.024547332897782326, Validation Loss: 0.029027095064520836\n",
      "Epoch 11, Batch: 51: Training Loss: 0.026822810992598534, Validation Loss: 0.02569827437400818\n",
      "Epoch 11, Batch: 52: Training Loss: 0.02403789944946766, Validation Loss: 0.026951143518090248\n",
      "Epoch 11, Batch: 53: Training Loss: 0.024076087400317192, Validation Loss: 0.024921493604779243\n",
      "Epoch 11, Batch: 54: Training Loss: 0.025177588686347008, Validation Loss: 0.024991577491164207\n",
      "Epoch 11, Batch: 55: Training Loss: 0.02209099382162094, Validation Loss: 0.028593657538294792\n",
      "Epoch 11, Batch: 56: Training Loss: 0.03049931488931179, Validation Loss: 0.02457502670586109\n",
      "Epoch 11, Batch: 57: Training Loss: 0.0268761795014143, Validation Loss: 0.025219926610589027\n",
      "Epoch 11, Batch: 58: Training Loss: 0.026052262634038925, Validation Loss: 0.024907568469643593\n",
      "Epoch 11, Batch: 59: Training Loss: 0.026015963405370712, Validation Loss: 0.02304522879421711\n",
      "Epoch 11, Batch: 60: Training Loss: 0.0286015085875988, Validation Loss: 0.02480974607169628\n",
      "Epoch 11, Batch: 61: Training Loss: 0.028485937044024467, Validation Loss: 0.027880126610398293\n",
      "Epoch 11, Batch: 62: Training Loss: 0.024675996974110603, Validation Loss: 0.025404922664165497\n",
      "Epoch 11, Batch: 63: Training Loss: 0.02455895207822323, Validation Loss: 0.02651098184287548\n",
      "Epoch 11, Batch: 64: Training Loss: 0.023767398670315742, Validation Loss: 0.024296186864376068\n",
      "Epoch 11, Batch: 65: Training Loss: 0.028427084907889366, Validation Loss: 0.026620743796229362\n",
      "Epoch 11, Batch: 66: Training Loss: 0.025066539645195007, Validation Loss: 0.026127193123102188\n",
      "Epoch 11, Batch: 67: Training Loss: 0.022713402286171913, Validation Loss: 0.026718685403466225\n",
      "Epoch 11, Batch: 68: Training Loss: 0.02783510833978653, Validation Loss: 0.02649994008243084\n",
      "Epoch 11, Batch: 69: Training Loss: 0.02421710640192032, Validation Loss: 0.02562635764479637\n",
      "Epoch 11, Batch: 70: Training Loss: 0.02510666288435459, Validation Loss: 0.026624972000718117\n",
      "Epoch 11, Batch: 71: Training Loss: 0.021383382380008698, Validation Loss: 0.024999285116791725\n",
      "Epoch 11, Batch: 72: Training Loss: 0.02732923813164234, Validation Loss: 0.02679304964840412\n",
      "Epoch 11, Batch: 73: Training Loss: 0.02350403554737568, Validation Loss: 0.026626868173480034\n",
      "Epoch 11, Batch: 74: Training Loss: 0.02284306474030018, Validation Loss: 0.025253966450691223\n",
      "Epoch 11, Batch: 75: Training Loss: 0.020375773310661316, Validation Loss: 0.025368697941303253\n",
      "Epoch 11, Batch: 76: Training Loss: 0.022071385756134987, Validation Loss: 0.024412134662270546\n",
      "Epoch 11, Batch: 77: Training Loss: 0.028136909008026123, Validation Loss: 0.024418991059064865\n",
      "Epoch 11, Batch: 78: Training Loss: 0.031415555626153946, Validation Loss: 0.02390424720942974\n",
      "Epoch 11, Batch: 79: Training Loss: 0.024692006409168243, Validation Loss: 0.026513412594795227\n",
      "Saving new best model w/ loss: 0.02172037586569786\n",
      "Epoch 11, Batch: 80: Training Loss: 0.023333696648478508, Validation Loss: 0.02172037586569786\n",
      "Epoch 11, Batch: 81: Training Loss: 0.0271003358066082, Validation Loss: 0.02435867115855217\n",
      "Epoch 11, Batch: 82: Training Loss: 0.025645077228546143, Validation Loss: 0.024847345426678658\n",
      "Epoch 11, Batch: 83: Training Loss: 0.023232782259583473, Validation Loss: 0.022566968575119972\n",
      "Epoch 11, Batch: 84: Training Loss: 0.025367239490151405, Validation Loss: 0.025756537914276123\n",
      "Epoch 11, Batch: 85: Training Loss: 0.0239529050886631, Validation Loss: 0.022240707650780678\n",
      "Epoch 11, Batch: 86: Training Loss: 0.02378881350159645, Validation Loss: 0.022637108340859413\n",
      "Epoch 11, Batch: 87: Training Loss: 0.025255508720874786, Validation Loss: 0.024288872256875038\n",
      "Epoch 11, Batch: 88: Training Loss: 0.02764148823916912, Validation Loss: 0.023177916184067726\n",
      "Epoch 11, Batch: 89: Training Loss: 0.0299584548920393, Validation Loss: 0.022679360583424568\n",
      "Epoch 11, Batch: 90: Training Loss: 0.025684978812932968, Validation Loss: 0.025159351527690887\n",
      "Epoch 11, Batch: 91: Training Loss: 0.030832361429929733, Validation Loss: 0.02704480290412903\n",
      "Epoch 11, Batch: 92: Training Loss: 0.03176200017333031, Validation Loss: 0.02386314980685711\n",
      "Epoch 11, Batch: 93: Training Loss: 0.027564339339733124, Validation Loss: 0.02632942795753479\n",
      "Epoch 11, Batch: 94: Training Loss: 0.028604213148355484, Validation Loss: 0.024931471794843674\n",
      "Epoch 11, Batch: 95: Training Loss: 0.023784272372722626, Validation Loss: 0.0247549619525671\n",
      "Epoch 11, Batch: 96: Training Loss: 0.020848209038376808, Validation Loss: 0.025964993983507156\n",
      "Saving new best model w/ loss: 0.021334705874323845\n",
      "Epoch 11, Batch: 97: Training Loss: 0.024098146706819534, Validation Loss: 0.021334705874323845\n",
      "Epoch 11, Batch: 98: Training Loss: 0.03008248098194599, Validation Loss: 0.022826146334409714\n",
      "Epoch 11, Batch: 99: Training Loss: 0.0209662988781929, Validation Loss: 0.022567497566342354\n",
      "Epoch 11, Batch: 100: Training Loss: 0.025489266961812973, Validation Loss: 0.022704146802425385\n",
      "Epoch 11, Batch: 101: Training Loss: 0.022324426099658012, Validation Loss: 0.025085480883717537\n",
      "Epoch 11, Batch: 102: Training Loss: 0.022738369181752205, Validation Loss: 0.024036500602960587\n",
      "Epoch 11, Batch: 103: Training Loss: 0.02557513304054737, Validation Loss: 0.022604843601584435\n",
      "Epoch 11, Batch: 104: Training Loss: 0.023612292483448982, Validation Loss: 0.02194611355662346\n",
      "Epoch 11, Batch: 105: Training Loss: 0.02114841155707836, Validation Loss: 0.02694074437022209\n",
      "Epoch 11, Batch: 106: Training Loss: 0.024074053391814232, Validation Loss: 0.023196503520011902\n",
      "Epoch 11, Batch: 107: Training Loss: 0.024646349251270294, Validation Loss: 0.022109176963567734\n",
      "Saving new best model w/ loss: 0.02102705091238022\n",
      "Epoch 11, Batch: 108: Training Loss: 0.023432446643710136, Validation Loss: 0.02102705091238022\n",
      "Epoch 11, Batch: 109: Training Loss: 0.027868149802088737, Validation Loss: 0.022030919790267944\n",
      "Epoch 11, Batch: 110: Training Loss: 0.02675950713455677, Validation Loss: 0.022749096155166626\n",
      "Epoch 11, Batch: 111: Training Loss: 0.022961661219596863, Validation Loss: 0.023945096880197525\n",
      "Epoch 11, Batch: 112: Training Loss: 0.0243136715143919, Validation Loss: 0.023558642715215683\n",
      "Epoch 11, Batch: 113: Training Loss: 0.0250322837382555, Validation Loss: 0.02305017039179802\n",
      "Epoch 11, Batch: 114: Training Loss: 0.02710595354437828, Validation Loss: 0.02462075650691986\n",
      "Epoch 11, Batch: 115: Training Loss: 0.022845938801765442, Validation Loss: 0.0234976839274168\n",
      "Epoch 11, Batch: 116: Training Loss: 0.023445090278983116, Validation Loss: 0.02231256291270256\n",
      "Saving new best model w/ loss: 0.020818719640374184\n",
      "Epoch 11, Batch: 117: Training Loss: 0.02534547448158264, Validation Loss: 0.020818719640374184\n",
      "Epoch 11, Batch: 118: Training Loss: 0.02036304958164692, Validation Loss: 0.0227800365537405\n",
      "Epoch 11, Batch: 119: Training Loss: 0.024609867483377457, Validation Loss: 0.02231575921177864\n",
      "Epoch 11, Batch: 120: Training Loss: 0.023686302825808525, Validation Loss: 0.0230900589376688\n",
      "Epoch 11, Batch: 121: Training Loss: 0.026268545538187027, Validation Loss: 0.023608285933732986\n",
      "Epoch 11, Batch: 122: Training Loss: 0.02705540508031845, Validation Loss: 0.023461170494556427\n",
      "Epoch 11, Batch: 123: Training Loss: 0.027097653597593307, Validation Loss: 0.02483624778687954\n",
      "Epoch 11, Batch: 124: Training Loss: 0.02327125146985054, Validation Loss: 0.02423064038157463\n",
      "Epoch 11, Batch: 125: Training Loss: 0.02347102016210556, Validation Loss: 0.027709204703569412\n",
      "Epoch 11, Batch: 126: Training Loss: 0.02508390322327614, Validation Loss: 0.024738851934671402\n",
      "Epoch 11, Batch: 127: Training Loss: 0.025893820449709892, Validation Loss: 0.027114935219287872\n",
      "Epoch 11, Batch: 128: Training Loss: 0.025036083534359932, Validation Loss: 0.02732263132929802\n",
      "Epoch 11, Batch: 129: Training Loss: 0.02303367480635643, Validation Loss: 0.02832336537539959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 130: Training Loss: 0.02903558313846588, Validation Loss: 0.02594323828816414\n",
      "Epoch 11, Batch: 131: Training Loss: 0.02306692861020565, Validation Loss: 0.025310812518000603\n",
      "Epoch 11, Batch: 132: Training Loss: 0.022588320076465607, Validation Loss: 0.02653142437338829\n",
      "Epoch 11, Batch: 133: Training Loss: 0.024453721940517426, Validation Loss: 0.027677984908223152\n",
      "Epoch 11, Batch: 134: Training Loss: 0.021554743871092796, Validation Loss: 0.02590705081820488\n",
      "Epoch 11, Batch: 135: Training Loss: 0.02382155880331993, Validation Loss: 0.026260336861014366\n",
      "Epoch 11, Batch: 136: Training Loss: 0.024521851912140846, Validation Loss: 0.026609187945723534\n",
      "Epoch 11, Batch: 137: Training Loss: 0.021881233900785446, Validation Loss: 0.02464808151125908\n",
      "Epoch 11, Batch: 138: Training Loss: 0.02516046352684498, Validation Loss: 0.0255868062376976\n",
      "Epoch 11, Batch: 139: Training Loss: 0.021272804588079453, Validation Loss: 0.026384752243757248\n",
      "Epoch 11, Batch: 140: Training Loss: 0.032472267746925354, Validation Loss: 0.02422596700489521\n",
      "Epoch 11, Batch: 141: Training Loss: 0.024911968037486076, Validation Loss: 0.022378526628017426\n",
      "Epoch 11, Batch: 142: Training Loss: 0.0224144384264946, Validation Loss: 0.02269583009183407\n",
      "Epoch 11, Batch: 143: Training Loss: 0.02409667707979679, Validation Loss: 0.023102594539523125\n",
      "Epoch 11, Batch: 144: Training Loss: 0.023670600727200508, Validation Loss: 0.022629227489233017\n",
      "Epoch 11, Batch: 145: Training Loss: 0.022321471944451332, Validation Loss: 0.02226843498647213\n",
      "Epoch 11, Batch: 146: Training Loss: 0.024221720173954964, Validation Loss: 0.0243033766746521\n",
      "Epoch 11, Batch: 147: Training Loss: 0.023493174463510513, Validation Loss: 0.02227840945124626\n",
      "Epoch 11, Batch: 148: Training Loss: 0.02401905134320259, Validation Loss: 0.02350631356239319\n",
      "Saving new best model w/ loss: 0.020400967448949814\n",
      "Epoch 11, Batch: 149: Training Loss: 0.02507842890918255, Validation Loss: 0.020400967448949814\n",
      "Epoch 11, Batch: 150: Training Loss: 0.027957096695899963, Validation Loss: 0.02241077460348606\n",
      "Epoch 11, Batch: 151: Training Loss: 0.023149799555540085, Validation Loss: 0.02402997389435768\n",
      "Epoch 11, Batch: 152: Training Loss: 0.026028970256447792, Validation Loss: 0.024955065920948982\n",
      "Epoch 11, Batch: 153: Training Loss: 0.024714084342122078, Validation Loss: 0.02282913401722908\n",
      "Epoch 11, Batch: 154: Training Loss: 0.030145829543471336, Validation Loss: 0.021393688395619392\n",
      "Saving new best model w/ loss: 0.02036193199455738\n",
      "Epoch 11, Batch: 155: Training Loss: 0.03470452502369881, Validation Loss: 0.02036193199455738\n",
      "Epoch 11, Batch: 156: Training Loss: 0.023819271475076675, Validation Loss: 0.02156888321042061\n",
      "Epoch 11, Batch: 157: Training Loss: 0.024276606738567352, Validation Loss: 0.023001231253147125\n",
      "Epoch 11, Batch: 158: Training Loss: 0.025218389928340912, Validation Loss: 0.02496136911213398\n",
      "Epoch 11, Batch: 159: Training Loss: 0.02493453584611416, Validation Loss: 0.02367377281188965\n",
      "Epoch 11, Batch: 160: Training Loss: 0.026297859847545624, Validation Loss: 0.02159596048295498\n",
      "Epoch 11, Batch: 161: Training Loss: 0.023556072264909744, Validation Loss: 0.02190423011779785\n",
      "Epoch 11, Batch: 162: Training Loss: 0.027031321078538895, Validation Loss: 0.024861447513103485\n",
      "Epoch 11, Batch: 163: Training Loss: 0.02663360722362995, Validation Loss: 0.022543320432305336\n",
      "Epoch 11, Batch: 164: Training Loss: 0.02298811823129654, Validation Loss: 0.024300333112478256\n",
      "Epoch 11, Batch: 165: Training Loss: 0.028975224122405052, Validation Loss: 0.024513138458132744\n",
      "Epoch 11, Batch: 166: Training Loss: 0.019984843209385872, Validation Loss: 0.02323588915169239\n",
      "Epoch 11, Batch: 167: Training Loss: 0.02413153275847435, Validation Loss: 0.02403668500483036\n",
      "Epoch 11, Batch: 168: Training Loss: 0.02473176270723343, Validation Loss: 0.022121857851743698\n",
      "Epoch 11, Batch: 169: Training Loss: 0.0238446407020092, Validation Loss: 0.02131718024611473\n",
      "Epoch 11, Batch: 170: Training Loss: 0.022271592170000076, Validation Loss: 0.021645329892635345\n",
      "Epoch 11, Batch: 171: Training Loss: 0.022321535274386406, Validation Loss: 0.02301541157066822\n",
      "Epoch 11, Batch: 172: Training Loss: 0.023369954898953438, Validation Loss: 0.021924564614892006\n",
      "Epoch 11, Batch: 173: Training Loss: 0.020506734028458595, Validation Loss: 0.023168427869677544\n",
      "Epoch 11, Batch: 174: Training Loss: 0.029521334916353226, Validation Loss: 0.02322714775800705\n",
      "Epoch 11, Batch: 175: Training Loss: 0.023380862548947334, Validation Loss: 0.023244531825184822\n",
      "Epoch 11, Batch: 176: Training Loss: 0.02605004981160164, Validation Loss: 0.021420344710350037\n",
      "Epoch 11, Batch: 177: Training Loss: 0.022195985540747643, Validation Loss: 0.021203020587563515\n",
      "Epoch 11, Batch: 178: Training Loss: 0.0297342911362648, Validation Loss: 0.0231612678617239\n",
      "Epoch 11, Batch: 179: Training Loss: 0.025206340476870537, Validation Loss: 0.022657295688986778\n",
      "Epoch 11, Batch: 180: Training Loss: 0.025252241641283035, Validation Loss: 0.022613296285271645\n",
      "Epoch 11, Batch: 181: Training Loss: 0.02281058020889759, Validation Loss: 0.0246761292219162\n",
      "Epoch 11, Batch: 182: Training Loss: 0.025351427495479584, Validation Loss: 0.02317676693201065\n",
      "Epoch 11, Batch: 183: Training Loss: 0.025224706158041954, Validation Loss: 0.022410079836845398\n",
      "Epoch 11, Batch: 184: Training Loss: 0.027388127520680428, Validation Loss: 0.022575560957193375\n",
      "Epoch 11, Batch: 185: Training Loss: 0.02779528498649597, Validation Loss: 0.025660119950771332\n",
      "Epoch 11, Batch: 186: Training Loss: 0.026976270601153374, Validation Loss: 0.02263871766626835\n",
      "Epoch 11, Batch: 187: Training Loss: 0.02422136254608631, Validation Loss: 0.0236201249063015\n",
      "Epoch 11, Batch: 188: Training Loss: 0.027384521439671516, Validation Loss: 0.026883002370595932\n",
      "Epoch 11, Batch: 189: Training Loss: 0.022386299446225166, Validation Loss: 0.02504248172044754\n",
      "Epoch 11, Batch: 190: Training Loss: 0.022242441773414612, Validation Loss: 0.02441992610692978\n",
      "Epoch 11, Batch: 191: Training Loss: 0.027202343568205833, Validation Loss: 0.02360120229423046\n",
      "Epoch 11, Batch: 192: Training Loss: 0.025218555703759193, Validation Loss: 0.024308474734425545\n",
      "Epoch 11, Batch: 193: Training Loss: 0.022774547338485718, Validation Loss: 0.024872440844774246\n",
      "Epoch 11, Batch: 194: Training Loss: 0.02532276324927807, Validation Loss: 0.02264420874416828\n",
      "Epoch 11, Batch: 195: Training Loss: 0.023654233664274216, Validation Loss: 0.023312561213970184\n",
      "Epoch 11, Batch: 196: Training Loss: 0.02372046373784542, Validation Loss: 0.02311316318809986\n",
      "Epoch 11, Batch: 197: Training Loss: 0.022518448531627655, Validation Loss: 0.02574555017054081\n",
      "Epoch 11, Batch: 198: Training Loss: 0.0234997421503067, Validation Loss: 0.023962438106536865\n",
      "Epoch 11, Batch: 199: Training Loss: 0.023705897852778435, Validation Loss: 0.02602909319102764\n",
      "Epoch 11, Batch: 200: Training Loss: 0.01983967423439026, Validation Loss: 0.024972224608063698\n",
      "Epoch 11, Batch: 201: Training Loss: 0.025917237624526024, Validation Loss: 0.022474512457847595\n",
      "Epoch 11, Batch: 202: Training Loss: 0.023766130208969116, Validation Loss: 0.026008090004324913\n",
      "Epoch 11, Batch: 203: Training Loss: 0.02743850275874138, Validation Loss: 0.022578569129109383\n",
      "Epoch 11, Batch: 204: Training Loss: 0.028543030843138695, Validation Loss: 0.02666168473660946\n",
      "Epoch 11, Batch: 205: Training Loss: 0.03043440915644169, Validation Loss: 0.025273561477661133\n",
      "Epoch 11, Batch: 206: Training Loss: 0.023673543706536293, Validation Loss: 0.024950996041297913\n",
      "Epoch 11, Batch: 207: Training Loss: 0.028394551947712898, Validation Loss: 0.02297123335301876\n",
      "Epoch 11, Batch: 208: Training Loss: 0.02778196893632412, Validation Loss: 0.023310599848628044\n",
      "Epoch 11, Batch: 209: Training Loss: 0.024432595819234848, Validation Loss: 0.023688076063990593\n",
      "Epoch 11, Batch: 210: Training Loss: 0.026823433116078377, Validation Loss: 0.024030447006225586\n",
      "Epoch 11, Batch: 211: Training Loss: 0.02489473856985569, Validation Loss: 0.023925593122839928\n",
      "Epoch 11, Batch: 212: Training Loss: 0.02492774836719036, Validation Loss: 0.027600660920143127\n",
      "Epoch 11, Batch: 213: Training Loss: 0.024837935343384743, Validation Loss: 0.023855190724134445\n",
      "Epoch 11, Batch: 214: Training Loss: 0.022592153400182724, Validation Loss: 0.026711763814091682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 215: Training Loss: 0.026273032650351524, Validation Loss: 0.022736938670277596\n",
      "Epoch 11, Batch: 216: Training Loss: 0.02482074499130249, Validation Loss: 0.0266626738011837\n",
      "Epoch 11, Batch: 217: Training Loss: 0.023176060989499092, Validation Loss: 0.024362346157431602\n",
      "Epoch 11, Batch: 218: Training Loss: 0.022816499695181847, Validation Loss: 0.022550877183675766\n",
      "Epoch 11, Batch: 219: Training Loss: 0.026877807453274727, Validation Loss: 0.023759838193655014\n",
      "Epoch 11, Batch: 220: Training Loss: 0.023072628304362297, Validation Loss: 0.023782916367053986\n",
      "Epoch 11, Batch: 221: Training Loss: 0.027473757043480873, Validation Loss: 0.025437813252210617\n",
      "Epoch 11, Batch: 222: Training Loss: 0.02354242466390133, Validation Loss: 0.02503449097275734\n",
      "Epoch 11, Batch: 223: Training Loss: 0.02405972220003605, Validation Loss: 0.02587764896452427\n",
      "Epoch 11, Batch: 224: Training Loss: 0.025130238384008408, Validation Loss: 0.02538626454770565\n",
      "Epoch 11, Batch: 225: Training Loss: 0.02184336818754673, Validation Loss: 0.02434268407523632\n",
      "Epoch 11, Batch: 226: Training Loss: 0.02516103908419609, Validation Loss: 0.022707166150212288\n",
      "Epoch 11, Batch: 227: Training Loss: 0.022305894643068314, Validation Loss: 0.02188967727124691\n",
      "Epoch 11, Batch: 228: Training Loss: 0.0244880523532629, Validation Loss: 0.023019125685095787\n",
      "Epoch 11, Batch: 229: Training Loss: 0.02406124584376812, Validation Loss: 0.021369870752096176\n",
      "Epoch 11, Batch: 230: Training Loss: 0.023102231323719025, Validation Loss: 0.02255316451191902\n",
      "Epoch 11, Batch: 231: Training Loss: 0.025812556967139244, Validation Loss: 0.02233102358877659\n",
      "Epoch 11, Batch: 232: Training Loss: 0.02710976079106331, Validation Loss: 0.022490324452519417\n",
      "Epoch 11, Batch: 233: Training Loss: 0.022356027737259865, Validation Loss: 0.024705206975340843\n",
      "Epoch 11, Batch: 234: Training Loss: 0.02670181170105934, Validation Loss: 0.023833375424146652\n",
      "Epoch 11, Batch: 235: Training Loss: 0.030292201787233353, Validation Loss: 0.024266159161925316\n",
      "Epoch 11, Batch: 236: Training Loss: 0.024564465507864952, Validation Loss: 0.02359624393284321\n",
      "Epoch 11, Batch: 237: Training Loss: 0.02649027295410633, Validation Loss: 0.027603240683674812\n",
      "Epoch 11, Batch: 238: Training Loss: 0.02259659767150879, Validation Loss: 0.025521542876958847\n",
      "Epoch 11, Batch: 239: Training Loss: 0.025916241109371185, Validation Loss: 0.026351720094680786\n",
      "Epoch 11, Batch: 240: Training Loss: 0.02514319121837616, Validation Loss: 0.02787320502102375\n",
      "Epoch 11, Batch: 241: Training Loss: 0.020898886024951935, Validation Loss: 0.026047902181744576\n",
      "Epoch 11, Batch: 242: Training Loss: 0.025385141372680664, Validation Loss: 0.02367515303194523\n",
      "Epoch 11, Batch: 243: Training Loss: 0.030831269919872284, Validation Loss: 0.02496839500963688\n",
      "Epoch 11, Batch: 244: Training Loss: 0.02388283982872963, Validation Loss: 0.025573864579200745\n",
      "Epoch 11, Batch: 245: Training Loss: 0.027919858694076538, Validation Loss: 0.025087743997573853\n",
      "Epoch 11, Batch: 246: Training Loss: 0.0233578160405159, Validation Loss: 0.02650701068341732\n",
      "Epoch 11, Batch: 247: Training Loss: 0.02676156535744667, Validation Loss: 0.02604297734797001\n",
      "Epoch 11, Batch: 248: Training Loss: 0.024632088840007782, Validation Loss: 0.02685573883354664\n",
      "Epoch 11, Batch: 249: Training Loss: 0.026560518890619278, Validation Loss: 0.027477039024233818\n",
      "Epoch 11, Batch: 250: Training Loss: 0.02751496434211731, Validation Loss: 0.0255656149238348\n",
      "Epoch 11, Batch: 251: Training Loss: 0.020060734823346138, Validation Loss: 0.028328686952590942\n",
      "Epoch 11, Batch: 252: Training Loss: 0.026268310844898224, Validation Loss: 0.027332035824656487\n",
      "Epoch 11, Batch: 253: Training Loss: 0.028387602418661118, Validation Loss: 0.028535893186926842\n",
      "Epoch 11, Batch: 254: Training Loss: 0.02300851047039032, Validation Loss: 0.02779308520257473\n",
      "Epoch 11, Batch: 255: Training Loss: 0.026703210547566414, Validation Loss: 0.030467476695775986\n",
      "Epoch 11, Batch: 256: Training Loss: 0.02805280312895775, Validation Loss: 0.030795497819781303\n",
      "Epoch 11, Batch: 257: Training Loss: 0.02535838633775711, Validation Loss: 0.025382941588759422\n",
      "Epoch 11, Batch: 258: Training Loss: 0.02776876837015152, Validation Loss: 0.023622125387191772\n",
      "Epoch 11, Batch: 259: Training Loss: 0.026664307340979576, Validation Loss: 0.027021517977118492\n",
      "Epoch 11, Batch: 260: Training Loss: 0.027192238718271255, Validation Loss: 0.02698858454823494\n",
      "Epoch 11, Batch: 261: Training Loss: 0.02622920460999012, Validation Loss: 0.02549329213798046\n",
      "Epoch 11, Batch: 262: Training Loss: 0.027086207643151283, Validation Loss: 0.02417931705713272\n",
      "Epoch 11, Batch: 263: Training Loss: 0.02251335047185421, Validation Loss: 0.024979986250400543\n",
      "Epoch 11, Batch: 264: Training Loss: 0.0253983736038208, Validation Loss: 0.026644635945558548\n",
      "Epoch 11, Batch: 265: Training Loss: 0.024773165583610535, Validation Loss: 0.025247588753700256\n",
      "Epoch 11, Batch: 266: Training Loss: 0.022803885862231255, Validation Loss: 0.027121569961309433\n",
      "Epoch 11, Batch: 267: Training Loss: 0.025030355900526047, Validation Loss: 0.02677883580327034\n",
      "Epoch 11, Batch: 268: Training Loss: 0.02389845810830593, Validation Loss: 0.027883276343345642\n",
      "Epoch 11, Batch: 269: Training Loss: 0.025388015434145927, Validation Loss: 0.025904441252350807\n",
      "Epoch 11, Batch: 270: Training Loss: 0.02317151054739952, Validation Loss: 0.025391235947608948\n",
      "Epoch 11, Batch: 271: Training Loss: 0.023980852216482162, Validation Loss: 0.024505440145730972\n",
      "Epoch 11, Batch: 272: Training Loss: 0.025674862787127495, Validation Loss: 0.024317672476172447\n",
      "Epoch 11, Batch: 273: Training Loss: 0.025739461183547974, Validation Loss: 0.026206916198134422\n",
      "Epoch 11, Batch: 274: Training Loss: 0.027399791404604912, Validation Loss: 0.025459837168455124\n",
      "Epoch 11, Batch: 275: Training Loss: 0.02374355122447014, Validation Loss: 0.024338625371456146\n",
      "Epoch 11, Batch: 276: Training Loss: 0.022821972146630287, Validation Loss: 0.024173833429813385\n",
      "Epoch 11, Batch: 277: Training Loss: 0.023959806188941002, Validation Loss: 0.024086138233542442\n",
      "Epoch 11, Batch: 278: Training Loss: 0.023830734193325043, Validation Loss: 0.025567715987563133\n",
      "Epoch 11, Batch: 279: Training Loss: 0.0258344579488039, Validation Loss: 0.025403795763850212\n",
      "Epoch 11, Batch: 280: Training Loss: 0.022010093554854393, Validation Loss: 0.025289982557296753\n",
      "Epoch 11, Batch: 281: Training Loss: 0.0222642719745636, Validation Loss: 0.025226492434740067\n",
      "Epoch 11, Batch: 282: Training Loss: 0.02162138558924198, Validation Loss: 0.028119711205363274\n",
      "Epoch 11, Batch: 283: Training Loss: 0.02550557628273964, Validation Loss: 0.02701367251574993\n",
      "Epoch 11, Batch: 284: Training Loss: 0.02492675557732582, Validation Loss: 0.02657185308635235\n",
      "Epoch 11, Batch: 285: Training Loss: 0.021807171404361725, Validation Loss: 0.024456849321722984\n",
      "Epoch 11, Batch: 286: Training Loss: 0.02478661760687828, Validation Loss: 0.02495683543384075\n",
      "Epoch 11, Batch: 287: Training Loss: 0.022653445601463318, Validation Loss: 0.02542710117995739\n",
      "Epoch 11, Batch: 288: Training Loss: 0.0258416086435318, Validation Loss: 0.02732478827238083\n",
      "Epoch 11, Batch: 289: Training Loss: 0.0255119688808918, Validation Loss: 0.024049673229455948\n",
      "Epoch 11, Batch: 290: Training Loss: 0.02643520012497902, Validation Loss: 0.026123015210032463\n",
      "Epoch 11, Batch: 291: Training Loss: 0.022352885454893112, Validation Loss: 0.02701588347554207\n",
      "Epoch 11, Batch: 292: Training Loss: 0.026722559705376625, Validation Loss: 0.025879008695483208\n",
      "Epoch 11, Batch: 293: Training Loss: 0.028943834826350212, Validation Loss: 0.026988692581653595\n",
      "Epoch 11, Batch: 294: Training Loss: 0.027250485494732857, Validation Loss: 0.028156772255897522\n",
      "Epoch 11, Batch: 295: Training Loss: 0.0254061222076416, Validation Loss: 0.027659323066473007\n",
      "Epoch 11, Batch: 296: Training Loss: 0.025782983750104904, Validation Loss: 0.025064954534173012\n",
      "Epoch 11, Batch: 297: Training Loss: 0.02491111308336258, Validation Loss: 0.024846771731972694\n",
      "Epoch 11, Batch: 298: Training Loss: 0.023036615923047066, Validation Loss: 0.024086862802505493\n",
      "Epoch 11, Batch: 299: Training Loss: 0.024222876876592636, Validation Loss: 0.025448132306337357\n",
      "Epoch 11, Batch: 300: Training Loss: 0.025220029056072235, Validation Loss: 0.024244237691164017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 301: Training Loss: 0.02275705710053444, Validation Loss: 0.024619068950414658\n",
      "Epoch 11, Batch: 302: Training Loss: 0.019131286069750786, Validation Loss: 0.026500213891267776\n",
      "Epoch 11, Batch: 303: Training Loss: 0.02375168912112713, Validation Loss: 0.025233447551727295\n",
      "Epoch 11, Batch: 304: Training Loss: 0.022266382351517677, Validation Loss: 0.02500140853226185\n",
      "Epoch 11, Batch: 305: Training Loss: 0.020998215302824974, Validation Loss: 0.02643837407231331\n",
      "Epoch 11, Batch: 306: Training Loss: 0.024161875247955322, Validation Loss: 0.026741717010736465\n",
      "Epoch 11, Batch: 307: Training Loss: 0.020190637558698654, Validation Loss: 0.026938922703266144\n",
      "Epoch 11, Batch: 308: Training Loss: 0.024214934557676315, Validation Loss: 0.024986961856484413\n",
      "Epoch 11, Batch: 309: Training Loss: 0.023779869079589844, Validation Loss: 0.024887511506676674\n",
      "Epoch 11, Batch: 310: Training Loss: 0.02399962767958641, Validation Loss: 0.02372145839035511\n",
      "Epoch 11, Batch: 311: Training Loss: 0.022640250623226166, Validation Loss: 0.02704036608338356\n",
      "Epoch 11, Batch: 312: Training Loss: 0.020980993285775185, Validation Loss: 0.02730143442749977\n",
      "Epoch 11, Batch: 313: Training Loss: 0.02340369112789631, Validation Loss: 0.027677128091454506\n",
      "Epoch 11, Batch: 314: Training Loss: 0.023807378485798836, Validation Loss: 0.02872951701283455\n",
      "Epoch 11, Batch: 315: Training Loss: 0.021156487986445427, Validation Loss: 0.02945936843752861\n",
      "Epoch 11, Batch: 316: Training Loss: 0.024665353819727898, Validation Loss: 0.02900240570306778\n",
      "Epoch 11, Batch: 317: Training Loss: 0.026566242799162865, Validation Loss: 0.02831318788230419\n",
      "Epoch 11, Batch: 318: Training Loss: 0.02302291989326477, Validation Loss: 0.026499468833208084\n",
      "Epoch 11, Batch: 319: Training Loss: 0.027531318366527557, Validation Loss: 0.027195772156119347\n",
      "Epoch 11, Batch: 320: Training Loss: 0.028409220278263092, Validation Loss: 0.026791471987962723\n",
      "Epoch 11, Batch: 321: Training Loss: 0.026027826592326164, Validation Loss: 0.02779400162398815\n",
      "Epoch 11, Batch: 322: Training Loss: 0.0244340430945158, Validation Loss: 0.026013078168034554\n",
      "Epoch 11, Batch: 323: Training Loss: 0.023204725235700607, Validation Loss: 0.026505302637815475\n",
      "Epoch 11, Batch: 324: Training Loss: 0.027654249221086502, Validation Loss: 0.024556171149015427\n",
      "Epoch 11, Batch: 325: Training Loss: 0.025717660784721375, Validation Loss: 0.02380351722240448\n",
      "Epoch 11, Batch: 326: Training Loss: 0.027474138885736465, Validation Loss: 0.026342036202549934\n",
      "Epoch 11, Batch: 327: Training Loss: 0.02187633141875267, Validation Loss: 0.027550552040338516\n",
      "Epoch 11, Batch: 328: Training Loss: 0.027365773916244507, Validation Loss: 0.026991674676537514\n",
      "Epoch 11, Batch: 329: Training Loss: 0.027239542454481125, Validation Loss: 0.02685272879898548\n",
      "Epoch 11, Batch: 330: Training Loss: 0.025294490158557892, Validation Loss: 0.024413272738456726\n",
      "Epoch 11, Batch: 331: Training Loss: 0.024255678057670593, Validation Loss: 0.025773564353585243\n",
      "Epoch 11, Batch: 332: Training Loss: 0.02169901318848133, Validation Loss: 0.024627648293972015\n",
      "Epoch 11, Batch: 333: Training Loss: 0.024840719997882843, Validation Loss: 0.025377823039889336\n",
      "Epoch 11, Batch: 334: Training Loss: 0.027566783130168915, Validation Loss: 0.026162637397646904\n",
      "Epoch 11, Batch: 335: Training Loss: 0.024624278768897057, Validation Loss: 0.025886734947562218\n",
      "Epoch 11, Batch: 336: Training Loss: 0.024389097467064857, Validation Loss: 0.027423402294516563\n",
      "Epoch 11, Batch: 337: Training Loss: 0.025368262082338333, Validation Loss: 0.02823350392282009\n",
      "Epoch 11, Batch: 338: Training Loss: 0.029998350888490677, Validation Loss: 0.024751311168074608\n",
      "Epoch 11, Batch: 339: Training Loss: 0.022906942293047905, Validation Loss: 0.025659598410129547\n",
      "Epoch 11, Batch: 340: Training Loss: 0.02607751078903675, Validation Loss: 0.024874337017536163\n",
      "Epoch 11, Batch: 341: Training Loss: 0.029459472745656967, Validation Loss: 0.0249837227165699\n",
      "Epoch 11, Batch: 342: Training Loss: 0.0294277835637331, Validation Loss: 0.02922525629401207\n",
      "Epoch 11, Batch: 343: Training Loss: 0.02459116466343403, Validation Loss: 0.026345456019043922\n",
      "Epoch 11, Batch: 344: Training Loss: 0.029821202158927917, Validation Loss: 0.027514053508639336\n",
      "Epoch 11, Batch: 345: Training Loss: 0.02653718739748001, Validation Loss: 0.026616161689162254\n",
      "Epoch 11, Batch: 346: Training Loss: 0.028792884200811386, Validation Loss: 0.027048883959650993\n",
      "Epoch 11, Batch: 347: Training Loss: 0.023023566231131554, Validation Loss: 0.026020824909210205\n",
      "Epoch 11, Batch: 348: Training Loss: 0.02656121365725994, Validation Loss: 0.022171251475811005\n",
      "Epoch 11, Batch: 349: Training Loss: 0.028529565781354904, Validation Loss: 0.02238413505256176\n",
      "Epoch 11, Batch: 350: Training Loss: 0.01997382752597332, Validation Loss: 0.02406892366707325\n",
      "Epoch 11, Batch: 351: Training Loss: 0.026645775884389877, Validation Loss: 0.02483118698000908\n",
      "Epoch 11, Batch: 352: Training Loss: 0.027464227750897408, Validation Loss: 0.02486569620668888\n",
      "Epoch 11, Batch: 353: Training Loss: 0.028224997222423553, Validation Loss: 0.027506867423653603\n",
      "Epoch 11, Batch: 354: Training Loss: 0.02518545649945736, Validation Loss: 0.02583625726401806\n",
      "Epoch 11, Batch: 355: Training Loss: 0.02416400983929634, Validation Loss: 0.024683933705091476\n",
      "Epoch 11, Batch: 356: Training Loss: 0.025063633918762207, Validation Loss: 0.02434457093477249\n",
      "Epoch 11, Batch: 357: Training Loss: 0.022625841200351715, Validation Loss: 0.02578705921769142\n",
      "Epoch 11, Batch: 358: Training Loss: 0.024145515635609627, Validation Loss: 0.02427675388753414\n",
      "Epoch 11, Batch: 359: Training Loss: 0.02621310204267502, Validation Loss: 0.023297341540455818\n",
      "Epoch 11, Batch: 360: Training Loss: 0.02561637945473194, Validation Loss: 0.02686440944671631\n",
      "Epoch 11, Batch: 361: Training Loss: 0.025541186332702637, Validation Loss: 0.025708824396133423\n",
      "Epoch 11, Batch: 362: Training Loss: 0.021265972405672073, Validation Loss: 0.025343604385852814\n",
      "Epoch 11, Batch: 363: Training Loss: 0.029596898704767227, Validation Loss: 0.0252044927328825\n",
      "Epoch 11, Batch: 364: Training Loss: 0.024216074496507645, Validation Loss: 0.023771706968545914\n",
      "Epoch 11, Batch: 365: Training Loss: 0.025465181097388268, Validation Loss: 0.02517891675233841\n",
      "Epoch 11, Batch: 366: Training Loss: 0.026000669226050377, Validation Loss: 0.0244024395942688\n",
      "Epoch 11, Batch: 367: Training Loss: 0.023365983739495277, Validation Loss: 0.026416081935167313\n",
      "Epoch 11, Batch: 368: Training Loss: 0.024089466780424118, Validation Loss: 0.023411141708493233\n",
      "Epoch 11, Batch: 369: Training Loss: 0.0229659304022789, Validation Loss: 0.02792249247431755\n",
      "Epoch 11, Batch: 370: Training Loss: 0.021198442205786705, Validation Loss: 0.02493221126496792\n",
      "Epoch 11, Batch: 371: Training Loss: 0.02508823573589325, Validation Loss: 0.02566593512892723\n",
      "Epoch 11, Batch: 372: Training Loss: 0.025202782824635506, Validation Loss: 0.023982569575309753\n",
      "Epoch 11, Batch: 373: Training Loss: 0.02688157558441162, Validation Loss: 0.02591656893491745\n",
      "Epoch 11, Batch: 374: Training Loss: 0.0215783528983593, Validation Loss: 0.024430295452475548\n",
      "Epoch 11, Batch: 375: Training Loss: 0.026507405564188957, Validation Loss: 0.025466306135058403\n",
      "Epoch 11, Batch: 376: Training Loss: 0.02217951789498329, Validation Loss: 0.0245036743581295\n",
      "Epoch 11, Batch: 377: Training Loss: 0.02437392808496952, Validation Loss: 0.024029692634940147\n",
      "Epoch 11, Batch: 378: Training Loss: 0.020475028082728386, Validation Loss: 0.027055883780121803\n",
      "Epoch 11, Batch: 379: Training Loss: 0.024158025160431862, Validation Loss: 0.023251809179782867\n",
      "Epoch 11, Batch: 380: Training Loss: 0.026030654087662697, Validation Loss: 0.02614526078104973\n",
      "Epoch 11, Batch: 381: Training Loss: 0.025004638358950615, Validation Loss: 0.02282627299427986\n",
      "Epoch 11, Batch: 382: Training Loss: 0.024084143340587616, Validation Loss: 0.025463910773396492\n",
      "Epoch 11, Batch: 383: Training Loss: 0.024945344775915146, Validation Loss: 0.028012994676828384\n",
      "Epoch 11, Batch: 384: Training Loss: 0.02376542054116726, Validation Loss: 0.023598456755280495\n",
      "Epoch 11, Batch: 385: Training Loss: 0.025511404499411583, Validation Loss: 0.02530488558113575\n",
      "Epoch 11, Batch: 386: Training Loss: 0.019487673416733742, Validation Loss: 0.026607440784573555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 387: Training Loss: 0.022070029750466347, Validation Loss: 0.02549021691083908\n",
      "Epoch 11, Batch: 388: Training Loss: 0.02530066855251789, Validation Loss: 0.026248114183545113\n",
      "Epoch 11, Batch: 389: Training Loss: 0.02708735503256321, Validation Loss: 0.025883717462420464\n",
      "Epoch 11, Batch: 390: Training Loss: 0.024197682738304138, Validation Loss: 0.025296766310930252\n",
      "Epoch 11, Batch: 391: Training Loss: 0.024531451985239983, Validation Loss: 0.025342362001538277\n",
      "Epoch 11, Batch: 392: Training Loss: 0.02291676588356495, Validation Loss: 0.027806714177131653\n",
      "Epoch 11, Batch: 393: Training Loss: 0.026311732828617096, Validation Loss: 0.02381272055208683\n",
      "Epoch 11, Batch: 394: Training Loss: 0.02225933037698269, Validation Loss: 0.026844214648008347\n",
      "Epoch 11, Batch: 395: Training Loss: 0.022195594385266304, Validation Loss: 0.024088306352496147\n",
      "Epoch 11, Batch: 396: Training Loss: 0.02679702639579773, Validation Loss: 0.023911822587251663\n",
      "Epoch 11, Batch: 397: Training Loss: 0.028865918517112732, Validation Loss: 0.022038821130990982\n",
      "Epoch 11, Batch: 398: Training Loss: 0.021301383152604103, Validation Loss: 0.024148546159267426\n",
      "Epoch 11, Batch: 399: Training Loss: 0.023983364924788475, Validation Loss: 0.02246810309588909\n",
      "Epoch 11, Batch: 400: Training Loss: 0.02933112345635891, Validation Loss: 0.02617640607059002\n",
      "Epoch 11, Batch: 401: Training Loss: 0.022502856329083443, Validation Loss: 0.025746669620275497\n",
      "Epoch 11, Batch: 402: Training Loss: 0.023354530334472656, Validation Loss: 0.02843007631599903\n",
      "Epoch 11, Batch: 403: Training Loss: 0.03253559395670891, Validation Loss: 0.026073623448610306\n",
      "Epoch 11, Batch: 404: Training Loss: 0.023238662630319595, Validation Loss: 0.026811933144927025\n",
      "Epoch 11, Batch: 405: Training Loss: 0.0250862929970026, Validation Loss: 0.023630142211914062\n",
      "Epoch 11, Batch: 406: Training Loss: 0.023529501631855965, Validation Loss: 0.025709165260195732\n",
      "Epoch 11, Batch: 407: Training Loss: 0.026637442409992218, Validation Loss: 0.025498026981949806\n",
      "Epoch 11, Batch: 408: Training Loss: 0.023720741271972656, Validation Loss: 0.022964417934417725\n",
      "Epoch 11, Batch: 409: Training Loss: 0.024266894906759262, Validation Loss: 0.02365352399647236\n",
      "Epoch 11, Batch: 410: Training Loss: 0.023398490622639656, Validation Loss: 0.02445213682949543\n",
      "Epoch 11, Batch: 411: Training Loss: 0.022300248965620995, Validation Loss: 0.024484271183609962\n",
      "Epoch 11, Batch: 412: Training Loss: 0.025072351098060608, Validation Loss: 0.024270573630928993\n",
      "Epoch 11, Batch: 413: Training Loss: 0.024480175226926804, Validation Loss: 0.02310139872133732\n",
      "Epoch 11, Batch: 414: Training Loss: 0.021960236132144928, Validation Loss: 0.023067781701683998\n",
      "Epoch 11, Batch: 415: Training Loss: 0.0260178055614233, Validation Loss: 0.024459049105644226\n",
      "Epoch 11, Batch: 416: Training Loss: 0.024641718715429306, Validation Loss: 0.02448391728103161\n",
      "Epoch 11, Batch: 417: Training Loss: 0.025322573259472847, Validation Loss: 0.02651912346482277\n",
      "Epoch 11, Batch: 418: Training Loss: 0.020209982991218567, Validation Loss: 0.025888578966259956\n",
      "Epoch 11, Batch: 419: Training Loss: 0.022922707721590996, Validation Loss: 0.026406537741422653\n",
      "Epoch 11, Batch: 420: Training Loss: 0.026386069133877754, Validation Loss: 0.023794323205947876\n",
      "Epoch 11, Batch: 421: Training Loss: 0.028957907110452652, Validation Loss: 0.024680178612470627\n",
      "Epoch 11, Batch: 422: Training Loss: 0.027352290228009224, Validation Loss: 0.025788564234972\n",
      "Epoch 11, Batch: 423: Training Loss: 0.028837081044912338, Validation Loss: 0.024213742464780807\n",
      "Epoch 11, Batch: 424: Training Loss: 0.02498490922152996, Validation Loss: 0.025023406371474266\n",
      "Epoch 11, Batch: 425: Training Loss: 0.02322806604206562, Validation Loss: 0.024245664477348328\n",
      "Epoch 11, Batch: 426: Training Loss: 0.026682965457439423, Validation Loss: 0.025990549474954605\n",
      "Epoch 11, Batch: 427: Training Loss: 0.02675372175872326, Validation Loss: 0.024227436631917953\n",
      "Epoch 11, Batch: 428: Training Loss: 0.024839002639055252, Validation Loss: 0.02329074963927269\n",
      "Epoch 11, Batch: 429: Training Loss: 0.024082543328404427, Validation Loss: 0.025633852928876877\n",
      "Epoch 11, Batch: 430: Training Loss: 0.02744906395673752, Validation Loss: 0.023796940222382545\n",
      "Epoch 11, Batch: 431: Training Loss: 0.02498573623597622, Validation Loss: 0.025300513952970505\n",
      "Epoch 11, Batch: 432: Training Loss: 0.02447028085589409, Validation Loss: 0.028285479173064232\n",
      "Epoch 11, Batch: 433: Training Loss: 0.02555984817445278, Validation Loss: 0.02412036620080471\n",
      "Epoch 11, Batch: 434: Training Loss: 0.027695247903466225, Validation Loss: 0.024410245940089226\n",
      "Epoch 11, Batch: 435: Training Loss: 0.024365901947021484, Validation Loss: 0.024041729047894478\n",
      "Epoch 11, Batch: 436: Training Loss: 0.02482607215642929, Validation Loss: 0.02614445798099041\n",
      "Epoch 11, Batch: 437: Training Loss: 0.020958557724952698, Validation Loss: 0.022975901141762733\n",
      "Epoch 11, Batch: 438: Training Loss: 0.026071898639202118, Validation Loss: 0.02451188676059246\n",
      "Epoch 11, Batch: 439: Training Loss: 0.02244662493467331, Validation Loss: 0.02413354255259037\n",
      "Epoch 11, Batch: 440: Training Loss: 0.030391788110136986, Validation Loss: 0.02523939125239849\n",
      "Epoch 11, Batch: 441: Training Loss: 0.026544641703367233, Validation Loss: 0.024662280455231667\n",
      "Epoch 11, Batch: 442: Training Loss: 0.022490791976451874, Validation Loss: 0.023750053718686104\n",
      "Epoch 11, Batch: 443: Training Loss: 0.021520428359508514, Validation Loss: 0.025483490899205208\n",
      "Epoch 11, Batch: 444: Training Loss: 0.021138295531272888, Validation Loss: 0.023764559999108315\n",
      "Epoch 11, Batch: 445: Training Loss: 0.02248426340520382, Validation Loss: 0.025689616799354553\n",
      "Epoch 11, Batch: 446: Training Loss: 0.031924471259117126, Validation Loss: 0.025787215679883957\n",
      "Epoch 11, Batch: 447: Training Loss: 0.02539242058992386, Validation Loss: 0.02420550212264061\n",
      "Epoch 11, Batch: 448: Training Loss: 0.0261396411806345, Validation Loss: 0.02552615851163864\n",
      "Epoch 11, Batch: 449: Training Loss: 0.023517850786447525, Validation Loss: 0.027942178770899773\n",
      "Epoch 11, Batch: 450: Training Loss: 0.02472260221838951, Validation Loss: 0.03335143253207207\n",
      "Epoch 11, Batch: 451: Training Loss: 0.032175250351428986, Validation Loss: 0.027453653514385223\n",
      "Epoch 11, Batch: 452: Training Loss: 0.028432443737983704, Validation Loss: 0.029204923659563065\n",
      "Epoch 11, Batch: 453: Training Loss: 0.02902485616505146, Validation Loss: 0.028709713369607925\n",
      "Epoch 11, Batch: 454: Training Loss: 0.02884681709110737, Validation Loss: 0.02716449461877346\n",
      "Epoch 11, Batch: 455: Training Loss: 0.02581869810819626, Validation Loss: 0.025203580036759377\n",
      "Epoch 11, Batch: 456: Training Loss: 0.023242497816681862, Validation Loss: 0.027363356202840805\n",
      "Epoch 11, Batch: 457: Training Loss: 0.025134652853012085, Validation Loss: 0.024914750829339027\n",
      "Epoch 11, Batch: 458: Training Loss: 0.020888840779662132, Validation Loss: 0.027065977454185486\n",
      "Epoch 11, Batch: 459: Training Loss: 0.02425968088209629, Validation Loss: 0.025449329987168312\n",
      "Epoch 11, Batch: 460: Training Loss: 0.02524297684431076, Validation Loss: 0.026806384325027466\n",
      "Epoch 11, Batch: 461: Training Loss: 0.02312185801565647, Validation Loss: 0.025817401707172394\n",
      "Epoch 11, Batch: 462: Training Loss: 0.02393224649131298, Validation Loss: 0.02450968138873577\n",
      "Epoch 11, Batch: 463: Training Loss: 0.02588447369635105, Validation Loss: 0.024953702464699745\n",
      "Epoch 11, Batch: 464: Training Loss: 0.02089381217956543, Validation Loss: 0.023422105237841606\n",
      "Epoch 11, Batch: 465: Training Loss: 0.02224772237241268, Validation Loss: 0.025737324729561806\n",
      "Epoch 11, Batch: 466: Training Loss: 0.022659989073872566, Validation Loss: 0.025049826130270958\n",
      "Epoch 11, Batch: 467: Training Loss: 0.027026670053601265, Validation Loss: 0.022456390783190727\n",
      "Epoch 11, Batch: 468: Training Loss: 0.026771612465381622, Validation Loss: 0.026263413950800896\n",
      "Epoch 11, Batch: 469: Training Loss: 0.02469722181558609, Validation Loss: 0.02501266822218895\n",
      "Epoch 11, Batch: 470: Training Loss: 0.024152051657438278, Validation Loss: 0.026202214881777763\n",
      "Epoch 11, Batch: 471: Training Loss: 0.02642831951379776, Validation Loss: 0.02658119425177574\n",
      "Epoch 11, Batch: 472: Training Loss: 0.026307033374905586, Validation Loss: 0.026117319241166115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch: 473: Training Loss: 0.022540772333741188, Validation Loss: 0.02441515028476715\n",
      "Epoch 11, Batch: 474: Training Loss: 0.019061755388975143, Validation Loss: 0.026069341227412224\n",
      "Epoch 11, Batch: 475: Training Loss: 0.028354745358228683, Validation Loss: 0.024377858266234398\n",
      "Epoch 11, Batch: 476: Training Loss: 0.024024398997426033, Validation Loss: 0.02893659844994545\n",
      "Epoch 11, Batch: 477: Training Loss: 0.026078611612319946, Validation Loss: 0.028029292821884155\n",
      "Epoch 11, Batch: 478: Training Loss: 0.024074938148260117, Validation Loss: 0.027182254940271378\n",
      "Epoch 11, Batch: 479: Training Loss: 0.023733358830213547, Validation Loss: 0.02659257873892784\n",
      "Epoch 11, Batch: 480: Training Loss: 0.029916968196630478, Validation Loss: 0.026149487122893333\n",
      "Epoch 11, Batch: 481: Training Loss: 0.02480147033929825, Validation Loss: 0.027530459687113762\n",
      "Epoch 11, Batch: 482: Training Loss: 0.02839597314596176, Validation Loss: 0.028532862663269043\n",
      "Epoch 11, Batch: 483: Training Loss: 0.028398193418979645, Validation Loss: 0.030492866411805153\n",
      "Epoch 11, Batch: 484: Training Loss: 0.030723435804247856, Validation Loss: 0.027905195951461792\n",
      "Epoch 11, Batch: 485: Training Loss: 0.028110109269618988, Validation Loss: 0.026881979778409004\n",
      "Epoch 11, Batch: 486: Training Loss: 0.029346207156777382, Validation Loss: 0.02594735287129879\n",
      "Epoch 11, Batch: 487: Training Loss: 0.02445903606712818, Validation Loss: 0.028894029557704926\n",
      "Epoch 11, Batch: 488: Training Loss: 0.025833558291196823, Validation Loss: 0.02550579234957695\n",
      "Epoch 11, Batch: 489: Training Loss: 0.03043247200548649, Validation Loss: 0.02657216228544712\n",
      "Epoch 11, Batch: 490: Training Loss: 0.028922509402036667, Validation Loss: 0.026318229734897614\n",
      "Epoch 11, Batch: 491: Training Loss: 0.02367081120610237, Validation Loss: 0.025475703179836273\n",
      "Epoch 11, Batch: 492: Training Loss: 0.026802774518728256, Validation Loss: 0.02810562215745449\n",
      "Epoch 11, Batch: 493: Training Loss: 0.027951626107096672, Validation Loss: 0.027411645278334618\n",
      "Epoch 11, Batch: 494: Training Loss: 0.03424892574548721, Validation Loss: 0.02740393579006195\n",
      "Epoch 11, Batch: 495: Training Loss: 0.025985322892665863, Validation Loss: 0.02933971770107746\n",
      "Epoch 11, Batch: 496: Training Loss: 0.02800728753209114, Validation Loss: 0.028883127495646477\n",
      "Epoch 11, Batch: 497: Training Loss: 0.02392934262752533, Validation Loss: 0.02907666005194187\n",
      "Epoch 11, Batch: 498: Training Loss: 0.026413945481181145, Validation Loss: 0.031660668551921844\n",
      "Epoch 11, Batch: 499: Training Loss: 0.02501302771270275, Validation Loss: 0.030260715633630753\n",
      "Epoch 12, Batch: 0: Training Loss: 0.026920953765511513, Validation Loss: 0.028727103024721146\n",
      "Epoch 12, Batch: 1: Training Loss: 0.024914104491472244, Validation Loss: 0.028314832597970963\n",
      "Epoch 12, Batch: 2: Training Loss: 0.03292027860879898, Validation Loss: 0.026026003062725067\n",
      "Epoch 12, Batch: 3: Training Loss: 0.029555222019553185, Validation Loss: 0.026162896305322647\n",
      "Epoch 12, Batch: 4: Training Loss: 0.02340422198176384, Validation Loss: 0.02614222653210163\n",
      "Epoch 12, Batch: 5: Training Loss: 0.025338483974337578, Validation Loss: 0.027705350890755653\n",
      "Epoch 12, Batch: 6: Training Loss: 0.023561770096421242, Validation Loss: 0.02833765745162964\n",
      "Epoch 12, Batch: 7: Training Loss: 0.025873230770230293, Validation Loss: 0.024976057931780815\n",
      "Epoch 12, Batch: 8: Training Loss: 0.02571280486881733, Validation Loss: 0.026822783052921295\n",
      "Epoch 12, Batch: 9: Training Loss: 0.025476159527897835, Validation Loss: 0.027553193271160126\n",
      "Epoch 12, Batch: 10: Training Loss: 0.02548936940729618, Validation Loss: 0.027877265587449074\n",
      "Epoch 12, Batch: 11: Training Loss: 0.029867641627788544, Validation Loss: 0.028823871165513992\n",
      "Epoch 12, Batch: 12: Training Loss: 0.023688893765211105, Validation Loss: 0.028283661231398582\n",
      "Epoch 12, Batch: 13: Training Loss: 0.030516261234879494, Validation Loss: 0.028201231732964516\n",
      "Epoch 12, Batch: 14: Training Loss: 0.024458887055516243, Validation Loss: 0.027683673426508904\n",
      "Epoch 12, Batch: 15: Training Loss: 0.02677306719124317, Validation Loss: 0.029150409623980522\n",
      "Epoch 12, Batch: 16: Training Loss: 0.02588069625198841, Validation Loss: 0.027121271938085556\n",
      "Epoch 12, Batch: 17: Training Loss: 0.02502242848277092, Validation Loss: 0.02919418178498745\n",
      "Epoch 12, Batch: 18: Training Loss: 0.024771448224782944, Validation Loss: 0.02732500061392784\n",
      "Epoch 12, Batch: 19: Training Loss: 0.019597958773374557, Validation Loss: 0.029277652502059937\n",
      "Epoch 12, Batch: 20: Training Loss: 0.02769867144525051, Validation Loss: 0.02631443738937378\n",
      "Epoch 12, Batch: 21: Training Loss: 0.02715023048222065, Validation Loss: 0.02782989852130413\n",
      "Epoch 12, Batch: 22: Training Loss: 0.028778020292520523, Validation Loss: 0.02945077046751976\n",
      "Epoch 12, Batch: 23: Training Loss: 0.02553028054535389, Validation Loss: 0.028182130306959152\n",
      "Epoch 12, Batch: 24: Training Loss: 0.02282346971333027, Validation Loss: 0.027737291529774666\n",
      "Epoch 12, Batch: 25: Training Loss: 0.022929102182388306, Validation Loss: 0.029179127886891365\n",
      "Epoch 12, Batch: 26: Training Loss: 0.02327840030193329, Validation Loss: 0.02727096527814865\n",
      "Epoch 12, Batch: 27: Training Loss: 0.026047175750136375, Validation Loss: 0.027238331735134125\n",
      "Epoch 12, Batch: 28: Training Loss: 0.026630040258169174, Validation Loss: 0.02780279703438282\n",
      "Epoch 12, Batch: 29: Training Loss: 0.02555192820727825, Validation Loss: 0.025538353249430656\n",
      "Epoch 12, Batch: 30: Training Loss: 0.02794690616428852, Validation Loss: 0.02664279378950596\n",
      "Epoch 12, Batch: 31: Training Loss: 0.028368398547172546, Validation Loss: 0.0274799857288599\n",
      "Epoch 12, Batch: 32: Training Loss: 0.02273002825677395, Validation Loss: 0.027312107384204865\n",
      "Epoch 12, Batch: 33: Training Loss: 0.02688138745725155, Validation Loss: 0.027479248121380806\n",
      "Epoch 12, Batch: 34: Training Loss: 0.02298922650516033, Validation Loss: 0.026440409943461418\n",
      "Epoch 12, Batch: 35: Training Loss: 0.026181191205978394, Validation Loss: 0.02807903289794922\n",
      "Epoch 12, Batch: 36: Training Loss: 0.022497110068798065, Validation Loss: 0.025445181876420975\n",
      "Epoch 12, Batch: 37: Training Loss: 0.022654913365840912, Validation Loss: 0.028803657740354538\n",
      "Epoch 12, Batch: 38: Training Loss: 0.02864527702331543, Validation Loss: 0.026823025196790695\n",
      "Epoch 12, Batch: 39: Training Loss: 0.02866874635219574, Validation Loss: 0.02825767919421196\n",
      "Epoch 12, Batch: 40: Training Loss: 0.02740509994328022, Validation Loss: 0.02872638590633869\n",
      "Epoch 12, Batch: 41: Training Loss: 0.025562766939401627, Validation Loss: 0.02749047242105007\n",
      "Epoch 12, Batch: 42: Training Loss: 0.022557532414793968, Validation Loss: 0.027756467461586\n",
      "Epoch 12, Batch: 43: Training Loss: 0.025250554084777832, Validation Loss: 0.02982889674603939\n",
      "Epoch 12, Batch: 44: Training Loss: 0.025794656947255135, Validation Loss: 0.02786640077829361\n",
      "Epoch 12, Batch: 45: Training Loss: 0.022285768762230873, Validation Loss: 0.027485912665724754\n",
      "Epoch 12, Batch: 46: Training Loss: 0.02394617162644863, Validation Loss: 0.025184834375977516\n",
      "Epoch 12, Batch: 47: Training Loss: 0.026354655623435974, Validation Loss: 0.025726037099957466\n",
      "Epoch 12, Batch: 48: Training Loss: 0.03002477064728737, Validation Loss: 0.026828503236174583\n",
      "Epoch 12, Batch: 49: Training Loss: 0.029661592096090317, Validation Loss: 0.027584383264183998\n",
      "Epoch 12, Batch: 50: Training Loss: 0.026712920516729355, Validation Loss: 0.028297780081629753\n",
      "Epoch 12, Batch: 51: Training Loss: 0.02878650650382042, Validation Loss: 0.027842557057738304\n",
      "Epoch 12, Batch: 52: Training Loss: 0.02078639157116413, Validation Loss: 0.02821306139230728\n",
      "Epoch 12, Batch: 53: Training Loss: 0.024348489940166473, Validation Loss: 0.026540482416749\n",
      "Epoch 12, Batch: 54: Training Loss: 0.026259053498506546, Validation Loss: 0.027466541156172752\n",
      "Epoch 12, Batch: 55: Training Loss: 0.023735756054520607, Validation Loss: 0.025744348764419556\n",
      "Epoch 12, Batch: 56: Training Loss: 0.027862854301929474, Validation Loss: 0.027173161506652832\n",
      "Epoch 12, Batch: 57: Training Loss: 0.023900754749774933, Validation Loss: 0.025172483175992966\n",
      "Epoch 12, Batch: 58: Training Loss: 0.029611218720674515, Validation Loss: 0.02672642469406128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 59: Training Loss: 0.021787526085972786, Validation Loss: 0.02339390479028225\n",
      "Epoch 12, Batch: 60: Training Loss: 0.022056907415390015, Validation Loss: 0.027111709117889404\n",
      "Epoch 12, Batch: 61: Training Loss: 0.030064361169934273, Validation Loss: 0.027590636163949966\n",
      "Epoch 12, Batch: 62: Training Loss: 0.026376238092780113, Validation Loss: 0.026464808732271194\n",
      "Epoch 12, Batch: 63: Training Loss: 0.026164619252085686, Validation Loss: 0.029101284220814705\n",
      "Epoch 12, Batch: 64: Training Loss: 0.025138400495052338, Validation Loss: 0.025293128564953804\n",
      "Epoch 12, Batch: 65: Training Loss: 0.025267846882343292, Validation Loss: 0.026096893474459648\n",
      "Epoch 12, Batch: 66: Training Loss: 0.023695843294262886, Validation Loss: 0.025072922930121422\n",
      "Epoch 12, Batch: 67: Training Loss: 0.02115894854068756, Validation Loss: 0.024171942844986916\n",
      "Epoch 12, Batch: 68: Training Loss: 0.02498014084994793, Validation Loss: 0.02675924450159073\n",
      "Epoch 12, Batch: 69: Training Loss: 0.024317020550370216, Validation Loss: 0.027073757722973824\n",
      "Epoch 12, Batch: 70: Training Loss: 0.03138696402311325, Validation Loss: 0.0258354339748621\n",
      "Epoch 12, Batch: 71: Training Loss: 0.02377701923251152, Validation Loss: 0.026262931525707245\n",
      "Epoch 12, Batch: 72: Training Loss: 0.02549729309976101, Validation Loss: 0.027175117284059525\n",
      "Epoch 12, Batch: 73: Training Loss: 0.025165630504488945, Validation Loss: 0.030164996162056923\n",
      "Epoch 12, Batch: 74: Training Loss: 0.02267802320420742, Validation Loss: 0.024411706253886223\n",
      "Epoch 12, Batch: 75: Training Loss: 0.020142249763011932, Validation Loss: 0.028028050437569618\n",
      "Epoch 12, Batch: 76: Training Loss: 0.022374412044882774, Validation Loss: 0.02687840536236763\n",
      "Epoch 12, Batch: 77: Training Loss: 0.026575181633234024, Validation Loss: 0.02566692605614662\n",
      "Epoch 12, Batch: 78: Training Loss: 0.025589773431420326, Validation Loss: 0.02846153825521469\n",
      "Epoch 12, Batch: 79: Training Loss: 0.024163180962204933, Validation Loss: 0.02891775220632553\n",
      "Epoch 12, Batch: 80: Training Loss: 0.025149201974272728, Validation Loss: 0.028573565185070038\n",
      "Epoch 12, Batch: 81: Training Loss: 0.02647152543067932, Validation Loss: 0.028912734240293503\n",
      "Epoch 12, Batch: 82: Training Loss: 0.02541119046509266, Validation Loss: 0.029043523594737053\n",
      "Epoch 12, Batch: 83: Training Loss: 0.025997187942266464, Validation Loss: 0.03143874183297157\n",
      "Epoch 12, Batch: 84: Training Loss: 0.02537515200674534, Validation Loss: 0.03020557574927807\n",
      "Epoch 12, Batch: 85: Training Loss: 0.026015527546405792, Validation Loss: 0.031278207898139954\n",
      "Epoch 12, Batch: 86: Training Loss: 0.028586046770215034, Validation Loss: 0.02849956974387169\n",
      "Epoch 12, Batch: 87: Training Loss: 0.026158012449741364, Validation Loss: 0.02673465572297573\n",
      "Epoch 12, Batch: 88: Training Loss: 0.02519570291042328, Validation Loss: 0.028365589678287506\n",
      "Epoch 12, Batch: 89: Training Loss: 0.030551791191101074, Validation Loss: 0.027710329741239548\n",
      "Epoch 12, Batch: 90: Training Loss: 0.021503273397684097, Validation Loss: 0.026483159512281418\n",
      "Epoch 12, Batch: 91: Training Loss: 0.02394624799489975, Validation Loss: 0.027143569663167\n",
      "Epoch 12, Batch: 92: Training Loss: 0.029646193608641624, Validation Loss: 0.02682986482977867\n",
      "Epoch 12, Batch: 93: Training Loss: 0.02990158647298813, Validation Loss: 0.02675335854291916\n",
      "Epoch 12, Batch: 94: Training Loss: 0.027583133429288864, Validation Loss: 0.025614436715841293\n",
      "Epoch 12, Batch: 95: Training Loss: 0.02558796852827072, Validation Loss: 0.026420114561915398\n",
      "Epoch 12, Batch: 96: Training Loss: 0.023385247215628624, Validation Loss: 0.024048862978816032\n",
      "Epoch 12, Batch: 97: Training Loss: 0.02437993884086609, Validation Loss: 0.02637805975973606\n",
      "Epoch 12, Batch: 98: Training Loss: 0.024128437042236328, Validation Loss: 0.025381043553352356\n",
      "Epoch 12, Batch: 99: Training Loss: 0.024141820147633553, Validation Loss: 0.025134015828371048\n",
      "Epoch 12, Batch: 100: Training Loss: 0.024062279611825943, Validation Loss: 0.028043366968631744\n",
      "Epoch 12, Batch: 101: Training Loss: 0.023720117285847664, Validation Loss: 0.02740068919956684\n",
      "Epoch 12, Batch: 102: Training Loss: 0.022737842053174973, Validation Loss: 0.024413298815488815\n",
      "Epoch 12, Batch: 103: Training Loss: 0.025465359911322594, Validation Loss: 0.02643360011279583\n",
      "Epoch 12, Batch: 104: Training Loss: 0.024234933778643608, Validation Loss: 0.025814400985836983\n",
      "Epoch 12, Batch: 105: Training Loss: 0.021622788161039352, Validation Loss: 0.024904990568757057\n",
      "Epoch 12, Batch: 106: Training Loss: 0.025430819019675255, Validation Loss: 0.02212459407746792\n",
      "Epoch 12, Batch: 107: Training Loss: 0.025650547817349434, Validation Loss: 0.024094367399811745\n",
      "Epoch 12, Batch: 108: Training Loss: 0.024406371638178825, Validation Loss: 0.02536957338452339\n",
      "Epoch 12, Batch: 109: Training Loss: 0.028162220492959023, Validation Loss: 0.02448444999754429\n",
      "Epoch 12, Batch: 110: Training Loss: 0.028262872248888016, Validation Loss: 0.023209000006318092\n",
      "Epoch 12, Batch: 111: Training Loss: 0.023380275815725327, Validation Loss: 0.024863924831151962\n",
      "Epoch 12, Batch: 112: Training Loss: 0.02225654199719429, Validation Loss: 0.023664401844143867\n",
      "Epoch 12, Batch: 113: Training Loss: 0.024390092119574547, Validation Loss: 0.02524406649172306\n",
      "Epoch 12, Batch: 114: Training Loss: 0.02358858846127987, Validation Loss: 0.0234686192125082\n",
      "Epoch 12, Batch: 115: Training Loss: 0.025989413261413574, Validation Loss: 0.025185946375131607\n",
      "Epoch 12, Batch: 116: Training Loss: 0.02234216220676899, Validation Loss: 0.024205723777413368\n",
      "Epoch 12, Batch: 117: Training Loss: 0.02519826777279377, Validation Loss: 0.02659892849624157\n",
      "Epoch 12, Batch: 118: Training Loss: 0.02249906025826931, Validation Loss: 0.0248732827603817\n",
      "Epoch 12, Batch: 119: Training Loss: 0.028159474954009056, Validation Loss: 0.025763995945453644\n",
      "Epoch 12, Batch: 120: Training Loss: 0.02059294283390045, Validation Loss: 0.02546384185552597\n",
      "Epoch 12, Batch: 121: Training Loss: 0.02299375832080841, Validation Loss: 0.02834150195121765\n",
      "Epoch 12, Batch: 122: Training Loss: 0.023718493059277534, Validation Loss: 0.02563428319990635\n",
      "Epoch 12, Batch: 123: Training Loss: 0.024160930886864662, Validation Loss: 0.024889344349503517\n",
      "Epoch 12, Batch: 124: Training Loss: 0.02299976535141468, Validation Loss: 0.0281683299690485\n",
      "Epoch 12, Batch: 125: Training Loss: 0.02168218418955803, Validation Loss: 0.027467679232358932\n",
      "Epoch 12, Batch: 126: Training Loss: 0.02397196553647518, Validation Loss: 0.02574857510626316\n",
      "Epoch 12, Batch: 127: Training Loss: 0.02445945329964161, Validation Loss: 0.02893129736185074\n",
      "Epoch 12, Batch: 128: Training Loss: 0.02436402253806591, Validation Loss: 0.029928425326943398\n",
      "Epoch 12, Batch: 129: Training Loss: 0.02412034198641777, Validation Loss: 0.028477203100919724\n",
      "Epoch 12, Batch: 130: Training Loss: 0.02821447141468525, Validation Loss: 0.028596999123692513\n",
      "Epoch 12, Batch: 131: Training Loss: 0.027694331482052803, Validation Loss: 0.02819068543612957\n",
      "Epoch 12, Batch: 132: Training Loss: 0.023332111537456512, Validation Loss: 0.028604939579963684\n",
      "Epoch 12, Batch: 133: Training Loss: 0.024784013628959656, Validation Loss: 0.027798982337117195\n",
      "Epoch 12, Batch: 134: Training Loss: 0.028134766966104507, Validation Loss: 0.027838084846735\n",
      "Epoch 12, Batch: 135: Training Loss: 0.022207247093319893, Validation Loss: 0.027807122096419334\n",
      "Epoch 12, Batch: 136: Training Loss: 0.023362629115581512, Validation Loss: 0.028920449316501617\n",
      "Epoch 12, Batch: 137: Training Loss: 0.022994719445705414, Validation Loss: 0.029971597716212273\n",
      "Epoch 12, Batch: 138: Training Loss: 0.025500290095806122, Validation Loss: 0.027169421315193176\n",
      "Epoch 12, Batch: 139: Training Loss: 0.02460610121488571, Validation Loss: 0.027337325736880302\n",
      "Epoch 12, Batch: 140: Training Loss: 0.0322188101708889, Validation Loss: 0.02656346745789051\n",
      "Epoch 12, Batch: 141: Training Loss: 0.029050733894109726, Validation Loss: 0.02662540227174759\n",
      "Epoch 12, Batch: 142: Training Loss: 0.029451023787260056, Validation Loss: 0.027522169053554535\n",
      "Epoch 12, Batch: 143: Training Loss: 0.02323523350059986, Validation Loss: 0.026098720729351044\n",
      "Epoch 12, Batch: 144: Training Loss: 0.02629260905086994, Validation Loss: 0.024139495566487312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 145: Training Loss: 0.021164007484912872, Validation Loss: 0.022743191570043564\n",
      "Epoch 12, Batch: 146: Training Loss: 0.023578615859150887, Validation Loss: 0.022672224789857864\n",
      "Epoch 12, Batch: 147: Training Loss: 0.024485576897859573, Validation Loss: 0.023747755214571953\n",
      "Epoch 12, Batch: 148: Training Loss: 0.025139857083559036, Validation Loss: 0.02529987134039402\n",
      "Epoch 12, Batch: 149: Training Loss: 0.026951119303703308, Validation Loss: 0.023548549041152\n",
      "Epoch 12, Batch: 150: Training Loss: 0.027601836249232292, Validation Loss: 0.02319965325295925\n",
      "Epoch 12, Batch: 151: Training Loss: 0.027522815391421318, Validation Loss: 0.023584716022014618\n",
      "Epoch 12, Batch: 152: Training Loss: 0.025227952748537064, Validation Loss: 0.02575135976076126\n",
      "Epoch 12, Batch: 153: Training Loss: 0.028126217424869537, Validation Loss: 0.024662436917424202\n",
      "Epoch 12, Batch: 154: Training Loss: 0.02505887672305107, Validation Loss: 0.02451809123158455\n",
      "Epoch 12, Batch: 155: Training Loss: 0.030449124053120613, Validation Loss: 0.024954361841082573\n",
      "Epoch 12, Batch: 156: Training Loss: 0.026842165738344193, Validation Loss: 0.026981007307767868\n",
      "Epoch 12, Batch: 157: Training Loss: 0.022719433531165123, Validation Loss: 0.02566729672253132\n",
      "Epoch 12, Batch: 158: Training Loss: 0.02642766386270523, Validation Loss: 0.026940081268548965\n",
      "Epoch 12, Batch: 159: Training Loss: 0.023744579404592514, Validation Loss: 0.025092005729675293\n",
      "Epoch 12, Batch: 160: Training Loss: 0.024898884817957878, Validation Loss: 0.023194139823317528\n",
      "Epoch 12, Batch: 161: Training Loss: 0.02546432800590992, Validation Loss: 0.026224341243505478\n",
      "Epoch 12, Batch: 162: Training Loss: 0.025320038199424744, Validation Loss: 0.027724536135792732\n",
      "Epoch 12, Batch: 163: Training Loss: 0.025043468922376633, Validation Loss: 0.0283011756837368\n",
      "Epoch 12, Batch: 164: Training Loss: 0.022875329479575157, Validation Loss: 0.025358455255627632\n",
      "Epoch 12, Batch: 165: Training Loss: 0.023242631927132607, Validation Loss: 0.027706490829586983\n",
      "Epoch 12, Batch: 166: Training Loss: 0.025193562731146812, Validation Loss: 0.0279275793582201\n",
      "Epoch 12, Batch: 167: Training Loss: 0.024139732122421265, Validation Loss: 0.02621888741850853\n",
      "Epoch 12, Batch: 168: Training Loss: 0.03132035955786705, Validation Loss: 0.027227379381656647\n",
      "Epoch 12, Batch: 169: Training Loss: 0.029781829565763474, Validation Loss: 0.027262946590781212\n",
      "Epoch 12, Batch: 170: Training Loss: 0.022628935053944588, Validation Loss: 0.026379061862826347\n",
      "Epoch 12, Batch: 171: Training Loss: 0.020564811304211617, Validation Loss: 0.026234665885567665\n",
      "Epoch 12, Batch: 172: Training Loss: 0.024054504930973053, Validation Loss: 0.02426040545105934\n",
      "Epoch 12, Batch: 173: Training Loss: 0.022129638120532036, Validation Loss: 0.025469297543168068\n",
      "Epoch 12, Batch: 174: Training Loss: 0.027728673070669174, Validation Loss: 0.02389645390212536\n",
      "Epoch 12, Batch: 175: Training Loss: 0.02513001300394535, Validation Loss: 0.024571815505623817\n",
      "Epoch 12, Batch: 176: Training Loss: 0.026740163564682007, Validation Loss: 0.024951400235295296\n",
      "Epoch 12, Batch: 177: Training Loss: 0.022638674825429916, Validation Loss: 0.026384670287370682\n",
      "Epoch 12, Batch: 178: Training Loss: 0.029583023861050606, Validation Loss: 0.026124412193894386\n",
      "Epoch 12, Batch: 179: Training Loss: 0.028608808293938637, Validation Loss: 0.027295444160699844\n",
      "Epoch 12, Batch: 180: Training Loss: 0.028008069843053818, Validation Loss: 0.02723967842757702\n",
      "Epoch 12, Batch: 181: Training Loss: 0.023588871583342552, Validation Loss: 0.02680087462067604\n",
      "Epoch 12, Batch: 182: Training Loss: 0.026173654943704605, Validation Loss: 0.026498127728700638\n",
      "Epoch 12, Batch: 183: Training Loss: 0.02555449865758419, Validation Loss: 0.026924433186650276\n",
      "Epoch 12, Batch: 184: Training Loss: 0.02519037015736103, Validation Loss: 0.024853147566318512\n",
      "Epoch 12, Batch: 185: Training Loss: 0.028715375810861588, Validation Loss: 0.0218346007168293\n",
      "Epoch 12, Batch: 186: Training Loss: 0.025083212181925774, Validation Loss: 0.025697022676467896\n",
      "Epoch 12, Batch: 187: Training Loss: 0.022453101351857185, Validation Loss: 0.02575146034359932\n",
      "Epoch 12, Batch: 188: Training Loss: 0.02349318377673626, Validation Loss: 0.026808567345142365\n",
      "Epoch 12, Batch: 189: Training Loss: 0.02384818345308304, Validation Loss: 0.025261929258704185\n",
      "Epoch 12, Batch: 190: Training Loss: 0.022852996364235878, Validation Loss: 0.024429403245449066\n",
      "Epoch 12, Batch: 191: Training Loss: 0.026419145986437798, Validation Loss: 0.027329564094543457\n",
      "Epoch 12, Batch: 192: Training Loss: 0.023952824994921684, Validation Loss: 0.02669537626206875\n",
      "Epoch 12, Batch: 193: Training Loss: 0.02579645998775959, Validation Loss: 0.024864578619599342\n",
      "Epoch 12, Batch: 194: Training Loss: 0.025387244299054146, Validation Loss: 0.025793688371777534\n",
      "Epoch 12, Batch: 195: Training Loss: 0.025856880471110344, Validation Loss: 0.0266641266644001\n",
      "Epoch 12, Batch: 196: Training Loss: 0.023791788145899773, Validation Loss: 0.02188766375184059\n",
      "Epoch 12, Batch: 197: Training Loss: 0.022538598626852036, Validation Loss: 0.023414436727762222\n",
      "Epoch 12, Batch: 198: Training Loss: 0.02081773430109024, Validation Loss: 0.02424454689025879\n",
      "Epoch 12, Batch: 199: Training Loss: 0.02301885187625885, Validation Loss: 0.026762405410408974\n",
      "Epoch 12, Batch: 200: Training Loss: 0.021726587787270546, Validation Loss: 0.022661006078124046\n",
      "Epoch 12, Batch: 201: Training Loss: 0.02594486065208912, Validation Loss: 0.023381918668746948\n",
      "Epoch 12, Batch: 202: Training Loss: 0.024513913318514824, Validation Loss: 0.022880611941218376\n",
      "Epoch 12, Batch: 203: Training Loss: 0.02566029131412506, Validation Loss: 0.02289949730038643\n",
      "Epoch 12, Batch: 204: Training Loss: 0.02641916647553444, Validation Loss: 0.026790892705321312\n",
      "Epoch 12, Batch: 205: Training Loss: 0.027732372283935547, Validation Loss: 0.02432863786816597\n",
      "Epoch 12, Batch: 206: Training Loss: 0.025336703285574913, Validation Loss: 0.025652730837464333\n",
      "Epoch 12, Batch: 207: Training Loss: 0.030876370146870613, Validation Loss: 0.02721165120601654\n",
      "Epoch 12, Batch: 208: Training Loss: 0.023521244525909424, Validation Loss: 0.025549234822392464\n",
      "Epoch 12, Batch: 209: Training Loss: 0.025058655068278313, Validation Loss: 0.025887535884976387\n",
      "Epoch 12, Batch: 210: Training Loss: 0.02856062911450863, Validation Loss: 0.023156573995947838\n",
      "Epoch 12, Batch: 211: Training Loss: 0.026859600096940994, Validation Loss: 0.02556844986975193\n",
      "Epoch 12, Batch: 212: Training Loss: 0.024784404784440994, Validation Loss: 0.025262754410505295\n",
      "Epoch 12, Batch: 213: Training Loss: 0.025265460833907127, Validation Loss: 0.025039227679371834\n",
      "Epoch 12, Batch: 214: Training Loss: 0.021303052082657814, Validation Loss: 0.027807964012026787\n",
      "Epoch 12, Batch: 215: Training Loss: 0.023679474368691444, Validation Loss: 0.022665495052933693\n",
      "Epoch 12, Batch: 216: Training Loss: 0.024574941024184227, Validation Loss: 0.025896569713950157\n",
      "Epoch 12, Batch: 217: Training Loss: 0.026332026347517967, Validation Loss: 0.02719677798449993\n",
      "Epoch 12, Batch: 218: Training Loss: 0.027168545871973038, Validation Loss: 0.024419857189059258\n",
      "Epoch 12, Batch: 219: Training Loss: 0.022514598444104195, Validation Loss: 0.02785097248852253\n",
      "Epoch 12, Batch: 220: Training Loss: 0.02637588605284691, Validation Loss: 0.02587219327688217\n",
      "Epoch 12, Batch: 221: Training Loss: 0.02642982453107834, Validation Loss: 0.025330470874905586\n",
      "Epoch 12, Batch: 222: Training Loss: 0.026191750541329384, Validation Loss: 0.026339998468756676\n",
      "Epoch 12, Batch: 223: Training Loss: 0.026932325214147568, Validation Loss: 0.02502506971359253\n",
      "Epoch 12, Batch: 224: Training Loss: 0.025664696469902992, Validation Loss: 0.023441333323717117\n",
      "Epoch 12, Batch: 225: Training Loss: 0.025408906862139702, Validation Loss: 0.02241169661283493\n",
      "Epoch 12, Batch: 226: Training Loss: 0.02767905220389366, Validation Loss: 0.02264329418540001\n",
      "Epoch 12, Batch: 227: Training Loss: 0.02431730180978775, Validation Loss: 0.027640467509627342\n",
      "Epoch 12, Batch: 228: Training Loss: 0.02793622389435768, Validation Loss: 0.02532091923058033\n",
      "Epoch 12, Batch: 229: Training Loss: 0.029299400746822357, Validation Loss: 0.02829604409635067\n",
      "Epoch 12, Batch: 230: Training Loss: 0.026030365377664566, Validation Loss: 0.02435600571334362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 231: Training Loss: 0.026764124631881714, Validation Loss: 0.025477616116404533\n",
      "Epoch 12, Batch: 232: Training Loss: 0.027593079954385757, Validation Loss: 0.027982257306575775\n",
      "Epoch 12, Batch: 233: Training Loss: 0.02362871542572975, Validation Loss: 0.025049906224012375\n",
      "Epoch 12, Batch: 234: Training Loss: 0.02629033476114273, Validation Loss: 0.027951525524258614\n",
      "Epoch 12, Batch: 235: Training Loss: 0.02618510089814663, Validation Loss: 0.028673946857452393\n",
      "Epoch 12, Batch: 236: Training Loss: 0.026184940710663795, Validation Loss: 0.030142098665237427\n",
      "Epoch 12, Batch: 237: Training Loss: 0.02648475021123886, Validation Loss: 0.027319904416799545\n",
      "Epoch 12, Batch: 238: Training Loss: 0.026971548795700073, Validation Loss: 0.025450339540839195\n",
      "Epoch 12, Batch: 239: Training Loss: 0.02630922943353653, Validation Loss: 0.028965763747692108\n",
      "Epoch 12, Batch: 240: Training Loss: 0.0253598652780056, Validation Loss: 0.028025344014167786\n",
      "Epoch 12, Batch: 241: Training Loss: 0.023000167682766914, Validation Loss: 0.026490891352295876\n",
      "Epoch 12, Batch: 242: Training Loss: 0.02634868584573269, Validation Loss: 0.02745489403605461\n",
      "Epoch 12, Batch: 243: Training Loss: 0.029225822538137436, Validation Loss: 0.02645166404545307\n",
      "Epoch 12, Batch: 244: Training Loss: 0.02522154152393341, Validation Loss: 0.02643800526857376\n",
      "Epoch 12, Batch: 245: Training Loss: 0.024812713265419006, Validation Loss: 0.027555610984563828\n",
      "Epoch 12, Batch: 246: Training Loss: 0.02586011029779911, Validation Loss: 0.024737536907196045\n",
      "Epoch 12, Batch: 247: Training Loss: 0.029149817302823067, Validation Loss: 0.026691842824220657\n",
      "Epoch 12, Batch: 248: Training Loss: 0.026536641642451286, Validation Loss: 0.027459079399704933\n",
      "Epoch 12, Batch: 249: Training Loss: 0.02809731476008892, Validation Loss: 0.026719681918621063\n",
      "Epoch 12, Batch: 250: Training Loss: 0.026465944945812225, Validation Loss: 0.027634838595986366\n",
      "Epoch 12, Batch: 251: Training Loss: 0.022495361045002937, Validation Loss: 0.02671363763511181\n",
      "Epoch 12, Batch: 252: Training Loss: 0.024961931630969048, Validation Loss: 0.028362657874822617\n",
      "Epoch 12, Batch: 253: Training Loss: 0.028043096885085106, Validation Loss: 0.02606641873717308\n",
      "Epoch 12, Batch: 254: Training Loss: 0.022543251514434814, Validation Loss: 0.02657599188387394\n",
      "Epoch 12, Batch: 255: Training Loss: 0.022542770951986313, Validation Loss: 0.027669519186019897\n",
      "Epoch 12, Batch: 256: Training Loss: 0.026913076639175415, Validation Loss: 0.02694769576191902\n",
      "Epoch 12, Batch: 257: Training Loss: 0.026098446920514107, Validation Loss: 0.026310613378882408\n",
      "Epoch 12, Batch: 258: Training Loss: 0.02698517218232155, Validation Loss: 0.023390550166368484\n",
      "Epoch 12, Batch: 259: Training Loss: 0.023878449574112892, Validation Loss: 0.026311125606298447\n",
      "Epoch 12, Batch: 260: Training Loss: 0.03235075995326042, Validation Loss: 0.02527940832078457\n",
      "Epoch 12, Batch: 261: Training Loss: 0.02691330574452877, Validation Loss: 0.025318529456853867\n",
      "Epoch 12, Batch: 262: Training Loss: 0.02532624639570713, Validation Loss: 0.024488326162099838\n",
      "Epoch 12, Batch: 263: Training Loss: 0.02529972605407238, Validation Loss: 0.025709988549351692\n",
      "Epoch 12, Batch: 264: Training Loss: 0.024293189868330956, Validation Loss: 0.023696552962064743\n",
      "Epoch 12, Batch: 265: Training Loss: 0.023991648107767105, Validation Loss: 0.025691309943795204\n",
      "Epoch 12, Batch: 266: Training Loss: 0.023484881967306137, Validation Loss: 0.024779528379440308\n",
      "Epoch 12, Batch: 267: Training Loss: 0.0259562935680151, Validation Loss: 0.024243010208010674\n",
      "Epoch 12, Batch: 268: Training Loss: 0.02268965356051922, Validation Loss: 0.023828895762562752\n",
      "Epoch 12, Batch: 269: Training Loss: 0.02589830569922924, Validation Loss: 0.02337951585650444\n",
      "Epoch 12, Batch: 270: Training Loss: 0.024089811369776726, Validation Loss: 0.023377154022455215\n",
      "Epoch 12, Batch: 271: Training Loss: 0.023315582424402237, Validation Loss: 0.024081774055957794\n",
      "Epoch 12, Batch: 272: Training Loss: 0.023526370525360107, Validation Loss: 0.026325590908527374\n",
      "Epoch 12, Batch: 273: Training Loss: 0.02661159448325634, Validation Loss: 0.02617703005671501\n",
      "Epoch 12, Batch: 274: Training Loss: 0.02783086523413658, Validation Loss: 0.024893613532185555\n",
      "Epoch 12, Batch: 275: Training Loss: 0.02479879930615425, Validation Loss: 0.028933320194482803\n",
      "Epoch 12, Batch: 276: Training Loss: 0.022573094815015793, Validation Loss: 0.02644716389477253\n",
      "Epoch 12, Batch: 277: Training Loss: 0.02289341576397419, Validation Loss: 0.025983452796936035\n",
      "Epoch 12, Batch: 278: Training Loss: 0.022415583953261375, Validation Loss: 0.02384110540151596\n",
      "Epoch 12, Batch: 279: Training Loss: 0.02780189923942089, Validation Loss: 0.026287544518709183\n",
      "Epoch 12, Batch: 280: Training Loss: 0.02322133630514145, Validation Loss: 0.02362227253615856\n",
      "Epoch 12, Batch: 281: Training Loss: 0.023115063086152077, Validation Loss: 0.026739295572042465\n",
      "Epoch 12, Batch: 282: Training Loss: 0.021827785298228264, Validation Loss: 0.025513440370559692\n",
      "Epoch 12, Batch: 283: Training Loss: 0.024574540555477142, Validation Loss: 0.026886288076639175\n",
      "Epoch 12, Batch: 284: Training Loss: 0.02421172522008419, Validation Loss: 0.024641752243041992\n",
      "Epoch 12, Batch: 285: Training Loss: 0.022913150489330292, Validation Loss: 0.023994870483875275\n",
      "Epoch 12, Batch: 286: Training Loss: 0.02306380681693554, Validation Loss: 0.024368980899453163\n",
      "Epoch 12, Batch: 287: Training Loss: 0.027350831776857376, Validation Loss: 0.024473413825035095\n",
      "Epoch 12, Batch: 288: Training Loss: 0.025337647646665573, Validation Loss: 0.02503601275384426\n",
      "Epoch 12, Batch: 289: Training Loss: 0.02818199433386326, Validation Loss: 0.026188423857092857\n",
      "Epoch 12, Batch: 290: Training Loss: 0.022750331088900566, Validation Loss: 0.025814807042479515\n",
      "Epoch 12, Batch: 291: Training Loss: 0.027879800647497177, Validation Loss: 0.02540748007595539\n",
      "Epoch 12, Batch: 292: Training Loss: 0.03086031600832939, Validation Loss: 0.02435985766351223\n",
      "Epoch 12, Batch: 293: Training Loss: 0.025320347398519516, Validation Loss: 0.02544105239212513\n",
      "Epoch 12, Batch: 294: Training Loss: 0.023180237039923668, Validation Loss: 0.02513887733221054\n",
      "Epoch 12, Batch: 295: Training Loss: 0.02366233803331852, Validation Loss: 0.023846639320254326\n",
      "Epoch 12, Batch: 296: Training Loss: 0.025981061160564423, Validation Loss: 0.026168562471866608\n",
      "Epoch 12, Batch: 297: Training Loss: 0.02672378160059452, Validation Loss: 0.02485473081469536\n",
      "Epoch 12, Batch: 298: Training Loss: 0.023720834404230118, Validation Loss: 0.026199258863925934\n",
      "Epoch 12, Batch: 299: Training Loss: 0.023029236122965813, Validation Loss: 0.024698754772543907\n",
      "Epoch 12, Batch: 300: Training Loss: 0.02597462758421898, Validation Loss: 0.02468607947230339\n",
      "Epoch 12, Batch: 301: Training Loss: 0.023571403697133064, Validation Loss: 0.02591485157608986\n",
      "Epoch 12, Batch: 302: Training Loss: 0.022396240383386612, Validation Loss: 0.024569833651185036\n",
      "Epoch 12, Batch: 303: Training Loss: 0.02286800555884838, Validation Loss: 0.02421683818101883\n",
      "Epoch 12, Batch: 304: Training Loss: 0.024593494832515717, Validation Loss: 0.02543221414089203\n",
      "Epoch 12, Batch: 305: Training Loss: 0.022570161148905754, Validation Loss: 0.023242831230163574\n",
      "Epoch 12, Batch: 306: Training Loss: 0.025575796142220497, Validation Loss: 0.023518282920122147\n",
      "Epoch 12, Batch: 307: Training Loss: 0.023986507207155228, Validation Loss: 0.02681017480790615\n",
      "Epoch 12, Batch: 308: Training Loss: 0.025758372619748116, Validation Loss: 0.02575071156024933\n",
      "Epoch 12, Batch: 309: Training Loss: 0.02565572038292885, Validation Loss: 0.025117183104157448\n",
      "Epoch 12, Batch: 310: Training Loss: 0.023934034630656242, Validation Loss: 0.025206299498677254\n",
      "Epoch 12, Batch: 311: Training Loss: 0.02397218905389309, Validation Loss: 0.024966467171907425\n",
      "Epoch 12, Batch: 312: Training Loss: 0.024674175307154655, Validation Loss: 0.025948509573936462\n",
      "Epoch 12, Batch: 313: Training Loss: 0.024518264457583427, Validation Loss: 0.024092255160212517\n",
      "Epoch 12, Batch: 314: Training Loss: 0.02494581788778305, Validation Loss: 0.025893602520227432\n",
      "Epoch 12, Batch: 315: Training Loss: 0.026179591193795204, Validation Loss: 0.024390235543251038\n",
      "Epoch 12, Batch: 316: Training Loss: 0.022394809871912003, Validation Loss: 0.026361936703324318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 317: Training Loss: 0.022659093141555786, Validation Loss: 0.025579601526260376\n",
      "Epoch 12, Batch: 318: Training Loss: 0.02336050197482109, Validation Loss: 0.024260595440864563\n",
      "Epoch 12, Batch: 319: Training Loss: 0.024449575692415237, Validation Loss: 0.02673862688243389\n",
      "Epoch 12, Batch: 320: Training Loss: 0.02524893544614315, Validation Loss: 0.02703399769961834\n",
      "Epoch 12, Batch: 321: Training Loss: 0.026233134791254997, Validation Loss: 0.02444748394191265\n",
      "Epoch 12, Batch: 322: Training Loss: 0.021878257393836975, Validation Loss: 0.02602277509868145\n",
      "Epoch 12, Batch: 323: Training Loss: 0.02445913664996624, Validation Loss: 0.023911938071250916\n",
      "Epoch 12, Batch: 324: Training Loss: 0.02451474778354168, Validation Loss: 0.024647168815135956\n",
      "Epoch 12, Batch: 325: Training Loss: 0.02497108094394207, Validation Loss: 0.025189505890011787\n",
      "Epoch 12, Batch: 326: Training Loss: 0.026889165863394737, Validation Loss: 0.02508113719522953\n",
      "Epoch 12, Batch: 327: Training Loss: 0.02206248976290226, Validation Loss: 0.024928122758865356\n",
      "Epoch 12, Batch: 328: Training Loss: 0.02306867390871048, Validation Loss: 0.02608812041580677\n",
      "Epoch 12, Batch: 329: Training Loss: 0.025896325707435608, Validation Loss: 0.025997169315814972\n",
      "Epoch 12, Batch: 330: Training Loss: 0.02425360679626465, Validation Loss: 0.025448553264141083\n",
      "Epoch 12, Batch: 331: Training Loss: 0.025816066190600395, Validation Loss: 0.02525929920375347\n",
      "Epoch 12, Batch: 332: Training Loss: 0.022854460403323174, Validation Loss: 0.025798464193940163\n",
      "Epoch 12, Batch: 333: Training Loss: 0.023806540295481682, Validation Loss: 0.023580024018883705\n",
      "Epoch 12, Batch: 334: Training Loss: 0.023770224303007126, Validation Loss: 0.025218555703759193\n",
      "Epoch 12, Batch: 335: Training Loss: 0.023545945063233376, Validation Loss: 0.02607097662985325\n",
      "Epoch 12, Batch: 336: Training Loss: 0.024203315377235413, Validation Loss: 0.026967519894242287\n",
      "Epoch 12, Batch: 337: Training Loss: 0.023217743262648582, Validation Loss: 0.026256805285811424\n",
      "Epoch 12, Batch: 338: Training Loss: 0.02659796178340912, Validation Loss: 0.026658937335014343\n",
      "Epoch 12, Batch: 339: Training Loss: 0.026051489636301994, Validation Loss: 0.02537606656551361\n",
      "Epoch 12, Batch: 340: Training Loss: 0.024666868150234222, Validation Loss: 0.02793562412261963\n",
      "Epoch 12, Batch: 341: Training Loss: 0.024217205122113228, Validation Loss: 0.024208413437008858\n",
      "Epoch 12, Batch: 342: Training Loss: 0.02558143436908722, Validation Loss: 0.025566425174474716\n",
      "Epoch 12, Batch: 343: Training Loss: 0.026317613199353218, Validation Loss: 0.028015784919261932\n",
      "Epoch 12, Batch: 344: Training Loss: 0.024944549426436424, Validation Loss: 0.027096567675471306\n",
      "Epoch 12, Batch: 345: Training Loss: 0.023852655664086342, Validation Loss: 0.027973448857665062\n",
      "Epoch 12, Batch: 346: Training Loss: 0.0286184661090374, Validation Loss: 0.024832434952259064\n",
      "Epoch 12, Batch: 347: Training Loss: 0.02318205125629902, Validation Loss: 0.027271650731563568\n",
      "Epoch 12, Batch: 348: Training Loss: 0.027600852772593498, Validation Loss: 0.025732360780239105\n",
      "Epoch 12, Batch: 349: Training Loss: 0.026680801063776016, Validation Loss: 0.026722008362412453\n",
      "Epoch 12, Batch: 350: Training Loss: 0.023259220644831657, Validation Loss: 0.02487664669752121\n",
      "Epoch 12, Batch: 351: Training Loss: 0.031028136610984802, Validation Loss: 0.025675896555185318\n",
      "Epoch 12, Batch: 352: Training Loss: 0.024053243920207024, Validation Loss: 0.02317502349615097\n",
      "Epoch 12, Batch: 353: Training Loss: 0.027673793956637383, Validation Loss: 0.022826608270406723\n",
      "Epoch 12, Batch: 354: Training Loss: 0.026051271706819534, Validation Loss: 0.022760076448321342\n",
      "Epoch 12, Batch: 355: Training Loss: 0.023107508197426796, Validation Loss: 0.021965334191918373\n",
      "Epoch 12, Batch: 356: Training Loss: 0.024790314957499504, Validation Loss: 0.023586656898260117\n",
      "Epoch 12, Batch: 357: Training Loss: 0.02458023466169834, Validation Loss: 0.023345638066530228\n",
      "Epoch 12, Batch: 358: Training Loss: 0.024659041315317154, Validation Loss: 0.023913633078336716\n",
      "Epoch 12, Batch: 359: Training Loss: 0.025150680914521217, Validation Loss: 0.02186385542154312\n",
      "Epoch 12, Batch: 360: Training Loss: 0.024578098207712173, Validation Loss: 0.024813750758767128\n",
      "Epoch 12, Batch: 361: Training Loss: 0.025660725310444832, Validation Loss: 0.022734789177775383\n",
      "Epoch 12, Batch: 362: Training Loss: 0.021571744233369827, Validation Loss: 0.0227164588868618\n",
      "Epoch 12, Batch: 363: Training Loss: 0.02560313232243061, Validation Loss: 0.02241510897874832\n",
      "Epoch 12, Batch: 364: Training Loss: 0.02272581309080124, Validation Loss: 0.02248588763177395\n",
      "Epoch 12, Batch: 365: Training Loss: 0.024376193061470985, Validation Loss: 0.024327857419848442\n",
      "Epoch 12, Batch: 366: Training Loss: 0.022825511172413826, Validation Loss: 0.02419252134859562\n",
      "Epoch 12, Batch: 367: Training Loss: 0.021424390375614166, Validation Loss: 0.0234358012676239\n",
      "Epoch 12, Batch: 368: Training Loss: 0.02167692966759205, Validation Loss: 0.02438437007367611\n",
      "Epoch 12, Batch: 369: Training Loss: 0.025276634842157364, Validation Loss: 0.02191174030303955\n",
      "Epoch 12, Batch: 370: Training Loss: 0.022173095494508743, Validation Loss: 0.02465434931218624\n",
      "Epoch 12, Batch: 371: Training Loss: 0.02323721908032894, Validation Loss: 0.0232833381742239\n",
      "Epoch 12, Batch: 372: Training Loss: 0.02422839030623436, Validation Loss: 0.023407919332385063\n",
      "Epoch 12, Batch: 373: Training Loss: 0.026785513386130333, Validation Loss: 0.025059524923563004\n",
      "Epoch 12, Batch: 374: Training Loss: 0.022339927032589912, Validation Loss: 0.023937448859214783\n",
      "Epoch 12, Batch: 375: Training Loss: 0.02604001946747303, Validation Loss: 0.022504514083266258\n",
      "Epoch 12, Batch: 376: Training Loss: 0.02097180113196373, Validation Loss: 0.024560634046792984\n",
      "Epoch 12, Batch: 377: Training Loss: 0.022640850394964218, Validation Loss: 0.022524716332554817\n",
      "Epoch 12, Batch: 378: Training Loss: 0.0204747524112463, Validation Loss: 0.022678310051560402\n",
      "Epoch 12, Batch: 379: Training Loss: 0.025234045460820198, Validation Loss: 0.021419228985905647\n",
      "Epoch 12, Batch: 380: Training Loss: 0.02882790006697178, Validation Loss: 0.023999541997909546\n",
      "Epoch 12, Batch: 381: Training Loss: 0.02267608605325222, Validation Loss: 0.025054674595594406\n",
      "Epoch 12, Batch: 382: Training Loss: 0.02425750531256199, Validation Loss: 0.02459082193672657\n",
      "Epoch 12, Batch: 383: Training Loss: 0.02549278363585472, Validation Loss: 0.023241698741912842\n",
      "Epoch 12, Batch: 384: Training Loss: 0.025905873626470566, Validation Loss: 0.022553298622369766\n",
      "Epoch 12, Batch: 385: Training Loss: 0.0219834316521883, Validation Loss: 0.024183452129364014\n",
      "Epoch 12, Batch: 386: Training Loss: 0.020179346203804016, Validation Loss: 0.02239575795829296\n",
      "Epoch 12, Batch: 387: Training Loss: 0.022461609914898872, Validation Loss: 0.02430172823369503\n",
      "Epoch 12, Batch: 388: Training Loss: 0.02396904118359089, Validation Loss: 0.025396374985575676\n",
      "Epoch 12, Batch: 389: Training Loss: 0.024880295619368553, Validation Loss: 0.024235324934124947\n",
      "Epoch 12, Batch: 390: Training Loss: 0.026578010991215706, Validation Loss: 0.02544444240629673\n",
      "Epoch 12, Batch: 391: Training Loss: 0.02303098887205124, Validation Loss: 0.02669774368405342\n",
      "Epoch 12, Batch: 392: Training Loss: 0.024469926953315735, Validation Loss: 0.024844570085406303\n",
      "Epoch 12, Batch: 393: Training Loss: 0.024090392515063286, Validation Loss: 0.022314542904496193\n",
      "Epoch 12, Batch: 394: Training Loss: 0.023585408926010132, Validation Loss: 0.02487127110362053\n",
      "Epoch 12, Batch: 395: Training Loss: 0.022279614582657814, Validation Loss: 0.02117229253053665\n",
      "Epoch 12, Batch: 396: Training Loss: 0.02489108219742775, Validation Loss: 0.024978993460536003\n",
      "Epoch 12, Batch: 397: Training Loss: 0.02436310425400734, Validation Loss: 0.025389129295945168\n",
      "Epoch 12, Batch: 398: Training Loss: 0.02364080399274826, Validation Loss: 0.024940503761172295\n",
      "Epoch 12, Batch: 399: Training Loss: 0.02491365373134613, Validation Loss: 0.022622263059020042\n",
      "Epoch 12, Batch: 400: Training Loss: 0.024753810837864876, Validation Loss: 0.02287309430539608\n",
      "Epoch 12, Batch: 401: Training Loss: 0.02291063964366913, Validation Loss: 0.023430416360497475\n",
      "Epoch 12, Batch: 402: Training Loss: 0.022750334814190865, Validation Loss: 0.02479991875588894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 403: Training Loss: 0.026475343853235245, Validation Loss: 0.024319777265191078\n",
      "Epoch 12, Batch: 404: Training Loss: 0.021334027871489525, Validation Loss: 0.024486921727657318\n",
      "Epoch 12, Batch: 405: Training Loss: 0.02456546016037464, Validation Loss: 0.023302122950553894\n",
      "Epoch 12, Batch: 406: Training Loss: 0.022415826097130775, Validation Loss: 0.02340659312903881\n",
      "Epoch 12, Batch: 407: Training Loss: 0.021305348724126816, Validation Loss: 0.023489005863666534\n",
      "Epoch 12, Batch: 408: Training Loss: 0.020613983273506165, Validation Loss: 0.022215325385332108\n",
      "Epoch 12, Batch: 409: Training Loss: 0.023805877193808556, Validation Loss: 0.022686973214149475\n",
      "Epoch 12, Batch: 410: Training Loss: 0.024100903421640396, Validation Loss: 0.02484162524342537\n",
      "Epoch 12, Batch: 411: Training Loss: 0.02459319867193699, Validation Loss: 0.024887362495064735\n",
      "Epoch 12, Batch: 412: Training Loss: 0.026917310431599617, Validation Loss: 0.022998670116066933\n",
      "Epoch 12, Batch: 413: Training Loss: 0.025197207927703857, Validation Loss: 0.02546251378953457\n",
      "Epoch 12, Batch: 414: Training Loss: 0.026099493727087975, Validation Loss: 0.026067206636071205\n",
      "Epoch 12, Batch: 415: Training Loss: 0.025118572637438774, Validation Loss: 0.02362431399524212\n",
      "Epoch 12, Batch: 416: Training Loss: 0.023350561037659645, Validation Loss: 0.02432156726717949\n",
      "Epoch 12, Batch: 417: Training Loss: 0.02334265224635601, Validation Loss: 0.02427666261792183\n",
      "Epoch 12, Batch: 418: Training Loss: 0.021338190883398056, Validation Loss: 0.02422851324081421\n",
      "Epoch 12, Batch: 419: Training Loss: 0.02503548189997673, Validation Loss: 0.023115038871765137\n",
      "Epoch 12, Batch: 420: Training Loss: 0.024050476029515266, Validation Loss: 0.0238486360758543\n",
      "Epoch 12, Batch: 421: Training Loss: 0.02531871385872364, Validation Loss: 0.02400796115398407\n",
      "Epoch 12, Batch: 422: Training Loss: 0.02640502341091633, Validation Loss: 0.026109997183084488\n",
      "Epoch 12, Batch: 423: Training Loss: 0.024814696982502937, Validation Loss: 0.025869619101285934\n",
      "Epoch 12, Batch: 424: Training Loss: 0.024865014478564262, Validation Loss: 0.021682007238268852\n",
      "Epoch 12, Batch: 425: Training Loss: 0.020390190184116364, Validation Loss: 0.02330092154443264\n",
      "Epoch 12, Batch: 426: Training Loss: 0.025588922202587128, Validation Loss: 0.025024980306625366\n",
      "Epoch 12, Batch: 427: Training Loss: 0.02685963362455368, Validation Loss: 0.028020160272717476\n",
      "Epoch 12, Batch: 428: Training Loss: 0.02219589240849018, Validation Loss: 0.024999156594276428\n",
      "Epoch 12, Batch: 429: Training Loss: 0.02127884328365326, Validation Loss: 0.023750238120555878\n",
      "Epoch 12, Batch: 430: Training Loss: 0.02314000017940998, Validation Loss: 0.024819185957312584\n",
      "Epoch 12, Batch: 431: Training Loss: 0.02184283919632435, Validation Loss: 0.024096308276057243\n",
      "Epoch 12, Batch: 432: Training Loss: 0.025991275906562805, Validation Loss: 0.025346048176288605\n",
      "Epoch 12, Batch: 433: Training Loss: 0.026759380474686623, Validation Loss: 0.024280350655317307\n",
      "Epoch 12, Batch: 434: Training Loss: 0.02354099415242672, Validation Loss: 0.024740364402532578\n",
      "Epoch 12, Batch: 435: Training Loss: 0.026248948648571968, Validation Loss: 0.02492607943713665\n",
      "Epoch 12, Batch: 436: Training Loss: 0.023104341700673103, Validation Loss: 0.022898834198713303\n",
      "Epoch 12, Batch: 437: Training Loss: 0.02490616776049137, Validation Loss: 0.024720560759305954\n",
      "Epoch 12, Batch: 438: Training Loss: 0.02542872168123722, Validation Loss: 0.026304941624403\n",
      "Epoch 12, Batch: 439: Training Loss: 0.022791368886828423, Validation Loss: 0.024539200589060783\n",
      "Epoch 12, Batch: 440: Training Loss: 0.028569862246513367, Validation Loss: 0.025767063722014427\n",
      "Epoch 12, Batch: 441: Training Loss: 0.028655728325247765, Validation Loss: 0.023514561355113983\n",
      "Epoch 12, Batch: 442: Training Loss: 0.026906967163085938, Validation Loss: 0.02688821405172348\n",
      "Epoch 12, Batch: 443: Training Loss: 0.023654840886592865, Validation Loss: 0.025217657908797264\n",
      "Epoch 12, Batch: 444: Training Loss: 0.022981559857726097, Validation Loss: 0.02588779106736183\n",
      "Epoch 12, Batch: 445: Training Loss: 0.02421693690121174, Validation Loss: 0.02703172340989113\n",
      "Epoch 12, Batch: 446: Training Loss: 0.026635555550456047, Validation Loss: 0.027112871408462524\n",
      "Epoch 12, Batch: 447: Training Loss: 0.022408220916986465, Validation Loss: 0.02621747925877571\n",
      "Epoch 12, Batch: 448: Training Loss: 0.025450389832258224, Validation Loss: 0.02620682306587696\n",
      "Epoch 12, Batch: 449: Training Loss: 0.023184573277831078, Validation Loss: 0.024140767753124237\n",
      "Epoch 12, Batch: 450: Training Loss: 0.02735655941069126, Validation Loss: 0.02416866272687912\n",
      "Epoch 12, Batch: 451: Training Loss: 0.024966882541775703, Validation Loss: 0.029555631801486015\n",
      "Epoch 12, Batch: 452: Training Loss: 0.029840609058737755, Validation Loss: 0.02722252532839775\n",
      "Epoch 12, Batch: 453: Training Loss: 0.022721968591213226, Validation Loss: 0.025231381878256798\n",
      "Epoch 12, Batch: 454: Training Loss: 0.0280190110206604, Validation Loss: 0.02585969679057598\n",
      "Epoch 12, Batch: 455: Training Loss: 0.023209672421216965, Validation Loss: 0.026528989896178246\n",
      "Epoch 12, Batch: 456: Training Loss: 0.027238423004746437, Validation Loss: 0.02779519557952881\n",
      "Epoch 12, Batch: 457: Training Loss: 0.027799129486083984, Validation Loss: 0.027050459757447243\n",
      "Epoch 12, Batch: 458: Training Loss: 0.020055722445249557, Validation Loss: 0.02481137029826641\n",
      "Epoch 12, Batch: 459: Training Loss: 0.022746659815311432, Validation Loss: 0.025472810491919518\n",
      "Epoch 12, Batch: 460: Training Loss: 0.02384106069803238, Validation Loss: 0.025577427819371223\n",
      "Epoch 12, Batch: 461: Training Loss: 0.025735948234796524, Validation Loss: 0.024504302069544792\n",
      "Epoch 12, Batch: 462: Training Loss: 0.02195265144109726, Validation Loss: 0.023250369355082512\n",
      "Epoch 12, Batch: 463: Training Loss: 0.027123484760522842, Validation Loss: 0.023828329518437386\n",
      "Epoch 12, Batch: 464: Training Loss: 0.024052424356341362, Validation Loss: 0.025075674057006836\n",
      "Epoch 12, Batch: 465: Training Loss: 0.024029714986681938, Validation Loss: 0.024968726560473442\n",
      "Epoch 12, Batch: 466: Training Loss: 0.023550907149910927, Validation Loss: 0.026043377816677094\n",
      "Epoch 12, Batch: 467: Training Loss: 0.02944023162126541, Validation Loss: 0.022658484056591988\n",
      "Epoch 12, Batch: 468: Training Loss: 0.030230022966861725, Validation Loss: 0.024405086413025856\n",
      "Epoch 12, Batch: 469: Training Loss: 0.0229725930839777, Validation Loss: 0.02433636412024498\n",
      "Epoch 12, Batch: 470: Training Loss: 0.02515995502471924, Validation Loss: 0.02495868317782879\n",
      "Epoch 12, Batch: 471: Training Loss: 0.027688369154930115, Validation Loss: 0.024745341390371323\n",
      "Epoch 12, Batch: 472: Training Loss: 0.028633909299969673, Validation Loss: 0.022946516051888466\n",
      "Epoch 12, Batch: 473: Training Loss: 0.02368238754570484, Validation Loss: 0.024726977571845055\n",
      "Epoch 12, Batch: 474: Training Loss: 0.024605916813015938, Validation Loss: 0.024079186841845512\n",
      "Epoch 12, Batch: 475: Training Loss: 0.0282016359269619, Validation Loss: 0.024098007008433342\n",
      "Epoch 12, Batch: 476: Training Loss: 0.02540282905101776, Validation Loss: 0.026762669906020164\n",
      "Epoch 12, Batch: 477: Training Loss: 0.02632828615605831, Validation Loss: 0.023515604436397552\n",
      "Epoch 12, Batch: 478: Training Loss: 0.024560347199440002, Validation Loss: 0.025037774816155434\n",
      "Epoch 12, Batch: 479: Training Loss: 0.024982627481222153, Validation Loss: 0.02591194398701191\n",
      "Epoch 12, Batch: 480: Training Loss: 0.023882688954472542, Validation Loss: 0.025498725473880768\n",
      "Epoch 12, Batch: 481: Training Loss: 0.02528764121234417, Validation Loss: 0.026414668187499046\n",
      "Epoch 12, Batch: 482: Training Loss: 0.028587335720658302, Validation Loss: 0.027073146775364876\n",
      "Epoch 12, Batch: 483: Training Loss: 0.0232752226293087, Validation Loss: 0.026625730097293854\n",
      "Epoch 12, Batch: 484: Training Loss: 0.025843624025583267, Validation Loss: 0.025864610448479652\n",
      "Epoch 12, Batch: 485: Training Loss: 0.02472011372447014, Validation Loss: 0.02635660208761692\n",
      "Epoch 12, Batch: 486: Training Loss: 0.02801603078842163, Validation Loss: 0.027301311492919922\n",
      "Epoch 12, Batch: 487: Training Loss: 0.02328014187514782, Validation Loss: 0.02463105134665966\n",
      "Epoch 12, Batch: 488: Training Loss: 0.025536488741636276, Validation Loss: 0.02499229833483696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch: 489: Training Loss: 0.02999039553105831, Validation Loss: 0.025155724957585335\n",
      "Epoch 12, Batch: 490: Training Loss: 0.027081485837697983, Validation Loss: 0.025928566232323647\n",
      "Epoch 12, Batch: 491: Training Loss: 0.02293507196009159, Validation Loss: 0.027040425688028336\n",
      "Epoch 12, Batch: 492: Training Loss: 0.028315408155322075, Validation Loss: 0.02501099929213524\n",
      "Epoch 12, Batch: 493: Training Loss: 0.028904074802994728, Validation Loss: 0.02455345168709755\n",
      "Epoch 12, Batch: 494: Training Loss: 0.028198067098855972, Validation Loss: 0.026071898639202118\n",
      "Epoch 12, Batch: 495: Training Loss: 0.027796689420938492, Validation Loss: 0.025267578661441803\n",
      "Epoch 12, Batch: 496: Training Loss: 0.02274012565612793, Validation Loss: 0.025296423584222794\n",
      "Epoch 12, Batch: 497: Training Loss: 0.02445034496486187, Validation Loss: 0.02499631978571415\n",
      "Epoch 12, Batch: 498: Training Loss: 0.022677786648273468, Validation Loss: 0.026796869933605194\n",
      "Epoch 12, Batch: 499: Training Loss: 0.022309964522719383, Validation Loss: 0.026861872524023056\n",
      "Epoch 13, Batch: 0: Training Loss: 0.023899974301457405, Validation Loss: 0.02564462274312973\n",
      "Epoch 13, Batch: 1: Training Loss: 0.0221471656113863, Validation Loss: 0.028995433822274208\n",
      "Epoch 13, Batch: 2: Training Loss: 0.026084933429956436, Validation Loss: 0.022702990099787712\n",
      "Epoch 13, Batch: 3: Training Loss: 0.02284054271876812, Validation Loss: 0.026444682851433754\n",
      "Epoch 13, Batch: 4: Training Loss: 0.023594895377755165, Validation Loss: 0.025564968585968018\n",
      "Epoch 13, Batch: 5: Training Loss: 0.02357734926044941, Validation Loss: 0.022596001625061035\n",
      "Epoch 13, Batch: 6: Training Loss: 0.022043582051992416, Validation Loss: 0.024511490017175674\n",
      "Epoch 13, Batch: 7: Training Loss: 0.023751692846417427, Validation Loss: 0.023335138335824013\n",
      "Epoch 13, Batch: 8: Training Loss: 0.026020241901278496, Validation Loss: 0.024991363286972046\n",
      "Epoch 13, Batch: 9: Training Loss: 0.022437408566474915, Validation Loss: 0.024435818195343018\n",
      "Epoch 13, Batch: 10: Training Loss: 0.028814788907766342, Validation Loss: 0.025909751653671265\n",
      "Epoch 13, Batch: 11: Training Loss: 0.024193666875362396, Validation Loss: 0.02513134479522705\n",
      "Epoch 13, Batch: 12: Training Loss: 0.024811245501041412, Validation Loss: 0.025757405906915665\n",
      "Epoch 13, Batch: 13: Training Loss: 0.02931608445942402, Validation Loss: 0.025364356115460396\n",
      "Epoch 13, Batch: 14: Training Loss: 0.026021314784884453, Validation Loss: 0.025505397468805313\n",
      "Epoch 13, Batch: 15: Training Loss: 0.026578718796372414, Validation Loss: 0.025520026683807373\n",
      "Epoch 13, Batch: 16: Training Loss: 0.027830203995108604, Validation Loss: 0.023916205391287804\n",
      "Epoch 13, Batch: 17: Training Loss: 0.02195511758327484, Validation Loss: 0.026768798008561134\n",
      "Epoch 13, Batch: 18: Training Loss: 0.02215389348566532, Validation Loss: 0.02391241118311882\n",
      "Epoch 13, Batch: 19: Training Loss: 0.022909002378582954, Validation Loss: 0.02320464327931404\n",
      "Epoch 13, Batch: 20: Training Loss: 0.021955182775855064, Validation Loss: 0.0252080038189888\n",
      "Epoch 13, Batch: 21: Training Loss: 0.02404012903571129, Validation Loss: 0.02509107068181038\n",
      "Epoch 13, Batch: 22: Training Loss: 0.026712000370025635, Validation Loss: 0.023068701848387718\n",
      "Epoch 13, Batch: 23: Training Loss: 0.02349868416786194, Validation Loss: 0.027444154024124146\n",
      "Epoch 13, Batch: 24: Training Loss: 0.024062247946858406, Validation Loss: 0.027156364172697067\n",
      "Epoch 13, Batch: 25: Training Loss: 0.024566739797592163, Validation Loss: 0.026210973039269447\n",
      "Epoch 13, Batch: 26: Training Loss: 0.02827199175953865, Validation Loss: 0.026980960741639137\n",
      "Epoch 13, Batch: 27: Training Loss: 0.02550152875483036, Validation Loss: 0.02701178379356861\n",
      "Epoch 13, Batch: 28: Training Loss: 0.02633860893547535, Validation Loss: 0.025040721520781517\n",
      "Epoch 13, Batch: 29: Training Loss: 0.026424551382660866, Validation Loss: 0.025853469967842102\n",
      "Epoch 13, Batch: 30: Training Loss: 0.02274825982749462, Validation Loss: 0.023287389427423477\n",
      "Epoch 13, Batch: 31: Training Loss: 0.026125095784664154, Validation Loss: 0.023411398753523827\n",
      "Epoch 13, Batch: 32: Training Loss: 0.023372871801257133, Validation Loss: 0.023926308378577232\n",
      "Epoch 13, Batch: 33: Training Loss: 0.022855494171380997, Validation Loss: 0.025392452254891396\n",
      "Epoch 13, Batch: 34: Training Loss: 0.022833645343780518, Validation Loss: 0.02271861955523491\n",
      "Epoch 13, Batch: 35: Training Loss: 0.025906290858983994, Validation Loss: 0.023925695568323135\n",
      "Epoch 13, Batch: 36: Training Loss: 0.02509911358356476, Validation Loss: 0.02182084508240223\n",
      "Epoch 13, Batch: 37: Training Loss: 0.02232394553720951, Validation Loss: 0.022796250879764557\n",
      "Epoch 13, Batch: 38: Training Loss: 0.027851620689034462, Validation Loss: 0.023750999942421913\n",
      "Epoch 13, Batch: 39: Training Loss: 0.026783786714076996, Validation Loss: 0.024052666500210762\n",
      "Epoch 13, Batch: 40: Training Loss: 0.025462564080953598, Validation Loss: 0.023906605318188667\n",
      "Epoch 13, Batch: 41: Training Loss: 0.025015758350491524, Validation Loss: 0.025567742064595222\n",
      "Epoch 13, Batch: 42: Training Loss: 0.023860661312937737, Validation Loss: 0.026299789547920227\n",
      "Epoch 13, Batch: 43: Training Loss: 0.023261835798621178, Validation Loss: 0.02576402574777603\n",
      "Epoch 13, Batch: 44: Training Loss: 0.025896022096276283, Validation Loss: 0.026713905856013298\n",
      "Epoch 13, Batch: 45: Training Loss: 0.023204956203699112, Validation Loss: 0.024160346016287804\n",
      "Epoch 13, Batch: 46: Training Loss: 0.02848796173930168, Validation Loss: 0.02527580037713051\n",
      "Epoch 13, Batch: 47: Training Loss: 0.028342485427856445, Validation Loss: 0.026608655229210854\n",
      "Epoch 13, Batch: 48: Training Loss: 0.028967203572392464, Validation Loss: 0.027601314708590508\n",
      "Epoch 13, Batch: 49: Training Loss: 0.025349298492074013, Validation Loss: 0.02558346837759018\n",
      "Epoch 13, Batch: 50: Training Loss: 0.02665393240749836, Validation Loss: 0.026874253526329994\n",
      "Epoch 13, Batch: 51: Training Loss: 0.024909598752856255, Validation Loss: 0.02635948173701763\n",
      "Epoch 13, Batch: 52: Training Loss: 0.022909624502062798, Validation Loss: 0.02338608168065548\n",
      "Epoch 13, Batch: 53: Training Loss: 0.020169254392385483, Validation Loss: 0.022720003500580788\n",
      "Epoch 13, Batch: 54: Training Loss: 0.025084519758820534, Validation Loss: 0.02347007393836975\n",
      "Epoch 13, Batch: 55: Training Loss: 0.021121200174093246, Validation Loss: 0.023961646482348442\n",
      "Epoch 13, Batch: 56: Training Loss: 0.025171376764774323, Validation Loss: 0.023218711838126183\n",
      "Epoch 13, Batch: 57: Training Loss: 0.021903526037931442, Validation Loss: 0.02266016975045204\n",
      "Epoch 13, Batch: 58: Training Loss: 0.022118929773569107, Validation Loss: 0.02569160796701908\n",
      "Epoch 13, Batch: 59: Training Loss: 0.01984706148505211, Validation Loss: 0.0246602613478899\n",
      "Epoch 13, Batch: 60: Training Loss: 0.022682929411530495, Validation Loss: 0.025572745129466057\n",
      "Epoch 13, Batch: 61: Training Loss: 0.025953225791454315, Validation Loss: 0.024121562018990517\n",
      "Epoch 13, Batch: 62: Training Loss: 0.02416904829442501, Validation Loss: 0.02563205547630787\n",
      "Epoch 13, Batch: 63: Training Loss: 0.025615299120545387, Validation Loss: 0.025238465517759323\n",
      "Epoch 13, Batch: 64: Training Loss: 0.022341061383485794, Validation Loss: 0.025299934670329094\n",
      "Epoch 13, Batch: 65: Training Loss: 0.028600992634892464, Validation Loss: 0.024138368666172028\n",
      "Epoch 13, Batch: 66: Training Loss: 0.02417764626443386, Validation Loss: 0.024020930752158165\n",
      "Epoch 13, Batch: 67: Training Loss: 0.02234581485390663, Validation Loss: 0.025971202179789543\n",
      "Epoch 13, Batch: 68: Training Loss: 0.02216923236846924, Validation Loss: 0.026376938447356224\n",
      "Epoch 13, Batch: 69: Training Loss: 0.024939866736531258, Validation Loss: 0.024421188980340958\n",
      "Epoch 13, Batch: 70: Training Loss: 0.02316245250403881, Validation Loss: 0.0258060060441494\n",
      "Epoch 13, Batch: 71: Training Loss: 0.021393362432718277, Validation Loss: 0.025685695931315422\n",
      "Epoch 13, Batch: 72: Training Loss: 0.021237444132566452, Validation Loss: 0.026647470891475677\n",
      "Epoch 13, Batch: 73: Training Loss: 0.020678086206316948, Validation Loss: 0.026136960834264755\n",
      "Epoch 13, Batch: 74: Training Loss: 0.025304490700364113, Validation Loss: 0.02608191780745983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch: 75: Training Loss: 0.020421920344233513, Validation Loss: 0.026964958757162094\n",
      "Epoch 13, Batch: 76: Training Loss: 0.021037278696894646, Validation Loss: 0.02590930089354515\n",
      "Epoch 13, Batch: 77: Training Loss: 0.026686597615480423, Validation Loss: 0.026551727205514908\n",
      "Epoch 13, Batch: 78: Training Loss: 0.02410268411040306, Validation Loss: 0.026482965797185898\n",
      "Epoch 13, Batch: 79: Training Loss: 0.02493100054562092, Validation Loss: 0.024933606386184692\n",
      "Epoch 13, Batch: 80: Training Loss: 0.02312658168375492, Validation Loss: 0.026004409417510033\n",
      "Epoch 13, Batch: 81: Training Loss: 0.026424508541822433, Validation Loss: 0.027775051072239876\n",
      "Epoch 13, Batch: 82: Training Loss: 0.024670762941241264, Validation Loss: 0.026996269822120667\n",
      "Epoch 13, Batch: 83: Training Loss: 0.025132501497864723, Validation Loss: 0.026964476332068443\n",
      "Epoch 13, Batch: 84: Training Loss: 0.026762494817376137, Validation Loss: 0.027212852612137794\n",
      "Epoch 13, Batch: 85: Training Loss: 0.020505966618657112, Validation Loss: 0.026753513142466545\n",
      "Epoch 13, Batch: 86: Training Loss: 0.02467874437570572, Validation Loss: 0.02794436551630497\n",
      "Epoch 13, Batch: 87: Training Loss: 0.025860168039798737, Validation Loss: 0.02647365815937519\n",
      "Epoch 13, Batch: 88: Training Loss: 0.024940932169556618, Validation Loss: 0.0251791812479496\n",
      "Epoch 13, Batch: 89: Training Loss: 0.028582630679011345, Validation Loss: 0.0220101960003376\n",
      "Epoch 13, Batch: 90: Training Loss: 0.02401016280055046, Validation Loss: 0.024120625108480453\n",
      "Epoch 13, Batch: 91: Training Loss: 0.02547955885529518, Validation Loss: 0.023201631382107735\n",
      "Epoch 13, Batch: 92: Training Loss: 0.027199137955904007, Validation Loss: 0.02273409441113472\n",
      "Epoch 13, Batch: 93: Training Loss: 0.024093080312013626, Validation Loss: 0.02279767394065857\n",
      "Epoch 13, Batch: 94: Training Loss: 0.029601633548736572, Validation Loss: 0.022700410336256027\n",
      "Epoch 13, Batch: 95: Training Loss: 0.024240480735898018, Validation Loss: 0.02172803319990635\n",
      "Epoch 13, Batch: 96: Training Loss: 0.021631542593240738, Validation Loss: 0.02175888605415821\n",
      "Epoch 13, Batch: 97: Training Loss: 0.02479948289692402, Validation Loss: 0.022638574242591858\n",
      "Epoch 13, Batch: 98: Training Loss: 0.026567289605736732, Validation Loss: 0.025483855977654457\n",
      "Epoch 13, Batch: 99: Training Loss: 0.026768138632178307, Validation Loss: 0.024698853492736816\n",
      "Epoch 13, Batch: 100: Training Loss: 0.028104430064558983, Validation Loss: 0.02074200101196766\n",
      "Epoch 13, Batch: 101: Training Loss: 0.024285899475216866, Validation Loss: 0.024440929293632507\n",
      "Epoch 13, Batch: 102: Training Loss: 0.026380576193332672, Validation Loss: 0.022478966042399406\n",
      "Epoch 13, Batch: 103: Training Loss: 0.026165718212723732, Validation Loss: 0.02632216550409794\n",
      "Epoch 13, Batch: 104: Training Loss: 0.022857964038848877, Validation Loss: 0.0255016777664423\n",
      "Epoch 13, Batch: 105: Training Loss: 0.021321354433894157, Validation Loss: 0.025334523990750313\n",
      "Epoch 13, Batch: 106: Training Loss: 0.024796033278107643, Validation Loss: 0.024802224710583687\n",
      "Epoch 13, Batch: 107: Training Loss: 0.02635807916522026, Validation Loss: 0.02667311578989029\n",
      "Epoch 13, Batch: 108: Training Loss: 0.0236645620316267, Validation Loss: 0.024861972779035568\n",
      "Epoch 13, Batch: 109: Training Loss: 0.025661954656243324, Validation Loss: 0.024391788989305496\n",
      "Epoch 13, Batch: 110: Training Loss: 0.024164162576198578, Validation Loss: 0.027812276035547256\n",
      "Epoch 13, Batch: 111: Training Loss: 0.025044184178113937, Validation Loss: 0.02688695304095745\n",
      "Epoch 13, Batch: 112: Training Loss: 0.02004845067858696, Validation Loss: 0.02544870227575302\n",
      "Epoch 13, Batch: 113: Training Loss: 0.02468356303870678, Validation Loss: 0.026598021388053894\n",
      "Epoch 13, Batch: 114: Training Loss: 0.024357687681913376, Validation Loss: 0.028657175600528717\n",
      "Epoch 13, Batch: 115: Training Loss: 0.027267882600426674, Validation Loss: 0.023262664675712585\n",
      "Epoch 13, Batch: 116: Training Loss: 0.02568957954645157, Validation Loss: 0.025379175320267677\n",
      "Epoch 13, Batch: 117: Training Loss: 0.022815750911831856, Validation Loss: 0.02664605714380741\n",
      "Epoch 13, Batch: 118: Training Loss: 0.023054230958223343, Validation Loss: 0.0260294321924448\n",
      "Epoch 13, Batch: 119: Training Loss: 0.024382058531045914, Validation Loss: 0.026160459965467453\n",
      "Epoch 13, Batch: 120: Training Loss: 0.0212550051510334, Validation Loss: 0.024962985888123512\n",
      "Epoch 13, Batch: 121: Training Loss: 0.02246692404150963, Validation Loss: 0.02536860480904579\n",
      "Epoch 13, Batch: 122: Training Loss: 0.022712906822562218, Validation Loss: 0.024476414546370506\n",
      "Epoch 13, Batch: 123: Training Loss: 0.027527378872036934, Validation Loss: 0.025675028562545776\n",
      "Epoch 13, Batch: 124: Training Loss: 0.022685393691062927, Validation Loss: 0.024346178397536278\n",
      "Epoch 13, Batch: 125: Training Loss: 0.01947455294430256, Validation Loss: 0.025691373273730278\n",
      "Epoch 13, Batch: 126: Training Loss: 0.0204261876642704, Validation Loss: 0.0249720998108387\n",
      "Epoch 13, Batch: 127: Training Loss: 0.019962431862950325, Validation Loss: 0.02489740215241909\n",
      "Epoch 13, Batch: 128: Training Loss: 0.02261640504002571, Validation Loss: 0.024852193892002106\n",
      "Epoch 13, Batch: 129: Training Loss: 0.020274879410862923, Validation Loss: 0.02488012984395027\n",
      "Epoch 13, Batch: 130: Training Loss: 0.025066988542675972, Validation Loss: 0.024347996339201927\n",
      "Epoch 13, Batch: 131: Training Loss: 0.0241959597915411, Validation Loss: 0.024552440270781517\n",
      "Epoch 13, Batch: 132: Training Loss: 0.025691544637084007, Validation Loss: 0.024630481377243996\n",
      "Epoch 13, Batch: 133: Training Loss: 0.024055158719420433, Validation Loss: 0.023667719215154648\n",
      "Epoch 13, Batch: 134: Training Loss: 0.022693248465657234, Validation Loss: 0.025045212358236313\n",
      "Epoch 13, Batch: 135: Training Loss: 0.023641817271709442, Validation Loss: 0.02276645414531231\n",
      "Epoch 13, Batch: 136: Training Loss: 0.020141974091529846, Validation Loss: 0.023581309244036674\n",
      "Epoch 13, Batch: 137: Training Loss: 0.020040364935994148, Validation Loss: 0.02542148157954216\n",
      "Epoch 13, Batch: 138: Training Loss: 0.022027820348739624, Validation Loss: 0.0259845107793808\n",
      "Epoch 13, Batch: 139: Training Loss: 0.019749416038393974, Validation Loss: 0.026117919012904167\n",
      "Epoch 13, Batch: 140: Training Loss: 0.026847099885344505, Validation Loss: 0.02522536925971508\n",
      "Epoch 13, Batch: 141: Training Loss: 0.02134968712925911, Validation Loss: 0.025089342147111893\n",
      "Epoch 13, Batch: 142: Training Loss: 0.025091474875807762, Validation Loss: 0.02445659600198269\n",
      "Epoch 13, Batch: 143: Training Loss: 0.022136101499199867, Validation Loss: 0.023851115256547928\n",
      "Epoch 13, Batch: 144: Training Loss: 0.02359922230243683, Validation Loss: 0.025512441992759705\n",
      "Epoch 13, Batch: 145: Training Loss: 0.021457284688949585, Validation Loss: 0.023135079070925713\n",
      "Epoch 13, Batch: 146: Training Loss: 0.022693807259202003, Validation Loss: 0.025157727301120758\n",
      "Epoch 13, Batch: 147: Training Loss: 0.02431030571460724, Validation Loss: 0.025592317804694176\n",
      "Epoch 13, Batch: 148: Training Loss: 0.023542365059256554, Validation Loss: 0.026905594393610954\n",
      "Epoch 13, Batch: 149: Training Loss: 0.021259242668747902, Validation Loss: 0.024152131751179695\n",
      "Epoch 13, Batch: 150: Training Loss: 0.02547374553978443, Validation Loss: 0.024057593196630478\n",
      "Epoch 13, Batch: 151: Training Loss: 0.028050916269421577, Validation Loss: 0.02598673664033413\n",
      "Epoch 13, Batch: 152: Training Loss: 0.026113446801900864, Validation Loss: 0.023898223415017128\n",
      "Epoch 13, Batch: 153: Training Loss: 0.02370956353843212, Validation Loss: 0.024468716233968735\n",
      "Epoch 13, Batch: 154: Training Loss: 0.02636643685400486, Validation Loss: 0.026129456236958504\n",
      "Epoch 13, Batch: 155: Training Loss: 0.028949707746505737, Validation Loss: 0.024128761142492294\n",
      "Epoch 13, Batch: 156: Training Loss: 0.022687837481498718, Validation Loss: 0.023961279541254044\n",
      "Epoch 13, Batch: 157: Training Loss: 0.020032968372106552, Validation Loss: 0.024784011766314507\n",
      "Epoch 13, Batch: 158: Training Loss: 0.021365424618124962, Validation Loss: 0.022861918434500694\n",
      "Epoch 13, Batch: 159: Training Loss: 0.021958421915769577, Validation Loss: 0.022321032360196114\n",
      "Epoch 13, Batch: 160: Training Loss: 0.02111518755555153, Validation Loss: 0.024860013276338577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch: 161: Training Loss: 0.021588684991002083, Validation Loss: 0.024235837161540985\n",
      "Epoch 13, Batch: 162: Training Loss: 0.02221991866827011, Validation Loss: 0.023650845512747765\n",
      "Epoch 13, Batch: 163: Training Loss: 0.019729139283299446, Validation Loss: 0.025682032108306885\n",
      "Epoch 13, Batch: 164: Training Loss: 0.02208043821156025, Validation Loss: 0.023990577086806297\n",
      "Epoch 13, Batch: 165: Training Loss: 0.02165110968053341, Validation Loss: 0.0247303768992424\n",
      "Epoch 13, Batch: 166: Training Loss: 0.0246629249304533, Validation Loss: 0.02425643615424633\n",
      "Epoch 13, Batch: 167: Training Loss: 0.02691398374736309, Validation Loss: 0.022679027169942856\n",
      "Epoch 13, Batch: 168: Training Loss: 0.024558305740356445, Validation Loss: 0.027705373242497444\n",
      "Epoch 13, Batch: 169: Training Loss: 0.021391620859503746, Validation Loss: 0.026547148823738098\n",
      "Epoch 13, Batch: 170: Training Loss: 0.023107115179300308, Validation Loss: 0.029180562123656273\n",
      "Epoch 13, Batch: 171: Training Loss: 0.023156248033046722, Validation Loss: 0.02885618805885315\n",
      "Epoch 13, Batch: 172: Training Loss: 0.022321544587612152, Validation Loss: 0.02779604122042656\n",
      "Epoch 13, Batch: 173: Training Loss: 0.023153910413384438, Validation Loss: 0.029515547677874565\n",
      "Epoch 13, Batch: 174: Training Loss: 0.027401426807045937, Validation Loss: 0.0289689302444458\n",
      "Epoch 13, Batch: 175: Training Loss: 0.023777194321155548, Validation Loss: 0.025789234787225723\n",
      "Epoch 13, Batch: 176: Training Loss: 0.026643214747309685, Validation Loss: 0.026580529287457466\n",
      "Epoch 13, Batch: 177: Training Loss: 0.026265855878591537, Validation Loss: 0.026185566559433937\n",
      "Epoch 13, Batch: 178: Training Loss: 0.02619941532611847, Validation Loss: 0.02760496735572815\n",
      "Epoch 13, Batch: 179: Training Loss: 0.026250824332237244, Validation Loss: 0.027748797088861465\n",
      "Epoch 13, Batch: 180: Training Loss: 0.023515837267041206, Validation Loss: 0.02574148029088974\n",
      "Epoch 13, Batch: 181: Training Loss: 0.02596428245306015, Validation Loss: 0.028831401839852333\n",
      "Epoch 13, Batch: 182: Training Loss: 0.027331652119755745, Validation Loss: 0.026934517547488213\n",
      "Epoch 13, Batch: 183: Training Loss: 0.0237501822412014, Validation Loss: 0.02684561163187027\n",
      "Epoch 13, Batch: 184: Training Loss: 0.02186775393784046, Validation Loss: 0.0247885063290596\n",
      "Epoch 13, Batch: 185: Training Loss: 0.026146292686462402, Validation Loss: 0.0251906905323267\n",
      "Epoch 13, Batch: 186: Training Loss: 0.03113621473312378, Validation Loss: 0.02600223198533058\n",
      "Epoch 13, Batch: 187: Training Loss: 0.021266832947731018, Validation Loss: 0.02596738189458847\n",
      "Epoch 13, Batch: 188: Training Loss: 0.024090925231575966, Validation Loss: 0.027491020038723946\n",
      "Epoch 13, Batch: 189: Training Loss: 0.024624161422252655, Validation Loss: 0.027095111086964607\n",
      "Epoch 13, Batch: 190: Training Loss: 0.025738785043358803, Validation Loss: 0.023195387795567513\n",
      "Epoch 13, Batch: 191: Training Loss: 0.025273481383919716, Validation Loss: 0.02486729621887207\n",
      "Epoch 13, Batch: 192: Training Loss: 0.024847429245710373, Validation Loss: 0.02482403814792633\n",
      "Epoch 13, Batch: 193: Training Loss: 0.02421628125011921, Validation Loss: 0.023242885246872902\n",
      "Epoch 13, Batch: 194: Training Loss: 0.02440636046230793, Validation Loss: 0.02616279385983944\n",
      "Epoch 13, Batch: 195: Training Loss: 0.02706747315824032, Validation Loss: 0.02458168752491474\n",
      "Epoch 13, Batch: 196: Training Loss: 0.027214674279093742, Validation Loss: 0.026016877964138985\n",
      "Epoch 13, Batch: 197: Training Loss: 0.027212491258978844, Validation Loss: 0.02874715067446232\n",
      "Epoch 13, Batch: 198: Training Loss: 0.02313302643597126, Validation Loss: 0.026069659739732742\n",
      "Epoch 13, Batch: 199: Training Loss: 0.025828231126070023, Validation Loss: 0.0284163486212492\n",
      "Epoch 13, Batch: 200: Training Loss: 0.021620508283376694, Validation Loss: 0.02433006465435028\n",
      "Epoch 13, Batch: 201: Training Loss: 0.028264151886105537, Validation Loss: 0.02638852782547474\n",
      "Epoch 13, Batch: 202: Training Loss: 0.025506509467959404, Validation Loss: 0.02478793077170849\n",
      "Epoch 13, Batch: 203: Training Loss: 0.024201100692152977, Validation Loss: 0.025962961837649345\n",
      "Epoch 13, Batch: 204: Training Loss: 0.02652924694120884, Validation Loss: 0.02630537562072277\n",
      "Epoch 13, Batch: 205: Training Loss: 0.027858922258019447, Validation Loss: 0.024959193542599678\n",
      "Epoch 13, Batch: 206: Training Loss: 0.021941116079688072, Validation Loss: 0.026545684784650803\n",
      "Epoch 13, Batch: 207: Training Loss: 0.026287589222192764, Validation Loss: 0.02496861107647419\n",
      "Epoch 13, Batch: 208: Training Loss: 0.025203730911016464, Validation Loss: 0.025437792763113976\n",
      "Epoch 13, Batch: 209: Training Loss: 0.02396491914987564, Validation Loss: 0.023337725549936295\n",
      "Epoch 13, Batch: 210: Training Loss: 0.02891901321709156, Validation Loss: 0.02443794719874859\n",
      "Epoch 13, Batch: 211: Training Loss: 0.025865323841571808, Validation Loss: 0.024169355630874634\n",
      "Epoch 13, Batch: 212: Training Loss: 0.024792959913611412, Validation Loss: 0.021720368415117264\n",
      "Epoch 13, Batch: 213: Training Loss: 0.027979347854852676, Validation Loss: 0.026481322944164276\n",
      "Epoch 13, Batch: 214: Training Loss: 0.021347451955080032, Validation Loss: 0.024873241782188416\n",
      "Epoch 13, Batch: 215: Training Loss: 0.0219613928347826, Validation Loss: 0.0221133753657341\n",
      "Epoch 13, Batch: 216: Training Loss: 0.026106106117367744, Validation Loss: 0.023186802864074707\n",
      "Epoch 13, Batch: 217: Training Loss: 0.024416154250502586, Validation Loss: 0.02399277873337269\n",
      "Epoch 13, Batch: 218: Training Loss: 0.02348736673593521, Validation Loss: 0.022631196305155754\n",
      "Epoch 13, Batch: 219: Training Loss: 0.023199250921607018, Validation Loss: 0.022839991375803947\n",
      "Epoch 13, Batch: 220: Training Loss: 0.024167027324438095, Validation Loss: 0.023989608511328697\n",
      "Epoch 13, Batch: 221: Training Loss: 0.023960599675774574, Validation Loss: 0.023014597594738007\n",
      "Epoch 13, Batch: 222: Training Loss: 0.021269990131258965, Validation Loss: 0.02643664926290512\n",
      "Epoch 13, Batch: 223: Training Loss: 0.02548081986606121, Validation Loss: 0.025070620700716972\n",
      "Epoch 13, Batch: 224: Training Loss: 0.02389536239206791, Validation Loss: 0.02365351840853691\n",
      "Epoch 13, Batch: 225: Training Loss: 0.02118297852575779, Validation Loss: 0.023151077330112457\n",
      "Epoch 13, Batch: 226: Training Loss: 0.023206191137433052, Validation Loss: 0.024127835407853127\n",
      "Epoch 13, Batch: 227: Training Loss: 0.02418818138539791, Validation Loss: 0.026856636628508568\n",
      "Epoch 13, Batch: 228: Training Loss: 0.0269937627017498, Validation Loss: 0.02327188476920128\n",
      "Epoch 13, Batch: 229: Training Loss: 0.024896694347262383, Validation Loss: 0.02604808285832405\n",
      "Epoch 13, Batch: 230: Training Loss: 0.021579831838607788, Validation Loss: 0.026635238900780678\n",
      "Epoch 13, Batch: 231: Training Loss: 0.028181839734315872, Validation Loss: 0.02485014498233795\n",
      "Epoch 13, Batch: 232: Training Loss: 0.02738204598426819, Validation Loss: 0.024004090577363968\n",
      "Epoch 13, Batch: 233: Training Loss: 0.02282591350376606, Validation Loss: 0.025586912408471107\n",
      "Epoch 13, Batch: 234: Training Loss: 0.02934280037879944, Validation Loss: 0.024392662569880486\n",
      "Epoch 13, Batch: 235: Training Loss: 0.027088787406682968, Validation Loss: 0.02353627048432827\n",
      "Epoch 13, Batch: 236: Training Loss: 0.02447298727929592, Validation Loss: 0.02204560860991478\n",
      "Epoch 13, Batch: 237: Training Loss: 0.023344989866018295, Validation Loss: 0.025650659576058388\n",
      "Epoch 13, Batch: 238: Training Loss: 0.024592870846390724, Validation Loss: 0.023396771401166916\n",
      "Epoch 13, Batch: 239: Training Loss: 0.02415798231959343, Validation Loss: 0.023035772144794464\n",
      "Epoch 13, Batch: 240: Training Loss: 0.022520916536450386, Validation Loss: 0.022644443437457085\n",
      "Epoch 13, Batch: 241: Training Loss: 0.02262151800096035, Validation Loss: 0.026147300377488136\n",
      "Epoch 13, Batch: 242: Training Loss: 0.022987520322203636, Validation Loss: 0.024334443733096123\n",
      "Epoch 13, Batch: 243: Training Loss: 0.026125285774469376, Validation Loss: 0.024564824998378754\n",
      "Epoch 13, Batch: 244: Training Loss: 0.022830670699477196, Validation Loss: 0.024167107418179512\n",
      "Epoch 13, Batch: 245: Training Loss: 0.024789439514279366, Validation Loss: 0.025113573297858238\n",
      "Epoch 13, Batch: 246: Training Loss: 0.023367706686258316, Validation Loss: 0.024452703073620796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch: 247: Training Loss: 0.024551261216402054, Validation Loss: 0.025093283504247665\n",
      "Epoch 13, Batch: 248: Training Loss: 0.02383984625339508, Validation Loss: 0.021757999435067177\n",
      "Epoch 13, Batch: 249: Training Loss: 0.021569978445768356, Validation Loss: 0.024245988577604294\n",
      "Epoch 13, Batch: 250: Training Loss: 0.020933102816343307, Validation Loss: 0.02632611244916916\n",
      "Epoch 13, Batch: 251: Training Loss: 0.024971308186650276, Validation Loss: 0.025657648220658302\n",
      "Epoch 13, Batch: 252: Training Loss: 0.02458270639181137, Validation Loss: 0.02639993466436863\n",
      "Epoch 13, Batch: 253: Training Loss: 0.025211984291672707, Validation Loss: 0.025571800768375397\n",
      "Epoch 13, Batch: 254: Training Loss: 0.021481355652213097, Validation Loss: 0.026464173570275307\n",
      "Epoch 13, Batch: 255: Training Loss: 0.026402628049254417, Validation Loss: 0.02401449903845787\n",
      "Epoch 13, Batch: 256: Training Loss: 0.026178795844316483, Validation Loss: 0.026624644175171852\n",
      "Epoch 13, Batch: 257: Training Loss: 0.027384711429476738, Validation Loss: 0.02452283538877964\n",
      "Epoch 13, Batch: 258: Training Loss: 0.027776114642620087, Validation Loss: 0.025109533220529556\n",
      "Epoch 13, Batch: 259: Training Loss: 0.02462639845907688, Validation Loss: 0.02917012944817543\n",
      "Epoch 13, Batch: 260: Training Loss: 0.02850216068327427, Validation Loss: 0.024096166715025902\n",
      "Epoch 13, Batch: 261: Training Loss: 0.027177847921848297, Validation Loss: 0.02643999271094799\n",
      "Epoch 13, Batch: 262: Training Loss: 0.025535263121128082, Validation Loss: 0.02687138505280018\n",
      "Epoch 13, Batch: 263: Training Loss: 0.024010246619582176, Validation Loss: 0.02333703637123108\n",
      "Epoch 13, Batch: 264: Training Loss: 0.02363896742463112, Validation Loss: 0.025560880079865456\n",
      "Epoch 13, Batch: 265: Training Loss: 0.024484997615218163, Validation Loss: 0.024060774594545364\n",
      "Epoch 13, Batch: 266: Training Loss: 0.021715519949793816, Validation Loss: 0.02427651174366474\n",
      "Epoch 13, Batch: 267: Training Loss: 0.023534687235951424, Validation Loss: 0.022837042808532715\n",
      "Epoch 13, Batch: 268: Training Loss: 0.02341667003929615, Validation Loss: 0.021635983139276505\n",
      "Epoch 13, Batch: 269: Training Loss: 0.025018123909831047, Validation Loss: 0.02479379251599312\n",
      "Epoch 13, Batch: 270: Training Loss: 0.023785021156072617, Validation Loss: 0.026373550295829773\n",
      "Epoch 13, Batch: 271: Training Loss: 0.021860884502530098, Validation Loss: 0.026531778275966644\n",
      "Epoch 13, Batch: 272: Training Loss: 0.02536736987531185, Validation Loss: 0.02466013841331005\n",
      "Epoch 13, Batch: 273: Training Loss: 0.024814443662762642, Validation Loss: 0.0255114808678627\n",
      "Epoch 13, Batch: 274: Training Loss: 0.024786625057458878, Validation Loss: 0.026038561016321182\n",
      "Epoch 13, Batch: 275: Training Loss: 0.023855404928326607, Validation Loss: 0.025534436106681824\n",
      "Epoch 13, Batch: 276: Training Loss: 0.022224821150302887, Validation Loss: 0.023842399939894676\n",
      "Epoch 13, Batch: 277: Training Loss: 0.025234628468751907, Validation Loss: 0.025534143671393394\n",
      "Epoch 13, Batch: 278: Training Loss: 0.022449728101491928, Validation Loss: 0.02660270407795906\n",
      "Epoch 13, Batch: 279: Training Loss: 0.026965046301484108, Validation Loss: 0.023736273869872093\n",
      "Epoch 13, Batch: 280: Training Loss: 0.02277970314025879, Validation Loss: 0.02392919361591339\n",
      "Epoch 13, Batch: 281: Training Loss: 0.024559425190091133, Validation Loss: 0.024212321266531944\n",
      "Epoch 13, Batch: 282: Training Loss: 0.026062477380037308, Validation Loss: 0.023832008242607117\n",
      "Epoch 13, Batch: 283: Training Loss: 0.025420349091291428, Validation Loss: 0.024594442918896675\n",
      "Epoch 13, Batch: 284: Training Loss: 0.02455204911530018, Validation Loss: 0.023247264325618744\n",
      "Epoch 13, Batch: 285: Training Loss: 0.02435152418911457, Validation Loss: 0.0247159693390131\n",
      "Epoch 13, Batch: 286: Training Loss: 0.026856927201151848, Validation Loss: 0.024263830855488777\n",
      "Epoch 13, Batch: 287: Training Loss: 0.02602021023631096, Validation Loss: 0.021637391299009323\n",
      "Epoch 13, Batch: 288: Training Loss: 0.02431553788483143, Validation Loss: 0.02503247745335102\n",
      "Epoch 13, Batch: 289: Training Loss: 0.02397569641470909, Validation Loss: 0.02423822320997715\n",
      "Epoch 13, Batch: 290: Training Loss: 0.023498788475990295, Validation Loss: 0.025573423132300377\n",
      "Epoch 13, Batch: 291: Training Loss: 0.020628929138183594, Validation Loss: 0.025007793679833412\n",
      "Epoch 13, Batch: 292: Training Loss: 0.026037847623229027, Validation Loss: 0.02618268132209778\n",
      "Epoch 13, Batch: 293: Training Loss: 0.02432241104543209, Validation Loss: 0.024042215198278427\n",
      "Epoch 13, Batch: 294: Training Loss: 0.021758783608675003, Validation Loss: 0.025703292340040207\n",
      "Epoch 13, Batch: 295: Training Loss: 0.02387009933590889, Validation Loss: 0.02642037533223629\n",
      "Epoch 13, Batch: 296: Training Loss: 0.02593247778713703, Validation Loss: 0.02388792484998703\n",
      "Epoch 13, Batch: 297: Training Loss: 0.02495899423956871, Validation Loss: 0.02387305721640587\n",
      "Epoch 13, Batch: 298: Training Loss: 0.02509145438671112, Validation Loss: 0.025882098823785782\n",
      "Epoch 13, Batch: 299: Training Loss: 0.021694311872124672, Validation Loss: 0.023187747225165367\n",
      "Epoch 13, Batch: 300: Training Loss: 0.023394502699375153, Validation Loss: 0.025403939187526703\n",
      "Epoch 13, Batch: 301: Training Loss: 0.02176099456846714, Validation Loss: 0.02371296100318432\n",
      "Epoch 13, Batch: 302: Training Loss: 0.02306697890162468, Validation Loss: 0.02403894253075123\n",
      "Epoch 13, Batch: 303: Training Loss: 0.02382955700159073, Validation Loss: 0.0224691741168499\n",
      "Epoch 13, Batch: 304: Training Loss: 0.02312626875936985, Validation Loss: 0.024604028090834618\n",
      "Epoch 13, Batch: 305: Training Loss: 0.019662097096443176, Validation Loss: 0.023249894380569458\n",
      "Epoch 13, Batch: 306: Training Loss: 0.023657983168959618, Validation Loss: 0.023844877257943153\n",
      "Epoch 13, Batch: 307: Training Loss: 0.02151683159172535, Validation Loss: 0.024803994223475456\n",
      "Epoch 13, Batch: 308: Training Loss: 0.02162966877222061, Validation Loss: 0.023203276097774506\n",
      "Epoch 13, Batch: 309: Training Loss: 0.021729636937379837, Validation Loss: 0.024523725733160973\n",
      "Epoch 13, Batch: 310: Training Loss: 0.0198809951543808, Validation Loss: 0.02352449856698513\n",
      "Epoch 13, Batch: 311: Training Loss: 0.02127717062830925, Validation Loss: 0.022745763882994652\n",
      "Epoch 13, Batch: 312: Training Loss: 0.019960712641477585, Validation Loss: 0.022737886756658554\n",
      "Epoch 13, Batch: 313: Training Loss: 0.02193262055516243, Validation Loss: 0.023641768842935562\n",
      "Epoch 13, Batch: 314: Training Loss: 0.02204405888915062, Validation Loss: 0.02436123415827751\n",
      "Epoch 13, Batch: 315: Training Loss: 0.021272603422403336, Validation Loss: 0.02398008480668068\n",
      "Epoch 13, Batch: 316: Training Loss: 0.024249259382486343, Validation Loss: 0.024264609441161156\n",
      "Epoch 13, Batch: 317: Training Loss: 0.01951439678668976, Validation Loss: 0.02411731332540512\n",
      "Epoch 13, Batch: 318: Training Loss: 0.023061655461788177, Validation Loss: 0.02457289583981037\n",
      "Epoch 13, Batch: 319: Training Loss: 0.025235336273908615, Validation Loss: 0.02339048683643341\n",
      "Epoch 13, Batch: 320: Training Loss: 0.02539871633052826, Validation Loss: 0.026077138260006905\n",
      "Epoch 13, Batch: 321: Training Loss: 0.02064788155257702, Validation Loss: 0.02782309427857399\n",
      "Epoch 13, Batch: 322: Training Loss: 0.022029176354408264, Validation Loss: 0.02640436217188835\n",
      "Epoch 13, Batch: 323: Training Loss: 0.02641921676695347, Validation Loss: 0.026802850887179375\n",
      "Epoch 13, Batch: 324: Training Loss: 0.023947907611727715, Validation Loss: 0.02436000667512417\n",
      "Epoch 13, Batch: 325: Training Loss: 0.02279597334563732, Validation Loss: 0.027323994785547256\n",
      "Epoch 13, Batch: 326: Training Loss: 0.031629856675863266, Validation Loss: 0.027188638225197792\n",
      "Epoch 13, Batch: 327: Training Loss: 0.024363771080970764, Validation Loss: 0.027410516515374184\n",
      "Epoch 13, Batch: 328: Training Loss: 0.025447610765695572, Validation Loss: 0.025612372905015945\n",
      "Epoch 13, Batch: 329: Training Loss: 0.022859349846839905, Validation Loss: 0.023816274479031563\n",
      "Epoch 13, Batch: 330: Training Loss: 0.022869866341352463, Validation Loss: 0.02552570402622223\n",
      "Epoch 13, Batch: 331: Training Loss: 0.02510816603899002, Validation Loss: 0.02656693570315838\n",
      "Epoch 13, Batch: 332: Training Loss: 0.027210785076022148, Validation Loss: 0.024060677736997604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch: 333: Training Loss: 0.02350221574306488, Validation Loss: 0.024339115247130394\n",
      "Epoch 13, Batch: 334: Training Loss: 0.02216140553355217, Validation Loss: 0.024682775139808655\n",
      "Epoch 13, Batch: 335: Training Loss: 0.020570747554302216, Validation Loss: 0.02738046832382679\n",
      "Epoch 13, Batch: 336: Training Loss: 0.02490786463022232, Validation Loss: 0.024867277592420578\n",
      "Epoch 13, Batch: 337: Training Loss: 0.019274456426501274, Validation Loss: 0.0241929329931736\n",
      "Epoch 13, Batch: 338: Training Loss: 0.02430667169392109, Validation Loss: 0.027253316715359688\n",
      "Epoch 13, Batch: 339: Training Loss: 0.019985076040029526, Validation Loss: 0.023897357285022736\n",
      "Epoch 13, Batch: 340: Training Loss: 0.022865047678351402, Validation Loss: 0.02554040588438511\n",
      "Epoch 13, Batch: 341: Training Loss: 0.022013645619153976, Validation Loss: 0.02420210652053356\n",
      "Epoch 13, Batch: 342: Training Loss: 0.022598909214138985, Validation Loss: 0.024910764768719673\n",
      "Epoch 13, Batch: 343: Training Loss: 0.025178875774145126, Validation Loss: 0.025869501754641533\n",
      "Epoch 13, Batch: 344: Training Loss: 0.02356279455125332, Validation Loss: 0.02445502206683159\n",
      "Epoch 13, Batch: 345: Training Loss: 0.022580061107873917, Validation Loss: 0.025851942598819733\n",
      "Epoch 13, Batch: 346: Training Loss: 0.022835560142993927, Validation Loss: 0.023908087983727455\n",
      "Epoch 13, Batch: 347: Training Loss: 0.024369345977902412, Validation Loss: 0.025681642815470695\n",
      "Epoch 13, Batch: 348: Training Loss: 0.023053638637065887, Validation Loss: 0.025112314149737358\n",
      "Epoch 13, Batch: 349: Training Loss: 0.024174487218260765, Validation Loss: 0.026838157325983047\n",
      "Epoch 13, Batch: 350: Training Loss: 0.020295822992920876, Validation Loss: 0.02408542111515999\n",
      "Epoch 13, Batch: 351: Training Loss: 0.02463860623538494, Validation Loss: 0.025268232449889183\n",
      "Epoch 13, Batch: 352: Training Loss: 0.024012427777051926, Validation Loss: 0.02484060823917389\n",
      "Epoch 13, Batch: 353: Training Loss: 0.025023963302373886, Validation Loss: 0.0231617484241724\n",
      "Epoch 13, Batch: 354: Training Loss: 0.023973682895302773, Validation Loss: 0.024876413866877556\n",
      "Epoch 13, Batch: 355: Training Loss: 0.021550865843892097, Validation Loss: 0.023597346618771553\n",
      "Epoch 13, Batch: 356: Training Loss: 0.024388059973716736, Validation Loss: 0.02328372746706009\n",
      "Epoch 13, Batch: 357: Training Loss: 0.02121743932366371, Validation Loss: 0.021831391379237175\n",
      "Epoch 13, Batch: 358: Training Loss: 0.02357838861644268, Validation Loss: 0.023854197934269905\n",
      "Epoch 13, Batch: 359: Training Loss: 0.023279478773474693, Validation Loss: 0.022771868854761124\n",
      "Epoch 13, Batch: 360: Training Loss: 0.026832206174731255, Validation Loss: 0.024952856823801994\n",
      "Epoch 13, Batch: 361: Training Loss: 0.022720949724316597, Validation Loss: 0.02543710358440876\n",
      "Epoch 13, Batch: 362: Training Loss: 0.023277580738067627, Validation Loss: 0.02356702648103237\n",
      "Epoch 13, Batch: 363: Training Loss: 0.023724118247628212, Validation Loss: 0.023440545424818993\n",
      "Epoch 13, Batch: 364: Training Loss: 0.02227720245718956, Validation Loss: 0.02361186593770981\n",
      "Epoch 13, Batch: 365: Training Loss: 0.020796246826648712, Validation Loss: 0.02378814108669758\n",
      "Epoch 13, Batch: 366: Training Loss: 0.024051252752542496, Validation Loss: 0.02277766354382038\n",
      "Epoch 13, Batch: 367: Training Loss: 0.020755287259817123, Validation Loss: 0.023106493055820465\n",
      "Epoch 13, Batch: 368: Training Loss: 0.02133072540163994, Validation Loss: 0.02503478340804577\n",
      "Epoch 13, Batch: 369: Training Loss: 0.02170068584382534, Validation Loss: 0.02280247025191784\n",
      "Epoch 13, Batch: 370: Training Loss: 0.025398818776011467, Validation Loss: 0.0229711402207613\n",
      "Epoch 13, Batch: 371: Training Loss: 0.022459568455815315, Validation Loss: 0.02202814817428589\n",
      "Epoch 13, Batch: 372: Training Loss: 0.02486526593565941, Validation Loss: 0.02216114103794098\n",
      "Epoch 13, Batch: 373: Training Loss: 0.02405770868062973, Validation Loss: 0.0208923127502203\n",
      "Epoch 13, Batch: 374: Training Loss: 0.01936313882470131, Validation Loss: 0.022915979847311974\n",
      "Epoch 13, Batch: 375: Training Loss: 0.026235580444335938, Validation Loss: 0.02294854447245598\n",
      "Epoch 13, Batch: 376: Training Loss: 0.02019950933754444, Validation Loss: 0.02326621301472187\n",
      "Epoch 13, Batch: 377: Training Loss: 0.022521957755088806, Validation Loss: 0.022849204018712044\n",
      "Epoch 13, Batch: 378: Training Loss: 0.021297745406627655, Validation Loss: 0.024348951876163483\n",
      "Epoch 13, Batch: 379: Training Loss: 0.022894389927387238, Validation Loss: 0.022922905161976814\n",
      "Epoch 13, Batch: 380: Training Loss: 0.02744103968143463, Validation Loss: 0.02396972104907036\n",
      "Epoch 13, Batch: 381: Training Loss: 0.024334480985999107, Validation Loss: 0.022274697199463844\n",
      "Epoch 13, Batch: 382: Training Loss: 0.023499468341469765, Validation Loss: 0.02217036299407482\n",
      "Epoch 13, Batch: 383: Training Loss: 0.02536984719336033, Validation Loss: 0.023527931421995163\n",
      "Epoch 13, Batch: 384: Training Loss: 0.02241140976548195, Validation Loss: 0.022802649065852165\n",
      "Epoch 13, Batch: 385: Training Loss: 0.021818554028868675, Validation Loss: 0.02206580340862274\n",
      "Epoch 13, Batch: 386: Training Loss: 0.021792154759168625, Validation Loss: 0.022891558706760406\n",
      "Epoch 13, Batch: 387: Training Loss: 0.02207063138484955, Validation Loss: 0.021881023421883583\n",
      "Epoch 13, Batch: 388: Training Loss: 0.024487027898430824, Validation Loss: 0.02290516160428524\n",
      "Epoch 13, Batch: 389: Training Loss: 0.027104396373033524, Validation Loss: 0.024825667962431908\n",
      "Epoch 13, Batch: 390: Training Loss: 0.02413944900035858, Validation Loss: 0.024951761588454247\n",
      "Epoch 13, Batch: 391: Training Loss: 0.024884674698114395, Validation Loss: 0.024435261264443398\n",
      "Epoch 13, Batch: 392: Training Loss: 0.0226459763944149, Validation Loss: 0.021778807044029236\n",
      "Epoch 13, Batch: 393: Training Loss: 0.023113492876291275, Validation Loss: 0.021488389000296593\n",
      "Epoch 13, Batch: 394: Training Loss: 0.023685112595558167, Validation Loss: 0.02372516691684723\n",
      "Epoch 13, Batch: 395: Training Loss: 0.02054455131292343, Validation Loss: 0.02314981445670128\n",
      "Epoch 13, Batch: 396: Training Loss: 0.024236027151346207, Validation Loss: 0.022068697959184647\n",
      "Epoch 13, Batch: 397: Training Loss: 0.0259416401386261, Validation Loss: 0.02550649270415306\n",
      "Epoch 13, Batch: 398: Training Loss: 0.02195923589169979, Validation Loss: 0.025598395615816116\n",
      "Epoch 13, Batch: 399: Training Loss: 0.023962024599313736, Validation Loss: 0.023525234311819077\n",
      "Epoch 13, Batch: 400: Training Loss: 0.02730909176170826, Validation Loss: 0.024490265175700188\n",
      "Epoch 13, Batch: 401: Training Loss: 0.0238307174295187, Validation Loss: 0.020663177594542503\n",
      "Epoch 13, Batch: 402: Training Loss: 0.023032037541270256, Validation Loss: 0.02390381507575512\n",
      "Epoch 13, Batch: 403: Training Loss: 0.02482820861041546, Validation Loss: 0.024826129898428917\n",
      "Epoch 13, Batch: 404: Training Loss: 0.024545127525925636, Validation Loss: 0.024502890184521675\n",
      "Epoch 13, Batch: 405: Training Loss: 0.02413862943649292, Validation Loss: 0.024950966238975525\n",
      "Epoch 13, Batch: 406: Training Loss: 0.023990783840417862, Validation Loss: 0.025333432480692863\n",
      "Epoch 13, Batch: 407: Training Loss: 0.0202792976051569, Validation Loss: 0.025829371064901352\n",
      "Epoch 13, Batch: 408: Training Loss: 0.021237004548311234, Validation Loss: 0.024486202746629715\n",
      "Epoch 13, Batch: 409: Training Loss: 0.02057325281202793, Validation Loss: 0.026328837499022484\n",
      "Epoch 13, Batch: 410: Training Loss: 0.026688789948821068, Validation Loss: 0.026919035241007805\n",
      "Epoch 13, Batch: 411: Training Loss: 0.025744996964931488, Validation Loss: 0.026816224679350853\n",
      "Epoch 13, Batch: 412: Training Loss: 0.029040126129984856, Validation Loss: 0.02397894114255905\n",
      "Epoch 13, Batch: 413: Training Loss: 0.025835081934928894, Validation Loss: 0.027151886373758316\n",
      "Epoch 13, Batch: 414: Training Loss: 0.024921130388975143, Validation Loss: 0.02583564817905426\n",
      "Epoch 13, Batch: 415: Training Loss: 0.028421930968761444, Validation Loss: 0.02642030455172062\n",
      "Epoch 13, Batch: 416: Training Loss: 0.022642899304628372, Validation Loss: 0.026530511677265167\n",
      "Epoch 13, Batch: 417: Training Loss: 0.024678044021129608, Validation Loss: 0.02701514959335327\n",
      "Epoch 13, Batch: 418: Training Loss: 0.023005550727248192, Validation Loss: 0.024143176153302193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch: 419: Training Loss: 0.022876042872667313, Validation Loss: 0.025066690519452095\n",
      "Epoch 13, Batch: 420: Training Loss: 0.027092531323432922, Validation Loss: 0.023709736764431\n",
      "Epoch 13, Batch: 421: Training Loss: 0.025468505918979645, Validation Loss: 0.02277923747897148\n",
      "Epoch 13, Batch: 422: Training Loss: 0.024425242096185684, Validation Loss: 0.02411867119371891\n",
      "Epoch 13, Batch: 423: Training Loss: 0.029828771948814392, Validation Loss: 0.024213287979364395\n",
      "Epoch 13, Batch: 424: Training Loss: 0.02374987304210663, Validation Loss: 0.024785742163658142\n",
      "Epoch 13, Batch: 425: Training Loss: 0.02459983341395855, Validation Loss: 0.026254858821630478\n",
      "Epoch 13, Batch: 426: Training Loss: 0.021975558251142502, Validation Loss: 0.026067979633808136\n",
      "Epoch 13, Batch: 427: Training Loss: 0.02504481002688408, Validation Loss: 0.025529853999614716\n",
      "Epoch 13, Batch: 428: Training Loss: 0.022533506155014038, Validation Loss: 0.028295625001192093\n",
      "Epoch 13, Batch: 429: Training Loss: 0.026235224679112434, Validation Loss: 0.02773008495569229\n",
      "Epoch 13, Batch: 430: Training Loss: 0.02614842727780342, Validation Loss: 0.02850659377872944\n",
      "Epoch 13, Batch: 431: Training Loss: 0.025499099865555763, Validation Loss: 0.026230385527014732\n",
      "Epoch 13, Batch: 432: Training Loss: 0.02641684003174305, Validation Loss: 0.026015980169177055\n",
      "Epoch 13, Batch: 433: Training Loss: 0.02408800646662712, Validation Loss: 0.027244720607995987\n",
      "Epoch 13, Batch: 434: Training Loss: 0.0279980655759573, Validation Loss: 0.030096285045146942\n",
      "Epoch 13, Batch: 435: Training Loss: 0.03048551455140114, Validation Loss: 0.027178851887583733\n",
      "Epoch 13, Batch: 436: Training Loss: 0.022899433970451355, Validation Loss: 0.02757454663515091\n",
      "Epoch 13, Batch: 437: Training Loss: 0.02622126415371895, Validation Loss: 0.026671944186091423\n",
      "Epoch 13, Batch: 438: Training Loss: 0.025542542338371277, Validation Loss: 0.02456359565258026\n",
      "Epoch 13, Batch: 439: Training Loss: 0.022512098774313927, Validation Loss: 0.02482217364013195\n",
      "Epoch 13, Batch: 440: Training Loss: 0.026619479060173035, Validation Loss: 0.023162713274359703\n",
      "Epoch 13, Batch: 441: Training Loss: 0.021933985874056816, Validation Loss: 0.025360941886901855\n",
      "Epoch 13, Batch: 442: Training Loss: 0.02678614854812622, Validation Loss: 0.024715738371014595\n",
      "Epoch 13, Batch: 443: Training Loss: 0.022844986990094185, Validation Loss: 0.02326297201216221\n",
      "Epoch 13, Batch: 444: Training Loss: 0.023965023458003998, Validation Loss: 0.023890040814876556\n",
      "Epoch 13, Batch: 445: Training Loss: 0.023968406021595, Validation Loss: 0.02446727268397808\n",
      "Epoch 13, Batch: 446: Training Loss: 0.027708711102604866, Validation Loss: 0.025721600279211998\n",
      "Epoch 13, Batch: 447: Training Loss: 0.02590242214500904, Validation Loss: 0.021889623254537582\n",
      "Epoch 13, Batch: 448: Training Loss: 0.02760450541973114, Validation Loss: 0.026814132928848267\n",
      "Epoch 13, Batch: 449: Training Loss: 0.023736385628581047, Validation Loss: 0.02431362122297287\n",
      "Epoch 13, Batch: 450: Training Loss: 0.023725897073745728, Validation Loss: 0.024312958121299744\n",
      "Epoch 13, Batch: 451: Training Loss: 0.02500748075544834, Validation Loss: 0.027283979579806328\n",
      "Epoch 13, Batch: 452: Training Loss: 0.02688935212790966, Validation Loss: 0.022402510046958923\n",
      "Epoch 13, Batch: 453: Training Loss: 0.026973683387041092, Validation Loss: 0.02598612941801548\n",
      "Epoch 13, Batch: 454: Training Loss: 0.02444801665842533, Validation Loss: 0.024396754801273346\n",
      "Epoch 13, Batch: 455: Training Loss: 0.0228621456772089, Validation Loss: 0.025188066065311432\n",
      "Epoch 13, Batch: 456: Training Loss: 0.025563208386301994, Validation Loss: 0.024766407907009125\n",
      "Epoch 13, Batch: 457: Training Loss: 0.029700523242354393, Validation Loss: 0.024598661810159683\n",
      "Epoch 13, Batch: 458: Training Loss: 0.017658956348896027, Validation Loss: 0.023603715002536774\n",
      "Epoch 13, Batch: 459: Training Loss: 0.023413322865962982, Validation Loss: 0.024655604735016823\n",
      "Epoch 13, Batch: 460: Training Loss: 0.02681196667253971, Validation Loss: 0.024012571200728416\n",
      "Epoch 13, Batch: 461: Training Loss: 0.022900376468896866, Validation Loss: 0.026069210842251778\n",
      "Epoch 13, Batch: 462: Training Loss: 0.020875923335552216, Validation Loss: 0.02548080123960972\n",
      "Epoch 13, Batch: 463: Training Loss: 0.023066645488142967, Validation Loss: 0.02469734661281109\n",
      "Epoch 13, Batch: 464: Training Loss: 0.022387336939573288, Validation Loss: 0.025773335248231888\n",
      "Epoch 13, Batch: 465: Training Loss: 0.025293884798884392, Validation Loss: 0.024876529350876808\n",
      "Epoch 13, Batch: 466: Training Loss: 0.021305276080965996, Validation Loss: 0.025346089154481888\n",
      "Epoch 13, Batch: 467: Training Loss: 0.028814265504479408, Validation Loss: 0.02575748972594738\n",
      "Epoch 13, Batch: 468: Training Loss: 0.024164507165551186, Validation Loss: 0.02405324950814247\n",
      "Epoch 13, Batch: 469: Training Loss: 0.022259581834077835, Validation Loss: 0.02223164029419422\n",
      "Epoch 13, Batch: 470: Training Loss: 0.024529147893190384, Validation Loss: 0.024841496720910072\n",
      "Epoch 13, Batch: 471: Training Loss: 0.02348044514656067, Validation Loss: 0.025394991040229797\n",
      "Epoch 13, Batch: 472: Training Loss: 0.02558535896241665, Validation Loss: 0.025660552084445953\n",
      "Epoch 13, Batch: 473: Training Loss: 0.021293045952916145, Validation Loss: 0.02636810950934887\n",
      "Epoch 13, Batch: 474: Training Loss: 0.023764753714203835, Validation Loss: 0.027263356372714043\n",
      "Epoch 13, Batch: 475: Training Loss: 0.02546921744942665, Validation Loss: 0.02745678648352623\n",
      "Epoch 13, Batch: 476: Training Loss: 0.020947782322764397, Validation Loss: 0.02505428157746792\n",
      "Epoch 13, Batch: 477: Training Loss: 0.025604531168937683, Validation Loss: 0.027079816907644272\n",
      "Epoch 13, Batch: 478: Training Loss: 0.02050207182765007, Validation Loss: 0.02585434541106224\n",
      "Epoch 13, Batch: 479: Training Loss: 0.02584139257669449, Validation Loss: 0.023271389305591583\n",
      "Epoch 13, Batch: 480: Training Loss: 0.025306744500994682, Validation Loss: 0.023901794105768204\n",
      "Epoch 13, Batch: 481: Training Loss: 0.021119317039847374, Validation Loss: 0.024029478430747986\n",
      "Epoch 13, Batch: 482: Training Loss: 0.025912022218108177, Validation Loss: 0.02618507854640484\n",
      "Epoch 13, Batch: 483: Training Loss: 0.023939944803714752, Validation Loss: 0.02592316083610058\n",
      "Epoch 13, Batch: 484: Training Loss: 0.02531339041888714, Validation Loss: 0.023219522088766098\n",
      "Epoch 13, Batch: 485: Training Loss: 0.025169024243950844, Validation Loss: 0.0229574516415596\n",
      "Epoch 13, Batch: 486: Training Loss: 0.02581840567290783, Validation Loss: 0.02263311855494976\n",
      "Epoch 13, Batch: 487: Training Loss: 0.022445442155003548, Validation Loss: 0.022195762023329735\n",
      "Epoch 13, Batch: 488: Training Loss: 0.02583680860698223, Validation Loss: 0.024907726794481277\n",
      "Epoch 13, Batch: 489: Training Loss: 0.025511860847473145, Validation Loss: 0.025219596922397614\n",
      "Epoch 13, Batch: 490: Training Loss: 0.025033561512827873, Validation Loss: 0.022865884006023407\n",
      "Epoch 13, Batch: 491: Training Loss: 0.019591419026255608, Validation Loss: 0.02379809319972992\n",
      "Epoch 13, Batch: 492: Training Loss: 0.02833126112818718, Validation Loss: 0.024925686419010162\n",
      "Epoch 13, Batch: 493: Training Loss: 0.025613166391849518, Validation Loss: 0.02485504187643528\n",
      "Epoch 13, Batch: 494: Training Loss: 0.027506792917847633, Validation Loss: 0.023228682577610016\n",
      "Epoch 13, Batch: 495: Training Loss: 0.02129438892006874, Validation Loss: 0.02602558210492134\n",
      "Epoch 13, Batch: 496: Training Loss: 0.02312062866985798, Validation Loss: 0.024172281846404076\n",
      "Epoch 13, Batch: 497: Training Loss: 0.02372082509100437, Validation Loss: 0.02427990362048149\n",
      "Epoch 13, Batch: 498: Training Loss: 0.024439744651317596, Validation Loss: 0.027192628011107445\n",
      "Epoch 13, Batch: 499: Training Loss: 0.022875076159834862, Validation Loss: 0.023875221610069275\n",
      "Epoch 14, Batch: 0: Training Loss: 0.021946851164102554, Validation Loss: 0.02408493310213089\n",
      "Epoch 14, Batch: 1: Training Loss: 0.02376706339418888, Validation Loss: 0.024015842005610466\n",
      "Epoch 14, Batch: 2: Training Loss: 0.023868625983595848, Validation Loss: 0.02369542606174946\n",
      "Epoch 14, Batch: 3: Training Loss: 0.02254548668861389, Validation Loss: 0.02500549517571926\n",
      "Epoch 14, Batch: 4: Training Loss: 0.020844442769885063, Validation Loss: 0.023782607167959213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 5: Training Loss: 0.023182600736618042, Validation Loss: 0.025158172473311424\n",
      "Epoch 14, Batch: 6: Training Loss: 0.023698067292571068, Validation Loss: 0.024799296632409096\n",
      "Epoch 14, Batch: 7: Training Loss: 0.024693338200449944, Validation Loss: 0.022371426224708557\n",
      "Epoch 14, Batch: 8: Training Loss: 0.02160116471350193, Validation Loss: 0.022215962409973145\n",
      "Epoch 14, Batch: 9: Training Loss: 0.022385967895388603, Validation Loss: 0.02227473258972168\n",
      "Epoch 14, Batch: 10: Training Loss: 0.024481074884533882, Validation Loss: 0.02243739925324917\n",
      "Epoch 14, Batch: 11: Training Loss: 0.02287503518164158, Validation Loss: 0.023178251460194588\n",
      "Epoch 14, Batch: 12: Training Loss: 0.023371759802103043, Validation Loss: 0.02409517765045166\n",
      "Epoch 14, Batch: 13: Training Loss: 0.027777666226029396, Validation Loss: 0.023639338091015816\n",
      "Epoch 14, Batch: 14: Training Loss: 0.02332122251391411, Validation Loss: 0.02495359256863594\n",
      "Epoch 14, Batch: 15: Training Loss: 0.02559593692421913, Validation Loss: 0.02458057552576065\n",
      "Epoch 14, Batch: 16: Training Loss: 0.026706798002123833, Validation Loss: 0.024520138278603554\n",
      "Epoch 14, Batch: 17: Training Loss: 0.02363005094230175, Validation Loss: 0.02418190985918045\n",
      "Epoch 14, Batch: 18: Training Loss: 0.023602521046996117, Validation Loss: 0.025443384423851967\n",
      "Epoch 14, Batch: 19: Training Loss: 0.02272764779627323, Validation Loss: 0.025490853935480118\n",
      "Epoch 14, Batch: 20: Training Loss: 0.02460166998207569, Validation Loss: 0.024379929527640343\n",
      "Epoch 14, Batch: 21: Training Loss: 0.026765795424580574, Validation Loss: 0.024592943489551544\n",
      "Epoch 14, Batch: 22: Training Loss: 0.02329164929687977, Validation Loss: 0.026209570467472076\n",
      "Epoch 14, Batch: 23: Training Loss: 0.02454392798244953, Validation Loss: 0.024370485916733742\n",
      "Epoch 14, Batch: 24: Training Loss: 0.027807440608739853, Validation Loss: 0.027175715193152428\n",
      "Epoch 14, Batch: 25: Training Loss: 0.020813746377825737, Validation Loss: 0.026743318885564804\n",
      "Epoch 14, Batch: 26: Training Loss: 0.02448672614991665, Validation Loss: 0.024700913578271866\n",
      "Epoch 14, Batch: 27: Training Loss: 0.02449622191488743, Validation Loss: 0.024210872128605843\n",
      "Epoch 14, Batch: 28: Training Loss: 0.029362965375185013, Validation Loss: 0.02576814591884613\n",
      "Epoch 14, Batch: 29: Training Loss: 0.02613631635904312, Validation Loss: 0.026065325364470482\n",
      "Epoch 14, Batch: 30: Training Loss: 0.02362128533422947, Validation Loss: 0.026754755526781082\n",
      "Epoch 14, Batch: 31: Training Loss: 0.0263779629021883, Validation Loss: 0.02469821646809578\n",
      "Epoch 14, Batch: 32: Training Loss: 0.024751581251621246, Validation Loss: 0.024861041456460953\n",
      "Epoch 14, Batch: 33: Training Loss: 0.02316305786371231, Validation Loss: 0.026456918567419052\n",
      "Epoch 14, Batch: 34: Training Loss: 0.021225234493613243, Validation Loss: 0.024732941761612892\n",
      "Epoch 14, Batch: 35: Training Loss: 0.024095255881547928, Validation Loss: 0.025334298610687256\n",
      "Epoch 14, Batch: 36: Training Loss: 0.02378241904079914, Validation Loss: 0.02514287456870079\n",
      "Epoch 14, Batch: 37: Training Loss: 0.022747457027435303, Validation Loss: 0.025380507111549377\n",
      "Epoch 14, Batch: 38: Training Loss: 0.027914950624108315, Validation Loss: 0.025086333975195885\n",
      "Epoch 14, Batch: 39: Training Loss: 0.025760989636182785, Validation Loss: 0.025243692100048065\n",
      "Epoch 14, Batch: 40: Training Loss: 0.025783604010939598, Validation Loss: 0.026065565645694733\n",
      "Epoch 14, Batch: 41: Training Loss: 0.026622988283634186, Validation Loss: 0.023330083116889\n",
      "Epoch 14, Batch: 42: Training Loss: 0.02407851628959179, Validation Loss: 0.025386061519384384\n",
      "Epoch 14, Batch: 43: Training Loss: 0.021924052387475967, Validation Loss: 0.02555595524609089\n",
      "Epoch 14, Batch: 44: Training Loss: 0.02404676377773285, Validation Loss: 0.02733413316309452\n",
      "Epoch 14, Batch: 45: Training Loss: 0.026155948638916016, Validation Loss: 0.02527853474020958\n",
      "Epoch 14, Batch: 46: Training Loss: 0.025798464193940163, Validation Loss: 0.024254390969872475\n",
      "Epoch 14, Batch: 47: Training Loss: 0.024537479504942894, Validation Loss: 0.024839023128151894\n",
      "Epoch 14, Batch: 48: Training Loss: 0.02690565586090088, Validation Loss: 0.02426488697528839\n",
      "Epoch 14, Batch: 49: Training Loss: 0.028541209176182747, Validation Loss: 0.02692701853811741\n",
      "Epoch 14, Batch: 50: Training Loss: 0.025743981823325157, Validation Loss: 0.024723611772060394\n",
      "Epoch 14, Batch: 51: Training Loss: 0.028451675549149513, Validation Loss: 0.023694410920143127\n",
      "Epoch 14, Batch: 52: Training Loss: 0.026730220764875412, Validation Loss: 0.023811880499124527\n",
      "Epoch 14, Batch: 53: Training Loss: 0.020413609221577644, Validation Loss: 0.02387191355228424\n",
      "Epoch 14, Batch: 54: Training Loss: 0.026006827130913734, Validation Loss: 0.023150471970438957\n",
      "Epoch 14, Batch: 55: Training Loss: 0.022824399173259735, Validation Loss: 0.02499844692647457\n",
      "Epoch 14, Batch: 56: Training Loss: 0.02564696967601776, Validation Loss: 0.02517830580472946\n",
      "Epoch 14, Batch: 57: Training Loss: 0.023979060351848602, Validation Loss: 0.025487612932920456\n",
      "Epoch 14, Batch: 58: Training Loss: 0.02325749211013317, Validation Loss: 0.02476111799478531\n",
      "Epoch 14, Batch: 59: Training Loss: 0.01983235962688923, Validation Loss: 0.026882311329245567\n",
      "Epoch 14, Batch: 60: Training Loss: 0.02326027676463127, Validation Loss: 0.024432232603430748\n",
      "Epoch 14, Batch: 61: Training Loss: 0.027729837223887444, Validation Loss: 0.02373940497636795\n",
      "Epoch 14, Batch: 62: Training Loss: 0.025232121348381042, Validation Loss: 0.024466916918754578\n",
      "Epoch 14, Batch: 63: Training Loss: 0.026949919760227203, Validation Loss: 0.024302948266267776\n",
      "Epoch 14, Batch: 64: Training Loss: 0.023700864985585213, Validation Loss: 0.022757994011044502\n",
      "Epoch 14, Batch: 65: Training Loss: 0.025594480335712433, Validation Loss: 0.028047999367117882\n",
      "Epoch 14, Batch: 66: Training Loss: 0.021068045869469643, Validation Loss: 0.0259616207331419\n",
      "Epoch 14, Batch: 67: Training Loss: 0.020737554877996445, Validation Loss: 0.026608234271407127\n",
      "Epoch 14, Batch: 68: Training Loss: 0.02256127819418907, Validation Loss: 0.02567150630056858\n",
      "Epoch 14, Batch: 69: Training Loss: 0.02427622675895691, Validation Loss: 0.02489117532968521\n",
      "Epoch 14, Batch: 70: Training Loss: 0.025118974968791008, Validation Loss: 0.02771497331559658\n",
      "Epoch 14, Batch: 71: Training Loss: 0.022695722058415413, Validation Loss: 0.025963276624679565\n",
      "Epoch 14, Batch: 72: Training Loss: 0.026518378406763077, Validation Loss: 0.027893712744116783\n",
      "Epoch 14, Batch: 73: Training Loss: 0.025073831900954247, Validation Loss: 0.029083790257573128\n",
      "Epoch 14, Batch: 74: Training Loss: 0.024709733203053474, Validation Loss: 0.026429081335663795\n",
      "Epoch 14, Batch: 75: Training Loss: 0.022224806249141693, Validation Loss: 0.02389649674296379\n",
      "Epoch 14, Batch: 76: Training Loss: 0.02069147862493992, Validation Loss: 0.02591484598815441\n",
      "Epoch 14, Batch: 77: Training Loss: 0.024279577657580376, Validation Loss: 0.023867275565862656\n",
      "Epoch 14, Batch: 78: Training Loss: 0.025974689051508904, Validation Loss: 0.02217215672135353\n",
      "Epoch 14, Batch: 79: Training Loss: 0.023709964007139206, Validation Loss: 0.026666736230254173\n",
      "Epoch 14, Batch: 80: Training Loss: 0.021923311054706573, Validation Loss: 0.024660008028149605\n",
      "Epoch 14, Batch: 81: Training Loss: 0.023876721039414406, Validation Loss: 0.02405552938580513\n",
      "Epoch 14, Batch: 82: Training Loss: 0.026328422129154205, Validation Loss: 0.025724345818161964\n",
      "Epoch 14, Batch: 83: Training Loss: 0.024979231879115105, Validation Loss: 0.025137197226285934\n",
      "Epoch 14, Batch: 84: Training Loss: 0.024250473827123642, Validation Loss: 0.024293899536132812\n",
      "Epoch 14, Batch: 85: Training Loss: 0.022817490622401237, Validation Loss: 0.0229217316955328\n",
      "Epoch 14, Batch: 86: Training Loss: 0.025300608947873116, Validation Loss: 0.0253959521651268\n",
      "Epoch 14, Batch: 87: Training Loss: 0.02780141681432724, Validation Loss: 0.025530850514769554\n",
      "Epoch 14, Batch: 88: Training Loss: 0.024449942633509636, Validation Loss: 0.025701865553855896\n",
      "Epoch 14, Batch: 89: Training Loss: 0.025287680327892303, Validation Loss: 0.025896964594721794\n",
      "Epoch 14, Batch: 90: Training Loss: 0.019676439464092255, Validation Loss: 0.026455866172909737\n",
      "Epoch 14, Batch: 91: Training Loss: 0.02546701394021511, Validation Loss: 0.026499150320887566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 92: Training Loss: 0.027111569419503212, Validation Loss: 0.026300622150301933\n",
      "Epoch 14, Batch: 93: Training Loss: 0.027319513261318207, Validation Loss: 0.02520732581615448\n",
      "Epoch 14, Batch: 94: Training Loss: 0.026505719870328903, Validation Loss: 0.0263509638607502\n",
      "Epoch 14, Batch: 95: Training Loss: 0.02369687147438526, Validation Loss: 0.024975037202239037\n",
      "Epoch 14, Batch: 96: Training Loss: 0.023184679448604584, Validation Loss: 0.024664398282766342\n",
      "Epoch 14, Batch: 97: Training Loss: 0.026259582489728928, Validation Loss: 0.026610074564814568\n",
      "Epoch 14, Batch: 98: Training Loss: 0.025030534714460373, Validation Loss: 0.026670491322875023\n",
      "Epoch 14, Batch: 99: Training Loss: 0.02376687154173851, Validation Loss: 0.023904358968138695\n",
      "Epoch 14, Batch: 100: Training Loss: 0.026486460119485855, Validation Loss: 0.024630412459373474\n",
      "Epoch 14, Batch: 101: Training Loss: 0.02171275019645691, Validation Loss: 0.023068176582455635\n",
      "Epoch 14, Batch: 102: Training Loss: 0.02330251783132553, Validation Loss: 0.024456102401018143\n",
      "Epoch 14, Batch: 103: Training Loss: 0.023851802572607994, Validation Loss: 0.024364575743675232\n",
      "Epoch 14, Batch: 104: Training Loss: 0.02137364260852337, Validation Loss: 0.023041322827339172\n",
      "Epoch 14, Batch: 105: Training Loss: 0.023473640903830528, Validation Loss: 0.022629689425230026\n",
      "Epoch 14, Batch: 106: Training Loss: 0.02442595176398754, Validation Loss: 0.023114556446671486\n",
      "Epoch 14, Batch: 107: Training Loss: 0.02071552909910679, Validation Loss: 0.02596943825483322\n",
      "Epoch 14, Batch: 108: Training Loss: 0.02323826588690281, Validation Loss: 0.024195244535803795\n",
      "Epoch 14, Batch: 109: Training Loss: 0.027309540659189224, Validation Loss: 0.022977275773882866\n",
      "Epoch 14, Batch: 110: Training Loss: 0.023977626115083694, Validation Loss: 0.024448327720165253\n",
      "Epoch 14, Batch: 111: Training Loss: 0.02202274277806282, Validation Loss: 0.025960169732570648\n",
      "Epoch 14, Batch: 112: Training Loss: 0.02289441041648388, Validation Loss: 0.02380172535777092\n",
      "Epoch 14, Batch: 113: Training Loss: 0.023994000628590584, Validation Loss: 0.02493671327829361\n",
      "Epoch 14, Batch: 114: Training Loss: 0.023228846490383148, Validation Loss: 0.02493264526128769\n",
      "Epoch 14, Batch: 115: Training Loss: 0.026580344885587692, Validation Loss: 0.0248127244412899\n",
      "Epoch 14, Batch: 116: Training Loss: 0.021616797894239426, Validation Loss: 0.02308077923953533\n",
      "Epoch 14, Batch: 117: Training Loss: 0.026432093232870102, Validation Loss: 0.027101917192339897\n",
      "Epoch 14, Batch: 118: Training Loss: 0.022820396348834038, Validation Loss: 0.025791944935917854\n",
      "Epoch 14, Batch: 119: Training Loss: 0.02406047098338604, Validation Loss: 0.02458181604743004\n",
      "Epoch 14, Batch: 120: Training Loss: 0.023092327639460564, Validation Loss: 0.02431514486670494\n",
      "Epoch 14, Batch: 121: Training Loss: 0.024164941161870956, Validation Loss: 0.02549290470778942\n",
      "Epoch 14, Batch: 122: Training Loss: 0.026798386126756668, Validation Loss: 0.025855589658021927\n",
      "Epoch 14, Batch: 123: Training Loss: 0.02394692413508892, Validation Loss: 0.023807087913155556\n",
      "Epoch 14, Batch: 124: Training Loss: 0.02477775327861309, Validation Loss: 0.025263598188757896\n",
      "Epoch 14, Batch: 125: Training Loss: 0.020145190879702568, Validation Loss: 0.02417529746890068\n",
      "Epoch 14, Batch: 126: Training Loss: 0.021320613101124763, Validation Loss: 0.026132583618164062\n",
      "Epoch 14, Batch: 127: Training Loss: 0.022490590810775757, Validation Loss: 0.02423182688653469\n",
      "Epoch 14, Batch: 128: Training Loss: 0.023456426337361336, Validation Loss: 0.022876804694533348\n",
      "Epoch 14, Batch: 129: Training Loss: 0.022752800956368446, Validation Loss: 0.024336475878953934\n",
      "Epoch 14, Batch: 130: Training Loss: 0.02326708473265171, Validation Loss: 0.02274010144174099\n",
      "Epoch 14, Batch: 131: Training Loss: 0.021369274705648422, Validation Loss: 0.025066932663321495\n",
      "Epoch 14, Batch: 132: Training Loss: 0.02149408496916294, Validation Loss: 0.027059992775321007\n",
      "Epoch 14, Batch: 133: Training Loss: 0.02142922952771187, Validation Loss: 0.024693071842193604\n",
      "Epoch 14, Batch: 134: Training Loss: 0.026048457249999046, Validation Loss: 0.02425886131823063\n",
      "Epoch 14, Batch: 135: Training Loss: 0.025355692952871323, Validation Loss: 0.02643921785056591\n",
      "Epoch 14, Batch: 136: Training Loss: 0.02245449274778366, Validation Loss: 0.02588084526360035\n",
      "Epoch 14, Batch: 137: Training Loss: 0.02007402293384075, Validation Loss: 0.023829778656363487\n",
      "Epoch 14, Batch: 138: Training Loss: 0.02402474358677864, Validation Loss: 0.02543054334819317\n",
      "Epoch 14, Batch: 139: Training Loss: 0.02270141988992691, Validation Loss: 0.022879522293806076\n",
      "Epoch 14, Batch: 140: Training Loss: 0.02589454874396324, Validation Loss: 0.024337317794561386\n",
      "Epoch 14, Batch: 141: Training Loss: 0.024701470509171486, Validation Loss: 0.024349864572286606\n",
      "Epoch 14, Batch: 142: Training Loss: 0.02410878799855709, Validation Loss: 0.02379588596522808\n",
      "Epoch 14, Batch: 143: Training Loss: 0.02012641169130802, Validation Loss: 0.02301190420985222\n",
      "Epoch 14, Batch: 144: Training Loss: 0.021400796249508858, Validation Loss: 0.023832514882087708\n",
      "Epoch 14, Batch: 145: Training Loss: 0.021717380732297897, Validation Loss: 0.021876400336623192\n",
      "Epoch 14, Batch: 146: Training Loss: 0.022955359891057014, Validation Loss: 0.02431989274919033\n",
      "Epoch 14, Batch: 147: Training Loss: 0.023187434300780296, Validation Loss: 0.023699454963207245\n",
      "Epoch 14, Batch: 148: Training Loss: 0.02363363467156887, Validation Loss: 0.021870866417884827\n",
      "Epoch 14, Batch: 149: Training Loss: 0.023679230362176895, Validation Loss: 0.021913889795541763\n",
      "Epoch 14, Batch: 150: Training Loss: 0.025020798668265343, Validation Loss: 0.0223055612295866\n",
      "Epoch 14, Batch: 151: Training Loss: 0.021713169291615486, Validation Loss: 0.025833401829004288\n",
      "Epoch 14, Batch: 152: Training Loss: 0.024793045595288277, Validation Loss: 0.024258529767394066\n",
      "Epoch 14, Batch: 153: Training Loss: 0.02305828407406807, Validation Loss: 0.02405298687517643\n",
      "Epoch 14, Batch: 154: Training Loss: 0.022663909941911697, Validation Loss: 0.024646934121847153\n",
      "Epoch 14, Batch: 155: Training Loss: 0.027470210567116737, Validation Loss: 0.024441992864012718\n",
      "Epoch 14, Batch: 156: Training Loss: 0.02249298430979252, Validation Loss: 0.026353565976023674\n",
      "Epoch 14, Batch: 157: Training Loss: 0.01777755655348301, Validation Loss: 0.02677694335579872\n",
      "Epoch 14, Batch: 158: Training Loss: 0.023139389231801033, Validation Loss: 0.025796137750148773\n",
      "Epoch 14, Batch: 159: Training Loss: 0.024154985323548317, Validation Loss: 0.025622298941016197\n",
      "Epoch 14, Batch: 160: Training Loss: 0.02148115076124668, Validation Loss: 0.02501668781042099\n",
      "Epoch 14, Batch: 161: Training Loss: 0.022488608956336975, Validation Loss: 0.025166049599647522\n",
      "Epoch 14, Batch: 162: Training Loss: 0.026573877781629562, Validation Loss: 0.021045316010713577\n",
      "Epoch 14, Batch: 163: Training Loss: 0.024497464299201965, Validation Loss: 0.025418926030397415\n",
      "Epoch 14, Batch: 164: Training Loss: 0.026264570653438568, Validation Loss: 0.02853422425687313\n",
      "Epoch 14, Batch: 165: Training Loss: 0.02054242603480816, Validation Loss: 0.026030441746115685\n",
      "Epoch 14, Batch: 166: Training Loss: 0.02125857025384903, Validation Loss: 0.026005249470472336\n",
      "Epoch 14, Batch: 167: Training Loss: 0.022463897243142128, Validation Loss: 0.026228638365864754\n",
      "Epoch 14, Batch: 168: Training Loss: 0.021780259907245636, Validation Loss: 0.025374453514814377\n",
      "Epoch 14, Batch: 169: Training Loss: 0.02395235374569893, Validation Loss: 0.024747489020228386\n",
      "Epoch 14, Batch: 170: Training Loss: 0.02241896465420723, Validation Loss: 0.025794249027967453\n",
      "Epoch 14, Batch: 171: Training Loss: 0.020514898002147675, Validation Loss: 0.02708355523645878\n",
      "Epoch 14, Batch: 172: Training Loss: 0.02220745012164116, Validation Loss: 0.025634130463004112\n",
      "Epoch 14, Batch: 173: Training Loss: 0.02235378697514534, Validation Loss: 0.02558121457695961\n",
      "Epoch 14, Batch: 174: Training Loss: 0.0229587834328413, Validation Loss: 0.024821806699037552\n",
      "Epoch 14, Batch: 175: Training Loss: 0.022794514894485474, Validation Loss: 0.026118997484445572\n",
      "Epoch 14, Batch: 176: Training Loss: 0.024196455255150795, Validation Loss: 0.024487940594553947\n",
      "Epoch 14, Batch: 177: Training Loss: 0.027970246970653534, Validation Loss: 0.027213390916585922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 178: Training Loss: 0.027580153197050095, Validation Loss: 0.026520077139139175\n",
      "Epoch 14, Batch: 179: Training Loss: 0.021543914452195168, Validation Loss: 0.024611718952655792\n",
      "Epoch 14, Batch: 180: Training Loss: 0.026740888133645058, Validation Loss: 0.025790756568312645\n",
      "Epoch 14, Batch: 181: Training Loss: 0.02678229846060276, Validation Loss: 0.024940861389040947\n",
      "Epoch 14, Batch: 182: Training Loss: 0.02927807718515396, Validation Loss: 0.02203310653567314\n",
      "Epoch 14, Batch: 183: Training Loss: 0.02479678951203823, Validation Loss: 0.023625558242201805\n",
      "Epoch 14, Batch: 184: Training Loss: 0.022335557267069817, Validation Loss: 0.024602947756648064\n",
      "Epoch 14, Batch: 185: Training Loss: 0.025024449452757835, Validation Loss: 0.02326967380940914\n",
      "Epoch 14, Batch: 186: Training Loss: 0.02284710668027401, Validation Loss: 0.022324861958622932\n",
      "Epoch 14, Batch: 187: Training Loss: 0.024567946791648865, Validation Loss: 0.02315448597073555\n",
      "Epoch 14, Batch: 188: Training Loss: 0.026773639023303986, Validation Loss: 0.025331245735287666\n",
      "Epoch 14, Batch: 189: Training Loss: 0.022238630801439285, Validation Loss: 0.02520262449979782\n",
      "Epoch 14, Batch: 190: Training Loss: 0.022833039984107018, Validation Loss: 0.023045850917696953\n",
      "Epoch 14, Batch: 191: Training Loss: 0.026388006284832954, Validation Loss: 0.024221662431955338\n",
      "Epoch 14, Batch: 192: Training Loss: 0.024818282574415207, Validation Loss: 0.02392522431910038\n",
      "Epoch 14, Batch: 193: Training Loss: 0.02502710558474064, Validation Loss: 0.02560940757393837\n",
      "Epoch 14, Batch: 194: Training Loss: 0.024160148575901985, Validation Loss: 0.023673586547374725\n",
      "Epoch 14, Batch: 195: Training Loss: 0.0223346296697855, Validation Loss: 0.02449086494743824\n",
      "Epoch 14, Batch: 196: Training Loss: 0.022882036864757538, Validation Loss: 0.025701474398374557\n",
      "Epoch 14, Batch: 197: Training Loss: 0.021676482632756233, Validation Loss: 0.026548244059085846\n",
      "Epoch 14, Batch: 198: Training Loss: 0.01882275380194187, Validation Loss: 0.0249929316341877\n",
      "Epoch 14, Batch: 199: Training Loss: 0.023072825744748116, Validation Loss: 0.024050602689385414\n",
      "Epoch 14, Batch: 200: Training Loss: 0.020218808203935623, Validation Loss: 0.024490216746926308\n",
      "Epoch 14, Batch: 201: Training Loss: 0.02686350792646408, Validation Loss: 0.025774791836738586\n",
      "Epoch 14, Batch: 202: Training Loss: 0.025906838476657867, Validation Loss: 0.02577797882258892\n",
      "Epoch 14, Batch: 203: Training Loss: 0.0257657989859581, Validation Loss: 0.026265621185302734\n",
      "Epoch 14, Batch: 204: Training Loss: 0.02359064482152462, Validation Loss: 0.024562934413552284\n",
      "Epoch 14, Batch: 205: Training Loss: 0.03093605488538742, Validation Loss: 0.025383012369275093\n",
      "Epoch 14, Batch: 206: Training Loss: 0.022413082420825958, Validation Loss: 0.02486681006848812\n",
      "Epoch 14, Batch: 207: Training Loss: 0.027114175260066986, Validation Loss: 0.024190304800868034\n",
      "Epoch 14, Batch: 208: Training Loss: 0.022448845207691193, Validation Loss: 0.026714127510786057\n",
      "Epoch 14, Batch: 209: Training Loss: 0.02335486002266407, Validation Loss: 0.025960147380828857\n",
      "Epoch 14, Batch: 210: Training Loss: 0.02529417723417282, Validation Loss: 0.02609679289162159\n",
      "Epoch 14, Batch: 211: Training Loss: 0.02434728853404522, Validation Loss: 0.023292798548936844\n",
      "Epoch 14, Batch: 212: Training Loss: 0.02217540331184864, Validation Loss: 0.024144044145941734\n",
      "Epoch 14, Batch: 213: Training Loss: 0.02535441145300865, Validation Loss: 0.02509581297636032\n",
      "Epoch 14, Batch: 214: Training Loss: 0.023765696212649345, Validation Loss: 0.021220441907644272\n",
      "Epoch 14, Batch: 215: Training Loss: 0.022573787719011307, Validation Loss: 0.022709202021360397\n",
      "Epoch 14, Batch: 216: Training Loss: 0.021868936717510223, Validation Loss: 0.025716522708535194\n",
      "Epoch 14, Batch: 217: Training Loss: 0.02747371792793274, Validation Loss: 0.02510237693786621\n",
      "Epoch 14, Batch: 218: Training Loss: 0.022802280262112617, Validation Loss: 0.021937482059001923\n",
      "Epoch 14, Batch: 219: Training Loss: 0.02733534574508667, Validation Loss: 0.022153964266180992\n",
      "Epoch 14, Batch: 220: Training Loss: 0.025400280952453613, Validation Loss: 0.023535575717687607\n",
      "Epoch 14, Batch: 221: Training Loss: 0.022629939019680023, Validation Loss: 0.02543463185429573\n",
      "Epoch 14, Batch: 222: Training Loss: 0.02490161918103695, Validation Loss: 0.023310866206884384\n",
      "Epoch 14, Batch: 223: Training Loss: 0.02417362667620182, Validation Loss: 0.024388151243329048\n",
      "Epoch 14, Batch: 224: Training Loss: 0.02572467364370823, Validation Loss: 0.025127841159701347\n",
      "Epoch 14, Batch: 225: Training Loss: 0.024093955755233765, Validation Loss: 0.020633652806282043\n",
      "Epoch 14, Batch: 226: Training Loss: 0.02446272224187851, Validation Loss: 0.023584598675370216\n",
      "Epoch 14, Batch: 227: Training Loss: 0.023862656205892563, Validation Loss: 0.025192758068442345\n",
      "Epoch 14, Batch: 228: Training Loss: 0.027427075430750847, Validation Loss: 0.026713037863373756\n",
      "Epoch 14, Batch: 229: Training Loss: 0.027279794216156006, Validation Loss: 0.024005040526390076\n",
      "Epoch 14, Batch: 230: Training Loss: 0.019866177812218666, Validation Loss: 0.023239685222506523\n",
      "Epoch 14, Batch: 231: Training Loss: 0.027559814974665642, Validation Loss: 0.023018375039100647\n",
      "Epoch 14, Batch: 232: Training Loss: 0.02363779954612255, Validation Loss: 0.024633396416902542\n",
      "Epoch 14, Batch: 233: Training Loss: 0.022080829367041588, Validation Loss: 0.02485799416899681\n",
      "Epoch 14, Batch: 234: Training Loss: 0.024285515770316124, Validation Loss: 0.024789322167634964\n",
      "Epoch 14, Batch: 235: Training Loss: 0.028326086699962616, Validation Loss: 0.02293243817985058\n",
      "Epoch 14, Batch: 236: Training Loss: 0.027160802856087685, Validation Loss: 0.023278286680579185\n",
      "Epoch 14, Batch: 237: Training Loss: 0.026160240173339844, Validation Loss: 0.022011512890458107\n",
      "Epoch 14, Batch: 238: Training Loss: 0.02308177575469017, Validation Loss: 0.022188788279891014\n",
      "Epoch 14, Batch: 239: Training Loss: 0.027729030698537827, Validation Loss: 0.02226770482957363\n",
      "Epoch 14, Batch: 240: Training Loss: 0.02307470515370369, Validation Loss: 0.02410943992435932\n",
      "Epoch 14, Batch: 241: Training Loss: 0.02072523720562458, Validation Loss: 0.023299258202314377\n",
      "Epoch 14, Batch: 242: Training Loss: 0.024668188765645027, Validation Loss: 0.02199474349617958\n",
      "Epoch 14, Batch: 243: Training Loss: 0.02821093238890171, Validation Loss: 0.02530885674059391\n",
      "Epoch 14, Batch: 244: Training Loss: 0.024829916656017303, Validation Loss: 0.02137506939470768\n",
      "Epoch 14, Batch: 245: Training Loss: 0.026480672881007195, Validation Loss: 0.025776535272598267\n",
      "Epoch 14, Batch: 246: Training Loss: 0.021009434014558792, Validation Loss: 0.023587947711348534\n",
      "Epoch 14, Batch: 247: Training Loss: 0.023367613554000854, Validation Loss: 0.02498767338693142\n",
      "Epoch 14, Batch: 248: Training Loss: 0.022801624611020088, Validation Loss: 0.02598249912261963\n",
      "Epoch 14, Batch: 249: Training Loss: 0.021576428785920143, Validation Loss: 0.027339709922671318\n",
      "Epoch 14, Batch: 250: Training Loss: 0.02530439756810665, Validation Loss: 0.025598956272006035\n",
      "Epoch 14, Batch: 251: Training Loss: 0.022189738228917122, Validation Loss: 0.024999937042593956\n",
      "Epoch 14, Batch: 252: Training Loss: 0.02716999314725399, Validation Loss: 0.026195624843239784\n",
      "Epoch 14, Batch: 253: Training Loss: 0.029768964275717735, Validation Loss: 0.025822680443525314\n",
      "Epoch 14, Batch: 254: Training Loss: 0.02131478860974312, Validation Loss: 0.024812811985611916\n",
      "Epoch 14, Batch: 255: Training Loss: 0.026966368779540062, Validation Loss: 0.028750350698828697\n",
      "Epoch 14, Batch: 256: Training Loss: 0.0279474426060915, Validation Loss: 0.025272250175476074\n",
      "Epoch 14, Batch: 257: Training Loss: 0.027477480471134186, Validation Loss: 0.025842223316431046\n",
      "Epoch 14, Batch: 258: Training Loss: 0.027776051312685013, Validation Loss: 0.025132644921541214\n",
      "Epoch 14, Batch: 259: Training Loss: 0.023805227130651474, Validation Loss: 0.02345500886440277\n",
      "Epoch 14, Batch: 260: Training Loss: 0.029844654724001884, Validation Loss: 0.02416200004518032\n",
      "Epoch 14, Batch: 261: Training Loss: 0.02630672976374626, Validation Loss: 0.023524479940533638\n",
      "Epoch 14, Batch: 262: Training Loss: 0.02623930759727955, Validation Loss: 0.02449597790837288\n",
      "Epoch 14, Batch: 263: Training Loss: 0.020975742489099503, Validation Loss: 0.02493223547935486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 264: Training Loss: 0.02269713394343853, Validation Loss: 0.02495240978896618\n",
      "Epoch 14, Batch: 265: Training Loss: 0.026074152439832687, Validation Loss: 0.025922279804944992\n",
      "Epoch 14, Batch: 266: Training Loss: 0.021322663873434067, Validation Loss: 0.022364046424627304\n",
      "Epoch 14, Batch: 267: Training Loss: 0.020382273942232132, Validation Loss: 0.024446379393339157\n",
      "Epoch 14, Batch: 268: Training Loss: 0.021342255175113678, Validation Loss: 0.022468721494078636\n",
      "Epoch 14, Batch: 269: Training Loss: 0.025868745520710945, Validation Loss: 0.021305670961737633\n",
      "Epoch 14, Batch: 270: Training Loss: 0.022482285276055336, Validation Loss: 0.023858459666371346\n",
      "Epoch 14, Batch: 271: Training Loss: 0.021432576701045036, Validation Loss: 0.022703872993588448\n",
      "Epoch 14, Batch: 272: Training Loss: 0.024898696690797806, Validation Loss: 0.023090269416570663\n",
      "Epoch 14, Batch: 273: Training Loss: 0.024668661877512932, Validation Loss: 0.022114451974630356\n",
      "Epoch 14, Batch: 274: Training Loss: 0.026872826740145683, Validation Loss: 0.027695735916495323\n",
      "Epoch 14, Batch: 275: Training Loss: 0.021879004314541817, Validation Loss: 0.02262778766453266\n",
      "Epoch 14, Batch: 276: Training Loss: 0.025987427681684494, Validation Loss: 0.024238280951976776\n",
      "Epoch 14, Batch: 277: Training Loss: 0.022762643173336983, Validation Loss: 0.023426495492458344\n",
      "Epoch 14, Batch: 278: Training Loss: 0.02248445898294449, Validation Loss: 0.023285450413823128\n",
      "Epoch 14, Batch: 279: Training Loss: 0.02484985627233982, Validation Loss: 0.022768178954720497\n",
      "Epoch 14, Batch: 280: Training Loss: 0.021510334685444832, Validation Loss: 0.02414359524846077\n",
      "Epoch 14, Batch: 281: Training Loss: 0.023884249851107597, Validation Loss: 0.025546962395310402\n",
      "Epoch 14, Batch: 282: Training Loss: 0.02208762802183628, Validation Loss: 0.024583984166383743\n",
      "Epoch 14, Batch: 283: Training Loss: 0.021294498816132545, Validation Loss: 0.023267675191164017\n",
      "Epoch 14, Batch: 284: Training Loss: 0.021704716607928276, Validation Loss: 0.023826749995350838\n",
      "Epoch 14, Batch: 285: Training Loss: 0.02052493952214718, Validation Loss: 0.024174803867936134\n",
      "Epoch 14, Batch: 286: Training Loss: 0.021789727732539177, Validation Loss: 0.023122435435652733\n",
      "Epoch 14, Batch: 287: Training Loss: 0.022431163117289543, Validation Loss: 0.025243790820240974\n",
      "Epoch 14, Batch: 288: Training Loss: 0.022929152473807335, Validation Loss: 0.022632690146565437\n",
      "Epoch 14, Batch: 289: Training Loss: 0.022435538470745087, Validation Loss: 0.02359970100224018\n",
      "Epoch 14, Batch: 290: Training Loss: 0.022483745589852333, Validation Loss: 0.022776100784540176\n",
      "Epoch 14, Batch: 291: Training Loss: 0.021512335166335106, Validation Loss: 0.021404551342129707\n",
      "Epoch 14, Batch: 292: Training Loss: 0.021982714533805847, Validation Loss: 0.022899415343999863\n",
      "Epoch 14, Batch: 293: Training Loss: 0.022576279938220978, Validation Loss: 0.023293092846870422\n",
      "Epoch 14, Batch: 294: Training Loss: 0.022342748939990997, Validation Loss: 0.025393906980752945\n",
      "Epoch 14, Batch: 295: Training Loss: 0.02256982959806919, Validation Loss: 0.025313187390565872\n",
      "Epoch 14, Batch: 296: Training Loss: 0.024882670491933823, Validation Loss: 0.025092216208577156\n",
      "Epoch 14, Batch: 297: Training Loss: 0.024027451872825623, Validation Loss: 0.026295961812138557\n",
      "Epoch 14, Batch: 298: Training Loss: 0.02713341824710369, Validation Loss: 0.025272449478507042\n",
      "Epoch 14, Batch: 299: Training Loss: 0.023139867931604385, Validation Loss: 0.025557279586791992\n",
      "Epoch 14, Batch: 300: Training Loss: 0.02354004606604576, Validation Loss: 0.024875393137335777\n",
      "Epoch 14, Batch: 301: Training Loss: 0.021252073347568512, Validation Loss: 0.025273222476243973\n",
      "Epoch 14, Batch: 302: Training Loss: 0.021749602630734444, Validation Loss: 0.02608892135322094\n",
      "Epoch 14, Batch: 303: Training Loss: 0.026299815624952316, Validation Loss: 0.025975635275244713\n",
      "Epoch 14, Batch: 304: Training Loss: 0.018731120973825455, Validation Loss: 0.027868181467056274\n",
      "Epoch 14, Batch: 305: Training Loss: 0.019411010667681694, Validation Loss: 0.026312505826354027\n",
      "Epoch 14, Batch: 306: Training Loss: 0.02329491823911667, Validation Loss: 0.02564402110874653\n",
      "Epoch 14, Batch: 307: Training Loss: 0.021946638822555542, Validation Loss: 0.025591662153601646\n",
      "Epoch 14, Batch: 308: Training Loss: 0.02341594733297825, Validation Loss: 0.026555683463811874\n",
      "Epoch 14, Batch: 309: Training Loss: 0.02367989346385002, Validation Loss: 0.02981998398900032\n",
      "Epoch 14, Batch: 310: Training Loss: 0.022814050316810608, Validation Loss: 0.0262661874294281\n",
      "Epoch 14, Batch: 311: Training Loss: 0.020563872531056404, Validation Loss: 0.025797981768846512\n",
      "Epoch 14, Batch: 312: Training Loss: 0.02461901120841503, Validation Loss: 0.025656847283244133\n",
      "Epoch 14, Batch: 313: Training Loss: 0.021198337897658348, Validation Loss: 0.02620663493871689\n",
      "Epoch 14, Batch: 314: Training Loss: 0.023929189890623093, Validation Loss: 0.02501622773706913\n",
      "Epoch 14, Batch: 315: Training Loss: 0.022654540836811066, Validation Loss: 0.028376616537570953\n",
      "Epoch 14, Batch: 316: Training Loss: 0.022049136459827423, Validation Loss: 0.026878029108047485\n",
      "Epoch 14, Batch: 317: Training Loss: 0.022143293172121048, Validation Loss: 0.026606479659676552\n",
      "Epoch 14, Batch: 318: Training Loss: 0.024428071454167366, Validation Loss: 0.027182718738913536\n",
      "Epoch 14, Batch: 319: Training Loss: 0.021648282185196877, Validation Loss: 0.024617863819003105\n",
      "Epoch 14, Batch: 320: Training Loss: 0.025277944281697273, Validation Loss: 0.022587310522794724\n",
      "Epoch 14, Batch: 321: Training Loss: 0.020276550203561783, Validation Loss: 0.024092644453048706\n",
      "Epoch 14, Batch: 322: Training Loss: 0.020483627915382385, Validation Loss: 0.0245524812489748\n",
      "Epoch 14, Batch: 323: Training Loss: 0.023067208006978035, Validation Loss: 0.02277747541666031\n",
      "Epoch 14, Batch: 324: Training Loss: 0.02297182008624077, Validation Loss: 0.021164197474718094\n",
      "Epoch 14, Batch: 325: Training Loss: 0.021990196779370308, Validation Loss: 0.024204125627875328\n",
      "Epoch 14, Batch: 326: Training Loss: 0.03218862786889076, Validation Loss: 0.026870619505643845\n",
      "Epoch 14, Batch: 327: Training Loss: 0.02280840277671814, Validation Loss: 0.023518914356827736\n",
      "Epoch 14, Batch: 328: Training Loss: 0.02680910937488079, Validation Loss: 0.02696223370730877\n",
      "Epoch 14, Batch: 329: Training Loss: 0.025417866185307503, Validation Loss: 0.022620011121034622\n",
      "Epoch 14, Batch: 330: Training Loss: 0.0227830708026886, Validation Loss: 0.02619236707687378\n",
      "Epoch 14, Batch: 331: Training Loss: 0.024691639468073845, Validation Loss: 0.02625766210258007\n",
      "Epoch 14, Batch: 332: Training Loss: 0.022805850952863693, Validation Loss: 0.024899553507566452\n",
      "Epoch 14, Batch: 333: Training Loss: 0.02677576243877411, Validation Loss: 0.0240316279232502\n",
      "Epoch 14, Batch: 334: Training Loss: 0.02207486517727375, Validation Loss: 0.0271515641361475\n",
      "Epoch 14, Batch: 335: Training Loss: 0.021621588617563248, Validation Loss: 0.027675064280629158\n",
      "Epoch 14, Batch: 336: Training Loss: 0.02459125965833664, Validation Loss: 0.025120191276073456\n",
      "Epoch 14, Batch: 337: Training Loss: 0.021064959466457367, Validation Loss: 0.024977954104542732\n",
      "Epoch 14, Batch: 338: Training Loss: 0.022002743557095528, Validation Loss: 0.026461848989129066\n",
      "Epoch 14, Batch: 339: Training Loss: 0.02055446244776249, Validation Loss: 0.02535230852663517\n",
      "Epoch 14, Batch: 340: Training Loss: 0.022198602557182312, Validation Loss: 0.026694374158978462\n",
      "Epoch 14, Batch: 341: Training Loss: 0.02297310344874859, Validation Loss: 0.024329062551259995\n",
      "Epoch 14, Batch: 342: Training Loss: 0.02565576322376728, Validation Loss: 0.0246884785592556\n",
      "Epoch 14, Batch: 343: Training Loss: 0.02264018915593624, Validation Loss: 0.026193497702479362\n",
      "Epoch 14, Batch: 344: Training Loss: 0.02283167466521263, Validation Loss: 0.024322224780917168\n",
      "Epoch 14, Batch: 345: Training Loss: 0.025092756375670433, Validation Loss: 0.026442334055900574\n",
      "Epoch 14, Batch: 346: Training Loss: 0.023503730073571205, Validation Loss: 0.02677915059030056\n",
      "Epoch 14, Batch: 347: Training Loss: 0.023670529946684837, Validation Loss: 0.028076717630028725\n",
      "Epoch 14, Batch: 348: Training Loss: 0.024132370948791504, Validation Loss: 0.025050662457942963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 349: Training Loss: 0.029167858883738518, Validation Loss: 0.024794185534119606\n",
      "Epoch 14, Batch: 350: Training Loss: 0.02370467595756054, Validation Loss: 0.02553413435816765\n",
      "Epoch 14, Batch: 351: Training Loss: 0.029313860461115837, Validation Loss: 0.02335377037525177\n",
      "Epoch 14, Batch: 352: Training Loss: 0.02391720563173294, Validation Loss: 0.02550632692873478\n",
      "Epoch 14, Batch: 353: Training Loss: 0.024761473760008812, Validation Loss: 0.023600773885846138\n",
      "Epoch 14, Batch: 354: Training Loss: 0.026464158669114113, Validation Loss: 0.024523211643099785\n",
      "Epoch 14, Batch: 355: Training Loss: 0.022197162732481956, Validation Loss: 0.025246737524867058\n",
      "Epoch 14, Batch: 356: Training Loss: 0.02476496435701847, Validation Loss: 0.024499619379639626\n",
      "Epoch 14, Batch: 357: Training Loss: 0.02210868149995804, Validation Loss: 0.025395724922418594\n",
      "Epoch 14, Batch: 358: Training Loss: 0.02305055595934391, Validation Loss: 0.025663720443844795\n",
      "Epoch 14, Batch: 359: Training Loss: 0.024016687646508217, Validation Loss: 0.026938853785395622\n",
      "Epoch 14, Batch: 360: Training Loss: 0.028927281498908997, Validation Loss: 0.0270843468606472\n",
      "Epoch 14, Batch: 361: Training Loss: 0.02304086461663246, Validation Loss: 0.028397485613822937\n",
      "Epoch 14, Batch: 362: Training Loss: 0.02612253837287426, Validation Loss: 0.029794618487358093\n",
      "Epoch 14, Batch: 363: Training Loss: 0.02698979526758194, Validation Loss: 0.028091976419091225\n",
      "Epoch 14, Batch: 364: Training Loss: 0.02224726229906082, Validation Loss: 0.025825465098023415\n",
      "Epoch 14, Batch: 365: Training Loss: 0.02293802797794342, Validation Loss: 0.028100434690713882\n",
      "Epoch 14, Batch: 366: Training Loss: 0.025014685466885567, Validation Loss: 0.02760162577033043\n",
      "Epoch 14, Batch: 367: Training Loss: 0.021068833768367767, Validation Loss: 0.02458692342042923\n",
      "Epoch 14, Batch: 368: Training Loss: 0.022618796676397324, Validation Loss: 0.025220882147550583\n",
      "Epoch 14, Batch: 369: Training Loss: 0.02531784400343895, Validation Loss: 0.02522750198841095\n",
      "Epoch 14, Batch: 370: Training Loss: 0.023658661171793938, Validation Loss: 0.024346254765987396\n",
      "Epoch 14, Batch: 371: Training Loss: 0.023532476276159286, Validation Loss: 0.024464953690767288\n",
      "Epoch 14, Batch: 372: Training Loss: 0.022922558709979057, Validation Loss: 0.023188088089227676\n",
      "Epoch 14, Batch: 373: Training Loss: 0.029396165162324905, Validation Loss: 0.024610647931694984\n",
      "Epoch 14, Batch: 374: Training Loss: 0.023822836577892303, Validation Loss: 0.023069821298122406\n",
      "Epoch 14, Batch: 375: Training Loss: 0.024183349683880806, Validation Loss: 0.02252408117055893\n",
      "Epoch 14, Batch: 376: Training Loss: 0.022098658606410027, Validation Loss: 0.022695306688547134\n",
      "Epoch 14, Batch: 377: Training Loss: 0.020812051370739937, Validation Loss: 0.02501831203699112\n",
      "Epoch 14, Batch: 378: Training Loss: 0.020877208560705185, Validation Loss: 0.025399204343557358\n",
      "Epoch 14, Batch: 379: Training Loss: 0.0214411411434412, Validation Loss: 0.02577899768948555\n",
      "Epoch 14, Batch: 380: Training Loss: 0.0245134886354208, Validation Loss: 0.025369547307491302\n",
      "Epoch 14, Batch: 381: Training Loss: 0.026881014928221703, Validation Loss: 0.024343395605683327\n",
      "Epoch 14, Batch: 382: Training Loss: 0.024133648723363876, Validation Loss: 0.023873314261436462\n",
      "Epoch 14, Batch: 383: Training Loss: 0.023732483386993408, Validation Loss: 0.025150040164589882\n",
      "Epoch 14, Batch: 384: Training Loss: 0.023561738431453705, Validation Loss: 0.02369534783065319\n",
      "Epoch 14, Batch: 385: Training Loss: 0.022125305607914925, Validation Loss: 0.023604966700077057\n",
      "Epoch 14, Batch: 386: Training Loss: 0.019523702561855316, Validation Loss: 0.024148842319846153\n",
      "Epoch 14, Batch: 387: Training Loss: 0.02033315785229206, Validation Loss: 0.02661311626434326\n",
      "Epoch 14, Batch: 388: Training Loss: 0.02395360916852951, Validation Loss: 0.02317453920841217\n",
      "Epoch 14, Batch: 389: Training Loss: 0.026145070791244507, Validation Loss: 0.02336815558373928\n",
      "Epoch 14, Batch: 390: Training Loss: 0.022752374410629272, Validation Loss: 0.023905962705612183\n",
      "Epoch 14, Batch: 391: Training Loss: 0.02321532368659973, Validation Loss: 0.027635155245661736\n",
      "Epoch 14, Batch: 392: Training Loss: 0.021786585450172424, Validation Loss: 0.022482093423604965\n",
      "Epoch 14, Batch: 393: Training Loss: 0.022495504468679428, Validation Loss: 0.02461836487054825\n",
      "Epoch 14, Batch: 394: Training Loss: 0.024469943717122078, Validation Loss: 0.02616657316684723\n",
      "Epoch 14, Batch: 395: Training Loss: 0.024124793708324432, Validation Loss: 0.025576066225767136\n",
      "Epoch 14, Batch: 396: Training Loss: 0.02731950953602791, Validation Loss: 0.024015329778194427\n",
      "Epoch 14, Batch: 397: Training Loss: 0.025176052004098892, Validation Loss: 0.026148641481995583\n",
      "Epoch 14, Batch: 398: Training Loss: 0.02114415541291237, Validation Loss: 0.02436440996825695\n",
      "Epoch 14, Batch: 399: Training Loss: 0.02324884757399559, Validation Loss: 0.02342008613049984\n",
      "Epoch 14, Batch: 400: Training Loss: 0.02773696556687355, Validation Loss: 0.0260110255330801\n",
      "Epoch 14, Batch: 401: Training Loss: 0.023244567215442657, Validation Loss: 0.024362288415431976\n",
      "Epoch 14, Batch: 402: Training Loss: 0.02114514820277691, Validation Loss: 0.025779571384191513\n",
      "Epoch 14, Batch: 403: Training Loss: 0.026992669329047203, Validation Loss: 0.028134921565651894\n",
      "Epoch 14, Batch: 404: Training Loss: 0.02410534769296646, Validation Loss: 0.02886875346302986\n",
      "Epoch 14, Batch: 405: Training Loss: 0.02755083329975605, Validation Loss: 0.02800574339926243\n",
      "Epoch 14, Batch: 406: Training Loss: 0.026397449895739555, Validation Loss: 0.025478405877947807\n",
      "Epoch 14, Batch: 407: Training Loss: 0.02875281125307083, Validation Loss: 0.02589588053524494\n",
      "Epoch 14, Batch: 408: Training Loss: 0.02390460856258869, Validation Loss: 0.026084551587700844\n",
      "Epoch 14, Batch: 409: Training Loss: 0.025299709290266037, Validation Loss: 0.025584222748875618\n",
      "Epoch 14, Batch: 410: Training Loss: 0.025453323498368263, Validation Loss: 0.026658672839403152\n",
      "Epoch 14, Batch: 411: Training Loss: 0.02688736468553543, Validation Loss: 0.0263530220836401\n",
      "Epoch 14, Batch: 412: Training Loss: 0.02537003718316555, Validation Loss: 0.024959446862339973\n",
      "Epoch 14, Batch: 413: Training Loss: 0.029602745547890663, Validation Loss: 0.026765378192067146\n",
      "Epoch 14, Batch: 414: Training Loss: 0.02581438608467579, Validation Loss: 0.027080485597252846\n",
      "Epoch 14, Batch: 415: Training Loss: 0.024373507127165794, Validation Loss: 0.025596274062991142\n",
      "Epoch 14, Batch: 416: Training Loss: 0.02249721996486187, Validation Loss: 0.024900270625948906\n",
      "Epoch 14, Batch: 417: Training Loss: 0.02347717247903347, Validation Loss: 0.026924023404717445\n",
      "Epoch 14, Batch: 418: Training Loss: 0.021729344502091408, Validation Loss: 0.024290692061185837\n",
      "Epoch 14, Batch: 419: Training Loss: 0.021734872832894325, Validation Loss: 0.02485688030719757\n",
      "Epoch 14, Batch: 420: Training Loss: 0.022135203704237938, Validation Loss: 0.02523769810795784\n",
      "Epoch 14, Batch: 421: Training Loss: 0.028988439589738846, Validation Loss: 0.024619020521640778\n",
      "Epoch 14, Batch: 422: Training Loss: 0.02787548303604126, Validation Loss: 0.025071371346712112\n",
      "Epoch 14, Batch: 423: Training Loss: 0.029511820524930954, Validation Loss: 0.02285963110625744\n",
      "Epoch 14, Batch: 424: Training Loss: 0.028490761294960976, Validation Loss: 0.024313615635037422\n",
      "Epoch 14, Batch: 425: Training Loss: 0.023671859875321388, Validation Loss: 0.025193767622113228\n",
      "Epoch 14, Batch: 426: Training Loss: 0.024726415053009987, Validation Loss: 0.0233297236263752\n",
      "Epoch 14, Batch: 427: Training Loss: 0.02527027390897274, Validation Loss: 0.027104219421744347\n",
      "Epoch 14, Batch: 428: Training Loss: 0.02123256027698517, Validation Loss: 0.024333344772458076\n",
      "Epoch 14, Batch: 429: Training Loss: 0.022161928936839104, Validation Loss: 0.02471906878054142\n",
      "Epoch 14, Batch: 430: Training Loss: 0.02168302796781063, Validation Loss: 0.023698780685663223\n",
      "Epoch 14, Batch: 431: Training Loss: 0.02687094546854496, Validation Loss: 0.027599340304732323\n",
      "Epoch 14, Batch: 432: Training Loss: 0.023194124922156334, Validation Loss: 0.025230322033166885\n",
      "Epoch 14, Batch: 433: Training Loss: 0.026441538706421852, Validation Loss: 0.02604033052921295\n",
      "Epoch 14, Batch: 434: Training Loss: 0.02299480512738228, Validation Loss: 0.026430776342749596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch: 435: Training Loss: 0.02364041842520237, Validation Loss: 0.024353601038455963\n",
      "Epoch 14, Batch: 436: Training Loss: 0.023996436968445778, Validation Loss: 0.026487097144126892\n",
      "Epoch 14, Batch: 437: Training Loss: 0.021433299407362938, Validation Loss: 0.025059161707758904\n",
      "Epoch 14, Batch: 438: Training Loss: 0.02409246936440468, Validation Loss: 0.02413288876414299\n",
      "Epoch 14, Batch: 439: Training Loss: 0.01958652399480343, Validation Loss: 0.025831574574112892\n",
      "Epoch 14, Batch: 440: Training Loss: 0.028374958783388138, Validation Loss: 0.026556679978966713\n",
      "Epoch 14, Batch: 441: Training Loss: 0.022025523707270622, Validation Loss: 0.026080334559082985\n",
      "Epoch 14, Batch: 442: Training Loss: 0.02615971863269806, Validation Loss: 0.025141654536128044\n",
      "Epoch 14, Batch: 443: Training Loss: 0.020842408761382103, Validation Loss: 0.02328411489725113\n",
      "Epoch 14, Batch: 444: Training Loss: 0.02340352162718773, Validation Loss: 0.025280771777033806\n",
      "Epoch 14, Batch: 445: Training Loss: 0.022717280313372612, Validation Loss: 0.024261824786663055\n",
      "Epoch 14, Batch: 446: Training Loss: 0.02992999367415905, Validation Loss: 0.02696363814175129\n",
      "Epoch 14, Batch: 447: Training Loss: 0.023188387975096703, Validation Loss: 0.025514988228678703\n",
      "Epoch 14, Batch: 448: Training Loss: 0.024641340598464012, Validation Loss: 0.02371266670525074\n",
      "Epoch 14, Batch: 449: Training Loss: 0.025534123182296753, Validation Loss: 0.02316180430352688\n",
      "Epoch 14, Batch: 450: Training Loss: 0.02088811993598938, Validation Loss: 0.02065056562423706\n",
      "Epoch 14, Batch: 451: Training Loss: 0.026673972606658936, Validation Loss: 0.024374214932322502\n",
      "Epoch 14, Batch: 452: Training Loss: 0.02417755126953125, Validation Loss: 0.022513575851917267\n",
      "Epoch 14, Batch: 453: Training Loss: 0.024820730090141296, Validation Loss: 0.022391492500901222\n",
      "Epoch 14, Batch: 454: Training Loss: 0.02451581321656704, Validation Loss: 0.02601233497262001\n",
      "Epoch 14, Batch: 455: Training Loss: 0.024093912914395332, Validation Loss: 0.02375863306224346\n",
      "Epoch 14, Batch: 456: Training Loss: 0.02350817620754242, Validation Loss: 0.025740576907992363\n",
      "Epoch 14, Batch: 457: Training Loss: 0.023023145273327827, Validation Loss: 0.021861527115106583\n",
      "Epoch 14, Batch: 458: Training Loss: 0.022613948211073875, Validation Loss: 0.02436196058988571\n",
      "Epoch 14, Batch: 459: Training Loss: 0.020809341222047806, Validation Loss: 0.02421211078763008\n",
      "Epoch 14, Batch: 460: Training Loss: 0.026513567194342613, Validation Loss: 0.025451939553022385\n",
      "Epoch 14, Batch: 461: Training Loss: 0.023463839665055275, Validation Loss: 0.023720156401395798\n",
      "Epoch 14, Batch: 462: Training Loss: 0.025893190875649452, Validation Loss: 0.023641221225261688\n",
      "Epoch 14, Batch: 463: Training Loss: 0.0254041850566864, Validation Loss: 0.023680610582232475\n",
      "Epoch 14, Batch: 464: Training Loss: 0.02083093300461769, Validation Loss: 0.024990472942590714\n",
      "Epoch 14, Batch: 465: Training Loss: 0.024904711171984673, Validation Loss: 0.025046344846487045\n",
      "Epoch 14, Batch: 466: Training Loss: 0.024480264633893967, Validation Loss: 0.02448130212724209\n",
      "Epoch 14, Batch: 467: Training Loss: 0.029144151136279106, Validation Loss: 0.02699717879295349\n",
      "Epoch 14, Batch: 468: Training Loss: 0.02832038328051567, Validation Loss: 0.02259177714586258\n",
      "Epoch 14, Batch: 469: Training Loss: 0.024659985676407814, Validation Loss: 0.024350078776478767\n",
      "Epoch 14, Batch: 470: Training Loss: 0.02053905837237835, Validation Loss: 0.02435806579887867\n",
      "Epoch 14, Batch: 471: Training Loss: 0.025942929089069366, Validation Loss: 0.025045739486813545\n",
      "Epoch 14, Batch: 472: Training Loss: 0.02113807387650013, Validation Loss: 0.025184815749526024\n",
      "Epoch 14, Batch: 473: Training Loss: 0.02187061496078968, Validation Loss: 0.022561337798833847\n",
      "Epoch 14, Batch: 474: Training Loss: 0.020276030525565147, Validation Loss: 0.024263938888907433\n",
      "Epoch 14, Batch: 475: Training Loss: 0.023536870256066322, Validation Loss: 0.02231048233807087\n",
      "Epoch 14, Batch: 476: Training Loss: 0.023229146376252174, Validation Loss: 0.02187158353626728\n",
      "Epoch 14, Batch: 477: Training Loss: 0.021906109526753426, Validation Loss: 0.02475144900381565\n",
      "Epoch 14, Batch: 478: Training Loss: 0.022982154041528702, Validation Loss: 0.023466400802135468\n",
      "Epoch 14, Batch: 479: Training Loss: 0.023895954713225365, Validation Loss: 0.02335806004703045\n",
      "Epoch 14, Batch: 480: Training Loss: 0.024717628955841064, Validation Loss: 0.023188773542642593\n",
      "Epoch 14, Batch: 481: Training Loss: 0.024661431089043617, Validation Loss: 0.02462853491306305\n",
      "Epoch 14, Batch: 482: Training Loss: 0.026740455999970436, Validation Loss: 0.024331092834472656\n",
      "Epoch 14, Batch: 483: Training Loss: 0.02265015058219433, Validation Loss: 0.024160737171769142\n",
      "Epoch 14, Batch: 484: Training Loss: 0.02362753450870514, Validation Loss: 0.025247754529118538\n",
      "Epoch 14, Batch: 485: Training Loss: 0.024408115074038506, Validation Loss: 0.022297589108347893\n",
      "Epoch 14, Batch: 486: Training Loss: 0.024602534249424934, Validation Loss: 0.025186581537127495\n",
      "Epoch 14, Batch: 487: Training Loss: 0.022231105715036392, Validation Loss: 0.023379633203148842\n",
      "Epoch 14, Batch: 488: Training Loss: 0.023027701303362846, Validation Loss: 0.026227625086903572\n",
      "Epoch 14, Batch: 489: Training Loss: 0.025974640622735023, Validation Loss: 0.023557515814900398\n",
      "Epoch 14, Batch: 490: Training Loss: 0.024028729647397995, Validation Loss: 0.02380954660475254\n",
      "Epoch 14, Batch: 491: Training Loss: 0.018204953521490097, Validation Loss: 0.021048223599791527\n",
      "Epoch 14, Batch: 492: Training Loss: 0.02658010460436344, Validation Loss: 0.024348285049200058\n",
      "Epoch 14, Batch: 493: Training Loss: 0.02249070256948471, Validation Loss: 0.024367786943912506\n",
      "Epoch 14, Batch: 494: Training Loss: 0.02656247466802597, Validation Loss: 0.02523471787571907\n",
      "Epoch 14, Batch: 495: Training Loss: 0.023886805400252342, Validation Loss: 0.023825759068131447\n",
      "Epoch 14, Batch: 496: Training Loss: 0.024178970605134964, Validation Loss: 0.021515924483537674\n",
      "Epoch 14, Batch: 497: Training Loss: 0.0221206434071064, Validation Loss: 0.022034354507923126\n",
      "Epoch 14, Batch: 498: Training Loss: 0.02228424698114395, Validation Loss: 0.0253929253667593\n",
      "Epoch 14, Batch: 499: Training Loss: 0.02202567830681801, Validation Loss: 0.02479255571961403\n",
      "Epoch 15, Batch: 0: Training Loss: 0.022928820922970772, Validation Loss: 0.02360592409968376\n",
      "Epoch 15, Batch: 1: Training Loss: 0.021977778524160385, Validation Loss: 0.023572592064738274\n",
      "Epoch 15, Batch: 2: Training Loss: 0.025826090946793556, Validation Loss: 0.025792144238948822\n",
      "Epoch 15, Batch: 3: Training Loss: 0.02091449312865734, Validation Loss: 0.023408178240060806\n",
      "Epoch 15, Batch: 4: Training Loss: 0.019527548924088478, Validation Loss: 0.02330050803720951\n",
      "Epoch 15, Batch: 5: Training Loss: 0.021778065711259842, Validation Loss: 0.024130122736096382\n",
      "Epoch 15, Batch: 6: Training Loss: 0.022535333409905434, Validation Loss: 0.023735566064715385\n",
      "Epoch 15, Batch: 7: Training Loss: 0.021664932370185852, Validation Loss: 0.022164853289723396\n",
      "Epoch 15, Batch: 8: Training Loss: 0.0235695019364357, Validation Loss: 0.02281768247485161\n",
      "Epoch 15, Batch: 9: Training Loss: 0.023093614727258682, Validation Loss: 0.02309170737862587\n",
      "Epoch 15, Batch: 10: Training Loss: 0.020743222907185555, Validation Loss: 0.02319442667067051\n",
      "Epoch 15, Batch: 11: Training Loss: 0.026224792003631592, Validation Loss: 0.023274725303053856\n",
      "Epoch 15, Batch: 12: Training Loss: 0.023388950154185295, Validation Loss: 0.023175567388534546\n",
      "Epoch 15, Batch: 13: Training Loss: 0.025392327457666397, Validation Loss: 0.02162577211856842\n",
      "Epoch 15, Batch: 14: Training Loss: 0.024332929402589798, Validation Loss: 0.022701280191540718\n",
      "Epoch 15, Batch: 15: Training Loss: 0.026570061221718788, Validation Loss: 0.025149712339043617\n",
      "Epoch 15, Batch: 16: Training Loss: 0.027609342709183693, Validation Loss: 0.025294644758105278\n",
      "Epoch 15, Batch: 17: Training Loss: 0.02348031848669052, Validation Loss: 0.023902861401438713\n",
      "Epoch 15, Batch: 18: Training Loss: 0.02184915915131569, Validation Loss: 0.024194227531552315\n",
      "Epoch 15, Batch: 19: Training Loss: 0.024110201746225357, Validation Loss: 0.02267729490995407\n",
      "Epoch 15, Batch: 20: Training Loss: 0.02500869892537594, Validation Loss: 0.0242949016392231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 21: Training Loss: 0.023674514144659042, Validation Loss: 0.024174965918064117\n",
      "Epoch 15, Batch: 22: Training Loss: 0.02332681603729725, Validation Loss: 0.02369232103228569\n",
      "Epoch 15, Batch: 23: Training Loss: 0.024211052805185318, Validation Loss: 0.02423175983130932\n",
      "Epoch 15, Batch: 24: Training Loss: 0.026996489614248276, Validation Loss: 0.023264579474925995\n",
      "Epoch 15, Batch: 25: Training Loss: 0.022127022966742516, Validation Loss: 0.026519613340497017\n",
      "Epoch 15, Batch: 26: Training Loss: 0.025704892352223396, Validation Loss: 0.02494356967508793\n",
      "Epoch 15, Batch: 27: Training Loss: 0.02576434798538685, Validation Loss: 0.02440510131418705\n",
      "Epoch 15, Batch: 28: Training Loss: 0.026066455990076065, Validation Loss: 0.0242040753364563\n",
      "Epoch 15, Batch: 29: Training Loss: 0.023532258346676826, Validation Loss: 0.026023421436548233\n",
      "Epoch 15, Batch: 30: Training Loss: 0.02130332589149475, Validation Loss: 0.02392595447599888\n",
      "Epoch 15, Batch: 31: Training Loss: 0.025829974561929703, Validation Loss: 0.024577129632234573\n",
      "Epoch 15, Batch: 32: Training Loss: 0.021600816398859024, Validation Loss: 0.02660462073981762\n",
      "Epoch 15, Batch: 33: Training Loss: 0.021871356293559074, Validation Loss: 0.027491407468914986\n",
      "Epoch 15, Batch: 34: Training Loss: 0.0209680013358593, Validation Loss: 0.024600276723504066\n",
      "Epoch 15, Batch: 35: Training Loss: 0.024157600477337837, Validation Loss: 0.02546090818941593\n",
      "Epoch 15, Batch: 36: Training Loss: 0.02474893257021904, Validation Loss: 0.026278140023350716\n",
      "Epoch 15, Batch: 37: Training Loss: 0.02550649456679821, Validation Loss: 0.026277104392647743\n",
      "Epoch 15, Batch: 38: Training Loss: 0.023592974990606308, Validation Loss: 0.028281239792704582\n",
      "Epoch 15, Batch: 39: Training Loss: 0.022889748215675354, Validation Loss: 0.029776424169540405\n",
      "Epoch 15, Batch: 40: Training Loss: 0.024167614057660103, Validation Loss: 0.027455966919660568\n",
      "Epoch 15, Batch: 41: Training Loss: 0.02531685121357441, Validation Loss: 0.02807333506643772\n",
      "Epoch 15, Batch: 42: Training Loss: 0.02172502502799034, Validation Loss: 0.028807803988456726\n",
      "Epoch 15, Batch: 43: Training Loss: 0.021286703646183014, Validation Loss: 0.028141427785158157\n",
      "Epoch 15, Batch: 44: Training Loss: 0.025490162894129753, Validation Loss: 0.026768065989017487\n",
      "Epoch 15, Batch: 45: Training Loss: 0.022020695731043816, Validation Loss: 0.027294671162962914\n",
      "Epoch 15, Batch: 46: Training Loss: 0.024242741987109184, Validation Loss: 0.029195686802268028\n",
      "Epoch 15, Batch: 47: Training Loss: 0.02430778183043003, Validation Loss: 0.027824802324175835\n",
      "Epoch 15, Batch: 48: Training Loss: 0.027473965659737587, Validation Loss: 0.02583719976246357\n",
      "Epoch 15, Batch: 49: Training Loss: 0.024912739172577858, Validation Loss: 0.02650856226682663\n",
      "Epoch 15, Batch: 50: Training Loss: 0.023721706122159958, Validation Loss: 0.025634916499257088\n",
      "Epoch 15, Batch: 51: Training Loss: 0.025134960189461708, Validation Loss: 0.025649504736065865\n",
      "Epoch 15, Batch: 52: Training Loss: 0.023047596216201782, Validation Loss: 0.024266695603728294\n",
      "Epoch 15, Batch: 53: Training Loss: 0.02177317440509796, Validation Loss: 0.02430291287600994\n",
      "Epoch 15, Batch: 54: Training Loss: 0.021353155374526978, Validation Loss: 0.022526955232024193\n",
      "Epoch 15, Batch: 55: Training Loss: 0.02206738106906414, Validation Loss: 0.023148126900196075\n",
      "Epoch 15, Batch: 56: Training Loss: 0.0263628289103508, Validation Loss: 0.023714939132332802\n",
      "Epoch 15, Batch: 57: Training Loss: 0.02060825191438198, Validation Loss: 0.02360861748456955\n",
      "Epoch 15, Batch: 58: Training Loss: 0.026355767622590065, Validation Loss: 0.025582199916243553\n",
      "Epoch 15, Batch: 59: Training Loss: 0.023621253669261932, Validation Loss: 0.024857351556420326\n",
      "Epoch 15, Batch: 60: Training Loss: 0.02404567040503025, Validation Loss: 0.02549475058913231\n",
      "Epoch 15, Batch: 61: Training Loss: 0.023004978895187378, Validation Loss: 0.02435946837067604\n",
      "Epoch 15, Batch: 62: Training Loss: 0.024807477369904518, Validation Loss: 0.024479763582348824\n",
      "Epoch 15, Batch: 63: Training Loss: 0.02263025753200054, Validation Loss: 0.024420611560344696\n",
      "Epoch 15, Batch: 64: Training Loss: 0.024487780407071114, Validation Loss: 0.022237030789256096\n",
      "Epoch 15, Batch: 65: Training Loss: 0.02821817249059677, Validation Loss: 0.02289803884923458\n",
      "Epoch 15, Batch: 66: Training Loss: 0.020870715379714966, Validation Loss: 0.025327486917376518\n",
      "Epoch 15, Batch: 67: Training Loss: 0.021936578676104546, Validation Loss: 0.0250822976231575\n",
      "Epoch 15, Batch: 68: Training Loss: 0.024082064628601074, Validation Loss: 0.023160507902503014\n",
      "Epoch 15, Batch: 69: Training Loss: 0.023658182471990585, Validation Loss: 0.02288287691771984\n",
      "Epoch 15, Batch: 70: Training Loss: 0.02816018834710121, Validation Loss: 0.02469625137746334\n",
      "Epoch 15, Batch: 71: Training Loss: 0.02164851687848568, Validation Loss: 0.024147436022758484\n",
      "Epoch 15, Batch: 72: Training Loss: 0.02411646395921707, Validation Loss: 0.02522708661854267\n",
      "Epoch 15, Batch: 73: Training Loss: 0.023295611143112183, Validation Loss: 0.02319216914474964\n",
      "Epoch 15, Batch: 74: Training Loss: 0.02136155590415001, Validation Loss: 0.025717422366142273\n",
      "Epoch 15, Batch: 75: Training Loss: 0.020936479791998863, Validation Loss: 0.023374376818537712\n",
      "Epoch 15, Batch: 76: Training Loss: 0.023458207026124, Validation Loss: 0.0255295280367136\n",
      "Epoch 15, Batch: 77: Training Loss: 0.026487793773412704, Validation Loss: 0.023619070649147034\n",
      "Epoch 15, Batch: 78: Training Loss: 0.02777024731040001, Validation Loss: 0.02439950220286846\n",
      "Epoch 15, Batch: 79: Training Loss: 0.02373705804347992, Validation Loss: 0.025954581797122955\n",
      "Epoch 15, Batch: 80: Training Loss: 0.021724041551351547, Validation Loss: 0.026239534839987755\n",
      "Epoch 15, Batch: 81: Training Loss: 0.025014603510499, Validation Loss: 0.027238888666033745\n",
      "Epoch 15, Batch: 82: Training Loss: 0.02586773969233036, Validation Loss: 0.02518230304121971\n",
      "Epoch 15, Batch: 83: Training Loss: 0.020678430795669556, Validation Loss: 0.026052385568618774\n",
      "Epoch 15, Batch: 84: Training Loss: 0.02345314621925354, Validation Loss: 0.023032939061522484\n",
      "Epoch 15, Batch: 85: Training Loss: 0.01995929889380932, Validation Loss: 0.02295369654893875\n",
      "Epoch 15, Batch: 86: Training Loss: 0.02295316942036152, Validation Loss: 0.024449465796351433\n",
      "Epoch 15, Batch: 87: Training Loss: 0.025832438841462135, Validation Loss: 0.025282762944698334\n",
      "Epoch 15, Batch: 88: Training Loss: 0.025310587137937546, Validation Loss: 0.025060275569558144\n",
      "Epoch 15, Batch: 89: Training Loss: 0.02614947035908699, Validation Loss: 0.025615273043513298\n",
      "Epoch 15, Batch: 90: Training Loss: 0.023688986897468567, Validation Loss: 0.02711247280240059\n",
      "Epoch 15, Batch: 91: Training Loss: 0.028574740514159203, Validation Loss: 0.02637406997382641\n",
      "Epoch 15, Batch: 92: Training Loss: 0.026432054117321968, Validation Loss: 0.02762918919324875\n",
      "Epoch 15, Batch: 93: Training Loss: 0.024775126948952675, Validation Loss: 0.026455704122781754\n",
      "Epoch 15, Batch: 94: Training Loss: 0.0251374039798975, Validation Loss: 0.02528437040746212\n",
      "Epoch 15, Batch: 95: Training Loss: 0.021880345419049263, Validation Loss: 0.02604059875011444\n",
      "Epoch 15, Batch: 96: Training Loss: 0.021814562380313873, Validation Loss: 0.024757755920290947\n",
      "Epoch 15, Batch: 97: Training Loss: 0.02448510378599167, Validation Loss: 0.026684628799557686\n",
      "Epoch 15, Batch: 98: Training Loss: 0.024918517097830772, Validation Loss: 0.026087641716003418\n",
      "Epoch 15, Batch: 99: Training Loss: 0.024397432804107666, Validation Loss: 0.023848917335271835\n",
      "Epoch 15, Batch: 100: Training Loss: 0.026280350983142853, Validation Loss: 0.02749243937432766\n",
      "Epoch 15, Batch: 101: Training Loss: 0.021447794511914253, Validation Loss: 0.02618008852005005\n",
      "Epoch 15, Batch: 102: Training Loss: 0.029740536585450172, Validation Loss: 0.024424642324447632\n",
      "Epoch 15, Batch: 103: Training Loss: 0.025874340906739235, Validation Loss: 0.026104072108864784\n",
      "Epoch 15, Batch: 104: Training Loss: 0.023702777922153473, Validation Loss: 0.03067406639456749\n",
      "Epoch 15, Batch: 105: Training Loss: 0.021320942789316177, Validation Loss: 0.02576388418674469\n",
      "Epoch 15, Batch: 106: Training Loss: 0.02556469291448593, Validation Loss: 0.023771017789840698\n",
      "Epoch 15, Batch: 107: Training Loss: 0.025797070935368538, Validation Loss: 0.02561098523437977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 108: Training Loss: 0.022030990570783615, Validation Loss: 0.02598126046359539\n",
      "Epoch 15, Batch: 109: Training Loss: 0.024314960464835167, Validation Loss: 0.025600571185350418\n",
      "Epoch 15, Batch: 110: Training Loss: 0.028868576511740685, Validation Loss: 0.026081981137394905\n",
      "Epoch 15, Batch: 111: Training Loss: 0.023324329406023026, Validation Loss: 0.02728402428328991\n",
      "Epoch 15, Batch: 112: Training Loss: 0.022804686799645424, Validation Loss: 0.023600377142429352\n",
      "Epoch 15, Batch: 113: Training Loss: 0.022973211482167244, Validation Loss: 0.025009164586663246\n",
      "Epoch 15, Batch: 114: Training Loss: 0.02242240123450756, Validation Loss: 0.024420054629445076\n",
      "Epoch 15, Batch: 115: Training Loss: 0.025520728901028633, Validation Loss: 0.022127198055386543\n",
      "Epoch 15, Batch: 116: Training Loss: 0.022562429308891296, Validation Loss: 0.025678128004074097\n",
      "Epoch 15, Batch: 117: Training Loss: 0.026448700577020645, Validation Loss: 0.025638602674007416\n",
      "Epoch 15, Batch: 118: Training Loss: 0.023897193372249603, Validation Loss: 0.023081790655851364\n",
      "Epoch 15, Batch: 119: Training Loss: 0.0222465880215168, Validation Loss: 0.024736866354942322\n",
      "Epoch 15, Batch: 120: Training Loss: 0.02216949500143528, Validation Loss: 0.02336457185447216\n",
      "Epoch 15, Batch: 121: Training Loss: 0.024054445326328278, Validation Loss: 0.024937309324741364\n",
      "Epoch 15, Batch: 122: Training Loss: 0.023455440998077393, Validation Loss: 0.025200581178069115\n",
      "Epoch 15, Batch: 123: Training Loss: 0.022389190271496773, Validation Loss: 0.02428375370800495\n",
      "Epoch 15, Batch: 124: Training Loss: 0.025174947455525398, Validation Loss: 0.02860907092690468\n",
      "Epoch 15, Batch: 125: Training Loss: 0.020355097949504852, Validation Loss: 0.026266520842909813\n",
      "Epoch 15, Batch: 126: Training Loss: 0.02233114466071129, Validation Loss: 0.026114171370863914\n",
      "Epoch 15, Batch: 127: Training Loss: 0.02215271070599556, Validation Loss: 0.024838564917445183\n",
      "Epoch 15, Batch: 128: Training Loss: 0.02165740728378296, Validation Loss: 0.02602279558777809\n",
      "Epoch 15, Batch: 129: Training Loss: 0.023440798744559288, Validation Loss: 0.023840561509132385\n",
      "Epoch 15, Batch: 130: Training Loss: 0.02448338083922863, Validation Loss: 0.027717161923646927\n",
      "Epoch 15, Batch: 131: Training Loss: 0.025654062628746033, Validation Loss: 0.02556067891418934\n",
      "Epoch 15, Batch: 132: Training Loss: 0.026173079386353493, Validation Loss: 0.029358239844441414\n",
      "Epoch 15, Batch: 133: Training Loss: 0.02528412453830242, Validation Loss: 0.02541772648692131\n",
      "Epoch 15, Batch: 134: Training Loss: 0.02360842004418373, Validation Loss: 0.02501862868666649\n",
      "Epoch 15, Batch: 135: Training Loss: 0.021982446312904358, Validation Loss: 0.02620207890868187\n",
      "Epoch 15, Batch: 136: Training Loss: 0.026297997683286667, Validation Loss: 0.0245567224919796\n",
      "Epoch 15, Batch: 137: Training Loss: 0.023465987294912338, Validation Loss: 0.02606745809316635\n",
      "Epoch 15, Batch: 138: Training Loss: 0.021610157564282417, Validation Loss: 0.025936946272850037\n",
      "Epoch 15, Batch: 139: Training Loss: 0.023520538583397865, Validation Loss: 0.026023879647254944\n",
      "Epoch 15, Batch: 140: Training Loss: 0.025789152830839157, Validation Loss: 0.024403948336839676\n",
      "Epoch 15, Batch: 141: Training Loss: 0.025338996201753616, Validation Loss: 0.023331528529524803\n",
      "Epoch 15, Batch: 142: Training Loss: 0.02078103832900524, Validation Loss: 0.024653689935803413\n",
      "Epoch 15, Batch: 143: Training Loss: 0.024895163252949715, Validation Loss: 0.026892460882663727\n",
      "Epoch 15, Batch: 144: Training Loss: 0.022703707218170166, Validation Loss: 0.024717392399907112\n",
      "Epoch 15, Batch: 145: Training Loss: 0.02472410909831524, Validation Loss: 0.026390762999653816\n",
      "Epoch 15, Batch: 146: Training Loss: 0.021785037592053413, Validation Loss: 0.025445032864809036\n",
      "Epoch 15, Batch: 147: Training Loss: 0.02298557385802269, Validation Loss: 0.025754796341061592\n",
      "Epoch 15, Batch: 148: Training Loss: 0.024543557316064835, Validation Loss: 0.022946884855628014\n",
      "Epoch 15, Batch: 149: Training Loss: 0.020561005920171738, Validation Loss: 0.023142755031585693\n",
      "Epoch 15, Batch: 150: Training Loss: 0.023390494287014008, Validation Loss: 0.023210933431982994\n",
      "Epoch 15, Batch: 151: Training Loss: 0.025632623583078384, Validation Loss: 0.02256234921514988\n",
      "Epoch 15, Batch: 152: Training Loss: 0.024454843252897263, Validation Loss: 0.024509772658348083\n",
      "Epoch 15, Batch: 153: Training Loss: 0.021916769444942474, Validation Loss: 0.023627065122127533\n",
      "Epoch 15, Batch: 154: Training Loss: 0.022069603204727173, Validation Loss: 0.022593095898628235\n",
      "Epoch 15, Batch: 155: Training Loss: 0.029589321464300156, Validation Loss: 0.023670541122555733\n",
      "Epoch 15, Batch: 156: Training Loss: 0.02631959319114685, Validation Loss: 0.021981973201036453\n",
      "Epoch 15, Batch: 157: Training Loss: 0.020956946536898613, Validation Loss: 0.02527143992483616\n",
      "Epoch 15, Batch: 158: Training Loss: 0.024660639464855194, Validation Loss: 0.02378174476325512\n",
      "Epoch 15, Batch: 159: Training Loss: 0.021399542689323425, Validation Loss: 0.023753048852086067\n",
      "Epoch 15, Batch: 160: Training Loss: 0.024307986721396446, Validation Loss: 0.024087252095341682\n",
      "Epoch 15, Batch: 161: Training Loss: 0.02606034092605114, Validation Loss: 0.02409556694328785\n",
      "Epoch 15, Batch: 162: Training Loss: 0.028642406687140465, Validation Loss: 0.02418545074760914\n",
      "Epoch 15, Batch: 163: Training Loss: 0.02486112155020237, Validation Loss: 0.025124503299593925\n",
      "Epoch 15, Batch: 164: Training Loss: 0.02647106535732746, Validation Loss: 0.024932678788900375\n",
      "Epoch 15, Batch: 165: Training Loss: 0.021052736788988113, Validation Loss: 0.024499187245965004\n",
      "Epoch 15, Batch: 166: Training Loss: 0.023641007021069527, Validation Loss: 0.02634831890463829\n",
      "Epoch 15, Batch: 167: Training Loss: 0.02500322461128235, Validation Loss: 0.026835590600967407\n",
      "Epoch 15, Batch: 168: Training Loss: 0.02374781109392643, Validation Loss: 0.02689843624830246\n",
      "Epoch 15, Batch: 169: Training Loss: 0.030954474583268166, Validation Loss: 0.027060290798544884\n",
      "Epoch 15, Batch: 170: Training Loss: 0.0221150703728199, Validation Loss: 0.02437283843755722\n",
      "Epoch 15, Batch: 171: Training Loss: 0.02264649048447609, Validation Loss: 0.024457549676299095\n",
      "Epoch 15, Batch: 172: Training Loss: 0.026363430544734, Validation Loss: 0.025957440957427025\n",
      "Epoch 15, Batch: 173: Training Loss: 0.02082294598221779, Validation Loss: 0.024054192006587982\n",
      "Epoch 15, Batch: 174: Training Loss: 0.02449405938386917, Validation Loss: 0.025783514603972435\n",
      "Epoch 15, Batch: 175: Training Loss: 0.023228123784065247, Validation Loss: 0.02180069498717785\n",
      "Epoch 15, Batch: 176: Training Loss: 0.023722216486930847, Validation Loss: 0.024579336866736412\n",
      "Epoch 15, Batch: 177: Training Loss: 0.020020538941025734, Validation Loss: 0.023427700623869896\n",
      "Epoch 15, Batch: 178: Training Loss: 0.02455110102891922, Validation Loss: 0.022052578628063202\n",
      "Epoch 15, Batch: 179: Training Loss: 0.020787321031093597, Validation Loss: 0.02529596909880638\n",
      "Epoch 15, Batch: 180: Training Loss: 0.02362879365682602, Validation Loss: 0.02284259907901287\n",
      "Epoch 15, Batch: 181: Training Loss: 0.023591788485646248, Validation Loss: 0.022437600418925285\n",
      "Epoch 15, Batch: 182: Training Loss: 0.025952253490686417, Validation Loss: 0.026126064360141754\n",
      "Epoch 15, Batch: 183: Training Loss: 0.025676660239696503, Validation Loss: 0.023531829938292503\n",
      "Epoch 15, Batch: 184: Training Loss: 0.02277110144495964, Validation Loss: 0.02279382199048996\n",
      "Epoch 15, Batch: 185: Training Loss: 0.024963364005088806, Validation Loss: 0.022488344460725784\n",
      "Epoch 15, Batch: 186: Training Loss: 0.02511126548051834, Validation Loss: 0.02348443679511547\n",
      "Epoch 15, Batch: 187: Training Loss: 0.02231304720044136, Validation Loss: 0.02308298647403717\n",
      "Epoch 15, Batch: 188: Training Loss: 0.02702057734131813, Validation Loss: 0.0210280679166317\n",
      "Epoch 15, Batch: 189: Training Loss: 0.02095114253461361, Validation Loss: 0.023277394473552704\n",
      "Epoch 15, Batch: 190: Training Loss: 0.025666113942861557, Validation Loss: 0.02329971455037594\n",
      "Epoch 15, Batch: 191: Training Loss: 0.026850147172808647, Validation Loss: 0.021794654428958893\n",
      "Epoch 15, Batch: 192: Training Loss: 0.026186006143689156, Validation Loss: 0.020887456834316254\n",
      "Epoch 15, Batch: 193: Training Loss: 0.02347906492650509, Validation Loss: 0.024838624522089958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 194: Training Loss: 0.024766772985458374, Validation Loss: 0.023601753637194633\n",
      "Epoch 15, Batch: 195: Training Loss: 0.023568185046315193, Validation Loss: 0.024031205102801323\n",
      "Epoch 15, Batch: 196: Training Loss: 0.02339610643684864, Validation Loss: 0.025098826736211777\n",
      "Epoch 15, Batch: 197: Training Loss: 0.02135966345667839, Validation Loss: 0.024359794333577156\n",
      "Epoch 15, Batch: 198: Training Loss: 0.020524762570858, Validation Loss: 0.02286725491285324\n",
      "Epoch 15, Batch: 199: Training Loss: 0.022984249517321587, Validation Loss: 0.028072532266378403\n",
      "Epoch 15, Batch: 200: Training Loss: 0.019842687994241714, Validation Loss: 0.023249413818120956\n",
      "Epoch 15, Batch: 201: Training Loss: 0.024695074185729027, Validation Loss: 0.02600417658686638\n",
      "Epoch 15, Batch: 202: Training Loss: 0.024877656251192093, Validation Loss: 0.025502992793917656\n",
      "Epoch 15, Batch: 203: Training Loss: 0.023471649736166, Validation Loss: 0.025384437292814255\n",
      "Epoch 15, Batch: 204: Training Loss: 0.02343176305294037, Validation Loss: 0.025790750980377197\n",
      "Epoch 15, Batch: 205: Training Loss: 0.030712824314832687, Validation Loss: 0.029139380902051926\n",
      "Epoch 15, Batch: 206: Training Loss: 0.02195884846150875, Validation Loss: 0.026142455637454987\n",
      "Epoch 15, Batch: 207: Training Loss: 0.026431888341903687, Validation Loss: 0.02711724303662777\n",
      "Epoch 15, Batch: 208: Training Loss: 0.027294550091028214, Validation Loss: 0.026101788505911827\n",
      "Epoch 15, Batch: 209: Training Loss: 0.028463050723075867, Validation Loss: 0.025796670466661453\n",
      "Epoch 15, Batch: 210: Training Loss: 0.02961823344230652, Validation Loss: 0.025336449965834618\n",
      "Epoch 15, Batch: 211: Training Loss: 0.023634742945432663, Validation Loss: 0.026347937062382698\n",
      "Epoch 15, Batch: 212: Training Loss: 0.023536987602710724, Validation Loss: 0.025802841410040855\n",
      "Epoch 15, Batch: 213: Training Loss: 0.026680758222937584, Validation Loss: 0.028175685554742813\n",
      "Epoch 15, Batch: 214: Training Loss: 0.02585548907518387, Validation Loss: 0.02883659303188324\n",
      "Epoch 15, Batch: 215: Training Loss: 0.027507023885846138, Validation Loss: 0.02631596475839615\n",
      "Epoch 15, Batch: 216: Training Loss: 0.025085387751460075, Validation Loss: 0.025634609162807465\n",
      "Epoch 15, Batch: 217: Training Loss: 0.02764209918677807, Validation Loss: 0.025881720706820488\n",
      "Epoch 15, Batch: 218: Training Loss: 0.023869695141911507, Validation Loss: 0.026100648567080498\n",
      "Epoch 15, Batch: 219: Training Loss: 0.02287914603948593, Validation Loss: 0.027253534644842148\n",
      "Epoch 15, Batch: 220: Training Loss: 0.02675400674343109, Validation Loss: 0.02514922060072422\n",
      "Epoch 15, Batch: 221: Training Loss: 0.025785576552152634, Validation Loss: 0.024501046165823936\n",
      "Epoch 15, Batch: 222: Training Loss: 0.02417452447116375, Validation Loss: 0.02707214653491974\n",
      "Epoch 15, Batch: 223: Training Loss: 0.020224643871188164, Validation Loss: 0.027452688664197922\n",
      "Epoch 15, Batch: 224: Training Loss: 0.022371739149093628, Validation Loss: 0.02828262746334076\n",
      "Epoch 15, Batch: 225: Training Loss: 0.02511635795235634, Validation Loss: 0.027039527893066406\n",
      "Epoch 15, Batch: 226: Training Loss: 0.02329600788652897, Validation Loss: 0.02678348869085312\n",
      "Epoch 15, Batch: 227: Training Loss: 0.022796563804149628, Validation Loss: 0.028907500207424164\n",
      "Epoch 15, Batch: 228: Training Loss: 0.029073866084218025, Validation Loss: 0.02951190620660782\n",
      "Epoch 15, Batch: 229: Training Loss: 0.030645985156297684, Validation Loss: 0.029434803873300552\n",
      "Epoch 15, Batch: 230: Training Loss: 0.025510158389806747, Validation Loss: 0.028850801289081573\n",
      "Epoch 15, Batch: 231: Training Loss: 0.02887672185897827, Validation Loss: 0.02712370827794075\n",
      "Epoch 15, Batch: 232: Training Loss: 0.02768108993768692, Validation Loss: 0.02763468027114868\n",
      "Epoch 15, Batch: 233: Training Loss: 0.01983305998146534, Validation Loss: 0.0268761795014143\n",
      "Epoch 15, Batch: 234: Training Loss: 0.02793039008975029, Validation Loss: 0.028146514669060707\n",
      "Epoch 15, Batch: 235: Training Loss: 0.026601722463965416, Validation Loss: 0.025682857260107994\n",
      "Epoch 15, Batch: 236: Training Loss: 0.02937663532793522, Validation Loss: 0.02576247975230217\n",
      "Epoch 15, Batch: 237: Training Loss: 0.025698792189359665, Validation Loss: 0.026440517976880074\n",
      "Epoch 15, Batch: 238: Training Loss: 0.026898009702563286, Validation Loss: 0.024334615096449852\n",
      "Epoch 15, Batch: 239: Training Loss: 0.022221675142645836, Validation Loss: 0.025114819407463074\n",
      "Epoch 15, Batch: 240: Training Loss: 0.02530723065137863, Validation Loss: 0.02631053514778614\n",
      "Epoch 15, Batch: 241: Training Loss: 0.022036489099264145, Validation Loss: 0.025750065222382545\n",
      "Epoch 15, Batch: 242: Training Loss: 0.024293173104524612, Validation Loss: 0.024438736960291862\n",
      "Epoch 15, Batch: 243: Training Loss: 0.025766462087631226, Validation Loss: 0.027306679636240005\n",
      "Epoch 15, Batch: 244: Training Loss: 0.02289498969912529, Validation Loss: 0.025197455659508705\n",
      "Epoch 15, Batch: 245: Training Loss: 0.02729632705450058, Validation Loss: 0.025936195626854897\n",
      "Epoch 15, Batch: 246: Training Loss: 0.023974498733878136, Validation Loss: 0.025777550414204597\n",
      "Epoch 15, Batch: 247: Training Loss: 0.024836495518684387, Validation Loss: 0.02585596777498722\n",
      "Epoch 15, Batch: 248: Training Loss: 0.022968148812651634, Validation Loss: 0.02598414197564125\n",
      "Epoch 15, Batch: 249: Training Loss: 0.025593210011720657, Validation Loss: 0.030345400795340538\n",
      "Epoch 15, Batch: 250: Training Loss: 0.02766450121998787, Validation Loss: 0.028329899534583092\n",
      "Epoch 15, Batch: 251: Training Loss: 0.021837417036294937, Validation Loss: 0.028081439435482025\n",
      "Epoch 15, Batch: 252: Training Loss: 0.02599363960325718, Validation Loss: 0.028353899717330933\n",
      "Epoch 15, Batch: 253: Training Loss: 0.03022165782749653, Validation Loss: 0.024443702772259712\n",
      "Epoch 15, Batch: 254: Training Loss: 0.02537897601723671, Validation Loss: 0.029453715309500694\n",
      "Epoch 15, Batch: 255: Training Loss: 0.026053985580801964, Validation Loss: 0.024009741842746735\n",
      "Epoch 15, Batch: 256: Training Loss: 0.024565527215600014, Validation Loss: 0.025404641404747963\n",
      "Epoch 15, Batch: 257: Training Loss: 0.022909386083483696, Validation Loss: 0.023736365139484406\n",
      "Epoch 15, Batch: 258: Training Loss: 0.027141284197568893, Validation Loss: 0.026477452367544174\n",
      "Epoch 15, Batch: 259: Training Loss: 0.025566043332219124, Validation Loss: 0.02478472702205181\n",
      "Epoch 15, Batch: 260: Training Loss: 0.030060574412345886, Validation Loss: 0.025113562121987343\n",
      "Epoch 15, Batch: 261: Training Loss: 0.024449501186609268, Validation Loss: 0.02622542716562748\n",
      "Epoch 15, Batch: 262: Training Loss: 0.026518147438764572, Validation Loss: 0.028788186609745026\n",
      "Epoch 15, Batch: 263: Training Loss: 0.025280822068452835, Validation Loss: 0.027101896703243256\n",
      "Epoch 15, Batch: 264: Training Loss: 0.02170015312731266, Validation Loss: 0.028501391410827637\n",
      "Epoch 15, Batch: 265: Training Loss: 0.027207594364881516, Validation Loss: 0.02667970396578312\n",
      "Epoch 15, Batch: 266: Training Loss: 0.019955454394221306, Validation Loss: 0.026529556140303612\n",
      "Epoch 15, Batch: 267: Training Loss: 0.023702871054410934, Validation Loss: 0.02576081082224846\n",
      "Epoch 15, Batch: 268: Training Loss: 0.02453221008181572, Validation Loss: 0.026806171983480453\n",
      "Epoch 15, Batch: 269: Training Loss: 0.02163056842982769, Validation Loss: 0.02709907293319702\n",
      "Epoch 15, Batch: 270: Training Loss: 0.025231320410966873, Validation Loss: 0.0265614315867424\n",
      "Epoch 15, Batch: 271: Training Loss: 0.02662218175828457, Validation Loss: 0.025892388075590134\n",
      "Epoch 15, Batch: 272: Training Loss: 0.02477961778640747, Validation Loss: 0.02522202953696251\n",
      "Epoch 15, Batch: 273: Training Loss: 0.026370445266366005, Validation Loss: 0.02633691392838955\n",
      "Epoch 15, Batch: 274: Training Loss: 0.0222729854285717, Validation Loss: 0.02452763356268406\n",
      "Epoch 15, Batch: 275: Training Loss: 0.02354448102414608, Validation Loss: 0.023818310350179672\n",
      "Epoch 15, Batch: 276: Training Loss: 0.024225154891610146, Validation Loss: 0.02465236932039261\n",
      "Epoch 15, Batch: 277: Training Loss: 0.02272734045982361, Validation Loss: 0.023409975692629814\n",
      "Epoch 15, Batch: 278: Training Loss: 0.024638766422867775, Validation Loss: 0.022601913660764694\n",
      "Epoch 15, Batch: 279: Training Loss: 0.026131875813007355, Validation Loss: 0.021549949422478676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 280: Training Loss: 0.01873967982828617, Validation Loss: 0.02351371757686138\n",
      "Epoch 15, Batch: 281: Training Loss: 0.023291638121008873, Validation Loss: 0.024064702913165092\n",
      "Epoch 15, Batch: 282: Training Loss: 0.022846020758152008, Validation Loss: 0.026932500302791595\n",
      "Epoch 15, Batch: 283: Training Loss: 0.024707648903131485, Validation Loss: 0.025456124916672707\n",
      "Epoch 15, Batch: 284: Training Loss: 0.02446628548204899, Validation Loss: 0.024854136630892754\n",
      "Epoch 15, Batch: 285: Training Loss: 0.02351328358054161, Validation Loss: 0.022969208657741547\n",
      "Epoch 15, Batch: 286: Training Loss: 0.02674710936844349, Validation Loss: 0.023981796577572823\n",
      "Epoch 15, Batch: 287: Training Loss: 0.022786524146795273, Validation Loss: 0.024982525035738945\n",
      "Epoch 15, Batch: 288: Training Loss: 0.025583259761333466, Validation Loss: 0.025210049003362656\n",
      "Epoch 15, Batch: 289: Training Loss: 0.025375688448548317, Validation Loss: 0.023240799084305763\n",
      "Epoch 15, Batch: 290: Training Loss: 0.023788174614310265, Validation Loss: 0.022633137181401253\n",
      "Epoch 15, Batch: 291: Training Loss: 0.02718045748770237, Validation Loss: 0.024038851261138916\n",
      "Epoch 15, Batch: 292: Training Loss: 0.02837073989212513, Validation Loss: 0.023050040006637573\n",
      "Epoch 15, Batch: 293: Training Loss: 0.026579339057207108, Validation Loss: 0.023025838658213615\n",
      "Epoch 15, Batch: 294: Training Loss: 0.02700413577258587, Validation Loss: 0.02131885662674904\n",
      "Epoch 15, Batch: 295: Training Loss: 0.02669561095535755, Validation Loss: 0.021078845486044884\n",
      "Epoch 15, Batch: 296: Training Loss: 0.02443557418882847, Validation Loss: 0.022073280066251755\n",
      "Epoch 15, Batch: 297: Training Loss: 0.024245329201221466, Validation Loss: 0.022875990718603134\n",
      "Epoch 15, Batch: 298: Training Loss: 0.020484410226345062, Validation Loss: 0.023988645523786545\n",
      "Epoch 15, Batch: 299: Training Loss: 0.020653871819376945, Validation Loss: 0.022522399201989174\n",
      "Epoch 15, Batch: 300: Training Loss: 0.025584541261196136, Validation Loss: 0.02227824553847313\n",
      "Epoch 15, Batch: 301: Training Loss: 0.0213022418320179, Validation Loss: 0.023261528462171555\n",
      "Epoch 15, Batch: 302: Training Loss: 0.020570458844304085, Validation Loss: 0.023882688954472542\n",
      "Epoch 15, Batch: 303: Training Loss: 0.025688190013170242, Validation Loss: 0.02312510460615158\n",
      "Epoch 15, Batch: 304: Training Loss: 0.02669193036854267, Validation Loss: 0.02343071438372135\n",
      "Epoch 15, Batch: 305: Training Loss: 0.019953051581978798, Validation Loss: 0.024059446528553963\n",
      "Epoch 15, Batch: 306: Training Loss: 0.02131422981619835, Validation Loss: 0.023984916508197784\n",
      "Epoch 15, Batch: 307: Training Loss: 0.022738277912139893, Validation Loss: 0.02216940000653267\n",
      "Epoch 15, Batch: 308: Training Loss: 0.025730324909090996, Validation Loss: 0.024936195462942123\n",
      "Epoch 15, Batch: 309: Training Loss: 0.01983896642923355, Validation Loss: 0.02461112104356289\n",
      "Epoch 15, Batch: 310: Training Loss: 0.0286672655493021, Validation Loss: 0.024081021547317505\n",
      "Epoch 15, Batch: 311: Training Loss: 0.019659126177430153, Validation Loss: 0.02385495975613594\n",
      "Epoch 15, Batch: 312: Training Loss: 0.024301813915371895, Validation Loss: 0.022565564140677452\n",
      "Epoch 15, Batch: 313: Training Loss: 0.022711481899023056, Validation Loss: 0.022441990673542023\n",
      "Epoch 15, Batch: 314: Training Loss: 0.023069366812705994, Validation Loss: 0.024133598431944847\n",
      "Epoch 15, Batch: 315: Training Loss: 0.02292950265109539, Validation Loss: 0.022476600483059883\n",
      "Epoch 15, Batch: 316: Training Loss: 0.020950905978679657, Validation Loss: 0.023130344226956367\n",
      "Epoch 15, Batch: 317: Training Loss: 0.02252386510372162, Validation Loss: 0.021518481895327568\n",
      "Epoch 15, Batch: 318: Training Loss: 0.022798296064138412, Validation Loss: 0.023336470127105713\n",
      "Epoch 15, Batch: 319: Training Loss: 0.02350740134716034, Validation Loss: 0.02292868308722973\n",
      "Epoch 15, Batch: 320: Training Loss: 0.027056589722633362, Validation Loss: 0.023921186104416847\n",
      "Epoch 15, Batch: 321: Training Loss: 0.02343583106994629, Validation Loss: 0.024640807881951332\n",
      "Epoch 15, Batch: 322: Training Loss: 0.02364891953766346, Validation Loss: 0.023763488978147507\n",
      "Epoch 15, Batch: 323: Training Loss: 0.02293984591960907, Validation Loss: 0.02650011144578457\n",
      "Epoch 15, Batch: 324: Training Loss: 0.025234226137399673, Validation Loss: 0.025480717420578003\n",
      "Epoch 15, Batch: 325: Training Loss: 0.02560913749039173, Validation Loss: 0.023273004218935966\n",
      "Epoch 15, Batch: 326: Training Loss: 0.029812322929501534, Validation Loss: 0.02643030695617199\n",
      "Epoch 15, Batch: 327: Training Loss: 0.02598443068563938, Validation Loss: 0.025063086301088333\n",
      "Epoch 15, Batch: 328: Training Loss: 0.02309490367770195, Validation Loss: 0.025542976334691048\n",
      "Epoch 15, Batch: 329: Training Loss: 0.024971846491098404, Validation Loss: 0.02637479268014431\n",
      "Epoch 15, Batch: 330: Training Loss: 0.02460058405995369, Validation Loss: 0.02442687377333641\n",
      "Epoch 15, Batch: 331: Training Loss: 0.024365661665797234, Validation Loss: 0.025811828672885895\n",
      "Epoch 15, Batch: 332: Training Loss: 0.02457011304795742, Validation Loss: 0.02574230171740055\n",
      "Epoch 15, Batch: 333: Training Loss: 0.023784881457686424, Validation Loss: 0.0277155339717865\n",
      "Epoch 15, Batch: 334: Training Loss: 0.026645783334970474, Validation Loss: 0.026381025090813637\n",
      "Epoch 15, Batch: 335: Training Loss: 0.023654652759432793, Validation Loss: 0.025585245341062546\n",
      "Epoch 15, Batch: 336: Training Loss: 0.023004863411188126, Validation Loss: 0.0223810113966465\n",
      "Epoch 15, Batch: 337: Training Loss: 0.025297168642282486, Validation Loss: 0.025243066251277924\n",
      "Epoch 15, Batch: 338: Training Loss: 0.022376127541065216, Validation Loss: 0.02586260810494423\n",
      "Epoch 15, Batch: 339: Training Loss: 0.02434426173567772, Validation Loss: 0.02474217116832733\n",
      "Epoch 15, Batch: 340: Training Loss: 0.029131093993782997, Validation Loss: 0.026009606197476387\n",
      "Epoch 15, Batch: 341: Training Loss: 0.026492631062865257, Validation Loss: 0.028784846886992455\n",
      "Epoch 15, Batch: 342: Training Loss: 0.025477249175310135, Validation Loss: 0.026561923325061798\n",
      "Epoch 15, Batch: 343: Training Loss: 0.027349522337317467, Validation Loss: 0.027253219857811928\n",
      "Epoch 15, Batch: 344: Training Loss: 0.02579493634402752, Validation Loss: 0.02682103030383587\n",
      "Epoch 15, Batch: 345: Training Loss: 0.023350518196821213, Validation Loss: 0.025687716901302338\n",
      "Epoch 15, Batch: 346: Training Loss: 0.024879002943634987, Validation Loss: 0.02528509497642517\n",
      "Epoch 15, Batch: 347: Training Loss: 0.022405734285712242, Validation Loss: 0.027495449408888817\n",
      "Epoch 15, Batch: 348: Training Loss: 0.022145437076687813, Validation Loss: 0.024672232568264008\n",
      "Epoch 15, Batch: 349: Training Loss: 0.03067360259592533, Validation Loss: 0.02701275795698166\n",
      "Epoch 15, Batch: 350: Training Loss: 0.02361946925520897, Validation Loss: 0.02661394141614437\n",
      "Epoch 15, Batch: 351: Training Loss: 0.031756456941366196, Validation Loss: 0.02697247453033924\n",
      "Epoch 15, Batch: 352: Training Loss: 0.024814892560243607, Validation Loss: 0.026549693197011948\n",
      "Epoch 15, Batch: 353: Training Loss: 0.02502044104039669, Validation Loss: 0.027622465044260025\n",
      "Epoch 15, Batch: 354: Training Loss: 0.027442922815680504, Validation Loss: 0.027579478919506073\n",
      "Epoch 15, Batch: 355: Training Loss: 0.02184620127081871, Validation Loss: 0.026359226554632187\n",
      "Epoch 15, Batch: 356: Training Loss: 0.02168360911309719, Validation Loss: 0.027856357395648956\n",
      "Epoch 15, Batch: 357: Training Loss: 0.023077452555298805, Validation Loss: 0.025300510227680206\n",
      "Epoch 15, Batch: 358: Training Loss: 0.024164965376257896, Validation Loss: 0.02683882974088192\n",
      "Epoch 15, Batch: 359: Training Loss: 0.0226493738591671, Validation Loss: 0.028633318841457367\n",
      "Epoch 15, Batch: 360: Training Loss: 0.024763932451605797, Validation Loss: 0.027866356074810028\n",
      "Epoch 15, Batch: 361: Training Loss: 0.023724714294075966, Validation Loss: 0.025559235364198685\n",
      "Epoch 15, Batch: 362: Training Loss: 0.025264466181397438, Validation Loss: 0.027987459674477577\n",
      "Epoch 15, Batch: 363: Training Loss: 0.032126206904649734, Validation Loss: 0.026728486642241478\n",
      "Epoch 15, Batch: 364: Training Loss: 0.02380969561636448, Validation Loss: 0.02798374928534031\n",
      "Epoch 15, Batch: 365: Training Loss: 0.025700198486447334, Validation Loss: 0.025963377207517624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 366: Training Loss: 0.027524087578058243, Validation Loss: 0.029476428404450417\n",
      "Epoch 15, Batch: 367: Training Loss: 0.021185923367738724, Validation Loss: 0.027248995378613472\n",
      "Epoch 15, Batch: 368: Training Loss: 0.021062364801764488, Validation Loss: 0.025866510346531868\n",
      "Epoch 15, Batch: 369: Training Loss: 0.02437714673578739, Validation Loss: 0.02598302625119686\n",
      "Epoch 15, Batch: 370: Training Loss: 0.024894841015338898, Validation Loss: 0.026495421305298805\n",
      "Epoch 15, Batch: 371: Training Loss: 0.024459265172481537, Validation Loss: 0.02806076779961586\n",
      "Epoch 15, Batch: 372: Training Loss: 0.024874670431017876, Validation Loss: 0.025705305859446526\n",
      "Epoch 15, Batch: 373: Training Loss: 0.025044139474630356, Validation Loss: 0.027942705899477005\n",
      "Epoch 15, Batch: 374: Training Loss: 0.024792533367872238, Validation Loss: 0.024286823347210884\n",
      "Epoch 15, Batch: 375: Training Loss: 0.024277791380882263, Validation Loss: 0.023226402699947357\n",
      "Epoch 15, Batch: 376: Training Loss: 0.023034077137708664, Validation Loss: 0.025245467200875282\n",
      "Epoch 15, Batch: 377: Training Loss: 0.024750689044594765, Validation Loss: 0.025404643267393112\n",
      "Epoch 15, Batch: 378: Training Loss: 0.02028283290565014, Validation Loss: 0.02671029046177864\n",
      "Epoch 15, Batch: 379: Training Loss: 0.024712659418582916, Validation Loss: 0.02764716185629368\n",
      "Epoch 15, Batch: 380: Training Loss: 0.025985490530729294, Validation Loss: 0.025971371680498123\n",
      "Epoch 15, Batch: 381: Training Loss: 0.022643981501460075, Validation Loss: 0.025385547429323196\n",
      "Epoch 15, Batch: 382: Training Loss: 0.02034131996333599, Validation Loss: 0.025967666879296303\n",
      "Epoch 15, Batch: 383: Training Loss: 0.02168332040309906, Validation Loss: 0.02579037845134735\n",
      "Epoch 15, Batch: 384: Training Loss: 0.02703637070953846, Validation Loss: 0.024367408826947212\n",
      "Epoch 15, Batch: 385: Training Loss: 0.023497354239225388, Validation Loss: 0.024869393557310104\n",
      "Epoch 15, Batch: 386: Training Loss: 0.020318305119872093, Validation Loss: 0.022951681166887283\n",
      "Epoch 15, Batch: 387: Training Loss: 0.022219516336917877, Validation Loss: 0.0251361895352602\n",
      "Epoch 15, Batch: 388: Training Loss: 0.024713482707738876, Validation Loss: 0.02456662431359291\n",
      "Epoch 15, Batch: 389: Training Loss: 0.023716498166322708, Validation Loss: 0.023625431582331657\n",
      "Epoch 15, Batch: 390: Training Loss: 0.025085704401135445, Validation Loss: 0.02285369299352169\n",
      "Epoch 15, Batch: 391: Training Loss: 0.02455180697143078, Validation Loss: 0.024987272918224335\n",
      "Epoch 15, Batch: 392: Training Loss: 0.02203589864075184, Validation Loss: 0.024747956544160843\n",
      "Epoch 15, Batch: 393: Training Loss: 0.02417946234345436, Validation Loss: 0.024586984887719154\n",
      "Epoch 15, Batch: 394: Training Loss: 0.024860890582203865, Validation Loss: 0.02228664420545101\n",
      "Epoch 15, Batch: 395: Training Loss: 0.022821854799985886, Validation Loss: 0.023478582501411438\n",
      "Epoch 15, Batch: 396: Training Loss: 0.022139642387628555, Validation Loss: 0.022949272766709328\n",
      "Epoch 15, Batch: 397: Training Loss: 0.024553803727030754, Validation Loss: 0.024806931614875793\n",
      "Epoch 15, Batch: 398: Training Loss: 0.02369466982781887, Validation Loss: 0.024087611585855484\n",
      "Epoch 15, Batch: 399: Training Loss: 0.02377968281507492, Validation Loss: 0.025897765532135963\n",
      "Epoch 15, Batch: 400: Training Loss: 0.024067414924502373, Validation Loss: 0.026153618469834328\n",
      "Epoch 15, Batch: 401: Training Loss: 0.026945270597934723, Validation Loss: 0.02655637450516224\n",
      "Epoch 15, Batch: 402: Training Loss: 0.022252444177865982, Validation Loss: 0.02838987298309803\n",
      "Epoch 15, Batch: 403: Training Loss: 0.02311716414988041, Validation Loss: 0.02615290880203247\n",
      "Epoch 15, Batch: 404: Training Loss: 0.025973474606871605, Validation Loss: 0.022627877071499825\n",
      "Epoch 15, Batch: 405: Training Loss: 0.02353617362678051, Validation Loss: 0.025248775258660316\n",
      "Epoch 15, Batch: 406: Training Loss: 0.0233193039894104, Validation Loss: 0.023980604484677315\n",
      "Epoch 15, Batch: 407: Training Loss: 0.024366792291402817, Validation Loss: 0.023455040529370308\n",
      "Epoch 15, Batch: 408: Training Loss: 0.023160969838500023, Validation Loss: 0.024528643116354942\n",
      "Epoch 15, Batch: 409: Training Loss: 0.023243185132741928, Validation Loss: 0.024223830550909042\n",
      "Epoch 15, Batch: 410: Training Loss: 0.02490117773413658, Validation Loss: 0.02462080866098404\n",
      "Epoch 15, Batch: 411: Training Loss: 0.022142766043543816, Validation Loss: 0.02422332763671875\n",
      "Epoch 15, Batch: 412: Training Loss: 0.027498843148350716, Validation Loss: 0.023857731372117996\n",
      "Epoch 15, Batch: 413: Training Loss: 0.025939250364899635, Validation Loss: 0.021879257634282112\n",
      "Epoch 15, Batch: 414: Training Loss: 0.025518245995044708, Validation Loss: 0.024165235459804535\n",
      "Epoch 15, Batch: 415: Training Loss: 0.025149062275886536, Validation Loss: 0.02703731320798397\n",
      "Epoch 15, Batch: 416: Training Loss: 0.025337131693959236, Validation Loss: 0.023460322991013527\n",
      "Epoch 15, Batch: 417: Training Loss: 0.027080800384283066, Validation Loss: 0.02527894452214241\n",
      "Epoch 15, Batch: 418: Training Loss: 0.024743445217609406, Validation Loss: 0.02511598728597164\n",
      "Epoch 15, Batch: 419: Training Loss: 0.02463577128946781, Validation Loss: 0.02575627900660038\n",
      "Epoch 15, Batch: 420: Training Loss: 0.02505270391702652, Validation Loss: 0.026777157559990883\n",
      "Epoch 15, Batch: 421: Training Loss: 0.027589065954089165, Validation Loss: 0.0276460200548172\n",
      "Epoch 15, Batch: 422: Training Loss: 0.028074078261852264, Validation Loss: 0.023759199306368828\n",
      "Epoch 15, Batch: 423: Training Loss: 0.02722865529358387, Validation Loss: 0.026109959930181503\n",
      "Epoch 15, Batch: 424: Training Loss: 0.026347357779741287, Validation Loss: 0.02545059472322464\n",
      "Epoch 15, Batch: 425: Training Loss: 0.024212488904595375, Validation Loss: 0.0270596481859684\n",
      "Epoch 15, Batch: 426: Training Loss: 0.024145785719156265, Validation Loss: 0.02639935538172722\n",
      "Epoch 15, Batch: 427: Training Loss: 0.025447066873311996, Validation Loss: 0.024337947368621826\n",
      "Epoch 15, Batch: 428: Training Loss: 0.02329837530851364, Validation Loss: 0.028141725808382034\n",
      "Epoch 15, Batch: 429: Training Loss: 0.023032499477267265, Validation Loss: 0.024986958131194115\n",
      "Epoch 15, Batch: 430: Training Loss: 0.021974753588438034, Validation Loss: 0.02630649134516716\n",
      "Epoch 15, Batch: 431: Training Loss: 0.023616915568709373, Validation Loss: 0.023891041055321693\n",
      "Epoch 15, Batch: 432: Training Loss: 0.02397807687520981, Validation Loss: 0.0270428154617548\n",
      "Epoch 15, Batch: 433: Training Loss: 0.02426978200674057, Validation Loss: 0.024313809350132942\n",
      "Epoch 15, Batch: 434: Training Loss: 0.027188187465071678, Validation Loss: 0.02297969162464142\n",
      "Epoch 15, Batch: 435: Training Loss: 0.02534879371523857, Validation Loss: 0.02644859068095684\n",
      "Epoch 15, Batch: 436: Training Loss: 0.021982669830322266, Validation Loss: 0.025390492752194405\n",
      "Epoch 15, Batch: 437: Training Loss: 0.02274446189403534, Validation Loss: 0.025838689878582954\n",
      "Epoch 15, Batch: 438: Training Loss: 0.023185856640338898, Validation Loss: 0.028307238593697548\n",
      "Epoch 15, Batch: 439: Training Loss: 0.023897286504507065, Validation Loss: 0.0243582371622324\n",
      "Epoch 15, Batch: 440: Training Loss: 0.025434711948037148, Validation Loss: 0.024249259382486343\n",
      "Epoch 15, Batch: 441: Training Loss: 0.02266409620642662, Validation Loss: 0.023784028366208076\n",
      "Epoch 15, Batch: 442: Training Loss: 0.02356615476310253, Validation Loss: 0.025704724714159966\n",
      "Epoch 15, Batch: 443: Training Loss: 0.02249194122850895, Validation Loss: 0.025262217968702316\n",
      "Epoch 15, Batch: 444: Training Loss: 0.020276442170143127, Validation Loss: 0.022453172132372856\n",
      "Epoch 15, Batch: 445: Training Loss: 0.020150139927864075, Validation Loss: 0.024015115574002266\n",
      "Epoch 15, Batch: 446: Training Loss: 0.027616048231720924, Validation Loss: 0.02413698099553585\n",
      "Epoch 15, Batch: 447: Training Loss: 0.022428756579756737, Validation Loss: 0.026072630658745766\n",
      "Epoch 15, Batch: 448: Training Loss: 0.024052303284406662, Validation Loss: 0.025211619213223457\n",
      "Epoch 15, Batch: 449: Training Loss: 0.02118387259542942, Validation Loss: 0.02640891633927822\n",
      "Epoch 15, Batch: 450: Training Loss: 0.022813107818365097, Validation Loss: 0.024491911754012108\n",
      "Epoch 15, Batch: 451: Training Loss: 0.027488332241773605, Validation Loss: 0.02334379032254219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch: 452: Training Loss: 0.028486939147114754, Validation Loss: 0.023362062871456146\n",
      "Epoch 15, Batch: 453: Training Loss: 0.028383061289787292, Validation Loss: 0.024254383519291878\n",
      "Epoch 15, Batch: 454: Training Loss: 0.02474183216691017, Validation Loss: 0.024084104225039482\n",
      "Epoch 15, Batch: 455: Training Loss: 0.022948598489165306, Validation Loss: 0.02221927046775818\n",
      "Epoch 15, Batch: 456: Training Loss: 0.02304888516664505, Validation Loss: 0.02220720611512661\n",
      "Epoch 15, Batch: 457: Training Loss: 0.02331913448870182, Validation Loss: 0.022833026945590973\n",
      "Epoch 15, Batch: 458: Training Loss: 0.01875833421945572, Validation Loss: 0.022502684965729713\n",
      "Epoch 15, Batch: 459: Training Loss: 0.02762773633003235, Validation Loss: 0.02295338362455368\n",
      "Epoch 15, Batch: 460: Training Loss: 0.022131584584712982, Validation Loss: 0.025438187643885612\n",
      "Epoch 15, Batch: 461: Training Loss: 0.02075209654867649, Validation Loss: 0.026863722130656242\n",
      "Epoch 15, Batch: 462: Training Loss: 0.01782573200762272, Validation Loss: 0.024387750774621964\n",
      "Epoch 15, Batch: 463: Training Loss: 0.022948550060391426, Validation Loss: 0.027704665437340736\n",
      "Epoch 15, Batch: 464: Training Loss: 0.02060898393392563, Validation Loss: 0.02458416298031807\n",
      "Epoch 15, Batch: 465: Training Loss: 0.02173495478928089, Validation Loss: 0.026761390268802643\n",
      "Epoch 15, Batch: 466: Training Loss: 0.02188156358897686, Validation Loss: 0.02953527122735977\n",
      "Epoch 15, Batch: 467: Training Loss: 0.029923781752586365, Validation Loss: 0.02581041492521763\n",
      "Epoch 15, Batch: 468: Training Loss: 0.02506214939057827, Validation Loss: 0.026567243039608\n",
      "Epoch 15, Batch: 469: Training Loss: 0.024781251326203346, Validation Loss: 0.02579992637038231\n",
      "Epoch 15, Batch: 470: Training Loss: 0.0220705047249794, Validation Loss: 0.025944562628865242\n",
      "Epoch 15, Batch: 471: Training Loss: 0.02716134488582611, Validation Loss: 0.02605329267680645\n",
      "Epoch 15, Batch: 472: Training Loss: 0.022849148139357567, Validation Loss: 0.026555806398391724\n",
      "Epoch 15, Batch: 473: Training Loss: 0.02248670905828476, Validation Loss: 0.027262065559625626\n",
      "Epoch 15, Batch: 474: Training Loss: 0.020697621628642082, Validation Loss: 0.025650136172771454\n",
      "Epoch 15, Batch: 475: Training Loss: 0.026975300163030624, Validation Loss: 0.024630049243569374\n",
      "Epoch 15, Batch: 476: Training Loss: 0.021585527807474136, Validation Loss: 0.02679475024342537\n",
      "Epoch 15, Batch: 477: Training Loss: 0.027142537757754326, Validation Loss: 0.029285822063684464\n",
      "Epoch 15, Batch: 478: Training Loss: 0.02336137555539608, Validation Loss: 0.028615297749638557\n",
      "Epoch 15, Batch: 479: Training Loss: 0.024486152455210686, Validation Loss: 0.026226213201880455\n",
      "Epoch 15, Batch: 480: Training Loss: 0.024449577555060387, Validation Loss: 0.02640463039278984\n",
      "Epoch 15, Batch: 481: Training Loss: 0.025007063522934914, Validation Loss: 0.02630174160003662\n",
      "Epoch 15, Batch: 482: Training Loss: 0.028027905151247978, Validation Loss: 0.02521098405122757\n",
      "Epoch 15, Batch: 483: Training Loss: 0.02238687500357628, Validation Loss: 0.029702261090278625\n",
      "Epoch 15, Batch: 484: Training Loss: 0.020888039842247963, Validation Loss: 0.027465766295790672\n",
      "Epoch 15, Batch: 485: Training Loss: 0.023522870615124702, Validation Loss: 0.028048032894730568\n",
      "Epoch 15, Batch: 486: Training Loss: 0.02574096992611885, Validation Loss: 0.027598736807703972\n",
      "Epoch 15, Batch: 487: Training Loss: 0.02352072298526764, Validation Loss: 0.02774142287671566\n",
      "Epoch 15, Batch: 488: Training Loss: 0.022924987599253654, Validation Loss: 0.028119521215558052\n",
      "Epoch 15, Batch: 489: Training Loss: 0.026869138702750206, Validation Loss: 0.02781429886817932\n",
      "Epoch 15, Batch: 490: Training Loss: 0.02786184474825859, Validation Loss: 0.027440713718533516\n",
      "Epoch 15, Batch: 491: Training Loss: 0.01947144791483879, Validation Loss: 0.02641305886209011\n",
      "Epoch 15, Batch: 492: Training Loss: 0.024939481168985367, Validation Loss: 0.02552144043147564\n",
      "Epoch 15, Batch: 493: Training Loss: 0.026535090059041977, Validation Loss: 0.025921205058693886\n",
      "Epoch 15, Batch: 494: Training Loss: 0.027369750663638115, Validation Loss: 0.02378244325518608\n",
      "Epoch 15, Batch: 495: Training Loss: 0.022875964641571045, Validation Loss: 0.024046294391155243\n",
      "Epoch 15, Batch: 496: Training Loss: 0.023288974538445473, Validation Loss: 0.02554955519735813\n",
      "Epoch 15, Batch: 497: Training Loss: 0.023365650326013565, Validation Loss: 0.02563663385808468\n",
      "Epoch 15, Batch: 498: Training Loss: 0.024300508201122284, Validation Loss: 0.026355264708399773\n",
      "Epoch 15, Batch: 499: Training Loss: 0.02183373272418976, Validation Loss: 0.024024078622460365\n",
      "Epoch 16, Batch: 0: Training Loss: 0.02523626573383808, Validation Loss: 0.026164500042796135\n",
      "Epoch 16, Batch: 1: Training Loss: 0.021870290860533714, Validation Loss: 0.025072330608963966\n",
      "Epoch 16, Batch: 2: Training Loss: 0.025315845385193825, Validation Loss: 0.02380230836570263\n",
      "Epoch 16, Batch: 3: Training Loss: 0.021457955241203308, Validation Loss: 0.02742858976125717\n",
      "Epoch 16, Batch: 4: Training Loss: 0.01870552822947502, Validation Loss: 0.024940751492977142\n",
      "Epoch 16, Batch: 5: Training Loss: 0.022600580006837845, Validation Loss: 0.02459116280078888\n",
      "Epoch 16, Batch: 6: Training Loss: 0.02060147374868393, Validation Loss: 0.02605515345931053\n",
      "Epoch 16, Batch: 7: Training Loss: 0.023071927949786186, Validation Loss: 0.024816693738102913\n",
      "Epoch 16, Batch: 8: Training Loss: 0.02505817823112011, Validation Loss: 0.02555132284760475\n",
      "Epoch 16, Batch: 9: Training Loss: 0.024604978039860725, Validation Loss: 0.025250034406781197\n",
      "Epoch 16, Batch: 10: Training Loss: 0.021661145612597466, Validation Loss: 0.028880493715405464\n",
      "Epoch 16, Batch: 11: Training Loss: 0.024308254942297935, Validation Loss: 0.026771076023578644\n",
      "Epoch 16, Batch: 12: Training Loss: 0.026986289769411087, Validation Loss: 0.024857191368937492\n",
      "Epoch 16, Batch: 13: Training Loss: 0.0261636171489954, Validation Loss: 0.026234420016407967\n",
      "Epoch 16, Batch: 14: Training Loss: 0.023345928639173508, Validation Loss: 0.024975290521979332\n",
      "Epoch 16, Batch: 15: Training Loss: 0.024320727214217186, Validation Loss: 0.02664759010076523\n",
      "Epoch 16, Batch: 16: Training Loss: 0.028037436306476593, Validation Loss: 0.0240221805870533\n",
      "Epoch 16, Batch: 17: Training Loss: 0.02325955592095852, Validation Loss: 0.025201287120580673\n",
      "Epoch 16, Batch: 18: Training Loss: 0.023568809032440186, Validation Loss: 0.024411983788013458\n",
      "Epoch 16, Batch: 19: Training Loss: 0.023053986951708794, Validation Loss: 0.025555165484547615\n",
      "Epoch 16, Batch: 20: Training Loss: 0.02512097917497158, Validation Loss: 0.026609862223267555\n",
      "Epoch 16, Batch: 21: Training Loss: 0.02541046403348446, Validation Loss: 0.02358248084783554\n",
      "Epoch 16, Batch: 22: Training Loss: 0.02387223020195961, Validation Loss: 0.02288355864584446\n",
      "Epoch 16, Batch: 23: Training Loss: 0.026580845937132835, Validation Loss: 0.026736820116639137\n",
      "Epoch 16, Batch: 24: Training Loss: 0.02167319878935814, Validation Loss: 0.024532053619623184\n",
      "Epoch 16, Batch: 25: Training Loss: 0.022814104333519936, Validation Loss: 0.029915036633610725\n",
      "Epoch 16, Batch: 26: Training Loss: 0.024043792858719826, Validation Loss: 0.027498390525579453\n",
      "Epoch 16, Batch: 27: Training Loss: 0.025633132085204124, Validation Loss: 0.026036133989691734\n",
      "Epoch 16, Batch: 28: Training Loss: 0.026984069496393204, Validation Loss: 0.02593056857585907\n",
      "Epoch 16, Batch: 29: Training Loss: 0.03141253814101219, Validation Loss: 0.027590231969952583\n",
      "Epoch 16, Batch: 30: Training Loss: 0.022629132494330406, Validation Loss: 0.02640054188668728\n",
      "Epoch 16, Batch: 31: Training Loss: 0.026538850739598274, Validation Loss: 0.024226857349276543\n",
      "Epoch 16, Batch: 32: Training Loss: 0.025179633870720863, Validation Loss: 0.027316667139530182\n",
      "Epoch 16, Batch: 33: Training Loss: 0.02421966753900051, Validation Loss: 0.0254615880548954\n",
      "Epoch 16, Batch: 34: Training Loss: 0.02177889458835125, Validation Loss: 0.024670900776982307\n",
      "Epoch 16, Batch: 35: Training Loss: 0.022488810122013092, Validation Loss: 0.02484915219247341\n",
      "Epoch 16, Batch: 36: Training Loss: 0.022376934066414833, Validation Loss: 0.024112505838274956\n",
      "Epoch 16, Batch: 37: Training Loss: 0.024206481873989105, Validation Loss: 0.025529509410262108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 38: Training Loss: 0.027350300922989845, Validation Loss: 0.02762337401509285\n",
      "Epoch 16, Batch: 39: Training Loss: 0.025603093206882477, Validation Loss: 0.025937270373106003\n",
      "Epoch 16, Batch: 40: Training Loss: 0.025863749906420708, Validation Loss: 0.026775337755680084\n",
      "Epoch 16, Batch: 41: Training Loss: 0.02586933970451355, Validation Loss: 0.026159407570958138\n",
      "Epoch 16, Batch: 42: Training Loss: 0.020922871306538582, Validation Loss: 0.024269724264740944\n",
      "Epoch 16, Batch: 43: Training Loss: 0.019367074593901634, Validation Loss: 0.026318954303860664\n",
      "Epoch 16, Batch: 44: Training Loss: 0.024774126708507538, Validation Loss: 0.025121470913290977\n",
      "Epoch 16, Batch: 45: Training Loss: 0.023270197212696075, Validation Loss: 0.026980416849255562\n",
      "Epoch 16, Batch: 46: Training Loss: 0.025025075301527977, Validation Loss: 0.025138206779956818\n",
      "Epoch 16, Batch: 47: Training Loss: 0.026984138414263725, Validation Loss: 0.026250960305333138\n",
      "Epoch 16, Batch: 48: Training Loss: 0.029130907729268074, Validation Loss: 0.027732575312256813\n",
      "Epoch 16, Batch: 49: Training Loss: 0.02304944582283497, Validation Loss: 0.027660196647047997\n",
      "Epoch 16, Batch: 50: Training Loss: 0.02330099046230316, Validation Loss: 0.02778356894850731\n",
      "Epoch 16, Batch: 51: Training Loss: 0.02437695488333702, Validation Loss: 0.0280466265976429\n",
      "Epoch 16, Batch: 52: Training Loss: 0.023879362270236015, Validation Loss: 0.02729431726038456\n",
      "Epoch 16, Batch: 53: Training Loss: 0.023848190903663635, Validation Loss: 0.02974643185734749\n",
      "Epoch 16, Batch: 54: Training Loss: 0.027145618572831154, Validation Loss: 0.027094122022390366\n",
      "Epoch 16, Batch: 55: Training Loss: 0.02176242507994175, Validation Loss: 0.026888418942689896\n",
      "Epoch 16, Batch: 56: Training Loss: 0.026634231209754944, Validation Loss: 0.028267426416277885\n",
      "Epoch 16, Batch: 57: Training Loss: 0.022374754771590233, Validation Loss: 0.027815265581011772\n",
      "Epoch 16, Batch: 58: Training Loss: 0.026896774768829346, Validation Loss: 0.025834862142801285\n",
      "Epoch 16, Batch: 59: Training Loss: 0.02227926254272461, Validation Loss: 0.028847528621554375\n",
      "Epoch 16, Batch: 60: Training Loss: 0.025532158091664314, Validation Loss: 0.026374738663434982\n",
      "Epoch 16, Batch: 61: Training Loss: 0.028882063925266266, Validation Loss: 0.02741858921945095\n",
      "Epoch 16, Batch: 62: Training Loss: 0.019788477569818497, Validation Loss: 0.031009728088974953\n",
      "Epoch 16, Batch: 63: Training Loss: 0.029101835563778877, Validation Loss: 0.028072571381926537\n",
      "Epoch 16, Batch: 64: Training Loss: 0.024524083361029625, Validation Loss: 0.024774985387921333\n",
      "Epoch 16, Batch: 65: Training Loss: 0.025531204417347908, Validation Loss: 0.024992389604449272\n",
      "Epoch 16, Batch: 66: Training Loss: 0.02197481133043766, Validation Loss: 0.0270672757178545\n",
      "Epoch 16, Batch: 67: Training Loss: 0.022495824843645096, Validation Loss: 0.02560543827712536\n",
      "Epoch 16, Batch: 68: Training Loss: 0.024321388453245163, Validation Loss: 0.02294396050274372\n",
      "Epoch 16, Batch: 69: Training Loss: 0.02419622801244259, Validation Loss: 0.022766564041376114\n",
      "Epoch 16, Batch: 70: Training Loss: 0.025367630645632744, Validation Loss: 0.023858552798628807\n",
      "Epoch 16, Batch: 71: Training Loss: 0.022258702665567398, Validation Loss: 0.0248621366918087\n",
      "Epoch 16, Batch: 72: Training Loss: 0.024594420567154884, Validation Loss: 0.023301975801587105\n",
      "Epoch 16, Batch: 73: Training Loss: 0.023493055254220963, Validation Loss: 0.02323159947991371\n",
      "Epoch 16, Batch: 74: Training Loss: 0.025312112644314766, Validation Loss: 0.02539675123989582\n",
      "Epoch 16, Batch: 75: Training Loss: 0.019913071766495705, Validation Loss: 0.026209093630313873\n",
      "Epoch 16, Batch: 76: Training Loss: 0.021845581009984016, Validation Loss: 0.026586655527353287\n",
      "Epoch 16, Batch: 77: Training Loss: 0.02657770738005638, Validation Loss: 0.02612127549946308\n",
      "Epoch 16, Batch: 78: Training Loss: 0.027399836108088493, Validation Loss: 0.02674742229282856\n",
      "Epoch 16, Batch: 79: Training Loss: 0.025371653959155083, Validation Loss: 0.028761986643075943\n",
      "Epoch 16, Batch: 80: Training Loss: 0.02339051477611065, Validation Loss: 0.02504069171845913\n",
      "Epoch 16, Batch: 81: Training Loss: 0.024460026994347572, Validation Loss: 0.028083795681595802\n",
      "Epoch 16, Batch: 82: Training Loss: 0.025141045451164246, Validation Loss: 0.027735164389014244\n",
      "Epoch 16, Batch: 83: Training Loss: 0.023499831557273865, Validation Loss: 0.02618706040084362\n",
      "Epoch 16, Batch: 84: Training Loss: 0.026739848777651787, Validation Loss: 0.024088386446237564\n",
      "Epoch 16, Batch: 85: Training Loss: 0.020935988053679466, Validation Loss: 0.026463095098733902\n",
      "Epoch 16, Batch: 86: Training Loss: 0.026762554422020912, Validation Loss: 0.027667246758937836\n",
      "Epoch 16, Batch: 87: Training Loss: 0.02942774072289467, Validation Loss: 0.02394404634833336\n",
      "Epoch 16, Batch: 88: Training Loss: 0.02596954070031643, Validation Loss: 0.023631853982806206\n",
      "Epoch 16, Batch: 89: Training Loss: 0.026119163259863853, Validation Loss: 0.025694431737065315\n",
      "Epoch 16, Batch: 90: Training Loss: 0.02041315846145153, Validation Loss: 0.025616083294153214\n",
      "Epoch 16, Batch: 91: Training Loss: 0.025351116433739662, Validation Loss: 0.02533666417002678\n",
      "Epoch 16, Batch: 92: Training Loss: 0.024491382762789726, Validation Loss: 0.0238250270485878\n",
      "Epoch 16, Batch: 93: Training Loss: 0.025183992460370064, Validation Loss: 0.024848923087120056\n",
      "Epoch 16, Batch: 94: Training Loss: 0.025206614285707474, Validation Loss: 0.025622840970754623\n",
      "Epoch 16, Batch: 95: Training Loss: 0.023585882037878036, Validation Loss: 0.02451726421713829\n",
      "Epoch 16, Batch: 96: Training Loss: 0.023479286581277847, Validation Loss: 0.025346791371703148\n",
      "Epoch 16, Batch: 97: Training Loss: 0.027607275173068047, Validation Loss: 0.02400851622223854\n",
      "Epoch 16, Batch: 98: Training Loss: 0.02243715710937977, Validation Loss: 0.02669582888484001\n",
      "Epoch 16, Batch: 99: Training Loss: 0.023721853271126747, Validation Loss: 0.026986997574567795\n",
      "Epoch 16, Batch: 100: Training Loss: 0.024628421291708946, Validation Loss: 0.02867085486650467\n",
      "Epoch 16, Batch: 101: Training Loss: 0.022290218621492386, Validation Loss: 0.02476891316473484\n",
      "Epoch 16, Batch: 102: Training Loss: 0.02560202218592167, Validation Loss: 0.026989692822098732\n",
      "Epoch 16, Batch: 103: Training Loss: 0.024877110496163368, Validation Loss: 0.026595374569296837\n",
      "Epoch 16, Batch: 104: Training Loss: 0.0219417791813612, Validation Loss: 0.024945365265011787\n",
      "Epoch 16, Batch: 105: Training Loss: 0.02099907211959362, Validation Loss: 0.025325601920485497\n",
      "Epoch 16, Batch: 106: Training Loss: 0.024623442441225052, Validation Loss: 0.02461310662329197\n",
      "Epoch 16, Batch: 107: Training Loss: 0.022028759121894836, Validation Loss: 0.022780481725931168\n",
      "Epoch 16, Batch: 108: Training Loss: 0.022048747166991234, Validation Loss: 0.02381000481545925\n",
      "Epoch 16, Batch: 109: Training Loss: 0.02432670257985592, Validation Loss: 0.025227859616279602\n",
      "Epoch 16, Batch: 110: Training Loss: 0.024365948513150215, Validation Loss: 0.02508266642689705\n",
      "Epoch 16, Batch: 111: Training Loss: 0.02299514412879944, Validation Loss: 0.02640361152589321\n",
      "Epoch 16, Batch: 112: Training Loss: 0.01792842335999012, Validation Loss: 0.024097437039017677\n",
      "Epoch 16, Batch: 113: Training Loss: 0.023948486894369125, Validation Loss: 0.024523451924324036\n",
      "Epoch 16, Batch: 114: Training Loss: 0.021393703296780586, Validation Loss: 0.022342460229992867\n",
      "Epoch 16, Batch: 115: Training Loss: 0.029210172593593597, Validation Loss: 0.02353891171514988\n",
      "Epoch 16, Batch: 116: Training Loss: 0.020534835755825043, Validation Loss: 0.024858159944415092\n",
      "Epoch 16, Batch: 117: Training Loss: 0.02540065161883831, Validation Loss: 0.026102129369974136\n",
      "Epoch 16, Batch: 118: Training Loss: 0.023269863799214363, Validation Loss: 0.025229550898075104\n",
      "Epoch 16, Batch: 119: Training Loss: 0.02495247684419155, Validation Loss: 0.026448385789990425\n",
      "Epoch 16, Batch: 120: Training Loss: 0.02078617736697197, Validation Loss: 0.02758585289120674\n",
      "Epoch 16, Batch: 121: Training Loss: 0.024783797562122345, Validation Loss: 0.02443980798125267\n",
      "Epoch 16, Batch: 122: Training Loss: 0.021689098328351974, Validation Loss: 0.02340809814631939\n",
      "Epoch 16, Batch: 123: Training Loss: 0.02346116304397583, Validation Loss: 0.02358013577759266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 124: Training Loss: 0.019809212535619736, Validation Loss: 0.02441755309700966\n",
      "Epoch 16, Batch: 125: Training Loss: 0.02405600994825363, Validation Loss: 0.02441374585032463\n",
      "Epoch 16, Batch: 126: Training Loss: 0.024344025179743767, Validation Loss: 0.024287024512887\n",
      "Epoch 16, Batch: 127: Training Loss: 0.02271658554673195, Validation Loss: 0.022805703803896904\n",
      "Epoch 16, Batch: 128: Training Loss: 0.023417392745614052, Validation Loss: 0.02238166332244873\n",
      "Epoch 16, Batch: 129: Training Loss: 0.021428102627396584, Validation Loss: 0.024195116013288498\n",
      "Epoch 16, Batch: 130: Training Loss: 0.024734286591410637, Validation Loss: 0.02435363456606865\n",
      "Epoch 16, Batch: 131: Training Loss: 0.023389950394630432, Validation Loss: 0.026961877942085266\n",
      "Epoch 16, Batch: 132: Training Loss: 0.023405645042657852, Validation Loss: 0.022376205772161484\n",
      "Epoch 16, Batch: 133: Training Loss: 0.024136066436767578, Validation Loss: 0.023208754137158394\n",
      "Epoch 16, Batch: 134: Training Loss: 0.026462377980351448, Validation Loss: 0.02334371581673622\n",
      "Epoch 16, Batch: 135: Training Loss: 0.023003926500678062, Validation Loss: 0.025192230939865112\n",
      "Epoch 16, Batch: 136: Training Loss: 0.023076333105564117, Validation Loss: 0.023648206144571304\n",
      "Epoch 16, Batch: 137: Training Loss: 0.019577359780669212, Validation Loss: 0.023453807458281517\n",
      "Epoch 16, Batch: 138: Training Loss: 0.023459376767277718, Validation Loss: 0.023953000083565712\n",
      "Epoch 16, Batch: 139: Training Loss: 0.02251763641834259, Validation Loss: 0.025093890726566315\n",
      "Epoch 16, Batch: 140: Training Loss: 0.0250038281083107, Validation Loss: 0.025067731738090515\n",
      "Epoch 16, Batch: 141: Training Loss: 0.023828430101275444, Validation Loss: 0.024304470047354698\n",
      "Epoch 16, Batch: 142: Training Loss: 0.020960930734872818, Validation Loss: 0.025449154898524284\n",
      "Epoch 16, Batch: 143: Training Loss: 0.023765696212649345, Validation Loss: 0.02784254029393196\n",
      "Epoch 16, Batch: 144: Training Loss: 0.02259809523820877, Validation Loss: 0.022990018129348755\n",
      "Epoch 16, Batch: 145: Training Loss: 0.024292372167110443, Validation Loss: 0.022692015394568443\n",
      "Epoch 16, Batch: 146: Training Loss: 0.024336697533726692, Validation Loss: 0.02349427342414856\n",
      "Epoch 16, Batch: 147: Training Loss: 0.02259153500199318, Validation Loss: 0.024261469021439552\n",
      "Epoch 16, Batch: 148: Training Loss: 0.022734615951776505, Validation Loss: 0.023382695391774178\n",
      "Epoch 16, Batch: 149: Training Loss: 0.023426266387104988, Validation Loss: 0.023113535717129707\n",
      "Epoch 16, Batch: 150: Training Loss: 0.026189489290118217, Validation Loss: 0.02550102397799492\n",
      "Epoch 16, Batch: 151: Training Loss: 0.025341937318444252, Validation Loss: 0.023469241335988045\n",
      "Epoch 16, Batch: 152: Training Loss: 0.02356065809726715, Validation Loss: 0.024008406326174736\n",
      "Epoch 16, Batch: 153: Training Loss: 0.021343378350138664, Validation Loss: 0.026648659259080887\n",
      "Epoch 16, Batch: 154: Training Loss: 0.02241913042962551, Validation Loss: 0.02452262118458748\n",
      "Epoch 16, Batch: 155: Training Loss: 0.0309511236846447, Validation Loss: 0.027124077081680298\n",
      "Epoch 16, Batch: 156: Training Loss: 0.023920603096485138, Validation Loss: 0.021399687975645065\n",
      "Epoch 16, Batch: 157: Training Loss: 0.021335329860448837, Validation Loss: 0.023611487820744514\n",
      "Epoch 16, Batch: 158: Training Loss: 0.0222550630569458, Validation Loss: 0.022848449647426605\n",
      "Epoch 16, Batch: 159: Training Loss: 0.021634675562381744, Validation Loss: 0.025442244485020638\n",
      "Epoch 16, Batch: 160: Training Loss: 0.02181837148964405, Validation Loss: 0.023816192522644997\n",
      "Epoch 16, Batch: 161: Training Loss: 0.0265068206936121, Validation Loss: 0.021810639649629593\n",
      "Epoch 16, Batch: 162: Training Loss: 0.026982519775629044, Validation Loss: 0.02159651555120945\n",
      "Epoch 16, Batch: 163: Training Loss: 0.022393077611923218, Validation Loss: 0.023316316306591034\n",
      "Epoch 16, Batch: 164: Training Loss: 0.021637527272105217, Validation Loss: 0.022574257105588913\n",
      "Epoch 16, Batch: 165: Training Loss: 0.022342298179864883, Validation Loss: 0.023506535217165947\n",
      "Epoch 16, Batch: 166: Training Loss: 0.01949239894747734, Validation Loss: 0.02501860074698925\n",
      "Epoch 16, Batch: 167: Training Loss: 0.022730203345417976, Validation Loss: 0.025205260142683983\n",
      "Epoch 16, Batch: 168: Training Loss: 0.023546399548649788, Validation Loss: 0.024158192798495293\n",
      "Epoch 16, Batch: 169: Training Loss: 0.026559436693787575, Validation Loss: 0.02429310232400894\n",
      "Epoch 16, Batch: 170: Training Loss: 0.023003894835710526, Validation Loss: 0.025977283716201782\n",
      "Epoch 16, Batch: 171: Training Loss: 0.01975199766457081, Validation Loss: 0.025327973067760468\n",
      "Epoch 16, Batch: 172: Training Loss: 0.02360674925148487, Validation Loss: 0.027262873947620392\n",
      "Epoch 16, Batch: 173: Training Loss: 0.022235525771975517, Validation Loss: 0.026203211396932602\n",
      "Epoch 16, Batch: 174: Training Loss: 0.02193436585366726, Validation Loss: 0.02279234491288662\n",
      "Epoch 16, Batch: 175: Training Loss: 0.023077717050909996, Validation Loss: 0.02511705830693245\n",
      "Epoch 16, Batch: 176: Training Loss: 0.02337854541838169, Validation Loss: 0.024066269397735596\n",
      "Epoch 16, Batch: 177: Training Loss: 0.024294447153806686, Validation Loss: 0.023729663342237473\n",
      "Epoch 16, Batch: 178: Training Loss: 0.024253401905298233, Validation Loss: 0.02400919608771801\n",
      "Epoch 16, Batch: 179: Training Loss: 0.020766396075487137, Validation Loss: 0.024720588698983192\n",
      "Epoch 16, Batch: 180: Training Loss: 0.02508811466395855, Validation Loss: 0.021459048613905907\n",
      "Epoch 16, Batch: 181: Training Loss: 0.02238371968269348, Validation Loss: 0.023381534963846207\n",
      "Epoch 16, Batch: 182: Training Loss: 0.02435276098549366, Validation Loss: 0.024287493899464607\n",
      "Epoch 16, Batch: 183: Training Loss: 0.020887233316898346, Validation Loss: 0.02248825691640377\n",
      "Epoch 16, Batch: 184: Training Loss: 0.022492805495858192, Validation Loss: 0.023167483508586884\n",
      "Epoch 16, Batch: 185: Training Loss: 0.029036100953817368, Validation Loss: 0.02323090471327305\n",
      "Epoch 16, Batch: 186: Training Loss: 0.023251831531524658, Validation Loss: 0.02265140600502491\n",
      "Epoch 16, Batch: 187: Training Loss: 0.025184419006109238, Validation Loss: 0.02264726161956787\n",
      "Epoch 16, Batch: 188: Training Loss: 0.02485804632306099, Validation Loss: 0.021542683243751526\n",
      "Epoch 16, Batch: 189: Training Loss: 0.023773126304149628, Validation Loss: 0.021789630874991417\n",
      "Epoch 16, Batch: 190: Training Loss: 0.025062905624508858, Validation Loss: 0.02370418794453144\n",
      "Epoch 16, Batch: 191: Training Loss: 0.027597587555646896, Validation Loss: 0.023688096553087234\n",
      "Epoch 16, Batch: 192: Training Loss: 0.02115619368851185, Validation Loss: 0.024099355563521385\n",
      "Epoch 16, Batch: 193: Training Loss: 0.026171527802944183, Validation Loss: 0.022672468796372414\n",
      "Epoch 16, Batch: 194: Training Loss: 0.02590458281338215, Validation Loss: 0.025166276842355728\n",
      "Epoch 16, Batch: 195: Training Loss: 0.023207593709230423, Validation Loss: 0.02239455096423626\n",
      "Epoch 16, Batch: 196: Training Loss: 0.02383544109761715, Validation Loss: 0.02326209843158722\n",
      "Epoch 16, Batch: 197: Training Loss: 0.023612193763256073, Validation Loss: 0.024265611544251442\n",
      "Epoch 16, Batch: 198: Training Loss: 0.022183097898960114, Validation Loss: 0.022104069590568542\n",
      "Epoch 16, Batch: 199: Training Loss: 0.02178971655666828, Validation Loss: 0.024760786443948746\n",
      "Epoch 16, Batch: 200: Training Loss: 0.020713260397315025, Validation Loss: 0.024600297212600708\n",
      "Epoch 16, Batch: 201: Training Loss: 0.027354005724191666, Validation Loss: 0.024713611230254173\n",
      "Epoch 16, Batch: 202: Training Loss: 0.02333282120525837, Validation Loss: 0.024001069366931915\n",
      "Epoch 16, Batch: 203: Training Loss: 0.026958433911204338, Validation Loss: 0.02413273975253105\n",
      "Epoch 16, Batch: 204: Training Loss: 0.02328426018357277, Validation Loss: 0.022578831762075424\n",
      "Epoch 16, Batch: 205: Training Loss: 0.025848006829619408, Validation Loss: 0.024177271872758865\n",
      "Epoch 16, Batch: 206: Training Loss: 0.024519333615899086, Validation Loss: 0.025963284075260162\n",
      "Epoch 16, Batch: 207: Training Loss: 0.02896112948656082, Validation Loss: 0.024971595034003258\n",
      "Epoch 16, Batch: 208: Training Loss: 0.022606587037444115, Validation Loss: 0.023159809410572052\n",
      "Epoch 16, Batch: 209: Training Loss: 0.022772422060370445, Validation Loss: 0.02817860059440136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 210: Training Loss: 0.02684139274060726, Validation Loss: 0.024537617340683937\n",
      "Epoch 16, Batch: 211: Training Loss: 0.02343744784593582, Validation Loss: 0.024454504251480103\n",
      "Epoch 16, Batch: 212: Training Loss: 0.02177131175994873, Validation Loss: 0.023087117820978165\n",
      "Epoch 16, Batch: 213: Training Loss: 0.02346002124249935, Validation Loss: 0.02625163458287716\n",
      "Epoch 16, Batch: 214: Training Loss: 0.02070709317922592, Validation Loss: 0.02524411305785179\n",
      "Epoch 16, Batch: 215: Training Loss: 0.025043847039341927, Validation Loss: 0.02634209766983986\n",
      "Epoch 16, Batch: 216: Training Loss: 0.022030191496014595, Validation Loss: 0.0241814237087965\n",
      "Epoch 16, Batch: 217: Training Loss: 0.028276653960347176, Validation Loss: 0.022475991398096085\n",
      "Epoch 16, Batch: 218: Training Loss: 0.022678138688206673, Validation Loss: 0.02323838509619236\n",
      "Epoch 16, Batch: 219: Training Loss: 0.0233854278922081, Validation Loss: 0.02166217938065529\n",
      "Epoch 16, Batch: 220: Training Loss: 0.022710472345352173, Validation Loss: 0.02248980477452278\n",
      "Epoch 16, Batch: 221: Training Loss: 0.019906047731637955, Validation Loss: 0.02320091426372528\n",
      "Epoch 16, Batch: 222: Training Loss: 0.024425042793154716, Validation Loss: 0.024107344448566437\n",
      "Epoch 16, Batch: 223: Training Loss: 0.022144271060824394, Validation Loss: 0.02298707142472267\n",
      "Epoch 16, Batch: 224: Training Loss: 0.02495034784078598, Validation Loss: 0.023034721612930298\n",
      "Epoch 16, Batch: 225: Training Loss: 0.022807523608207703, Validation Loss: 0.02074606530368328\n",
      "Epoch 16, Batch: 226: Training Loss: 0.0230779517441988, Validation Loss: 0.023900650441646576\n",
      "Epoch 16, Batch: 227: Training Loss: 0.022788982838392258, Validation Loss: 0.024338752031326294\n",
      "Epoch 16, Batch: 228: Training Loss: 0.021791523322463036, Validation Loss: 0.02446380816400051\n",
      "Epoch 16, Batch: 229: Training Loss: 0.02629997394979, Validation Loss: 0.026502305641770363\n",
      "Epoch 16, Batch: 230: Training Loss: 0.020087772980332375, Validation Loss: 0.026262154802680016\n",
      "Epoch 16, Batch: 231: Training Loss: 0.0264261607080698, Validation Loss: 0.024423737078905106\n",
      "Epoch 16, Batch: 232: Training Loss: 0.024845188483595848, Validation Loss: 0.024183480069041252\n",
      "Epoch 16, Batch: 233: Training Loss: 0.019168296828866005, Validation Loss: 0.025242622941732407\n",
      "Epoch 16, Batch: 234: Training Loss: 0.026658011600375175, Validation Loss: 0.024949420243501663\n",
      "Epoch 16, Batch: 235: Training Loss: 0.02273550070822239, Validation Loss: 0.02428915537893772\n",
      "Epoch 16, Batch: 236: Training Loss: 0.02449467033147812, Validation Loss: 0.02578103542327881\n",
      "Epoch 16, Batch: 237: Training Loss: 0.02544821798801422, Validation Loss: 0.02346544712781906\n",
      "Epoch 16, Batch: 238: Training Loss: 0.023538941517472267, Validation Loss: 0.025852585211396217\n",
      "Epoch 16, Batch: 239: Training Loss: 0.022789031267166138, Validation Loss: 0.025793367996811867\n",
      "Epoch 16, Batch: 240: Training Loss: 0.022416308522224426, Validation Loss: 0.024139096960425377\n",
      "Epoch 16, Batch: 241: Training Loss: 0.021939165890216827, Validation Loss: 0.0245786365121603\n",
      "Epoch 16, Batch: 242: Training Loss: 0.02098125033080578, Validation Loss: 0.026956601068377495\n",
      "Epoch 16, Batch: 243: Training Loss: 0.0246170274913311, Validation Loss: 0.024515585973858833\n",
      "Epoch 16, Batch: 244: Training Loss: 0.02615051344037056, Validation Loss: 0.025687530636787415\n",
      "Epoch 16, Batch: 245: Training Loss: 0.025032492354512215, Validation Loss: 0.025915369391441345\n",
      "Epoch 16, Batch: 246: Training Loss: 0.024120954796671867, Validation Loss: 0.024263665080070496\n",
      "Epoch 16, Batch: 247: Training Loss: 0.025493528693914413, Validation Loss: 0.025674771517515182\n",
      "Epoch 16, Batch: 248: Training Loss: 0.02464229054749012, Validation Loss: 0.024579888209700584\n",
      "Epoch 16, Batch: 249: Training Loss: 0.02101944200694561, Validation Loss: 0.024418802931904793\n",
      "Epoch 16, Batch: 250: Training Loss: 0.02459000237286091, Validation Loss: 0.02299126610159874\n",
      "Epoch 16, Batch: 251: Training Loss: 0.020084980875253677, Validation Loss: 0.02427935041487217\n",
      "Epoch 16, Batch: 252: Training Loss: 0.02565833553671837, Validation Loss: 0.02398068457841873\n",
      "Epoch 16, Batch: 253: Training Loss: 0.02503930777311325, Validation Loss: 0.022670499980449677\n",
      "Epoch 16, Batch: 254: Training Loss: 0.020435547456145287, Validation Loss: 0.025999557226896286\n",
      "Epoch 16, Batch: 255: Training Loss: 0.026138080283999443, Validation Loss: 0.02324572764337063\n",
      "Epoch 16, Batch: 256: Training Loss: 0.022353800013661385, Validation Loss: 0.02299293503165245\n",
      "Epoch 16, Batch: 257: Training Loss: 0.02397499978542328, Validation Loss: 0.02451496757566929\n",
      "Epoch 16, Batch: 258: Training Loss: 0.02419806271791458, Validation Loss: 0.02400224469602108\n",
      "Epoch 16, Batch: 259: Training Loss: 0.02179604582488537, Validation Loss: 0.02339242771267891\n",
      "Epoch 16, Batch: 260: Training Loss: 0.027478810399770737, Validation Loss: 0.02442215196788311\n",
      "Epoch 16, Batch: 261: Training Loss: 0.02316528744995594, Validation Loss: 0.02393629215657711\n",
      "Epoch 16, Batch: 262: Training Loss: 0.02405916340649128, Validation Loss: 0.021589189767837524\n",
      "Epoch 16, Batch: 263: Training Loss: 0.019844528287649155, Validation Loss: 0.02246217057108879\n",
      "Epoch 16, Batch: 264: Training Loss: 0.01974993571639061, Validation Loss: 0.023340025916695595\n",
      "Epoch 16, Batch: 265: Training Loss: 0.023446690291166306, Validation Loss: 0.022325903177261353\n",
      "Epoch 16, Batch: 266: Training Loss: 0.020896397531032562, Validation Loss: 0.02536718174815178\n",
      "Epoch 16, Batch: 267: Training Loss: 0.021044380962848663, Validation Loss: 0.022753259167075157\n",
      "Epoch 16, Batch: 268: Training Loss: 0.020942045375704765, Validation Loss: 0.023205729201436043\n",
      "Epoch 16, Batch: 269: Training Loss: 0.021460838615894318, Validation Loss: 0.021825723350048065\n",
      "Epoch 16, Batch: 270: Training Loss: 0.023084156215190887, Validation Loss: 0.02294415608048439\n",
      "Epoch 16, Batch: 271: Training Loss: 0.019910259172320366, Validation Loss: 0.023667821660637856\n",
      "Epoch 16, Batch: 272: Training Loss: 0.021265525370836258, Validation Loss: 0.023835763335227966\n",
      "Epoch 16, Batch: 273: Training Loss: 0.02510182373225689, Validation Loss: 0.02394394390285015\n",
      "Epoch 16, Batch: 274: Training Loss: 0.022368382662534714, Validation Loss: 0.025719063356518745\n",
      "Epoch 16, Batch: 275: Training Loss: 0.022551266476511955, Validation Loss: 0.02408473752439022\n",
      "Epoch 16, Batch: 276: Training Loss: 0.020340029150247574, Validation Loss: 0.025923406705260277\n",
      "Epoch 16, Batch: 277: Training Loss: 0.021711140871047974, Validation Loss: 0.02334783785045147\n",
      "Epoch 16, Batch: 278: Training Loss: 0.022858910262584686, Validation Loss: 0.02419004961848259\n",
      "Epoch 16, Batch: 279: Training Loss: 0.02639143541455269, Validation Loss: 0.023346038535237312\n",
      "Epoch 16, Batch: 280: Training Loss: 0.01920143887400627, Validation Loss: 0.025449438020586967\n",
      "Epoch 16, Batch: 281: Training Loss: 0.022163890302181244, Validation Loss: 0.024290669709444046\n",
      "Epoch 16, Batch: 282: Training Loss: 0.021832747384905815, Validation Loss: 0.024088816717267036\n",
      "Epoch 16, Batch: 283: Training Loss: 0.023139867931604385, Validation Loss: 0.025221068412065506\n",
      "Epoch 16, Batch: 284: Training Loss: 0.025509875267744064, Validation Loss: 0.022999878972768784\n",
      "Epoch 16, Batch: 285: Training Loss: 0.0237771924585104, Validation Loss: 0.024219956248998642\n",
      "Epoch 16, Batch: 286: Training Loss: 0.02174154482781887, Validation Loss: 0.02048410102725029\n",
      "Epoch 16, Batch: 287: Training Loss: 0.023119909688830376, Validation Loss: 0.02093162201344967\n",
      "Epoch 16, Batch: 288: Training Loss: 0.02323065511882305, Validation Loss: 0.022675558924674988\n",
      "Epoch 16, Batch: 289: Training Loss: 0.022393150255084038, Validation Loss: 0.022881492972373962\n",
      "Epoch 16, Batch: 290: Training Loss: 0.01967456564307213, Validation Loss: 0.02111874148249626\n",
      "Epoch 16, Batch: 291: Training Loss: 0.02132289484143257, Validation Loss: 0.022774314507842064\n",
      "Epoch 16, Batch: 292: Training Loss: 0.024656832218170166, Validation Loss: 0.02334652468562126\n",
      "Epoch 16, Batch: 293: Training Loss: 0.02339024282991886, Validation Loss: 0.023050859570503235\n",
      "Epoch 16, Batch: 294: Training Loss: 0.021606072783470154, Validation Loss: 0.022997653111815453\n",
      "Epoch 16, Batch: 295: Training Loss: 0.022463975474238396, Validation Loss: 0.02455373853445053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 296: Training Loss: 0.02181546948850155, Validation Loss: 0.02323433943092823\n",
      "Epoch 16, Batch: 297: Training Loss: 0.023899171501398087, Validation Loss: 0.02286093309521675\n",
      "Epoch 16, Batch: 298: Training Loss: 0.023466747254133224, Validation Loss: 0.02382580190896988\n",
      "Epoch 16, Batch: 299: Training Loss: 0.020419416949152946, Validation Loss: 0.021191252395510674\n",
      "Epoch 16, Batch: 300: Training Loss: 0.02572840265929699, Validation Loss: 0.022615060210227966\n",
      "Epoch 16, Batch: 301: Training Loss: 0.020461643114686012, Validation Loss: 0.023677630349993706\n",
      "Epoch 16, Batch: 302: Training Loss: 0.020968591794371605, Validation Loss: 0.022898811846971512\n",
      "Epoch 16, Batch: 303: Training Loss: 0.021922919899225235, Validation Loss: 0.024165935814380646\n",
      "Epoch 16, Batch: 304: Training Loss: 0.021160904318094254, Validation Loss: 0.021950721740722656\n",
      "Epoch 16, Batch: 305: Training Loss: 0.018430041149258614, Validation Loss: 0.02168024331331253\n",
      "Epoch 16, Batch: 306: Training Loss: 0.021366877481341362, Validation Loss: 0.02330085076391697\n",
      "Epoch 16, Batch: 307: Training Loss: 0.021422313526272774, Validation Loss: 0.02349640056490898\n",
      "Epoch 16, Batch: 308: Training Loss: 0.02256130799651146, Validation Loss: 0.023169152438640594\n",
      "Epoch 16, Batch: 309: Training Loss: 0.021137183532118797, Validation Loss: 0.021947335451841354\n",
      "Epoch 16, Batch: 310: Training Loss: 0.023127345368266106, Validation Loss: 0.023100189864635468\n",
      "Epoch 16, Batch: 311: Training Loss: 0.01873166859149933, Validation Loss: 0.02272062376141548\n",
      "Epoch 16, Batch: 312: Training Loss: 0.022903483361005783, Validation Loss: 0.02357044816017151\n",
      "Epoch 16, Batch: 313: Training Loss: 0.016986824572086334, Validation Loss: 0.023246174678206444\n",
      "Epoch 16, Batch: 314: Training Loss: 0.02097976766526699, Validation Loss: 0.02280709706246853\n",
      "Epoch 16, Batch: 315: Training Loss: 0.021550826728343964, Validation Loss: 0.024414608255028725\n",
      "Epoch 16, Batch: 316: Training Loss: 0.022398879751563072, Validation Loss: 0.022871678695082664\n",
      "Epoch 16, Batch: 317: Training Loss: 0.021896623075008392, Validation Loss: 0.022419137880206108\n",
      "Epoch 16, Batch: 318: Training Loss: 0.01991300843656063, Validation Loss: 0.02362857572734356\n",
      "Epoch 16, Batch: 319: Training Loss: 0.021542221307754517, Validation Loss: 0.021054750308394432\n",
      "Epoch 16, Batch: 320: Training Loss: 0.021086404100060463, Validation Loss: 0.022817082703113556\n",
      "Epoch 16, Batch: 321: Training Loss: 0.023143693804740906, Validation Loss: 0.024948852136731148\n",
      "Epoch 16, Batch: 322: Training Loss: 0.021173609420657158, Validation Loss: 0.02254685014486313\n",
      "Epoch 16, Batch: 323: Training Loss: 0.020739680156111717, Validation Loss: 0.02276449091732502\n",
      "Epoch 16, Batch: 324: Training Loss: 0.02302786521613598, Validation Loss: 0.021427741274237633\n",
      "Epoch 16, Batch: 325: Training Loss: 0.020303983241319656, Validation Loss: 0.02303202636539936\n",
      "Epoch 16, Batch: 326: Training Loss: 0.023364506661891937, Validation Loss: 0.02361130341887474\n",
      "Epoch 16, Batch: 327: Training Loss: 0.019092297181487083, Validation Loss: 0.023356670513749123\n",
      "Epoch 16, Batch: 328: Training Loss: 0.02299327403306961, Validation Loss: 0.021359816193580627\n",
      "Epoch 16, Batch: 329: Training Loss: 0.020100640133023262, Validation Loss: 0.0239355880767107\n",
      "Saving new best model w/ loss: 0.019672254100441933\n",
      "Epoch 16, Batch: 330: Training Loss: 0.021739648655056953, Validation Loss: 0.019672254100441933\n",
      "Epoch 16, Batch: 331: Training Loss: 0.022858498618006706, Validation Loss: 0.020853301510214806\n",
      "Epoch 16, Batch: 332: Training Loss: 0.019080815836787224, Validation Loss: 0.02213265746831894\n",
      "Epoch 16, Batch: 333: Training Loss: 0.024085350334644318, Validation Loss: 0.0220938827842474\n",
      "Epoch 16, Batch: 334: Training Loss: 0.021540947258472443, Validation Loss: 0.020635155960917473\n",
      "Epoch 16, Batch: 335: Training Loss: 0.021241478621959686, Validation Loss: 0.022036010399460793\n",
      "Epoch 16, Batch: 336: Training Loss: 0.022578498348593712, Validation Loss: 0.021886218339204788\n",
      "Epoch 16, Batch: 337: Training Loss: 0.020106879994273186, Validation Loss: 0.020533109083771706\n",
      "Epoch 16, Batch: 338: Training Loss: 0.020485365763306618, Validation Loss: 0.02150154300034046\n",
      "Epoch 16, Batch: 339: Training Loss: 0.02311905100941658, Validation Loss: 0.022760476917028427\n",
      "Epoch 16, Batch: 340: Training Loss: 0.02437548339366913, Validation Loss: 0.022503726184368134\n",
      "Epoch 16, Batch: 341: Training Loss: 0.021579023450613022, Validation Loss: 0.022141695022583008\n",
      "Epoch 16, Batch: 342: Training Loss: 0.023104865103960037, Validation Loss: 0.0206783227622509\n",
      "Epoch 16, Batch: 343: Training Loss: 0.02281811647117138, Validation Loss: 0.022176045924425125\n",
      "Epoch 16, Batch: 344: Training Loss: 0.0209648497402668, Validation Loss: 0.020211007446050644\n",
      "Epoch 16, Batch: 345: Training Loss: 0.023157233372330666, Validation Loss: 0.022252164781093597\n",
      "Epoch 16, Batch: 346: Training Loss: 0.025723183527588844, Validation Loss: 0.022577138617634773\n",
      "Epoch 16, Batch: 347: Training Loss: 0.019017035141587257, Validation Loss: 0.023186208680272102\n",
      "Epoch 16, Batch: 348: Training Loss: 0.02375110611319542, Validation Loss: 0.023003919050097466\n",
      "Epoch 16, Batch: 349: Training Loss: 0.026514122262597084, Validation Loss: 0.023577360436320305\n",
      "Epoch 16, Batch: 350: Training Loss: 0.018793778494000435, Validation Loss: 0.02312040887773037\n",
      "Epoch 16, Batch: 351: Training Loss: 0.026339037343859673, Validation Loss: 0.023289980366826057\n",
      "Epoch 16, Batch: 352: Training Loss: 0.023190394043922424, Validation Loss: 0.021764911711215973\n",
      "Epoch 16, Batch: 353: Training Loss: 0.02331637404859066, Validation Loss: 0.02221565693616867\n",
      "Epoch 16, Batch: 354: Training Loss: 0.020989425480365753, Validation Loss: 0.020986713469028473\n",
      "Epoch 16, Batch: 355: Training Loss: 0.02283310331404209, Validation Loss: 0.02391967922449112\n",
      "Epoch 16, Batch: 356: Training Loss: 0.022706175222992897, Validation Loss: 0.02056076191365719\n",
      "Epoch 16, Batch: 357: Training Loss: 0.021463336423039436, Validation Loss: 0.021804457530379295\n",
      "Epoch 16, Batch: 358: Training Loss: 0.02411877177655697, Validation Loss: 0.02113368734717369\n",
      "Epoch 16, Batch: 359: Training Loss: 0.02274031564593315, Validation Loss: 0.020822731778025627\n",
      "Epoch 16, Batch: 360: Training Loss: 0.021825142204761505, Validation Loss: 0.02176998183131218\n",
      "Epoch 16, Batch: 361: Training Loss: 0.020757898688316345, Validation Loss: 0.022627010941505432\n",
      "Epoch 16, Batch: 362: Training Loss: 0.021723639219999313, Validation Loss: 0.023584820330142975\n",
      "Epoch 16, Batch: 363: Training Loss: 0.02621135115623474, Validation Loss: 0.021803410723805428\n",
      "Epoch 16, Batch: 364: Training Loss: 0.023074902594089508, Validation Loss: 0.022236939519643784\n",
      "Epoch 16, Batch: 365: Training Loss: 0.023227114230394363, Validation Loss: 0.02258981764316559\n",
      "Epoch 16, Batch: 366: Training Loss: 0.025536641478538513, Validation Loss: 0.02195601910352707\n",
      "Epoch 16, Batch: 367: Training Loss: 0.021122202277183533, Validation Loss: 0.024789748713374138\n",
      "Epoch 16, Batch: 368: Training Loss: 0.02000320889055729, Validation Loss: 0.022721000015735626\n",
      "Epoch 16, Batch: 369: Training Loss: 0.024071745574474335, Validation Loss: 0.0223684124648571\n",
      "Epoch 16, Batch: 370: Training Loss: 0.02134569175541401, Validation Loss: 0.023088380694389343\n",
      "Epoch 16, Batch: 371: Training Loss: 0.022506272420287132, Validation Loss: 0.024248715490102768\n",
      "Epoch 16, Batch: 372: Training Loss: 0.022548701614141464, Validation Loss: 0.021132443100214005\n",
      "Epoch 16, Batch: 373: Training Loss: 0.02361978404223919, Validation Loss: 0.02302377298474312\n",
      "Epoch 16, Batch: 374: Training Loss: 0.021231792867183685, Validation Loss: 0.021884985268115997\n",
      "Epoch 16, Batch: 375: Training Loss: 0.027384251356124878, Validation Loss: 0.025674257427453995\n",
      "Epoch 16, Batch: 376: Training Loss: 0.02274804562330246, Validation Loss: 0.024698203429579735\n",
      "Epoch 16, Batch: 377: Training Loss: 0.022131940349936485, Validation Loss: 0.023796387016773224\n",
      "Epoch 16, Batch: 378: Training Loss: 0.019460951909422874, Validation Loss: 0.02317255549132824\n",
      "Epoch 16, Batch: 379: Training Loss: 0.023063164204359055, Validation Loss: 0.02200467698276043\n",
      "Epoch 16, Batch: 380: Training Loss: 0.024776937440037727, Validation Loss: 0.02466495893895626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 381: Training Loss: 0.024757876992225647, Validation Loss: 0.01968151517212391\n",
      "Epoch 16, Batch: 382: Training Loss: 0.02303922548890114, Validation Loss: 0.02103961445391178\n",
      "Epoch 16, Batch: 383: Training Loss: 0.01978052593767643, Validation Loss: 0.022106671705842018\n",
      "Epoch 16, Batch: 384: Training Loss: 0.022518204525113106, Validation Loss: 0.024364063516259193\n",
      "Epoch 16, Batch: 385: Training Loss: 0.020165564492344856, Validation Loss: 0.023814817890524864\n",
      "Epoch 16, Batch: 386: Training Loss: 0.021351082250475883, Validation Loss: 0.023303845897316933\n",
      "Epoch 16, Batch: 387: Training Loss: 0.022534072399139404, Validation Loss: 0.025384776294231415\n",
      "Epoch 16, Batch: 388: Training Loss: 0.023005906492471695, Validation Loss: 0.024235881865024567\n",
      "Epoch 16, Batch: 389: Training Loss: 0.025502661243081093, Validation Loss: 0.023862196132540703\n",
      "Epoch 16, Batch: 390: Training Loss: 0.02293761819601059, Validation Loss: 0.023939277976751328\n",
      "Epoch 16, Batch: 391: Training Loss: 0.027044221758842468, Validation Loss: 0.02394658327102661\n",
      "Epoch 16, Batch: 392: Training Loss: 0.022897448390722275, Validation Loss: 0.021650204434990883\n",
      "Epoch 16, Batch: 393: Training Loss: 0.021209832280874252, Validation Loss: 0.02443779818713665\n",
      "Epoch 16, Batch: 394: Training Loss: 0.02212607115507126, Validation Loss: 0.022662140429019928\n",
      "Epoch 16, Batch: 395: Training Loss: 0.019030002877116203, Validation Loss: 0.02064438723027706\n",
      "Epoch 16, Batch: 396: Training Loss: 0.0222705639898777, Validation Loss: 0.02252332866191864\n",
      "Epoch 16, Batch: 397: Training Loss: 0.0228491872549057, Validation Loss: 0.02231057547032833\n",
      "Epoch 16, Batch: 398: Training Loss: 0.020145105198025703, Validation Loss: 0.02222810685634613\n",
      "Epoch 16, Batch: 399: Training Loss: 0.02107158675789833, Validation Loss: 0.02156459167599678\n",
      "Epoch 16, Batch: 400: Training Loss: 0.022008301690220833, Validation Loss: 0.021860362961888313\n",
      "Epoch 16, Batch: 401: Training Loss: 0.01896601915359497, Validation Loss: 0.02176586538553238\n",
      "Epoch 16, Batch: 402: Training Loss: 0.02318526618182659, Validation Loss: 0.02122425101697445\n",
      "Epoch 16, Batch: 403: Training Loss: 0.02583969384431839, Validation Loss: 0.023686345666646957\n",
      "Epoch 16, Batch: 404: Training Loss: 0.01972970925271511, Validation Loss: 0.022909317165613174\n",
      "Epoch 16, Batch: 405: Training Loss: 0.020219724625349045, Validation Loss: 0.022918667644262314\n",
      "Epoch 16, Batch: 406: Training Loss: 0.02321765385568142, Validation Loss: 0.023275716230273247\n",
      "Epoch 16, Batch: 407: Training Loss: 0.021973147988319397, Validation Loss: 0.022969139739871025\n",
      "Epoch 16, Batch: 408: Training Loss: 0.020845361053943634, Validation Loss: 0.02190902829170227\n",
      "Epoch 16, Batch: 409: Training Loss: 0.02249833382666111, Validation Loss: 0.022716321051120758\n",
      "Epoch 16, Batch: 410: Training Loss: 0.024851227179169655, Validation Loss: 0.02238134853541851\n",
      "Epoch 16, Batch: 411: Training Loss: 0.022059990093111992, Validation Loss: 0.022451890632510185\n",
      "Epoch 16, Batch: 412: Training Loss: 0.029463302344083786, Validation Loss: 0.023655401542782784\n",
      "Epoch 16, Batch: 413: Training Loss: 0.02554556168615818, Validation Loss: 0.02328435145318508\n",
      "Epoch 16, Batch: 414: Training Loss: 0.029105549678206444, Validation Loss: 0.022901663556694984\n",
      "Epoch 16, Batch: 415: Training Loss: 0.023501761257648468, Validation Loss: 0.023754287511110306\n",
      "Epoch 16, Batch: 416: Training Loss: 0.021236922591924667, Validation Loss: 0.02332707867026329\n",
      "Epoch 16, Batch: 417: Training Loss: 0.02115524560213089, Validation Loss: 0.023044917732477188\n",
      "Epoch 16, Batch: 418: Training Loss: 0.023196356371045113, Validation Loss: 0.02402517944574356\n",
      "Epoch 16, Batch: 419: Training Loss: 0.022124003618955612, Validation Loss: 0.02352108620107174\n",
      "Epoch 16, Batch: 420: Training Loss: 0.022723037749528885, Validation Loss: 0.024252980947494507\n",
      "Epoch 16, Batch: 421: Training Loss: 0.024007443338632584, Validation Loss: 0.02526700310409069\n",
      "Epoch 16, Batch: 422: Training Loss: 0.026986252516508102, Validation Loss: 0.024350591003894806\n",
      "Epoch 16, Batch: 423: Training Loss: 0.02651325613260269, Validation Loss: 0.023639492690563202\n",
      "Epoch 16, Batch: 424: Training Loss: 0.022512096911668777, Validation Loss: 0.02269747480750084\n",
      "Epoch 16, Batch: 425: Training Loss: 0.02255670540034771, Validation Loss: 0.02246714197099209\n",
      "Epoch 16, Batch: 426: Training Loss: 0.019864525645971298, Validation Loss: 0.022683337330818176\n",
      "Epoch 16, Batch: 427: Training Loss: 0.02344554290175438, Validation Loss: 0.02123880758881569\n",
      "Epoch 16, Batch: 428: Training Loss: 0.023139039054512978, Validation Loss: 0.024104630574584007\n",
      "Epoch 16, Batch: 429: Training Loss: 0.023534195497632027, Validation Loss: 0.021956322714686394\n",
      "Epoch 16, Batch: 430: Training Loss: 0.02322743460536003, Validation Loss: 0.022987637668848038\n",
      "Epoch 16, Batch: 431: Training Loss: 0.021171947941184044, Validation Loss: 0.023071881383657455\n",
      "Epoch 16, Batch: 432: Training Loss: 0.025037746876478195, Validation Loss: 0.022135401144623756\n",
      "Epoch 16, Batch: 433: Training Loss: 0.022609157487750053, Validation Loss: 0.023047372698783875\n",
      "Epoch 16, Batch: 434: Training Loss: 0.02102740667760372, Validation Loss: 0.021327953785657883\n",
      "Epoch 16, Batch: 435: Training Loss: 0.02150350622832775, Validation Loss: 0.023291703313589096\n",
      "Epoch 16, Batch: 436: Training Loss: 0.02618309110403061, Validation Loss: 0.022407036274671555\n",
      "Epoch 16, Batch: 437: Training Loss: 0.020390795543789864, Validation Loss: 0.023726727813482285\n",
      "Epoch 16, Batch: 438: Training Loss: 0.02305968478322029, Validation Loss: 0.024431494995951653\n",
      "Epoch 16, Batch: 439: Training Loss: 0.019890518859028816, Validation Loss: 0.021413005888462067\n",
      "Epoch 16, Batch: 440: Training Loss: 0.025128396227955818, Validation Loss: 0.022220121696591377\n",
      "Epoch 16, Batch: 441: Training Loss: 0.022304031997919083, Validation Loss: 0.02037750370800495\n",
      "Epoch 16, Batch: 442: Training Loss: 0.026728207245469093, Validation Loss: 0.024628177285194397\n",
      "Epoch 16, Batch: 443: Training Loss: 0.021597350016236305, Validation Loss: 0.022994529455900192\n",
      "Epoch 16, Batch: 444: Training Loss: 0.0220294538885355, Validation Loss: 0.021527105942368507\n",
      "Epoch 16, Batch: 445: Training Loss: 0.02127482369542122, Validation Loss: 0.02320924587547779\n",
      "Epoch 16, Batch: 446: Training Loss: 0.026155462488532066, Validation Loss: 0.022886205464601517\n",
      "Epoch 16, Batch: 447: Training Loss: 0.024937357753515244, Validation Loss: 0.024528881534934044\n",
      "Epoch 16, Batch: 448: Training Loss: 0.024267306551337242, Validation Loss: 0.022946050390601158\n",
      "Epoch 16, Batch: 449: Training Loss: 0.022959843277931213, Validation Loss: 0.022251946851611137\n",
      "Epoch 16, Batch: 450: Training Loss: 0.021628282964229584, Validation Loss: 0.023413175716996193\n",
      "Epoch 16, Batch: 451: Training Loss: 0.02299327589571476, Validation Loss: 0.025449130684137344\n",
      "Epoch 16, Batch: 452: Training Loss: 0.026882754638791084, Validation Loss: 0.02247265726327896\n",
      "Epoch 16, Batch: 453: Training Loss: 0.023351453244686127, Validation Loss: 0.02476383000612259\n",
      "Epoch 16, Batch: 454: Training Loss: 0.02306284010410309, Validation Loss: 0.022269509732723236\n",
      "Epoch 16, Batch: 455: Training Loss: 0.021972347050905228, Validation Loss: 0.02407275140285492\n",
      "Epoch 16, Batch: 456: Training Loss: 0.022854531183838844, Validation Loss: 0.022863546386361122\n",
      "Epoch 16, Batch: 457: Training Loss: 0.025812985375523567, Validation Loss: 0.021102163940668106\n",
      "Epoch 16, Batch: 458: Training Loss: 0.017964044585824013, Validation Loss: 0.022017288953065872\n",
      "Epoch 16, Batch: 459: Training Loss: 0.02750307321548462, Validation Loss: 0.023909762501716614\n",
      "Epoch 16, Batch: 460: Training Loss: 0.02254209667444229, Validation Loss: 0.020317457616329193\n",
      "Epoch 16, Batch: 461: Training Loss: 0.023398512974381447, Validation Loss: 0.021280355751514435\n",
      "Epoch 16, Batch: 462: Training Loss: 0.02041153982281685, Validation Loss: 0.020702963694930077\n",
      "Epoch 16, Batch: 463: Training Loss: 0.024058865383267403, Validation Loss: 0.021655846387147903\n",
      "Epoch 16, Batch: 464: Training Loss: 0.021994762122631073, Validation Loss: 0.02045063115656376\n",
      "Epoch 16, Batch: 465: Training Loss: 0.024490613490343094, Validation Loss: 0.021726181730628014\n",
      "Epoch 16, Batch: 466: Training Loss: 0.021833673119544983, Validation Loss: 0.021542444825172424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch: 467: Training Loss: 0.029760664328932762, Validation Loss: 0.021765440702438354\n",
      "Epoch 16, Batch: 468: Training Loss: 0.027353787794709206, Validation Loss: 0.02420191839337349\n",
      "Epoch 16, Batch: 469: Training Loss: 0.021719256415963173, Validation Loss: 0.025216421112418175\n",
      "Epoch 16, Batch: 470: Training Loss: 0.023950712755322456, Validation Loss: 0.024450363591313362\n",
      "Epoch 16, Batch: 471: Training Loss: 0.026472140103578568, Validation Loss: 0.022964175790548325\n",
      "Epoch 16, Batch: 472: Training Loss: 0.024411793798208237, Validation Loss: 0.024647768586874008\n",
      "Epoch 16, Batch: 473: Training Loss: 0.02293241210281849, Validation Loss: 0.025595705956220627\n",
      "Epoch 16, Batch: 474: Training Loss: 0.024144699797034264, Validation Loss: 0.025463830679655075\n",
      "Epoch 16, Batch: 475: Training Loss: 0.025923507288098335, Validation Loss: 0.021771270781755447\n",
      "Epoch 16, Batch: 476: Training Loss: 0.0257682166993618, Validation Loss: 0.025847915560007095\n",
      "Epoch 16, Batch: 477: Training Loss: 0.023856086656451225, Validation Loss: 0.024961303919553757\n",
      "Epoch 16, Batch: 478: Training Loss: 0.024168115109205246, Validation Loss: 0.02620590105652809\n",
      "Epoch 16, Batch: 479: Training Loss: 0.02668045461177826, Validation Loss: 0.026593249291181564\n",
      "Epoch 16, Batch: 480: Training Loss: 0.024719886481761932, Validation Loss: 0.025454113259911537\n",
      "Epoch 16, Batch: 481: Training Loss: 0.021748710423707962, Validation Loss: 0.024461304768919945\n",
      "Epoch 16, Batch: 482: Training Loss: 0.026554832234978676, Validation Loss: 0.023974105715751648\n",
      "Epoch 16, Batch: 483: Training Loss: 0.02201722003519535, Validation Loss: 0.022988678887486458\n",
      "Epoch 16, Batch: 484: Training Loss: 0.024297840893268585, Validation Loss: 0.022777071222662926\n",
      "Epoch 16, Batch: 485: Training Loss: 0.02373853139579296, Validation Loss: 0.022652778774499893\n",
      "Epoch 16, Batch: 486: Training Loss: 0.02637888863682747, Validation Loss: 0.025380630046129227\n",
      "Epoch 16, Batch: 487: Training Loss: 0.021487213671207428, Validation Loss: 0.02226538583636284\n",
      "Epoch 16, Batch: 488: Training Loss: 0.024309225380420685, Validation Loss: 0.023323804140090942\n",
      "Epoch 16, Batch: 489: Training Loss: 0.025623943656682968, Validation Loss: 0.021431805565953255\n",
      "Epoch 16, Batch: 490: Training Loss: 0.02601626142859459, Validation Loss: 0.022382766008377075\n",
      "Epoch 16, Batch: 491: Training Loss: 0.019736431539058685, Validation Loss: 0.022838499397039413\n",
      "Epoch 16, Batch: 492: Training Loss: 0.0266994871199131, Validation Loss: 0.023519260808825493\n",
      "Epoch 16, Batch: 493: Training Loss: 0.02404264733195305, Validation Loss: 0.02176506631076336\n",
      "Epoch 16, Batch: 494: Training Loss: 0.02414034493267536, Validation Loss: 0.0227571539580822\n",
      "Epoch 16, Batch: 495: Training Loss: 0.02187403477728367, Validation Loss: 0.023311732336878777\n",
      "Epoch 16, Batch: 496: Training Loss: 0.022098423913121223, Validation Loss: 0.02191261760890484\n",
      "Epoch 16, Batch: 497: Training Loss: 0.019673271104693413, Validation Loss: 0.02253454551100731\n",
      "Epoch 16, Batch: 498: Training Loss: 0.02348640188574791, Validation Loss: 0.02126379869878292\n",
      "Epoch 16, Batch: 499: Training Loss: 0.02092287316918373, Validation Loss: 0.022939184680581093\n",
      "Epoch 17, Batch: 0: Training Loss: 0.01921430602669716, Validation Loss: 0.023301314562559128\n",
      "Saving new best model w/ loss: 0.019667068496346474\n",
      "Epoch 17, Batch: 1: Training Loss: 0.0204808097332716, Validation Loss: 0.019667068496346474\n",
      "Epoch 17, Batch: 2: Training Loss: 0.026674753054976463, Validation Loss: 0.023510407656431198\n",
      "Epoch 17, Batch: 3: Training Loss: 0.02221548929810524, Validation Loss: 0.023332171142101288\n",
      "Epoch 17, Batch: 4: Training Loss: 0.020241310819983482, Validation Loss: 0.024496186524629593\n",
      "Epoch 17, Batch: 5: Training Loss: 0.019870173186063766, Validation Loss: 0.023692646995186806\n",
      "Epoch 17, Batch: 6: Training Loss: 0.02236351929605007, Validation Loss: 0.024480294436216354\n",
      "Epoch 17, Batch: 7: Training Loss: 0.022514311596751213, Validation Loss: 0.02309739962220192\n",
      "Epoch 17, Batch: 8: Training Loss: 0.022841261699795723, Validation Loss: 0.023296212777495384\n",
      "Epoch 17, Batch: 9: Training Loss: 0.02311853878200054, Validation Loss: 0.02317078970372677\n",
      "Epoch 17, Batch: 10: Training Loss: 0.021825416013598442, Validation Loss: 0.023133745416998863\n",
      "Epoch 17, Batch: 11: Training Loss: 0.023417040705680847, Validation Loss: 0.023475313559174538\n",
      "Epoch 17, Batch: 12: Training Loss: 0.025747330859303474, Validation Loss: 0.023840760812163353\n",
      "Epoch 17, Batch: 13: Training Loss: 0.027372363954782486, Validation Loss: 0.027264658361673355\n",
      "Epoch 17, Batch: 14: Training Loss: 0.024019375443458557, Validation Loss: 0.024884728714823723\n",
      "Epoch 17, Batch: 15: Training Loss: 0.022868897765874863, Validation Loss: 0.026472648605704308\n",
      "Epoch 17, Batch: 16: Training Loss: 0.024748848751187325, Validation Loss: 0.024936310946941376\n",
      "Epoch 17, Batch: 17: Training Loss: 0.02063503861427307, Validation Loss: 0.025184068828821182\n",
      "Epoch 17, Batch: 18: Training Loss: 0.019659671932458878, Validation Loss: 0.025143546983599663\n",
      "Epoch 17, Batch: 19: Training Loss: 0.023731732740998268, Validation Loss: 0.024735944345593452\n",
      "Epoch 17, Batch: 20: Training Loss: 0.027076685801148415, Validation Loss: 0.023744260892271996\n",
      "Epoch 17, Batch: 21: Training Loss: 0.021863119676709175, Validation Loss: 0.02574174851179123\n",
      "Epoch 17, Batch: 22: Training Loss: 0.0223509781062603, Validation Loss: 0.02557840757071972\n",
      "Epoch 17, Batch: 23: Training Loss: 0.022148795425891876, Validation Loss: 0.023018455132842064\n",
      "Epoch 17, Batch: 24: Training Loss: 0.024522144347429276, Validation Loss: 0.02359902486205101\n",
      "Epoch 17, Batch: 25: Training Loss: 0.02199551649391651, Validation Loss: 0.024754980579018593\n",
      "Epoch 17, Batch: 26: Training Loss: 0.021365854889154434, Validation Loss: 0.022686777636408806\n",
      "Epoch 17, Batch: 27: Training Loss: 0.02354832924902439, Validation Loss: 0.02393326535820961\n",
      "Epoch 17, Batch: 28: Training Loss: 0.0253966823220253, Validation Loss: 0.023589609190821648\n",
      "Epoch 17, Batch: 29: Training Loss: 0.02133304625749588, Validation Loss: 0.02275092527270317\n",
      "Epoch 17, Batch: 30: Training Loss: 0.023141663521528244, Validation Loss: 0.026414450258016586\n",
      "Epoch 17, Batch: 31: Training Loss: 0.027760902419686317, Validation Loss: 0.02536677196621895\n",
      "Epoch 17, Batch: 32: Training Loss: 0.02531696856021881, Validation Loss: 0.02322613075375557\n",
      "Epoch 17, Batch: 33: Training Loss: 0.021252894774079323, Validation Loss: 0.02398422546684742\n",
      "Epoch 17, Batch: 34: Training Loss: 0.02113744616508484, Validation Loss: 0.02288985811173916\n",
      "Epoch 17, Batch: 35: Training Loss: 0.022819573059678078, Validation Loss: 0.023937517777085304\n",
      "Epoch 17, Batch: 36: Training Loss: 0.021685658022761345, Validation Loss: 0.024584554135799408\n",
      "Epoch 17, Batch: 37: Training Loss: 0.024767253547906876, Validation Loss: 0.02465120144188404\n",
      "Epoch 17, Batch: 38: Training Loss: 0.0231929961591959, Validation Loss: 0.02610396407544613\n",
      "Epoch 17, Batch: 39: Training Loss: 0.025554006919264793, Validation Loss: 0.024601662531495094\n",
      "Epoch 17, Batch: 40: Training Loss: 0.02492765709757805, Validation Loss: 0.02480207197368145\n",
      "Epoch 17, Batch: 41: Training Loss: 0.02254514954984188, Validation Loss: 0.026311734691262245\n",
      "Epoch 17, Batch: 42: Training Loss: 0.020941806957125664, Validation Loss: 0.023815656080842018\n",
      "Epoch 17, Batch: 43: Training Loss: 0.020087629556655884, Validation Loss: 0.027082102373242378\n",
      "Epoch 17, Batch: 44: Training Loss: 0.02188151143491268, Validation Loss: 0.02646709233522415\n",
      "Epoch 17, Batch: 45: Training Loss: 0.023403871804475784, Validation Loss: 0.024733269587159157\n",
      "Epoch 17, Batch: 46: Training Loss: 0.023839851841330528, Validation Loss: 0.027494356036186218\n",
      "Epoch 17, Batch: 47: Training Loss: 0.021913543343544006, Validation Loss: 0.026934301480650902\n",
      "Epoch 17, Batch: 48: Training Loss: 0.026929179206490517, Validation Loss: 0.024792201817035675\n",
      "Epoch 17, Batch: 49: Training Loss: 0.021127969026565552, Validation Loss: 0.026112738996744156\n",
      "Epoch 17, Batch: 50: Training Loss: 0.02213309146463871, Validation Loss: 0.025060830637812614\n",
      "Epoch 17, Batch: 51: Training Loss: 0.02267979271709919, Validation Loss: 0.024772893637418747\n",
      "Epoch 17, Batch: 52: Training Loss: 0.02148151583969593, Validation Loss: 0.02706867828965187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 53: Training Loss: 0.025228844955563545, Validation Loss: 0.027030663564801216\n",
      "Epoch 17, Batch: 54: Training Loss: 0.0236872136592865, Validation Loss: 0.02372673712670803\n",
      "Epoch 17, Batch: 55: Training Loss: 0.019556924700737, Validation Loss: 0.02654237113893032\n",
      "Epoch 17, Batch: 56: Training Loss: 0.024572135880589485, Validation Loss: 0.024274976924061775\n",
      "Epoch 17, Batch: 57: Training Loss: 0.021371720358729362, Validation Loss: 0.022725243121385574\n",
      "Epoch 17, Batch: 58: Training Loss: 0.020725080743432045, Validation Loss: 0.023239078000187874\n",
      "Epoch 17, Batch: 59: Training Loss: 0.02114902436733246, Validation Loss: 0.025489890947937965\n",
      "Epoch 17, Batch: 60: Training Loss: 0.02240985631942749, Validation Loss: 0.02622917667031288\n",
      "Epoch 17, Batch: 61: Training Loss: 0.02461603470146656, Validation Loss: 0.02441810816526413\n",
      "Epoch 17, Batch: 62: Training Loss: 0.02189825102686882, Validation Loss: 0.024108193814754486\n",
      "Epoch 17, Batch: 63: Training Loss: 0.027486566454172134, Validation Loss: 0.027091335505247116\n",
      "Epoch 17, Batch: 64: Training Loss: 0.02374969981610775, Validation Loss: 0.023232625797390938\n",
      "Epoch 17, Batch: 65: Training Loss: 0.024959325790405273, Validation Loss: 0.023647192865610123\n",
      "Epoch 17, Batch: 66: Training Loss: 0.024671612307429314, Validation Loss: 0.020363766700029373\n",
      "Epoch 17, Batch: 67: Training Loss: 0.019834568724036217, Validation Loss: 0.023385750129818916\n",
      "Epoch 17, Batch: 68: Training Loss: 0.024556342512369156, Validation Loss: 0.02554086223244667\n",
      "Epoch 17, Batch: 69: Training Loss: 0.02318347431719303, Validation Loss: 0.023226052522659302\n",
      "Epoch 17, Batch: 70: Training Loss: 0.02731163054704666, Validation Loss: 0.0222416203469038\n",
      "Epoch 17, Batch: 71: Training Loss: 0.02424713410437107, Validation Loss: 0.0240895114839077\n",
      "Epoch 17, Batch: 72: Training Loss: 0.02217433974146843, Validation Loss: 0.02505090832710266\n",
      "Epoch 17, Batch: 73: Training Loss: 0.024022161960601807, Validation Loss: 0.025544894859194756\n",
      "Epoch 17, Batch: 74: Training Loss: 0.02096983976662159, Validation Loss: 0.023686576634645462\n",
      "Epoch 17, Batch: 75: Training Loss: 0.018978876993060112, Validation Loss: 0.02345067448914051\n",
      "Epoch 17, Batch: 76: Training Loss: 0.02370111271739006, Validation Loss: 0.024763014167547226\n",
      "Epoch 17, Batch: 77: Training Loss: 0.027563802897930145, Validation Loss: 0.02727428823709488\n",
      "Epoch 17, Batch: 78: Training Loss: 0.02735172212123871, Validation Loss: 0.0250055231153965\n",
      "Epoch 17, Batch: 79: Training Loss: 0.023827260360121727, Validation Loss: 0.02686234936118126\n",
      "Epoch 17, Batch: 80: Training Loss: 0.023178091272711754, Validation Loss: 0.026460833847522736\n",
      "Epoch 17, Batch: 81: Training Loss: 0.02281002141535282, Validation Loss: 0.026447683572769165\n",
      "Epoch 17, Batch: 82: Training Loss: 0.024258974939584732, Validation Loss: 0.02549908496439457\n",
      "Epoch 17, Batch: 83: Training Loss: 0.020290900021791458, Validation Loss: 0.02528054267168045\n",
      "Epoch 17, Batch: 84: Training Loss: 0.025563357397913933, Validation Loss: 0.026340913027524948\n",
      "Epoch 17, Batch: 85: Training Loss: 0.02062143385410309, Validation Loss: 0.024502677842974663\n",
      "Epoch 17, Batch: 86: Training Loss: 0.026902956888079643, Validation Loss: 0.025328224524855614\n",
      "Epoch 17, Batch: 87: Training Loss: 0.02807377651333809, Validation Loss: 0.026132607832551003\n",
      "Epoch 17, Batch: 88: Training Loss: 0.021848056465387344, Validation Loss: 0.024782095104455948\n",
      "Epoch 17, Batch: 89: Training Loss: 0.024230943992733955, Validation Loss: 0.02399941347539425\n",
      "Epoch 17, Batch: 90: Training Loss: 0.02183595485985279, Validation Loss: 0.023877955973148346\n",
      "Epoch 17, Batch: 91: Training Loss: 0.023396139964461327, Validation Loss: 0.023521151393651962\n",
      "Epoch 17, Batch: 92: Training Loss: 0.025606872513890266, Validation Loss: 0.021418454125523567\n",
      "Epoch 17, Batch: 93: Training Loss: 0.023786701261997223, Validation Loss: 0.023297542706131935\n",
      "Epoch 17, Batch: 94: Training Loss: 0.027452144771814346, Validation Loss: 0.022784551605582237\n",
      "Epoch 17, Batch: 95: Training Loss: 0.019511131569743156, Validation Loss: 0.023428665474057198\n",
      "Epoch 17, Batch: 96: Training Loss: 0.024610625579953194, Validation Loss: 0.02311500534415245\n",
      "Epoch 17, Batch: 97: Training Loss: 0.025143535807728767, Validation Loss: 0.02248154953122139\n",
      "Epoch 17, Batch: 98: Training Loss: 0.026502100750803947, Validation Loss: 0.021806493401527405\n",
      "Epoch 17, Batch: 99: Training Loss: 0.024977490305900574, Validation Loss: 0.02163134142756462\n",
      "Epoch 17, Batch: 100: Training Loss: 0.026641804724931717, Validation Loss: 0.021528493613004684\n",
      "Epoch 17, Batch: 101: Training Loss: 0.022662995383143425, Validation Loss: 0.022835031151771545\n",
      "Epoch 17, Batch: 102: Training Loss: 0.022237850353121758, Validation Loss: 0.024180935695767403\n",
      "Epoch 17, Batch: 103: Training Loss: 0.02788737043738365, Validation Loss: 0.023889878764748573\n",
      "Epoch 17, Batch: 104: Training Loss: 0.021139051765203476, Validation Loss: 0.025163425132632256\n",
      "Epoch 17, Batch: 105: Training Loss: 0.019892800599336624, Validation Loss: 0.021167634055018425\n",
      "Epoch 17, Batch: 106: Training Loss: 0.021644141525030136, Validation Loss: 0.021789228543639183\n",
      "Epoch 17, Batch: 107: Training Loss: 0.026865683495998383, Validation Loss: 0.022146068513393402\n",
      "Epoch 17, Batch: 108: Training Loss: 0.023518817499279976, Validation Loss: 0.02216903120279312\n",
      "Epoch 17, Batch: 109: Training Loss: 0.02441173978149891, Validation Loss: 0.022106926888227463\n",
      "Epoch 17, Batch: 110: Training Loss: 0.025519121438264847, Validation Loss: 0.021596504375338554\n",
      "Epoch 17, Batch: 111: Training Loss: 0.02522859163582325, Validation Loss: 0.023633699864149094\n",
      "Epoch 17, Batch: 112: Training Loss: 0.021845709532499313, Validation Loss: 0.02395877055823803\n",
      "Epoch 17, Batch: 113: Training Loss: 0.025720050558447838, Validation Loss: 0.02330315113067627\n",
      "Epoch 17, Batch: 114: Training Loss: 0.020688241347670555, Validation Loss: 0.02567446231842041\n",
      "Epoch 17, Batch: 115: Training Loss: 0.025428196415305138, Validation Loss: 0.023434748873114586\n",
      "Epoch 17, Batch: 116: Training Loss: 0.020981047302484512, Validation Loss: 0.024525577202439308\n",
      "Epoch 17, Batch: 117: Training Loss: 0.02386350929737091, Validation Loss: 0.025078479200601578\n",
      "Epoch 17, Batch: 118: Training Loss: 0.022954769432544708, Validation Loss: 0.024266114458441734\n",
      "Epoch 17, Batch: 119: Training Loss: 0.026372667402029037, Validation Loss: 0.024218369275331497\n",
      "Epoch 17, Batch: 120: Training Loss: 0.024169297888875008, Validation Loss: 0.022974805906414986\n",
      "Epoch 17, Batch: 121: Training Loss: 0.02546374313533306, Validation Loss: 0.02412613295018673\n",
      "Epoch 17, Batch: 122: Training Loss: 0.025417137891054153, Validation Loss: 0.024726351723074913\n",
      "Epoch 17, Batch: 123: Training Loss: 0.0237480066716671, Validation Loss: 0.02213459275662899\n",
      "Epoch 17, Batch: 124: Training Loss: 0.022919366136193275, Validation Loss: 0.024829303845763206\n",
      "Epoch 17, Batch: 125: Training Loss: 0.0227386225014925, Validation Loss: 0.025811800733208656\n",
      "Epoch 17, Batch: 126: Training Loss: 0.021175431087613106, Validation Loss: 0.022493280470371246\n",
      "Epoch 17, Batch: 127: Training Loss: 0.022801129147410393, Validation Loss: 0.025362679734826088\n",
      "Epoch 17, Batch: 128: Training Loss: 0.023248303681612015, Validation Loss: 0.02373296581208706\n",
      "Epoch 17, Batch: 129: Training Loss: 0.01977475918829441, Validation Loss: 0.02534833364188671\n",
      "Epoch 17, Batch: 130: Training Loss: 0.0236881785094738, Validation Loss: 0.023241760209202766\n",
      "Epoch 17, Batch: 131: Training Loss: 0.026582345366477966, Validation Loss: 0.024500953033566475\n",
      "Epoch 17, Batch: 132: Training Loss: 0.02242627926170826, Validation Loss: 0.02257612533867359\n",
      "Epoch 17, Batch: 133: Training Loss: 0.02182711288332939, Validation Loss: 0.023168956860899925\n",
      "Epoch 17, Batch: 134: Training Loss: 0.026428254321217537, Validation Loss: 0.023721985518932343\n",
      "Epoch 17, Batch: 135: Training Loss: 0.02563720941543579, Validation Loss: 0.023480424657464027\n",
      "Epoch 17, Batch: 136: Training Loss: 0.022236188873648643, Validation Loss: 0.02472255565226078\n",
      "Epoch 17, Batch: 137: Training Loss: 0.02334168553352356, Validation Loss: 0.021547695621848106\n",
      "Epoch 17, Batch: 138: Training Loss: 0.02522743120789528, Validation Loss: 0.02224786952137947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 139: Training Loss: 0.020833570510149002, Validation Loss: 0.019911227747797966\n",
      "Epoch 17, Batch: 140: Training Loss: 0.027186306193470955, Validation Loss: 0.02419593743979931\n",
      "Epoch 17, Batch: 141: Training Loss: 0.022788645699620247, Validation Loss: 0.02442026138305664\n",
      "Epoch 17, Batch: 142: Training Loss: 0.023514868691563606, Validation Loss: 0.023426620289683342\n",
      "Epoch 17, Batch: 143: Training Loss: 0.021765606477856636, Validation Loss: 0.025996966287493706\n",
      "Epoch 17, Batch: 144: Training Loss: 0.022255921736359596, Validation Loss: 0.024406906217336655\n",
      "Epoch 17, Batch: 145: Training Loss: 0.023596471175551414, Validation Loss: 0.02482498623430729\n",
      "Epoch 17, Batch: 146: Training Loss: 0.025242798030376434, Validation Loss: 0.026286322623491287\n",
      "Epoch 17, Batch: 147: Training Loss: 0.024825632572174072, Validation Loss: 0.026018979027867317\n",
      "Epoch 17, Batch: 148: Training Loss: 0.022124797105789185, Validation Loss: 0.024900799617171288\n",
      "Epoch 17, Batch: 149: Training Loss: 0.025703642517328262, Validation Loss: 0.023852121084928513\n",
      "Epoch 17, Batch: 150: Training Loss: 0.024657489731907845, Validation Loss: 0.02599393017590046\n",
      "Epoch 17, Batch: 151: Training Loss: 0.022485673427581787, Validation Loss: 0.024383461102843285\n",
      "Epoch 17, Batch: 152: Training Loss: 0.025694046169519424, Validation Loss: 0.025006508454680443\n",
      "Epoch 17, Batch: 153: Training Loss: 0.022113565355539322, Validation Loss: 0.025131940841674805\n",
      "Epoch 17, Batch: 154: Training Loss: 0.024546759203076363, Validation Loss: 0.02710789255797863\n",
      "Epoch 17, Batch: 155: Training Loss: 0.026808584108948708, Validation Loss: 0.023849504068493843\n",
      "Epoch 17, Batch: 156: Training Loss: 0.023519478738307953, Validation Loss: 0.02482893504202366\n",
      "Epoch 17, Batch: 157: Training Loss: 0.01914347894489765, Validation Loss: 0.027741899713873863\n",
      "Epoch 17, Batch: 158: Training Loss: 0.021491240710020065, Validation Loss: 0.024058319628238678\n",
      "Epoch 17, Batch: 159: Training Loss: 0.022864757105708122, Validation Loss: 0.024196133017539978\n",
      "Epoch 17, Batch: 160: Training Loss: 0.02146797440946102, Validation Loss: 0.023414555937051773\n",
      "Epoch 17, Batch: 161: Training Loss: 0.024901937693357468, Validation Loss: 0.02340449020266533\n",
      "Epoch 17, Batch: 162: Training Loss: 0.023510141298174858, Validation Loss: 0.02495885267853737\n",
      "Epoch 17, Batch: 163: Training Loss: 0.0256748478859663, Validation Loss: 0.024813750758767128\n",
      "Epoch 17, Batch: 164: Training Loss: 0.026994841173291206, Validation Loss: 0.0244124922901392\n",
      "Epoch 17, Batch: 165: Training Loss: 0.023992009460926056, Validation Loss: 0.02402658760547638\n",
      "Epoch 17, Batch: 166: Training Loss: 0.021210219711065292, Validation Loss: 0.024829799309372902\n",
      "Epoch 17, Batch: 167: Training Loss: 0.019878998398780823, Validation Loss: 0.02464376948773861\n",
      "Epoch 17, Batch: 168: Training Loss: 0.02096017450094223, Validation Loss: 0.02416323870420456\n",
      "Epoch 17, Batch: 169: Training Loss: 0.025447852909564972, Validation Loss: 0.024858975782990456\n",
      "Epoch 17, Batch: 170: Training Loss: 0.0232979916036129, Validation Loss: 0.024997653439641\n",
      "Epoch 17, Batch: 171: Training Loss: 0.02063446305692196, Validation Loss: 0.024019649252295494\n",
      "Epoch 17, Batch: 172: Training Loss: 0.02234422042965889, Validation Loss: 0.022220322862267494\n",
      "Epoch 17, Batch: 173: Training Loss: 0.02010822668671608, Validation Loss: 0.02491990104317665\n",
      "Epoch 17, Batch: 174: Training Loss: 0.023840954527258873, Validation Loss: 0.022011732682585716\n",
      "Epoch 17, Batch: 175: Training Loss: 0.021706949919462204, Validation Loss: 0.02361307665705681\n",
      "Epoch 17, Batch: 176: Training Loss: 0.023373786360025406, Validation Loss: 0.02230653166770935\n",
      "Epoch 17, Batch: 177: Training Loss: 0.024591553956270218, Validation Loss: 0.02193533070385456\n",
      "Epoch 17, Batch: 178: Training Loss: 0.02382527105510235, Validation Loss: 0.02171797677874565\n",
      "Epoch 17, Batch: 179: Training Loss: 0.019776202738285065, Validation Loss: 0.023346876725554466\n",
      "Epoch 17, Batch: 180: Training Loss: 0.02439098432660103, Validation Loss: 0.02471010386943817\n",
      "Epoch 17, Batch: 181: Training Loss: 0.023825954645872116, Validation Loss: 0.023639017716050148\n",
      "Epoch 17, Batch: 182: Training Loss: 0.025321295484900475, Validation Loss: 0.023050928488373756\n",
      "Epoch 17, Batch: 183: Training Loss: 0.022475307807326317, Validation Loss: 0.023375490680336952\n",
      "Epoch 17, Batch: 184: Training Loss: 0.0214997548609972, Validation Loss: 0.023072369396686554\n",
      "Epoch 17, Batch: 185: Training Loss: 0.026264790445566177, Validation Loss: 0.024484816938638687\n",
      "Epoch 17, Batch: 186: Training Loss: 0.02585480362176895, Validation Loss: 0.02179976925253868\n",
      "Epoch 17, Batch: 187: Training Loss: 0.020253529772162437, Validation Loss: 0.0227060429751873\n",
      "Epoch 17, Batch: 188: Training Loss: 0.021380966529250145, Validation Loss: 0.02363726496696472\n",
      "Epoch 17, Batch: 189: Training Loss: 0.020640484988689423, Validation Loss: 0.022980133071541786\n",
      "Epoch 17, Batch: 190: Training Loss: 0.02236034721136093, Validation Loss: 0.022590676322579384\n",
      "Epoch 17, Batch: 191: Training Loss: 0.023644104599952698, Validation Loss: 0.020688576623797417\n",
      "Epoch 17, Batch: 192: Training Loss: 0.02375098504126072, Validation Loss: 0.023503290489315987\n",
      "Epoch 17, Batch: 193: Training Loss: 0.022269610315561295, Validation Loss: 0.021383007988333702\n",
      "Epoch 17, Batch: 194: Training Loss: 0.023615963757038116, Validation Loss: 0.02489880658686161\n",
      "Epoch 17, Batch: 195: Training Loss: 0.020471490919589996, Validation Loss: 0.024268997833132744\n",
      "Epoch 17, Batch: 196: Training Loss: 0.026574475690722466, Validation Loss: 0.02145904116332531\n",
      "Epoch 17, Batch: 197: Training Loss: 0.020893827080726624, Validation Loss: 0.021908555179834366\n",
      "Epoch 17, Batch: 198: Training Loss: 0.0195389986038208, Validation Loss: 0.02295839414000511\n",
      "Epoch 17, Batch: 199: Training Loss: 0.020475950092077255, Validation Loss: 0.023642664775252342\n",
      "Epoch 17, Batch: 200: Training Loss: 0.019668197259306908, Validation Loss: 0.024055074900388718\n",
      "Epoch 17, Batch: 201: Training Loss: 0.027016835287213326, Validation Loss: 0.02184055931866169\n",
      "Epoch 17, Batch: 202: Training Loss: 0.020220858976244926, Validation Loss: 0.023775340989232063\n",
      "Epoch 17, Batch: 203: Training Loss: 0.02331194467842579, Validation Loss: 0.02432628907263279\n",
      "Epoch 17, Batch: 204: Training Loss: 0.018696950748562813, Validation Loss: 0.024256613105535507\n",
      "Epoch 17, Batch: 205: Training Loss: 0.027602694928646088, Validation Loss: 0.024540849030017853\n",
      "Epoch 17, Batch: 206: Training Loss: 0.02355973981320858, Validation Loss: 0.025802217423915863\n",
      "Epoch 17, Batch: 207: Training Loss: 0.02655675821006298, Validation Loss: 0.02728538028895855\n",
      "Epoch 17, Batch: 208: Training Loss: 0.022487636655569077, Validation Loss: 0.02555246278643608\n",
      "Epoch 17, Batch: 209: Training Loss: 0.023720819503068924, Validation Loss: 0.023016080260276794\n",
      "Epoch 17, Batch: 210: Training Loss: 0.02440587617456913, Validation Loss: 0.02359667234122753\n",
      "Epoch 17, Batch: 211: Training Loss: 0.02253573015332222, Validation Loss: 0.024128664284944534\n",
      "Epoch 17, Batch: 212: Training Loss: 0.024222178384661674, Validation Loss: 0.02407575026154518\n",
      "Epoch 17, Batch: 213: Training Loss: 0.022246232256293297, Validation Loss: 0.02159573882818222\n",
      "Epoch 17, Batch: 214: Training Loss: 0.020868374034762383, Validation Loss: 0.023730598390102386\n",
      "Epoch 17, Batch: 215: Training Loss: 0.02390545979142189, Validation Loss: 0.02162262052297592\n",
      "Epoch 17, Batch: 216: Training Loss: 0.02348031848669052, Validation Loss: 0.022157447412610054\n",
      "Epoch 17, Batch: 217: Training Loss: 0.030356459319591522, Validation Loss: 0.021063730120658875\n",
      "Epoch 17, Batch: 218: Training Loss: 0.022650176659226418, Validation Loss: 0.02271527796983719\n",
      "Epoch 17, Batch: 219: Training Loss: 0.022901032119989395, Validation Loss: 0.02172173000872135\n",
      "Epoch 17, Batch: 220: Training Loss: 0.023590540513396263, Validation Loss: 0.021022729575634003\n",
      "Epoch 17, Batch: 221: Training Loss: 0.022413192316889763, Validation Loss: 0.022549228742718697\n",
      "Epoch 17, Batch: 222: Training Loss: 0.022055478766560555, Validation Loss: 0.021269377321004868\n",
      "Epoch 17, Batch: 223: Training Loss: 0.02457532100379467, Validation Loss: 0.02019638568162918\n",
      "Epoch 17, Batch: 224: Training Loss: 0.021929359063506126, Validation Loss: 0.021254833787679672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 225: Training Loss: 0.022762306034564972, Validation Loss: 0.02191147953271866\n",
      "Saving new best model w/ loss: 0.01948748528957367\n",
      "Epoch 17, Batch: 226: Training Loss: 0.02190808393061161, Validation Loss: 0.01948748528957367\n",
      "Epoch 17, Batch: 227: Training Loss: 0.024142365902662277, Validation Loss: 0.022668639197945595\n",
      "Epoch 17, Batch: 228: Training Loss: 0.023878566920757294, Validation Loss: 0.02198832854628563\n",
      "Epoch 17, Batch: 229: Training Loss: 0.022708553820848465, Validation Loss: 0.022138329222798347\n",
      "Epoch 17, Batch: 230: Training Loss: 0.02126842923462391, Validation Loss: 0.022369183599948883\n",
      "Epoch 17, Batch: 231: Training Loss: 0.023174988105893135, Validation Loss: 0.024242796003818512\n",
      "Epoch 17, Batch: 232: Training Loss: 0.02540610171854496, Validation Loss: 0.021951353177428246\n",
      "Epoch 17, Batch: 233: Training Loss: 0.018387850373983383, Validation Loss: 0.02331620268523693\n",
      "Epoch 17, Batch: 234: Training Loss: 0.02437315136194229, Validation Loss: 0.022058388218283653\n",
      "Epoch 17, Batch: 235: Training Loss: 0.02546175941824913, Validation Loss: 0.02403055690228939\n",
      "Epoch 17, Batch: 236: Training Loss: 0.026708828285336494, Validation Loss: 0.022576291114091873\n",
      "Epoch 17, Batch: 237: Training Loss: 0.025086909532546997, Validation Loss: 0.021220561116933823\n",
      "Epoch 17, Batch: 238: Training Loss: 0.024750540032982826, Validation Loss: 0.02322285808622837\n",
      "Epoch 17, Batch: 239: Training Loss: 0.02355780638754368, Validation Loss: 0.022065043449401855\n",
      "Epoch 17, Batch: 240: Training Loss: 0.02097599394619465, Validation Loss: 0.02086527645587921\n",
      "Epoch 17, Batch: 241: Training Loss: 0.021304188296198845, Validation Loss: 0.020921360701322556\n",
      "Epoch 17, Batch: 242: Training Loss: 0.02309015579521656, Validation Loss: 0.023995883762836456\n",
      "Epoch 17, Batch: 243: Training Loss: 0.026191383600234985, Validation Loss: 0.022166507318615913\n",
      "Epoch 17, Batch: 244: Training Loss: 0.021774785593152046, Validation Loss: 0.02291867323219776\n",
      "Epoch 17, Batch: 245: Training Loss: 0.024993248283863068, Validation Loss: 0.0203423872590065\n",
      "Epoch 17, Batch: 246: Training Loss: 0.022901540622115135, Validation Loss: 0.021273836493492126\n",
      "Epoch 17, Batch: 247: Training Loss: 0.024740757420659065, Validation Loss: 0.024106059223413467\n",
      "Epoch 17, Batch: 248: Training Loss: 0.020796677097678185, Validation Loss: 0.024935327470302582\n",
      "Epoch 17, Batch: 249: Training Loss: 0.02466459944844246, Validation Loss: 0.02176498807966709\n",
      "Epoch 17, Batch: 250: Training Loss: 0.022156741470098495, Validation Loss: 0.022546418011188507\n",
      "Epoch 17, Batch: 251: Training Loss: 0.02275392971932888, Validation Loss: 0.02212238684296608\n",
      "Epoch 17, Batch: 252: Training Loss: 0.023448480293154716, Validation Loss: 0.02359967678785324\n",
      "Epoch 17, Batch: 253: Training Loss: 0.02538824826478958, Validation Loss: 0.02157500386238098\n",
      "Epoch 17, Batch: 254: Training Loss: 0.0199279747903347, Validation Loss: 0.02252086251974106\n",
      "Epoch 17, Batch: 255: Training Loss: 0.02230813167989254, Validation Loss: 0.02420133352279663\n",
      "Epoch 17, Batch: 256: Training Loss: 0.027061961591243744, Validation Loss: 0.0246487595140934\n",
      "Epoch 17, Batch: 257: Training Loss: 0.020997080951929092, Validation Loss: 0.02290024422109127\n",
      "Epoch 17, Batch: 258: Training Loss: 0.024366414174437523, Validation Loss: 0.023776408284902573\n",
      "Epoch 17, Batch: 259: Training Loss: 0.02246510237455368, Validation Loss: 0.025630740448832512\n",
      "Epoch 17, Batch: 260: Training Loss: 0.025123586878180504, Validation Loss: 0.021863970905542374\n",
      "Epoch 17, Batch: 261: Training Loss: 0.02370363287627697, Validation Loss: 0.021552439779043198\n",
      "Epoch 17, Batch: 262: Training Loss: 0.022654803469777107, Validation Loss: 0.02449203096330166\n",
      "Epoch 17, Batch: 263: Training Loss: 0.020019743591547012, Validation Loss: 0.023862365633249283\n",
      "Epoch 17, Batch: 264: Training Loss: 0.021354960277676582, Validation Loss: 0.024223903194069862\n",
      "Epoch 17, Batch: 265: Training Loss: 0.02377801202237606, Validation Loss: 0.023898910731077194\n",
      "Epoch 17, Batch: 266: Training Loss: 0.018999505788087845, Validation Loss: 0.022233810275793076\n",
      "Epoch 17, Batch: 267: Training Loss: 0.020869940519332886, Validation Loss: 0.02480076439678669\n",
      "Epoch 17, Batch: 268: Training Loss: 0.02197408303618431, Validation Loss: 0.023344406858086586\n",
      "Epoch 17, Batch: 269: Training Loss: 0.023400671780109406, Validation Loss: 0.021510159596800804\n",
      "Epoch 17, Batch: 270: Training Loss: 0.019521154463291168, Validation Loss: 0.022357964888215065\n",
      "Epoch 17, Batch: 271: Training Loss: 0.02182754874229431, Validation Loss: 0.024592913687229156\n",
      "Epoch 17, Batch: 272: Training Loss: 0.024937577545642853, Validation Loss: 0.022488702088594437\n",
      "Epoch 17, Batch: 273: Training Loss: 0.023112455382943153, Validation Loss: 0.02300143428146839\n",
      "Epoch 17, Batch: 274: Training Loss: 0.023185910657048225, Validation Loss: 0.02475578524172306\n",
      "Epoch 17, Batch: 275: Training Loss: 0.02092314139008522, Validation Loss: 0.024309203028678894\n",
      "Epoch 17, Batch: 276: Training Loss: 0.022137081250548363, Validation Loss: 0.022813305258750916\n",
      "Epoch 17, Batch: 277: Training Loss: 0.02053608000278473, Validation Loss: 0.02343936264514923\n",
      "Epoch 17, Batch: 278: Training Loss: 0.021599188446998596, Validation Loss: 0.024056006222963333\n",
      "Epoch 17, Batch: 279: Training Loss: 0.02420954965054989, Validation Loss: 0.024113677442073822\n",
      "Epoch 17, Batch: 280: Training Loss: 0.020337160676717758, Validation Loss: 0.0225206408649683\n",
      "Epoch 17, Batch: 281: Training Loss: 0.021897755563259125, Validation Loss: 0.02343650907278061\n",
      "Epoch 17, Batch: 282: Training Loss: 0.022556841373443604, Validation Loss: 0.021962983533740044\n",
      "Epoch 17, Batch: 283: Training Loss: 0.020367635414004326, Validation Loss: 0.02286231331527233\n",
      "Epoch 17, Batch: 284: Training Loss: 0.02437969669699669, Validation Loss: 0.020950643345713615\n",
      "Epoch 17, Batch: 285: Training Loss: 0.023027047514915466, Validation Loss: 0.022781621664762497\n",
      "Epoch 17, Batch: 286: Training Loss: 0.02371104620397091, Validation Loss: 0.02212214469909668\n",
      "Epoch 17, Batch: 287: Training Loss: 0.024664495140314102, Validation Loss: 0.022702958434820175\n",
      "Epoch 17, Batch: 288: Training Loss: 0.019924085587263107, Validation Loss: 0.02360643446445465\n",
      "Epoch 17, Batch: 289: Training Loss: 0.023100169375538826, Validation Loss: 0.023005811497569084\n",
      "Epoch 17, Batch: 290: Training Loss: 0.02164560928940773, Validation Loss: 0.025535404682159424\n",
      "Epoch 17, Batch: 291: Training Loss: 0.023414025083184242, Validation Loss: 0.025899525731801987\n",
      "Epoch 17, Batch: 292: Training Loss: 0.023706747218966484, Validation Loss: 0.024832235649228096\n",
      "Epoch 17, Batch: 293: Training Loss: 0.025661993771791458, Validation Loss: 0.024920862168073654\n",
      "Epoch 17, Batch: 294: Training Loss: 0.020327582955360413, Validation Loss: 0.02308589406311512\n",
      "Epoch 17, Batch: 295: Training Loss: 0.021372070536017418, Validation Loss: 0.02365982159972191\n",
      "Epoch 17, Batch: 296: Training Loss: 0.023887999355793, Validation Loss: 0.022298192605376244\n",
      "Epoch 17, Batch: 297: Training Loss: 0.020496618002653122, Validation Loss: 0.022839533165097237\n",
      "Epoch 17, Batch: 298: Training Loss: 0.0211014486849308, Validation Loss: 0.023770755156874657\n",
      "Epoch 17, Batch: 299: Training Loss: 0.02255871519446373, Validation Loss: 0.024661097675561905\n",
      "Epoch 17, Batch: 300: Training Loss: 0.025570284575223923, Validation Loss: 0.024069465696811676\n",
      "Epoch 17, Batch: 301: Training Loss: 0.021649230271577835, Validation Loss: 0.022057654336094856\n",
      "Epoch 17, Batch: 302: Training Loss: 0.018891405314207077, Validation Loss: 0.020495759323239326\n",
      "Epoch 17, Batch: 303: Training Loss: 0.02249760739505291, Validation Loss: 0.021813493221998215\n",
      "Epoch 17, Batch: 304: Training Loss: 0.02389458380639553, Validation Loss: 0.02210448682308197\n",
      "Epoch 17, Batch: 305: Training Loss: 0.01913420669734478, Validation Loss: 0.022174231708049774\n",
      "Epoch 17, Batch: 306: Training Loss: 0.021408353000879288, Validation Loss: 0.023066552355885506\n",
      "Epoch 17, Batch: 307: Training Loss: 0.022783590480685234, Validation Loss: 0.022129889577627182\n",
      "Epoch 17, Batch: 308: Training Loss: 0.02234984189271927, Validation Loss: 0.02199660986661911\n",
      "Epoch 17, Batch: 309: Training Loss: 0.019463758915662766, Validation Loss: 0.02169916406273842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 310: Training Loss: 0.022407379001379013, Validation Loss: 0.022084955126047134\n",
      "Epoch 17, Batch: 311: Training Loss: 0.022015728056430817, Validation Loss: 0.02151305228471756\n",
      "Epoch 17, Batch: 312: Training Loss: 0.022931411862373352, Validation Loss: 0.02262938767671585\n",
      "Epoch 17, Batch: 313: Training Loss: 0.020597141236066818, Validation Loss: 0.02287113294005394\n",
      "Epoch 17, Batch: 314: Training Loss: 0.023469869047403336, Validation Loss: 0.02295834757387638\n",
      "Epoch 17, Batch: 315: Training Loss: 0.022061863914132118, Validation Loss: 0.023481905460357666\n",
      "Epoch 17, Batch: 316: Training Loss: 0.020118895918130875, Validation Loss: 0.025513555854558945\n",
      "Epoch 17, Batch: 317: Training Loss: 0.02277139574289322, Validation Loss: 0.02367708832025528\n",
      "Epoch 17, Batch: 318: Training Loss: 0.021932872012257576, Validation Loss: 0.022366447374224663\n",
      "Epoch 17, Batch: 319: Training Loss: 0.021301642060279846, Validation Loss: 0.023765400052070618\n",
      "Epoch 17, Batch: 320: Training Loss: 0.023587973788380623, Validation Loss: 0.02362038753926754\n",
      "Epoch 17, Batch: 321: Training Loss: 0.02077361010015011, Validation Loss: 0.022717686370015144\n",
      "Epoch 17, Batch: 322: Training Loss: 0.0215243399143219, Validation Loss: 0.02367013320326805\n",
      "Epoch 17, Batch: 323: Training Loss: 0.01973658800125122, Validation Loss: 0.02484813891351223\n",
      "Epoch 17, Batch: 324: Training Loss: 0.022435437887907028, Validation Loss: 0.024201814085245132\n",
      "Epoch 17, Batch: 325: Training Loss: 0.02162224054336548, Validation Loss: 0.024881606921553612\n",
      "Epoch 17, Batch: 326: Training Loss: 0.028055725619196892, Validation Loss: 0.025082984939217567\n",
      "Epoch 17, Batch: 327: Training Loss: 0.01936199888586998, Validation Loss: 0.027409518137574196\n",
      "Epoch 17, Batch: 328: Training Loss: 0.02335614524781704, Validation Loss: 0.02424430474638939\n",
      "Epoch 17, Batch: 329: Training Loss: 0.02458389662206173, Validation Loss: 0.022435791790485382\n",
      "Epoch 17, Batch: 330: Training Loss: 0.023339668288826942, Validation Loss: 0.02401440404355526\n",
      "Epoch 17, Batch: 331: Training Loss: 0.022687824442982674, Validation Loss: 0.02507483772933483\n",
      "Epoch 17, Batch: 332: Training Loss: 0.02495863288640976, Validation Loss: 0.0250406377017498\n",
      "Epoch 17, Batch: 333: Training Loss: 0.021945612505078316, Validation Loss: 0.024314476177096367\n",
      "Epoch 17, Batch: 334: Training Loss: 0.021334314718842506, Validation Loss: 0.024609142914414406\n",
      "Epoch 17, Batch: 335: Training Loss: 0.01726922206580639, Validation Loss: 0.023800456896424294\n",
      "Epoch 17, Batch: 336: Training Loss: 0.020992537960410118, Validation Loss: 0.02500585839152336\n",
      "Epoch 17, Batch: 337: Training Loss: 0.021915361285209656, Validation Loss: 0.023986147716641426\n",
      "Epoch 17, Batch: 338: Training Loss: 0.021114066243171692, Validation Loss: 0.024690499529242516\n",
      "Epoch 17, Batch: 339: Training Loss: 0.02365979366004467, Validation Loss: 0.0243418887257576\n",
      "Epoch 17, Batch: 340: Training Loss: 0.020507024601101875, Validation Loss: 0.025654779747128487\n",
      "Epoch 17, Batch: 341: Training Loss: 0.021575434133410454, Validation Loss: 0.026325510814785957\n",
      "Epoch 17, Batch: 342: Training Loss: 0.025967493653297424, Validation Loss: 0.023014230653643608\n",
      "Epoch 17, Batch: 343: Training Loss: 0.026605261489748955, Validation Loss: 0.02664732001721859\n",
      "Epoch 17, Batch: 344: Training Loss: 0.023258181288838387, Validation Loss: 0.024908974766731262\n",
      "Epoch 17, Batch: 345: Training Loss: 0.02371661178767681, Validation Loss: 0.02369537390768528\n",
      "Epoch 17, Batch: 346: Training Loss: 0.025650840252637863, Validation Loss: 0.02420915476977825\n",
      "Epoch 17, Batch: 347: Training Loss: 0.021696787327528, Validation Loss: 0.025582000613212585\n",
      "Epoch 17, Batch: 348: Training Loss: 0.020651794970035553, Validation Loss: 0.024473022669553757\n",
      "Epoch 17, Batch: 349: Training Loss: 0.027042651548981667, Validation Loss: 0.024609936401247978\n",
      "Epoch 17, Batch: 350: Training Loss: 0.022683966904878616, Validation Loss: 0.02322436310350895\n",
      "Epoch 17, Batch: 351: Training Loss: 0.02860262431204319, Validation Loss: 0.022675275802612305\n",
      "Epoch 17, Batch: 352: Training Loss: 0.022280678153038025, Validation Loss: 0.024192657321691513\n",
      "Epoch 17, Batch: 353: Training Loss: 0.023194480687379837, Validation Loss: 0.02368258684873581\n",
      "Epoch 17, Batch: 354: Training Loss: 0.024944083765149117, Validation Loss: 0.025492656975984573\n",
      "Epoch 17, Batch: 355: Training Loss: 0.025227993726730347, Validation Loss: 0.02578592300415039\n",
      "Epoch 17, Batch: 356: Training Loss: 0.022927653044462204, Validation Loss: 0.026704058051109314\n",
      "Epoch 17, Batch: 357: Training Loss: 0.024249618873000145, Validation Loss: 0.027957886457443237\n",
      "Epoch 17, Batch: 358: Training Loss: 0.024471348151564598, Validation Loss: 0.02598971128463745\n",
      "Epoch 17, Batch: 359: Training Loss: 0.027803393080830574, Validation Loss: 0.024647729471325874\n",
      "Epoch 17, Batch: 360: Training Loss: 0.027268070727586746, Validation Loss: 0.026133036240935326\n",
      "Epoch 17, Batch: 361: Training Loss: 0.020243002101778984, Validation Loss: 0.02708141691982746\n",
      "Epoch 17, Batch: 362: Training Loss: 0.021381257101893425, Validation Loss: 0.0255800299346447\n",
      "Epoch 17, Batch: 363: Training Loss: 0.031190156936645508, Validation Loss: 0.028419002890586853\n",
      "Epoch 17, Batch: 364: Training Loss: 0.0229543037712574, Validation Loss: 0.026805270463228226\n",
      "Epoch 17, Batch: 365: Training Loss: 0.019094061106443405, Validation Loss: 0.024630246683955193\n",
      "Epoch 17, Batch: 366: Training Loss: 0.026675617322325706, Validation Loss: 0.026692364364862442\n",
      "Epoch 17, Batch: 367: Training Loss: 0.020221253857016563, Validation Loss: 0.028527438640594482\n",
      "Epoch 17, Batch: 368: Training Loss: 0.021084964275360107, Validation Loss: 0.02330104447901249\n",
      "Epoch 17, Batch: 369: Training Loss: 0.02344714105129242, Validation Loss: 0.025080757215619087\n",
      "Epoch 17, Batch: 370: Training Loss: 0.02447548694908619, Validation Loss: 0.0274448674172163\n",
      "Epoch 17, Batch: 371: Training Loss: 0.023617936298251152, Validation Loss: 0.028043515980243683\n",
      "Epoch 17, Batch: 372: Training Loss: 0.021492036059498787, Validation Loss: 0.026815632358193398\n",
      "Epoch 17, Batch: 373: Training Loss: 0.0231388621032238, Validation Loss: 0.026437122374773026\n",
      "Epoch 17, Batch: 374: Training Loss: 0.02044513076543808, Validation Loss: 0.025419974699616432\n",
      "Epoch 17, Batch: 375: Training Loss: 0.02372809313237667, Validation Loss: 0.0253757257014513\n",
      "Epoch 17, Batch: 376: Training Loss: 0.021663306280970573, Validation Loss: 0.023328492417931557\n",
      "Epoch 17, Batch: 377: Training Loss: 0.018993744626641273, Validation Loss: 0.02571524679660797\n",
      "Epoch 17, Batch: 378: Training Loss: 0.021998459473252296, Validation Loss: 0.02551168017089367\n",
      "Epoch 17, Batch: 379: Training Loss: 0.024420613422989845, Validation Loss: 0.024149293079972267\n",
      "Epoch 17, Batch: 380: Training Loss: 0.026064028963446617, Validation Loss: 0.02220555767416954\n",
      "Epoch 17, Batch: 381: Training Loss: 0.023743661120533943, Validation Loss: 0.021525632590055466\n",
      "Epoch 17, Batch: 382: Training Loss: 0.020254915580153465, Validation Loss: 0.02432389371097088\n",
      "Epoch 17, Batch: 383: Training Loss: 0.020884308964014053, Validation Loss: 0.022514594718813896\n",
      "Epoch 17, Batch: 384: Training Loss: 0.023309020325541496, Validation Loss: 0.023226257413625717\n",
      "Epoch 17, Batch: 385: Training Loss: 0.02324136346578598, Validation Loss: 0.02227715402841568\n",
      "Epoch 17, Batch: 386: Training Loss: 0.019604064524173737, Validation Loss: 0.022080717608332634\n",
      "Epoch 17, Batch: 387: Training Loss: 0.021696938201785088, Validation Loss: 0.023602191358804703\n",
      "Epoch 17, Batch: 388: Training Loss: 0.023682206869125366, Validation Loss: 0.0241191815584898\n",
      "Epoch 17, Batch: 389: Training Loss: 0.021235167980194092, Validation Loss: 0.01994318701326847\n",
      "Epoch 17, Batch: 390: Training Loss: 0.02105272375047207, Validation Loss: 0.023875052109360695\n",
      "Epoch 17, Batch: 391: Training Loss: 0.0230657197535038, Validation Loss: 0.022841360419988632\n",
      "Epoch 17, Batch: 392: Training Loss: 0.02155047096312046, Validation Loss: 0.023812057450413704\n",
      "Epoch 17, Batch: 393: Training Loss: 0.022088853642344475, Validation Loss: 0.02460736781358719\n",
      "Epoch 17, Batch: 394: Training Loss: 0.02445567026734352, Validation Loss: 0.02445937506854534\n",
      "Epoch 17, Batch: 395: Training Loss: 0.021761763840913773, Validation Loss: 0.02570180781185627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 396: Training Loss: 0.02237018756568432, Validation Loss: 0.02422102354466915\n",
      "Epoch 17, Batch: 397: Training Loss: 0.02186831273138523, Validation Loss: 0.02499285526573658\n",
      "Epoch 17, Batch: 398: Training Loss: 0.019673597067594528, Validation Loss: 0.024001305922865868\n",
      "Epoch 17, Batch: 399: Training Loss: 0.020475290715694427, Validation Loss: 0.023299366235733032\n",
      "Epoch 17, Batch: 400: Training Loss: 0.021456478163599968, Validation Loss: 0.0250460933893919\n",
      "Epoch 17, Batch: 401: Training Loss: 0.023568781092762947, Validation Loss: 0.023748155683279037\n",
      "Epoch 17, Batch: 402: Training Loss: 0.022433260455727577, Validation Loss: 0.023506538942456245\n",
      "Epoch 17, Batch: 403: Training Loss: 0.023142840713262558, Validation Loss: 0.02663620561361313\n",
      "Epoch 17, Batch: 404: Training Loss: 0.02204132080078125, Validation Loss: 0.024385400116443634\n",
      "Epoch 17, Batch: 405: Training Loss: 0.02182554267346859, Validation Loss: 0.02366015501320362\n",
      "Epoch 17, Batch: 406: Training Loss: 0.022263171151280403, Validation Loss: 0.025647569447755814\n",
      "Epoch 17, Batch: 407: Training Loss: 0.01991966739296913, Validation Loss: 0.026105953380465508\n",
      "Epoch 17, Batch: 408: Training Loss: 0.02032831870019436, Validation Loss: 0.022867215797305107\n",
      "Epoch 17, Batch: 409: Training Loss: 0.021441005170345306, Validation Loss: 0.025842703878879547\n",
      "Epoch 17, Batch: 410: Training Loss: 0.02126401662826538, Validation Loss: 0.021689455956220627\n",
      "Epoch 17, Batch: 411: Training Loss: 0.018792079761624336, Validation Loss: 0.023907195776700974\n",
      "Epoch 17, Batch: 412: Training Loss: 0.02655917778611183, Validation Loss: 0.024396901950240135\n",
      "Epoch 17, Batch: 413: Training Loss: 0.027773234993219376, Validation Loss: 0.025130875408649445\n",
      "Epoch 17, Batch: 414: Training Loss: 0.02190753072500229, Validation Loss: 0.024045221507549286\n",
      "Epoch 17, Batch: 415: Training Loss: 0.02496677078306675, Validation Loss: 0.023360008373856544\n",
      "Epoch 17, Batch: 416: Training Loss: 0.02328275516629219, Validation Loss: 0.02391844056546688\n",
      "Epoch 17, Batch: 417: Training Loss: 0.028962839394807816, Validation Loss: 0.02472602017223835\n",
      "Epoch 17, Batch: 418: Training Loss: 0.020026640966534615, Validation Loss: 0.022717753425240517\n",
      "Epoch 17, Batch: 419: Training Loss: 0.024649184197187424, Validation Loss: 0.02510162629187107\n",
      "Epoch 17, Batch: 420: Training Loss: 0.02673822082579136, Validation Loss: 0.02372581698000431\n",
      "Epoch 17, Batch: 421: Training Loss: 0.029250493273139, Validation Loss: 0.024684971198439598\n",
      "Epoch 17, Batch: 422: Training Loss: 0.02641124464571476, Validation Loss: 0.024221453815698624\n",
      "Epoch 17, Batch: 423: Training Loss: 0.026461128145456314, Validation Loss: 0.024659935384988785\n",
      "Epoch 17, Batch: 424: Training Loss: 0.0235615111887455, Validation Loss: 0.0239216648042202\n",
      "Epoch 17, Batch: 425: Training Loss: 0.02290261536836624, Validation Loss: 0.02188454568386078\n",
      "Epoch 17, Batch: 426: Training Loss: 0.02365473285317421, Validation Loss: 0.02413392812013626\n",
      "Epoch 17, Batch: 427: Training Loss: 0.02248513698577881, Validation Loss: 0.023758647963404655\n",
      "Epoch 17, Batch: 428: Training Loss: 0.020167727023363113, Validation Loss: 0.02438162826001644\n",
      "Epoch 17, Batch: 429: Training Loss: 0.02009388990700245, Validation Loss: 0.02631423808634281\n",
      "Epoch 17, Batch: 430: Training Loss: 0.018468869850039482, Validation Loss: 0.023248057812452316\n",
      "Epoch 17, Batch: 431: Training Loss: 0.019889775663614273, Validation Loss: 0.02462032064795494\n",
      "Epoch 17, Batch: 432: Training Loss: 0.021007679402828217, Validation Loss: 0.02388305962085724\n",
      "Epoch 17, Batch: 433: Training Loss: 0.02400878816843033, Validation Loss: 0.025506597012281418\n",
      "Epoch 17, Batch: 434: Training Loss: 0.026156101375818253, Validation Loss: 0.02402416430413723\n",
      "Epoch 17, Batch: 435: Training Loss: 0.023549877107143402, Validation Loss: 0.026554370298981667\n",
      "Epoch 17, Batch: 436: Training Loss: 0.020426297560334206, Validation Loss: 0.02505175769329071\n",
      "Epoch 17, Batch: 437: Training Loss: 0.0203897375613451, Validation Loss: 0.02588418312370777\n",
      "Epoch 17, Batch: 438: Training Loss: 0.024255968630313873, Validation Loss: 0.024185804650187492\n",
      "Epoch 17, Batch: 439: Training Loss: 0.018679209053516388, Validation Loss: 0.024838248267769814\n",
      "Epoch 17, Batch: 440: Training Loss: 0.024297330528497696, Validation Loss: 0.023921644315123558\n",
      "Epoch 17, Batch: 441: Training Loss: 0.01938415877521038, Validation Loss: 0.02501184493303299\n",
      "Epoch 17, Batch: 442: Training Loss: 0.027031296864151955, Validation Loss: 0.02850181981921196\n",
      "Epoch 17, Batch: 443: Training Loss: 0.02655596286058426, Validation Loss: 0.02631131000816822\n",
      "Epoch 17, Batch: 444: Training Loss: 0.023644758388400078, Validation Loss: 0.02682284452021122\n",
      "Epoch 17, Batch: 445: Training Loss: 0.019894685596227646, Validation Loss: 0.02633344568312168\n",
      "Epoch 17, Batch: 446: Training Loss: 0.026328349485993385, Validation Loss: 0.024535952135920525\n",
      "Epoch 17, Batch: 447: Training Loss: 0.024523314088582993, Validation Loss: 0.024600103497505188\n",
      "Epoch 17, Batch: 448: Training Loss: 0.020863404497504234, Validation Loss: 0.024826103821396828\n",
      "Epoch 17, Batch: 449: Training Loss: 0.023266391828656197, Validation Loss: 0.025978440418839455\n",
      "Epoch 17, Batch: 450: Training Loss: 0.025425974279642105, Validation Loss: 0.023520782589912415\n",
      "Epoch 17, Batch: 451: Training Loss: 0.025742176920175552, Validation Loss: 0.022053563967347145\n",
      "Epoch 17, Batch: 452: Training Loss: 0.02569533884525299, Validation Loss: 0.02280876412987709\n",
      "Epoch 17, Batch: 453: Training Loss: 0.024702543392777443, Validation Loss: 0.026796644553542137\n",
      "Epoch 17, Batch: 454: Training Loss: 0.02357635088264942, Validation Loss: 0.025161243975162506\n",
      "Epoch 17, Batch: 455: Training Loss: 0.02287334017455578, Validation Loss: 0.023251289501786232\n",
      "Epoch 17, Batch: 456: Training Loss: 0.02612951025366783, Validation Loss: 0.02120649255812168\n",
      "Epoch 17, Batch: 457: Training Loss: 0.02292190119624138, Validation Loss: 0.02287626825273037\n",
      "Epoch 17, Batch: 458: Training Loss: 0.02323863096535206, Validation Loss: 0.024616975337266922\n",
      "Epoch 17, Batch: 459: Training Loss: 0.018858974799513817, Validation Loss: 0.02333468198776245\n",
      "Epoch 17, Batch: 460: Training Loss: 0.021026374772191048, Validation Loss: 0.023640895262360573\n",
      "Epoch 17, Batch: 461: Training Loss: 0.024481894448399544, Validation Loss: 0.023069284856319427\n",
      "Epoch 17, Batch: 462: Training Loss: 0.02288958802819252, Validation Loss: 0.021224405616521835\n",
      "Epoch 17, Batch: 463: Training Loss: 0.019231854006648064, Validation Loss: 0.02398238517343998\n",
      "Epoch 17, Batch: 464: Training Loss: 0.024343980476260185, Validation Loss: 0.02363790012896061\n",
      "Epoch 17, Batch: 465: Training Loss: 0.023336753249168396, Validation Loss: 0.02320212498307228\n",
      "Epoch 17, Batch: 466: Training Loss: 0.023151205852627754, Validation Loss: 0.023797810077667236\n",
      "Epoch 17, Batch: 467: Training Loss: 0.031160416081547737, Validation Loss: 0.02549896389245987\n",
      "Epoch 17, Batch: 468: Training Loss: 0.024146653711795807, Validation Loss: 0.02263839729130268\n",
      "Epoch 17, Batch: 469: Training Loss: 0.022844860330224037, Validation Loss: 0.0227276012301445\n",
      "Epoch 17, Batch: 470: Training Loss: 0.02060563862323761, Validation Loss: 0.02362855151295662\n",
      "Epoch 17, Batch: 471: Training Loss: 0.020045196637511253, Validation Loss: 0.02359878458082676\n",
      "Epoch 17, Batch: 472: Training Loss: 0.021842332556843758, Validation Loss: 0.02450026012957096\n",
      "Epoch 17, Batch: 473: Training Loss: 0.019401991739869118, Validation Loss: 0.025965582579374313\n",
      "Epoch 17, Batch: 474: Training Loss: 0.02135784924030304, Validation Loss: 0.02415984682738781\n",
      "Epoch 17, Batch: 475: Training Loss: 0.021496374160051346, Validation Loss: 0.02251247502863407\n",
      "Epoch 17, Batch: 476: Training Loss: 0.021283702924847603, Validation Loss: 0.02365095727145672\n",
      "Epoch 17, Batch: 477: Training Loss: 0.023897318169474602, Validation Loss: 0.023437660187482834\n",
      "Epoch 17, Batch: 478: Training Loss: 0.023139916360378265, Validation Loss: 0.023252977058291435\n",
      "Epoch 17, Batch: 479: Training Loss: 0.02363303117454052, Validation Loss: 0.022802436724305153\n",
      "Epoch 17, Batch: 480: Training Loss: 0.02297065779566765, Validation Loss: 0.023629821836948395\n",
      "Epoch 17, Batch: 481: Training Loss: 0.02310526929795742, Validation Loss: 0.02327415719628334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch: 482: Training Loss: 0.02443978749215603, Validation Loss: 0.02258002758026123\n",
      "Epoch 17, Batch: 483: Training Loss: 0.024532439187169075, Validation Loss: 0.022567851468920708\n",
      "Epoch 17, Batch: 484: Training Loss: 0.023482657968997955, Validation Loss: 0.023042360320687294\n",
      "Epoch 17, Batch: 485: Training Loss: 0.02527298405766487, Validation Loss: 0.022087814286351204\n",
      "Epoch 17, Batch: 486: Training Loss: 0.024680478498339653, Validation Loss: 0.022631675004959106\n",
      "Epoch 17, Batch: 487: Training Loss: 0.020080702379345894, Validation Loss: 0.021210024133324623\n",
      "Epoch 17, Batch: 488: Training Loss: 0.020743396133184433, Validation Loss: 0.023019226267933846\n",
      "Epoch 17, Batch: 489: Training Loss: 0.0294661782681942, Validation Loss: 0.0228100698441267\n",
      "Epoch 17, Batch: 490: Training Loss: 0.022828303277492523, Validation Loss: 0.02174375206232071\n",
      "Epoch 17, Batch: 491: Training Loss: 0.020114894956350327, Validation Loss: 0.023345068097114563\n",
      "Epoch 17, Batch: 492: Training Loss: 0.025816846638917923, Validation Loss: 0.023269981145858765\n",
      "Epoch 17, Batch: 493: Training Loss: 0.022719936445355415, Validation Loss: 0.022475631907582283\n",
      "Epoch 17, Batch: 494: Training Loss: 0.02433302253484726, Validation Loss: 0.022693248465657234\n",
      "Epoch 17, Batch: 495: Training Loss: 0.02347549796104431, Validation Loss: 0.02227846346795559\n",
      "Epoch 17, Batch: 496: Training Loss: 0.022649819031357765, Validation Loss: 0.020908109843730927\n",
      "Epoch 17, Batch: 497: Training Loss: 0.02150355465710163, Validation Loss: 0.023126063868403435\n",
      "Epoch 17, Batch: 498: Training Loss: 0.020824046805500984, Validation Loss: 0.02086680196225643\n",
      "Epoch 17, Batch: 499: Training Loss: 0.019938776269555092, Validation Loss: 0.02248283103108406\n",
      "Epoch 18, Batch: 0: Training Loss: 0.022124718874692917, Validation Loss: 0.022192543372511864\n",
      "Epoch 18, Batch: 1: Training Loss: 0.023005256429314613, Validation Loss: 0.02369990013539791\n",
      "Epoch 18, Batch: 2: Training Loss: 0.022253891453146935, Validation Loss: 0.023602178320288658\n",
      "Epoch 18, Batch: 3: Training Loss: 0.021494267508387566, Validation Loss: 0.023913737386465073\n",
      "Epoch 18, Batch: 4: Training Loss: 0.017899012193083763, Validation Loss: 0.023370971903204918\n",
      "Epoch 18, Batch: 5: Training Loss: 0.023310041055083275, Validation Loss: 0.023475296795368195\n",
      "Epoch 18, Batch: 6: Training Loss: 0.02173684723675251, Validation Loss: 0.022644074633717537\n",
      "Epoch 18, Batch: 7: Training Loss: 0.020380383357405663, Validation Loss: 0.02487627975642681\n",
      "Epoch 18, Batch: 8: Training Loss: 0.021602865308523178, Validation Loss: 0.02376020886003971\n",
      "Epoch 18, Batch: 9: Training Loss: 0.01938612014055252, Validation Loss: 0.022309262305498123\n",
      "Epoch 18, Batch: 10: Training Loss: 0.02123217284679413, Validation Loss: 0.0207683052867651\n",
      "Epoch 18, Batch: 11: Training Loss: 0.02671327255666256, Validation Loss: 0.02100665494799614\n",
      "Epoch 18, Batch: 12: Training Loss: 0.02209649607539177, Validation Loss: 0.022680966183543205\n",
      "Epoch 18, Batch: 13: Training Loss: 0.026892824098467827, Validation Loss: 0.02107589691877365\n",
      "Epoch 18, Batch: 14: Training Loss: 0.027078641578555107, Validation Loss: 0.02438184805214405\n",
      "Epoch 18, Batch: 15: Training Loss: 0.023039257153868675, Validation Loss: 0.02426972985267639\n",
      "Epoch 18, Batch: 16: Training Loss: 0.024891262874007225, Validation Loss: 0.025695864111185074\n",
      "Epoch 18, Batch: 17: Training Loss: 0.020428871735930443, Validation Loss: 0.023126356303691864\n",
      "Epoch 18, Batch: 18: Training Loss: 0.020929500460624695, Validation Loss: 0.024354074150323868\n",
      "Epoch 18, Batch: 19: Training Loss: 0.019005613401532173, Validation Loss: 0.02501515857875347\n",
      "Epoch 18, Batch: 20: Training Loss: 0.02123325876891613, Validation Loss: 0.025079213082790375\n",
      "Epoch 18, Batch: 21: Training Loss: 0.020620644092559814, Validation Loss: 0.024146927520632744\n",
      "Epoch 18, Batch: 22: Training Loss: 0.024706220254302025, Validation Loss: 0.026101890951395035\n",
      "Epoch 18, Batch: 23: Training Loss: 0.0249706469476223, Validation Loss: 0.025934802368283272\n",
      "Epoch 18, Batch: 24: Training Loss: 0.022199859842658043, Validation Loss: 0.028315018862485886\n",
      "Epoch 18, Batch: 25: Training Loss: 0.021302998065948486, Validation Loss: 0.026902316138148308\n",
      "Epoch 18, Batch: 26: Training Loss: 0.023633012548089027, Validation Loss: 0.023393305018544197\n",
      "Epoch 18, Batch: 27: Training Loss: 0.02223031409084797, Validation Loss: 0.02362600713968277\n",
      "Epoch 18, Batch: 28: Training Loss: 0.029786663129925728, Validation Loss: 0.026893556118011475\n",
      "Epoch 18, Batch: 29: Training Loss: 0.02475515380501747, Validation Loss: 0.02599967084825039\n",
      "Epoch 18, Batch: 30: Training Loss: 0.020400360226631165, Validation Loss: 0.025358133018016815\n",
      "Epoch 18, Batch: 31: Training Loss: 0.02382591739296913, Validation Loss: 0.022865472361445427\n",
      "Epoch 18, Batch: 32: Training Loss: 0.023418588563799858, Validation Loss: 0.02491474710404873\n",
      "Epoch 18, Batch: 33: Training Loss: 0.019388793036341667, Validation Loss: 0.024093138054013252\n",
      "Epoch 18, Batch: 34: Training Loss: 0.019903549924492836, Validation Loss: 0.02256605215370655\n",
      "Epoch 18, Batch: 35: Training Loss: 0.019440118223428726, Validation Loss: 0.024317022413015366\n",
      "Epoch 18, Batch: 36: Training Loss: 0.019558971747756004, Validation Loss: 0.02277255430817604\n",
      "Epoch 18, Batch: 37: Training Loss: 0.02372446283698082, Validation Loss: 0.023102592676877975\n",
      "Epoch 18, Batch: 38: Training Loss: 0.020663965493440628, Validation Loss: 0.021405940875411034\n",
      "Epoch 18, Batch: 39: Training Loss: 0.022936629131436348, Validation Loss: 0.022490253672003746\n",
      "Epoch 18, Batch: 40: Training Loss: 0.025631841272115707, Validation Loss: 0.023397929966449738\n",
      "Epoch 18, Batch: 41: Training Loss: 0.023251179605722427, Validation Loss: 0.02525295689702034\n",
      "Epoch 18, Batch: 42: Training Loss: 0.021577803418040276, Validation Loss: 0.027019713073968887\n",
      "Epoch 18, Batch: 43: Training Loss: 0.02075197733938694, Validation Loss: 0.025199195370078087\n",
      "Epoch 18, Batch: 44: Training Loss: 0.024268846958875656, Validation Loss: 0.025132542476058006\n",
      "Epoch 18, Batch: 45: Training Loss: 0.02311370149254799, Validation Loss: 0.02605394646525383\n",
      "Epoch 18, Batch: 46: Training Loss: 0.022237924858927727, Validation Loss: 0.026414738968014717\n",
      "Epoch 18, Batch: 47: Training Loss: 0.02534306049346924, Validation Loss: 0.026850061491131783\n",
      "Epoch 18, Batch: 48: Training Loss: 0.025947168469429016, Validation Loss: 0.024868417531251907\n",
      "Epoch 18, Batch: 49: Training Loss: 0.02280239202082157, Validation Loss: 0.025245998054742813\n",
      "Epoch 18, Batch: 50: Training Loss: 0.02253878116607666, Validation Loss: 0.025288963690400124\n",
      "Epoch 18, Batch: 51: Training Loss: 0.026243213564157486, Validation Loss: 0.026079336181282997\n",
      "Epoch 18, Batch: 52: Training Loss: 0.023681966587901115, Validation Loss: 0.02632879465818405\n",
      "Epoch 18, Batch: 53: Training Loss: 0.019879620522260666, Validation Loss: 0.026402998715639114\n",
      "Epoch 18, Batch: 54: Training Loss: 0.022739272564649582, Validation Loss: 0.0285097174346447\n",
      "Epoch 18, Batch: 55: Training Loss: 0.022416502237319946, Validation Loss: 0.0264737531542778\n",
      "Epoch 18, Batch: 56: Training Loss: 0.023613987490534782, Validation Loss: 0.028897395357489586\n",
      "Epoch 18, Batch: 57: Training Loss: 0.022276660427451134, Validation Loss: 0.025395913049578667\n",
      "Epoch 18, Batch: 58: Training Loss: 0.023115186020731926, Validation Loss: 0.025562837719917297\n",
      "Epoch 18, Batch: 59: Training Loss: 0.020946726202964783, Validation Loss: 0.026168907061219215\n",
      "Epoch 18, Batch: 60: Training Loss: 0.021790342405438423, Validation Loss: 0.025964591652154922\n",
      "Epoch 18, Batch: 61: Training Loss: 0.02479475364089012, Validation Loss: 0.023658260703086853\n",
      "Epoch 18, Batch: 62: Training Loss: 0.021487023681402206, Validation Loss: 0.025760233402252197\n",
      "Epoch 18, Batch: 63: Training Loss: 0.025790780782699585, Validation Loss: 0.02440226823091507\n",
      "Epoch 18, Batch: 64: Training Loss: 0.02148040384054184, Validation Loss: 0.026460517197847366\n",
      "Epoch 18, Batch: 65: Training Loss: 0.024237368255853653, Validation Loss: 0.0266589168459177\n",
      "Epoch 18, Batch: 66: Training Loss: 0.021153524518013, Validation Loss: 0.02424618788063526\n",
      "Epoch 18, Batch: 67: Training Loss: 0.021318688988685608, Validation Loss: 0.025639984756708145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 68: Training Loss: 0.02372346818447113, Validation Loss: 0.025024116039276123\n",
      "Epoch 18, Batch: 69: Training Loss: 0.023400895297527313, Validation Loss: 0.023169491440057755\n",
      "Epoch 18, Batch: 70: Training Loss: 0.025016073137521744, Validation Loss: 0.022725658491253853\n",
      "Epoch 18, Batch: 71: Training Loss: 0.019253602251410484, Validation Loss: 0.024040743708610535\n",
      "Epoch 18, Batch: 72: Training Loss: 0.02539283223450184, Validation Loss: 0.02432880364358425\n",
      "Epoch 18, Batch: 73: Training Loss: 0.020803259685635567, Validation Loss: 0.0239701010286808\n",
      "Epoch 18, Batch: 74: Training Loss: 0.01999099738895893, Validation Loss: 0.024042077362537384\n",
      "Epoch 18, Batch: 75: Training Loss: 0.022201767191290855, Validation Loss: 0.025234701111912727\n",
      "Epoch 18, Batch: 76: Training Loss: 0.020489808171987534, Validation Loss: 0.022742796689271927\n",
      "Epoch 18, Batch: 77: Training Loss: 0.03185811638832092, Validation Loss: 0.02375471591949463\n",
      "Epoch 18, Batch: 78: Training Loss: 0.027271971106529236, Validation Loss: 0.023877058178186417\n",
      "Epoch 18, Batch: 79: Training Loss: 0.023993894457817078, Validation Loss: 0.02459806762635708\n",
      "Epoch 18, Batch: 80: Training Loss: 0.02326427400112152, Validation Loss: 0.025205014273524284\n",
      "Epoch 18, Batch: 81: Training Loss: 0.022739263251423836, Validation Loss: 0.0265413299202919\n",
      "Epoch 18, Batch: 82: Training Loss: 0.02586822398006916, Validation Loss: 0.024456525221467018\n",
      "Epoch 18, Batch: 83: Training Loss: 0.022719018161296844, Validation Loss: 0.025338679552078247\n",
      "Epoch 18, Batch: 84: Training Loss: 0.025109821930527687, Validation Loss: 0.02292136289179325\n",
      "Epoch 18, Batch: 85: Training Loss: 0.023560475558042526, Validation Loss: 0.024170774966478348\n",
      "Epoch 18, Batch: 86: Training Loss: 0.021894721314311028, Validation Loss: 0.024257225915789604\n",
      "Epoch 18, Batch: 87: Training Loss: 0.027760779485106468, Validation Loss: 0.021782251074910164\n",
      "Epoch 18, Batch: 88: Training Loss: 0.027599884197115898, Validation Loss: 0.022621382027864456\n",
      "Epoch 18, Batch: 89: Training Loss: 0.02520078793168068, Validation Loss: 0.023716401308774948\n",
      "Epoch 18, Batch: 90: Training Loss: 0.021682873368263245, Validation Loss: 0.02300375886261463\n",
      "Epoch 18, Batch: 91: Training Loss: 0.02456318587064743, Validation Loss: 0.023875467479228973\n",
      "Epoch 18, Batch: 92: Training Loss: 0.023767465725541115, Validation Loss: 0.023775164037942886\n",
      "Epoch 18, Batch: 93: Training Loss: 0.021270930767059326, Validation Loss: 0.027400393038988113\n",
      "Epoch 18, Batch: 94: Training Loss: 0.02428734116256237, Validation Loss: 0.023030435666441917\n",
      "Epoch 18, Batch: 95: Training Loss: 0.02249770238995552, Validation Loss: 0.022905047982931137\n",
      "Epoch 18, Batch: 96: Training Loss: 0.02105793170630932, Validation Loss: 0.025274459272623062\n",
      "Epoch 18, Batch: 97: Training Loss: 0.023243479430675507, Validation Loss: 0.024404503405094147\n",
      "Epoch 18, Batch: 98: Training Loss: 0.027691088616847992, Validation Loss: 0.025061363354325294\n",
      "Epoch 18, Batch: 99: Training Loss: 0.0268655214458704, Validation Loss: 0.02642716094851494\n",
      "Epoch 18, Batch: 100: Training Loss: 0.025752365589141846, Validation Loss: 0.02511693350970745\n",
      "Epoch 18, Batch: 101: Training Loss: 0.026012476533651352, Validation Loss: 0.02540377341210842\n",
      "Epoch 18, Batch: 102: Training Loss: 0.025768937543034554, Validation Loss: 0.024593288078904152\n",
      "Epoch 18, Batch: 103: Training Loss: 0.02527797594666481, Validation Loss: 0.024740824475884438\n",
      "Epoch 18, Batch: 104: Training Loss: 0.0206668209284544, Validation Loss: 0.022971052676439285\n",
      "Epoch 18, Batch: 105: Training Loss: 0.021016201004385948, Validation Loss: 0.024688176810741425\n",
      "Epoch 18, Batch: 106: Training Loss: 0.02404184453189373, Validation Loss: 0.023804623633623123\n",
      "Epoch 18, Batch: 107: Training Loss: 0.022806961089372635, Validation Loss: 0.02246829867362976\n",
      "Epoch 18, Batch: 108: Training Loss: 0.022561686113476753, Validation Loss: 0.023161571472883224\n",
      "Epoch 18, Batch: 109: Training Loss: 0.022831542417407036, Validation Loss: 0.02411779575049877\n",
      "Epoch 18, Batch: 110: Training Loss: 0.027327464893460274, Validation Loss: 0.02461620792746544\n",
      "Epoch 18, Batch: 111: Training Loss: 0.022592348977923393, Validation Loss: 0.023492688313126564\n",
      "Epoch 18, Batch: 112: Training Loss: 0.021273711696267128, Validation Loss: 0.025368735194206238\n",
      "Epoch 18, Batch: 113: Training Loss: 0.02357117272913456, Validation Loss: 0.02536783739924431\n",
      "Epoch 18, Batch: 114: Training Loss: 0.02249094657599926, Validation Loss: 0.02962789684534073\n",
      "Epoch 18, Batch: 115: Training Loss: 0.026425136253237724, Validation Loss: 0.02417798899114132\n",
      "Epoch 18, Batch: 116: Training Loss: 0.019748667255043983, Validation Loss: 0.025361318141222\n",
      "Epoch 18, Batch: 117: Training Loss: 0.022081132978200912, Validation Loss: 0.02824985794723034\n",
      "Epoch 18, Batch: 118: Training Loss: 0.023807967081665993, Validation Loss: 0.027915168553590775\n",
      "Epoch 18, Batch: 119: Training Loss: 0.023855945095419884, Validation Loss: 0.028093157336115837\n",
      "Epoch 18, Batch: 120: Training Loss: 0.020979493856430054, Validation Loss: 0.024985382333397865\n",
      "Epoch 18, Batch: 121: Training Loss: 0.023030508309602737, Validation Loss: 0.0237874872982502\n",
      "Epoch 18, Batch: 122: Training Loss: 0.021697018295526505, Validation Loss: 0.023310136049985886\n",
      "Epoch 18, Batch: 123: Training Loss: 0.022557223215699196, Validation Loss: 0.02400691621005535\n",
      "Epoch 18, Batch: 124: Training Loss: 0.021119827404618263, Validation Loss: 0.02494620531797409\n",
      "Epoch 18, Batch: 125: Training Loss: 0.019083920866250992, Validation Loss: 0.023905718699097633\n",
      "Epoch 18, Batch: 126: Training Loss: 0.020632782950997353, Validation Loss: 0.024046914651989937\n",
      "Epoch 18, Batch: 127: Training Loss: 0.023526187986135483, Validation Loss: 0.02274768240749836\n",
      "Epoch 18, Batch: 128: Training Loss: 0.021545207127928734, Validation Loss: 0.02129967138171196\n",
      "Epoch 18, Batch: 129: Training Loss: 0.021577708423137665, Validation Loss: 0.022769274190068245\n",
      "Epoch 18, Batch: 130: Training Loss: 0.02266635000705719, Validation Loss: 0.02173653244972229\n",
      "Epoch 18, Batch: 131: Training Loss: 0.022037215530872345, Validation Loss: 0.02322102151811123\n",
      "Epoch 18, Batch: 132: Training Loss: 0.02299165166914463, Validation Loss: 0.022351078689098358\n",
      "Epoch 18, Batch: 133: Training Loss: 0.021066149696707726, Validation Loss: 0.02322496846318245\n",
      "Epoch 18, Batch: 134: Training Loss: 0.02267513796687126, Validation Loss: 0.023786790668964386\n",
      "Epoch 18, Batch: 135: Training Loss: 0.02156931720674038, Validation Loss: 0.02549165114760399\n",
      "Epoch 18, Batch: 136: Training Loss: 0.021624751389026642, Validation Loss: 0.022516658529639244\n",
      "Epoch 18, Batch: 137: Training Loss: 0.022222645580768585, Validation Loss: 0.026006344705820084\n",
      "Epoch 18, Batch: 138: Training Loss: 0.025019945576786995, Validation Loss: 0.02610068768262863\n",
      "Epoch 18, Batch: 139: Training Loss: 0.02057093009352684, Validation Loss: 0.02544115111231804\n",
      "Epoch 18, Batch: 140: Training Loss: 0.025959663093090057, Validation Loss: 0.02228158339858055\n",
      "Epoch 18, Batch: 141: Training Loss: 0.02315981686115265, Validation Loss: 0.02318851836025715\n",
      "Epoch 18, Batch: 142: Training Loss: 0.023308349773287773, Validation Loss: 0.02198515087366104\n",
      "Epoch 18, Batch: 143: Training Loss: 0.02400343306362629, Validation Loss: 0.022027332335710526\n",
      "Epoch 18, Batch: 144: Training Loss: 0.020091427490115166, Validation Loss: 0.0230602715164423\n",
      "Epoch 18, Batch: 145: Training Loss: 0.023032454773783684, Validation Loss: 0.02193673886358738\n",
      "Epoch 18, Batch: 146: Training Loss: 0.021176015958189964, Validation Loss: 0.024080749601125717\n",
      "Epoch 18, Batch: 147: Training Loss: 0.020376717671751976, Validation Loss: 0.02536003477871418\n",
      "Epoch 18, Batch: 148: Training Loss: 0.01998266763985157, Validation Loss: 0.02214103192090988\n",
      "Epoch 18, Batch: 149: Training Loss: 0.024734588339924812, Validation Loss: 0.02380777895450592\n",
      "Epoch 18, Batch: 150: Training Loss: 0.025728002190589905, Validation Loss: 0.026704786345362663\n",
      "Epoch 18, Batch: 151: Training Loss: 0.022740887477993965, Validation Loss: 0.02367108128964901\n",
      "Epoch 18, Batch: 152: Training Loss: 0.024446137249469757, Validation Loss: 0.025969551876187325\n",
      "Epoch 18, Batch: 153: Training Loss: 0.024998877197504044, Validation Loss: 0.022721529006958008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 154: Training Loss: 0.025735780596733093, Validation Loss: 0.02380279265344143\n",
      "Epoch 18, Batch: 155: Training Loss: 0.026578465476632118, Validation Loss: 0.024453841149806976\n",
      "Epoch 18, Batch: 156: Training Loss: 0.021445082500576973, Validation Loss: 0.023408666253089905\n",
      "Epoch 18, Batch: 157: Training Loss: 0.024487216025590897, Validation Loss: 0.022495053708553314\n",
      "Epoch 18, Batch: 158: Training Loss: 0.021681131795048714, Validation Loss: 0.023148301988840103\n",
      "Epoch 18, Batch: 159: Training Loss: 0.02521885372698307, Validation Loss: 0.022701887413859367\n",
      "Epoch 18, Batch: 160: Training Loss: 0.022057291120290756, Validation Loss: 0.020750675350427628\n",
      "Epoch 18, Batch: 161: Training Loss: 0.0219107773154974, Validation Loss: 0.022886818274855614\n",
      "Epoch 18, Batch: 162: Training Loss: 0.022239232435822487, Validation Loss: 0.026152772828936577\n",
      "Epoch 18, Batch: 163: Training Loss: 0.02481217496097088, Validation Loss: 0.024596819654107094\n",
      "Epoch 18, Batch: 164: Training Loss: 0.020571593195199966, Validation Loss: 0.02191486768424511\n",
      "Epoch 18, Batch: 165: Training Loss: 0.02176256850361824, Validation Loss: 0.027772875502705574\n",
      "Epoch 18, Batch: 166: Training Loss: 0.023080073297023773, Validation Loss: 0.02764221653342247\n",
      "Epoch 18, Batch: 167: Training Loss: 0.021765384823083878, Validation Loss: 0.02550891786813736\n",
      "Epoch 18, Batch: 168: Training Loss: 0.023821761831641197, Validation Loss: 0.02521349862217903\n",
      "Epoch 18, Batch: 169: Training Loss: 0.02531735971570015, Validation Loss: 0.0256723053753376\n",
      "Epoch 18, Batch: 170: Training Loss: 0.023425377905368805, Validation Loss: 0.025825923308730125\n",
      "Epoch 18, Batch: 171: Training Loss: 0.02178005501627922, Validation Loss: 0.02850603498518467\n",
      "Epoch 18, Batch: 172: Training Loss: 0.021584337577223778, Validation Loss: 0.029433922842144966\n",
      "Epoch 18, Batch: 173: Training Loss: 0.021742261946201324, Validation Loss: 0.02875802293419838\n",
      "Epoch 18, Batch: 174: Training Loss: 0.0254376120865345, Validation Loss: 0.025540493428707123\n",
      "Epoch 18, Batch: 175: Training Loss: 0.021448839455842972, Validation Loss: 0.02775140292942524\n",
      "Epoch 18, Batch: 176: Training Loss: 0.022630665451288223, Validation Loss: 0.027677791193127632\n",
      "Epoch 18, Batch: 177: Training Loss: 0.02272041328251362, Validation Loss: 0.026546895503997803\n",
      "Epoch 18, Batch: 178: Training Loss: 0.023949699476361275, Validation Loss: 0.028939802199602127\n",
      "Epoch 18, Batch: 179: Training Loss: 0.0211541336029768, Validation Loss: 0.02724362537264824\n",
      "Epoch 18, Batch: 180: Training Loss: 0.021775957196950912, Validation Loss: 0.030972810462117195\n",
      "Epoch 18, Batch: 181: Training Loss: 0.02235810086131096, Validation Loss: 0.02993815951049328\n",
      "Epoch 18, Batch: 182: Training Loss: 0.02128016948699951, Validation Loss: 0.028430065140128136\n",
      "Epoch 18, Batch: 183: Training Loss: 0.02672525867819786, Validation Loss: 0.02773396670818329\n",
      "Epoch 18, Batch: 184: Training Loss: 0.027569536119699478, Validation Loss: 0.027942631393671036\n",
      "Epoch 18, Batch: 185: Training Loss: 0.02564583532512188, Validation Loss: 0.030271559953689575\n",
      "Epoch 18, Batch: 186: Training Loss: 0.02475745975971222, Validation Loss: 0.02644510008394718\n",
      "Epoch 18, Batch: 187: Training Loss: 0.021937206387519836, Validation Loss: 0.02822716347873211\n",
      "Epoch 18, Batch: 188: Training Loss: 0.02780091017484665, Validation Loss: 0.02761124074459076\n",
      "Epoch 18, Batch: 189: Training Loss: 0.02160564251244068, Validation Loss: 0.026673978194594383\n",
      "Epoch 18, Batch: 190: Training Loss: 0.023771077394485474, Validation Loss: 0.024053482338786125\n",
      "Epoch 18, Batch: 191: Training Loss: 0.02427821420133114, Validation Loss: 0.026916369795799255\n",
      "Epoch 18, Batch: 192: Training Loss: 0.026009803637862206, Validation Loss: 0.027255302295088768\n",
      "Epoch 18, Batch: 193: Training Loss: 0.024760019034147263, Validation Loss: 0.025564584881067276\n",
      "Epoch 18, Batch: 194: Training Loss: 0.02849980629980564, Validation Loss: 0.028438854962587357\n",
      "Epoch 18, Batch: 195: Training Loss: 0.023211048915982246, Validation Loss: 0.02679470367729664\n",
      "Epoch 18, Batch: 196: Training Loss: 0.024124225601553917, Validation Loss: 0.025608429685235023\n",
      "Epoch 18, Batch: 197: Training Loss: 0.02089824713766575, Validation Loss: 0.024479348212480545\n",
      "Epoch 18, Batch: 198: Training Loss: 0.021945012733340263, Validation Loss: 0.025006970390677452\n",
      "Epoch 18, Batch: 199: Training Loss: 0.022895285859704018, Validation Loss: 0.027587134391069412\n",
      "Epoch 18, Batch: 200: Training Loss: 0.01955096237361431, Validation Loss: 0.024323947727680206\n",
      "Epoch 18, Batch: 201: Training Loss: 0.027880912646651268, Validation Loss: 0.02495020627975464\n",
      "Epoch 18, Batch: 202: Training Loss: 0.026032935827970505, Validation Loss: 0.026605922728776932\n",
      "Epoch 18, Batch: 203: Training Loss: 0.023266974836587906, Validation Loss: 0.026550915092229843\n",
      "Epoch 18, Batch: 204: Training Loss: 0.021986454725265503, Validation Loss: 0.026491424068808556\n",
      "Epoch 18, Batch: 205: Training Loss: 0.0303609911352396, Validation Loss: 0.024464357644319534\n",
      "Epoch 18, Batch: 206: Training Loss: 0.027121208608150482, Validation Loss: 0.023764096200466156\n",
      "Epoch 18, Batch: 207: Training Loss: 0.02479826845228672, Validation Loss: 0.02591901458799839\n",
      "Epoch 18, Batch: 208: Training Loss: 0.02361457422375679, Validation Loss: 0.024118686094880104\n",
      "Epoch 18, Batch: 209: Training Loss: 0.025174152106046677, Validation Loss: 0.0235439483076334\n",
      "Epoch 18, Batch: 210: Training Loss: 0.024708032608032227, Validation Loss: 0.02373499795794487\n",
      "Epoch 18, Batch: 211: Training Loss: 0.02407967485487461, Validation Loss: 0.02326892875134945\n",
      "Epoch 18, Batch: 212: Training Loss: 0.025224721059203148, Validation Loss: 0.02687969245016575\n",
      "Epoch 18, Batch: 213: Training Loss: 0.02914787270128727, Validation Loss: 0.02462887018918991\n",
      "Epoch 18, Batch: 214: Training Loss: 0.023590413853526115, Validation Loss: 0.025004589930176735\n",
      "Epoch 18, Batch: 215: Training Loss: 0.024386940523982048, Validation Loss: 0.024309484288096428\n",
      "Epoch 18, Batch: 216: Training Loss: 0.025769542902708054, Validation Loss: 0.024894172325730324\n",
      "Epoch 18, Batch: 217: Training Loss: 0.0236993245780468, Validation Loss: 0.02628498710691929\n",
      "Epoch 18, Batch: 218: Training Loss: 0.021212639287114143, Validation Loss: 0.023815583437681198\n",
      "Epoch 18, Batch: 219: Training Loss: 0.023116691038012505, Validation Loss: 0.024356864392757416\n",
      "Epoch 18, Batch: 220: Training Loss: 0.024350393563508987, Validation Loss: 0.024911439046263695\n",
      "Epoch 18, Batch: 221: Training Loss: 0.02156275324523449, Validation Loss: 0.025599146261811256\n",
      "Epoch 18, Batch: 222: Training Loss: 0.025129996240139008, Validation Loss: 0.025538112968206406\n",
      "Epoch 18, Batch: 223: Training Loss: 0.02336539328098297, Validation Loss: 0.026156367734074593\n",
      "Epoch 18, Batch: 224: Training Loss: 0.024262085556983948, Validation Loss: 0.024383898824453354\n",
      "Epoch 18, Batch: 225: Training Loss: 0.02384542115032673, Validation Loss: 0.022274024784564972\n",
      "Epoch 18, Batch: 226: Training Loss: 0.022834865376353264, Validation Loss: 0.023423144593834877\n",
      "Epoch 18, Batch: 227: Training Loss: 0.023074453696608543, Validation Loss: 0.02151719108223915\n",
      "Epoch 18, Batch: 228: Training Loss: 0.024377087131142616, Validation Loss: 0.02345268800854683\n",
      "Epoch 18, Batch: 229: Training Loss: 0.024495702236890793, Validation Loss: 0.023797566071152687\n",
      "Epoch 18, Batch: 230: Training Loss: 0.020656855776906013, Validation Loss: 0.02234461158514023\n",
      "Epoch 18, Batch: 231: Training Loss: 0.023976584896445274, Validation Loss: 0.022131068632006645\n",
      "Epoch 18, Batch: 232: Training Loss: 0.025410303846001625, Validation Loss: 0.026820354163646698\n",
      "Epoch 18, Batch: 233: Training Loss: 0.01806219480931759, Validation Loss: 0.023981520906090736\n",
      "Epoch 18, Batch: 234: Training Loss: 0.02528434433043003, Validation Loss: 0.02363491989672184\n",
      "Epoch 18, Batch: 235: Training Loss: 0.024917082861065865, Validation Loss: 0.023502713069319725\n",
      "Epoch 18, Batch: 236: Training Loss: 0.024000301957130432, Validation Loss: 0.025065170601010323\n",
      "Epoch 18, Batch: 237: Training Loss: 0.02419411577284336, Validation Loss: 0.025706827640533447\n",
      "Epoch 18, Batch: 238: Training Loss: 0.026068970561027527, Validation Loss: 0.025246605277061462\n",
      "Epoch 18, Batch: 239: Training Loss: 0.024240175262093544, Validation Loss: 0.022556934505701065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 240: Training Loss: 0.02152380347251892, Validation Loss: 0.0225242730230093\n",
      "Epoch 18, Batch: 241: Training Loss: 0.023587295785546303, Validation Loss: 0.02572542615234852\n",
      "Epoch 18, Batch: 242: Training Loss: 0.020970426499843597, Validation Loss: 0.022209767252206802\n",
      "Epoch 18, Batch: 243: Training Loss: 0.028009042143821716, Validation Loss: 0.022466296330094337\n",
      "Epoch 18, Batch: 244: Training Loss: 0.021621890366077423, Validation Loss: 0.023471063002943993\n",
      "Epoch 18, Batch: 245: Training Loss: 0.023449720814824104, Validation Loss: 0.024993404746055603\n",
      "Epoch 18, Batch: 246: Training Loss: 0.022298643365502357, Validation Loss: 0.023518336936831474\n",
      "Epoch 18, Batch: 247: Training Loss: 0.023894501850008965, Validation Loss: 0.022206563502550125\n",
      "Epoch 18, Batch: 248: Training Loss: 0.020575011149048805, Validation Loss: 0.02431461587548256\n",
      "Epoch 18, Batch: 249: Training Loss: 0.020432516932487488, Validation Loss: 0.02347288653254509\n",
      "Epoch 18, Batch: 250: Training Loss: 0.021794917061924934, Validation Loss: 0.023389682173728943\n",
      "Epoch 18, Batch: 251: Training Loss: 0.019430087879300117, Validation Loss: 0.022286338731646538\n",
      "Epoch 18, Batch: 252: Training Loss: 0.02358121983706951, Validation Loss: 0.023150520399212837\n",
      "Epoch 18, Batch: 253: Training Loss: 0.022360702976584435, Validation Loss: 0.020370684564113617\n",
      "Epoch 18, Batch: 254: Training Loss: 0.018920540809631348, Validation Loss: 0.02453911118209362\n",
      "Epoch 18, Batch: 255: Training Loss: 0.02267412282526493, Validation Loss: 0.02270525135099888\n",
      "Epoch 18, Batch: 256: Training Loss: 0.020620722323656082, Validation Loss: 0.022061893716454506\n",
      "Epoch 18, Batch: 257: Training Loss: 0.022950630635023117, Validation Loss: 0.023451900109648705\n",
      "Epoch 18, Batch: 258: Training Loss: 0.024432139471173286, Validation Loss: 0.0229999627918005\n",
      "Epoch 18, Batch: 259: Training Loss: 0.024915557354688644, Validation Loss: 0.022214055061340332\n",
      "Epoch 18, Batch: 260: Training Loss: 0.023380693048238754, Validation Loss: 0.0238232109695673\n",
      "Epoch 18, Batch: 261: Training Loss: 0.023623354732990265, Validation Loss: 0.02689182385802269\n",
      "Epoch 18, Batch: 262: Training Loss: 0.02162284031510353, Validation Loss: 0.0274807121604681\n",
      "Epoch 18, Batch: 263: Training Loss: 0.0186688881367445, Validation Loss: 0.02760923095047474\n",
      "Epoch 18, Batch: 264: Training Loss: 0.018948866054415703, Validation Loss: 0.026025546714663506\n",
      "Epoch 18, Batch: 265: Training Loss: 0.025526899844408035, Validation Loss: 0.026406869292259216\n",
      "Epoch 18, Batch: 266: Training Loss: 0.02077796868979931, Validation Loss: 0.026256266981363297\n",
      "Epoch 18, Batch: 267: Training Loss: 0.018430646508932114, Validation Loss: 0.024844499304890633\n",
      "Epoch 18, Batch: 268: Training Loss: 0.021730657666921616, Validation Loss: 0.02656116709113121\n",
      "Epoch 18, Batch: 269: Training Loss: 0.021464141085743904, Validation Loss: 0.030065353959798813\n",
      "Epoch 18, Batch: 270: Training Loss: 0.021523920819163322, Validation Loss: 0.027025775983929634\n",
      "Epoch 18, Batch: 271: Training Loss: 0.02486623264849186, Validation Loss: 0.02698826603591442\n",
      "Epoch 18, Batch: 272: Training Loss: 0.02215316891670227, Validation Loss: 0.02761884592473507\n",
      "Epoch 18, Batch: 273: Training Loss: 0.021355008706450462, Validation Loss: 0.029720716178417206\n",
      "Epoch 18, Batch: 274: Training Loss: 0.02321246638894081, Validation Loss: 0.024225080385804176\n",
      "Epoch 18, Batch: 275: Training Loss: 0.021459046751260757, Validation Loss: 0.02349740080535412\n",
      "Epoch 18, Batch: 276: Training Loss: 0.02139325439929962, Validation Loss: 0.024369878694415092\n",
      "Epoch 18, Batch: 277: Training Loss: 0.021188193932175636, Validation Loss: 0.023155633360147476\n",
      "Epoch 18, Batch: 278: Training Loss: 0.021824752911925316, Validation Loss: 0.024032553657889366\n",
      "Epoch 18, Batch: 279: Training Loss: 0.022881686687469482, Validation Loss: 0.02624824270606041\n",
      "Epoch 18, Batch: 280: Training Loss: 0.022841565310955048, Validation Loss: 0.0231692623347044\n",
      "Epoch 18, Batch: 281: Training Loss: 0.0233786478638649, Validation Loss: 0.022294431924819946\n",
      "Epoch 18, Batch: 282: Training Loss: 0.02326193079352379, Validation Loss: 0.02167803794145584\n",
      "Epoch 18, Batch: 283: Training Loss: 0.02504541538655758, Validation Loss: 0.02392541989684105\n",
      "Saving new best model w/ loss: 0.01943676359951496\n",
      "Epoch 18, Batch: 284: Training Loss: 0.021967822685837746, Validation Loss: 0.01943676359951496\n",
      "Epoch 18, Batch: 285: Training Loss: 0.02233239635825157, Validation Loss: 0.02102651633322239\n",
      "Epoch 18, Batch: 286: Training Loss: 0.021244194358587265, Validation Loss: 0.021170755848288536\n",
      "Epoch 18, Batch: 287: Training Loss: 0.023528004065155983, Validation Loss: 0.023032095283269882\n",
      "Epoch 18, Batch: 288: Training Loss: 0.020774582400918007, Validation Loss: 0.024164222180843353\n",
      "Epoch 18, Batch: 289: Training Loss: 0.024264466017484665, Validation Loss: 0.022934377193450928\n",
      "Epoch 18, Batch: 290: Training Loss: 0.02374551258981228, Validation Loss: 0.021661126986145973\n",
      "Epoch 18, Batch: 291: Training Loss: 0.021597079932689667, Validation Loss: 0.02149198018014431\n",
      "Epoch 18, Batch: 292: Training Loss: 0.026209883391857147, Validation Loss: 0.023769233375787735\n",
      "Epoch 18, Batch: 293: Training Loss: 0.02544730342924595, Validation Loss: 0.021207185462117195\n",
      "Epoch 18, Batch: 294: Training Loss: 0.020725319162011147, Validation Loss: 0.023816358298063278\n",
      "Epoch 18, Batch: 295: Training Loss: 0.02374088019132614, Validation Loss: 0.021455369889736176\n",
      "Epoch 18, Batch: 296: Training Loss: 0.021753018721938133, Validation Loss: 0.019852571189403534\n",
      "Epoch 18, Batch: 297: Training Loss: 0.024623338133096695, Validation Loss: 0.020981837064027786\n",
      "Epoch 18, Batch: 298: Training Loss: 0.019530365243554115, Validation Loss: 0.023857450112700462\n",
      "Epoch 18, Batch: 299: Training Loss: 0.018840519711375237, Validation Loss: 0.02208721823990345\n",
      "Epoch 18, Batch: 300: Training Loss: 0.02441013790667057, Validation Loss: 0.024286577478051186\n",
      "Epoch 18, Batch: 301: Training Loss: 0.018395094200968742, Validation Loss: 0.024233603850007057\n",
      "Epoch 18, Batch: 302: Training Loss: 0.02103053592145443, Validation Loss: 0.02276928350329399\n",
      "Epoch 18, Batch: 303: Training Loss: 0.022788293659687042, Validation Loss: 0.02437993697822094\n",
      "Epoch 18, Batch: 304: Training Loss: 0.023133624345064163, Validation Loss: 0.023617522791028023\n",
      "Epoch 18, Batch: 305: Training Loss: 0.022564927116036415, Validation Loss: 0.021857531741261482\n",
      "Epoch 18, Batch: 306: Training Loss: 0.022409386932849884, Validation Loss: 0.023032421246170998\n",
      "Epoch 18, Batch: 307: Training Loss: 0.02013222686946392, Validation Loss: 0.023752866312861443\n",
      "Epoch 18, Batch: 308: Training Loss: 0.022790217772126198, Validation Loss: 0.022431788966059685\n",
      "Epoch 18, Batch: 309: Training Loss: 0.0223060455173254, Validation Loss: 0.02315608412027359\n",
      "Epoch 18, Batch: 310: Training Loss: 0.02074585109949112, Validation Loss: 0.023815257474780083\n",
      "Epoch 18, Batch: 311: Training Loss: 0.0220234002918005, Validation Loss: 0.022061282768845558\n",
      "Epoch 18, Batch: 312: Training Loss: 0.02301996387541294, Validation Loss: 0.02320677414536476\n",
      "Epoch 18, Batch: 313: Training Loss: 0.01997331529855728, Validation Loss: 0.023486044257879257\n",
      "Epoch 18, Batch: 314: Training Loss: 0.02201000787317753, Validation Loss: 0.02262132801115513\n",
      "Epoch 18, Batch: 315: Training Loss: 0.022620949894189835, Validation Loss: 0.020025257021188736\n",
      "Epoch 18, Batch: 316: Training Loss: 0.020815929397940636, Validation Loss: 0.023058615624904633\n",
      "Epoch 18, Batch: 317: Training Loss: 0.019408807158470154, Validation Loss: 0.021836090832948685\n",
      "Epoch 18, Batch: 318: Training Loss: 0.019561896100640297, Validation Loss: 0.023851865902543068\n",
      "Epoch 18, Batch: 319: Training Loss: 0.022708792239427567, Validation Loss: 0.022499166429042816\n",
      "Epoch 18, Batch: 320: Training Loss: 0.022182514891028404, Validation Loss: 0.020235609263181686\n",
      "Epoch 18, Batch: 321: Training Loss: 0.01905079185962677, Validation Loss: 0.020329434424638748\n",
      "Epoch 18, Batch: 322: Training Loss: 0.022869043052196503, Validation Loss: 0.02118554338812828\n",
      "Epoch 18, Batch: 323: Training Loss: 0.02080106921494007, Validation Loss: 0.02387269027531147\n",
      "Epoch 18, Batch: 324: Training Loss: 0.02637479268014431, Validation Loss: 0.024070020765066147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 325: Training Loss: 0.020125607028603554, Validation Loss: 0.023078229278326035\n",
      "Epoch 18, Batch: 326: Training Loss: 0.02465072274208069, Validation Loss: 0.025909824296832085\n",
      "Epoch 18, Batch: 327: Training Loss: 0.01843932829797268, Validation Loss: 0.02191932499408722\n",
      "Epoch 18, Batch: 328: Training Loss: 0.023620974272489548, Validation Loss: 0.024059126153588295\n",
      "Epoch 18, Batch: 329: Training Loss: 0.024302514269948006, Validation Loss: 0.02395157516002655\n",
      "Epoch 18, Batch: 330: Training Loss: 0.027839237824082375, Validation Loss: 0.026366641744971275\n",
      "Epoch 18, Batch: 331: Training Loss: 0.0230249036103487, Validation Loss: 0.022721750661730766\n",
      "Epoch 18, Batch: 332: Training Loss: 0.022416945546865463, Validation Loss: 0.023035824298858643\n",
      "Epoch 18, Batch: 333: Training Loss: 0.022721027955412865, Validation Loss: 0.025103840976953506\n",
      "Epoch 18, Batch: 334: Training Loss: 0.023016590625047684, Validation Loss: 0.025963913649320602\n",
      "Epoch 18, Batch: 335: Training Loss: 0.022910896688699722, Validation Loss: 0.02506447583436966\n",
      "Epoch 18, Batch: 336: Training Loss: 0.024627353996038437, Validation Loss: 0.027052534744143486\n",
      "Epoch 18, Batch: 337: Training Loss: 0.021996311843395233, Validation Loss: 0.027213236317038536\n",
      "Epoch 18, Batch: 338: Training Loss: 0.024954605847597122, Validation Loss: 0.027085082605481148\n",
      "Epoch 18, Batch: 339: Training Loss: 0.020908169448375702, Validation Loss: 0.026577113196253777\n",
      "Epoch 18, Batch: 340: Training Loss: 0.0244798231869936, Validation Loss: 0.025643810629844666\n",
      "Epoch 18, Batch: 341: Training Loss: 0.02377934567630291, Validation Loss: 0.02698967233300209\n",
      "Epoch 18, Batch: 342: Training Loss: 0.02488435059785843, Validation Loss: 0.026960870251059532\n",
      "Epoch 18, Batch: 343: Training Loss: 0.025235651060938835, Validation Loss: 0.02571774832904339\n",
      "Epoch 18, Batch: 344: Training Loss: 0.02519139088690281, Validation Loss: 0.02462715655565262\n",
      "Epoch 18, Batch: 345: Training Loss: 0.022814899682998657, Validation Loss: 0.02728375419974327\n",
      "Epoch 18, Batch: 346: Training Loss: 0.023932209238409996, Validation Loss: 0.026881184428930283\n",
      "Epoch 18, Batch: 347: Training Loss: 0.02200094237923622, Validation Loss: 0.026054533198475838\n",
      "Epoch 18, Batch: 348: Training Loss: 0.022081812843680382, Validation Loss: 0.025997117161750793\n",
      "Epoch 18, Batch: 349: Training Loss: 0.024472961202263832, Validation Loss: 0.024622974917292595\n",
      "Epoch 18, Batch: 350: Training Loss: 0.017444390803575516, Validation Loss: 0.026926623657345772\n",
      "Epoch 18, Batch: 351: Training Loss: 0.026622965931892395, Validation Loss: 0.025856440886855125\n",
      "Epoch 18, Batch: 352: Training Loss: 0.02354971505701542, Validation Loss: 0.025192702189087868\n",
      "Epoch 18, Batch: 353: Training Loss: 0.02485075034201145, Validation Loss: 0.025451714172959328\n",
      "Epoch 18, Batch: 354: Training Loss: 0.02413986809551716, Validation Loss: 0.0258750319480896\n",
      "Epoch 18, Batch: 355: Training Loss: 0.01942504197359085, Validation Loss: 0.025574781000614166\n",
      "Epoch 18, Batch: 356: Training Loss: 0.024050017818808556, Validation Loss: 0.02708546258509159\n",
      "Epoch 18, Batch: 357: Training Loss: 0.019779350608587265, Validation Loss: 0.023892270401120186\n",
      "Epoch 18, Batch: 358: Training Loss: 0.023465203121304512, Validation Loss: 0.02758024074137211\n",
      "Epoch 18, Batch: 359: Training Loss: 0.02405800297856331, Validation Loss: 0.026325033977627754\n",
      "Epoch 18, Batch: 360: Training Loss: 0.02370557375252247, Validation Loss: 0.028980033472180367\n",
      "Epoch 18, Batch: 361: Training Loss: 0.021627524867653847, Validation Loss: 0.026890013366937637\n",
      "Epoch 18, Batch: 362: Training Loss: 0.02293797954916954, Validation Loss: 0.026583930477499962\n",
      "Epoch 18, Batch: 363: Training Loss: 0.027991512790322304, Validation Loss: 0.024112269282341003\n",
      "Epoch 18, Batch: 364: Training Loss: 0.0253608375787735, Validation Loss: 0.023227786645293236\n",
      "Epoch 18, Batch: 365: Training Loss: 0.024118194356560707, Validation Loss: 0.024035809561610222\n",
      "Epoch 18, Batch: 366: Training Loss: 0.022737832739949226, Validation Loss: 0.025758836418390274\n",
      "Epoch 18, Batch: 367: Training Loss: 0.020938249304890633, Validation Loss: 0.02616526186466217\n",
      "Epoch 18, Batch: 368: Training Loss: 0.02135423943400383, Validation Loss: 0.02377791702747345\n",
      "Epoch 18, Batch: 369: Training Loss: 0.021680429577827454, Validation Loss: 0.02678499184548855\n",
      "Epoch 18, Batch: 370: Training Loss: 0.023404663428664207, Validation Loss: 0.02507033757865429\n",
      "Epoch 18, Batch: 371: Training Loss: 0.020274359732866287, Validation Loss: 0.023006673902273178\n",
      "Epoch 18, Batch: 372: Training Loss: 0.02073882706463337, Validation Loss: 0.0232290867716074\n",
      "Epoch 18, Batch: 373: Training Loss: 0.02472088672220707, Validation Loss: 0.023021267727017403\n",
      "Epoch 18, Batch: 374: Training Loss: 0.020416932180523872, Validation Loss: 0.025242267176508904\n",
      "Epoch 18, Batch: 375: Training Loss: 0.024070657789707184, Validation Loss: 0.023772310465574265\n",
      "Epoch 18, Batch: 376: Training Loss: 0.02455197088420391, Validation Loss: 0.022751765325665474\n",
      "Epoch 18, Batch: 377: Training Loss: 0.02079283446073532, Validation Loss: 0.022620992735028267\n",
      "Epoch 18, Batch: 378: Training Loss: 0.02053184248507023, Validation Loss: 0.024511948227882385\n",
      "Epoch 18, Batch: 379: Training Loss: 0.02328440546989441, Validation Loss: 0.024694165214896202\n",
      "Epoch 18, Batch: 380: Training Loss: 0.025694068521261215, Validation Loss: 0.022692812606692314\n",
      "Epoch 18, Batch: 381: Training Loss: 0.021776938810944557, Validation Loss: 0.025118693709373474\n",
      "Epoch 18, Batch: 382: Training Loss: 0.022586826235055923, Validation Loss: 0.02252550609409809\n",
      "Epoch 18, Batch: 383: Training Loss: 0.02210766263306141, Validation Loss: 0.023805832490324974\n",
      "Epoch 18, Batch: 384: Training Loss: 0.0256517231464386, Validation Loss: 0.023806540295481682\n",
      "Epoch 18, Batch: 385: Training Loss: 0.022907458245754242, Validation Loss: 0.026076633483171463\n",
      "Epoch 18, Batch: 386: Training Loss: 0.01649739407002926, Validation Loss: 0.024988263845443726\n",
      "Epoch 18, Batch: 387: Training Loss: 0.02460268698632717, Validation Loss: 0.02172897383570671\n",
      "Epoch 18, Batch: 388: Training Loss: 0.020592179149389267, Validation Loss: 0.023375455290079117\n",
      "Epoch 18, Batch: 389: Training Loss: 0.02260272391140461, Validation Loss: 0.023121302947402\n",
      "Epoch 18, Batch: 390: Training Loss: 0.021017789840698242, Validation Loss: 0.024484757333993912\n",
      "Epoch 18, Batch: 391: Training Loss: 0.02440287545323372, Validation Loss: 0.024281037971377373\n",
      "Epoch 18, Batch: 392: Training Loss: 0.019649095833301544, Validation Loss: 0.023572690784931183\n",
      "Epoch 18, Batch: 393: Training Loss: 0.022479087114334106, Validation Loss: 0.0245609562844038\n",
      "Epoch 18, Batch: 394: Training Loss: 0.02215718850493431, Validation Loss: 0.024112151935696602\n",
      "Epoch 18, Batch: 395: Training Loss: 0.021155567839741707, Validation Loss: 0.021765178069472313\n",
      "Epoch 18, Batch: 396: Training Loss: 0.021843498572707176, Validation Loss: 0.024087779223918915\n",
      "Epoch 18, Batch: 397: Training Loss: 0.022779276594519615, Validation Loss: 0.023723023012280464\n",
      "Epoch 18, Batch: 398: Training Loss: 0.02117372304201126, Validation Loss: 0.022913848981261253\n",
      "Epoch 18, Batch: 399: Training Loss: 0.022285686805844307, Validation Loss: 0.0241154246032238\n",
      "Epoch 18, Batch: 400: Training Loss: 0.022750766947865486, Validation Loss: 0.02555001527070999\n",
      "Epoch 18, Batch: 401: Training Loss: 0.020531833171844482, Validation Loss: 0.022937776520848274\n",
      "Epoch 18, Batch: 402: Training Loss: 0.021465936675667763, Validation Loss: 0.024770954623818398\n",
      "Epoch 18, Batch: 403: Training Loss: 0.026142483577132225, Validation Loss: 0.024887334555387497\n",
      "Epoch 18, Batch: 404: Training Loss: 0.0211689043790102, Validation Loss: 0.024222413077950478\n",
      "Epoch 18, Batch: 405: Training Loss: 0.024287477135658264, Validation Loss: 0.02291989140212536\n",
      "Epoch 18, Batch: 406: Training Loss: 0.023312536999583244, Validation Loss: 0.02576834335923195\n",
      "Epoch 18, Batch: 407: Training Loss: 0.021665111184120178, Validation Loss: 0.02387586236000061\n",
      "Epoch 18, Batch: 408: Training Loss: 0.01839226856827736, Validation Loss: 0.024294337257742882\n",
      "Epoch 18, Batch: 409: Training Loss: 0.021379156038165092, Validation Loss: 0.02274438366293907\n",
      "Epoch 18, Batch: 410: Training Loss: 0.022308332845568657, Validation Loss: 0.01979517750442028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 411: Training Loss: 0.02048652246594429, Validation Loss: 0.022037042304873466\n",
      "Epoch 18, Batch: 412: Training Loss: 0.023729447275400162, Validation Loss: 0.02223316766321659\n",
      "Epoch 18, Batch: 413: Training Loss: 0.024806296452879906, Validation Loss: 0.024809449911117554\n",
      "Epoch 18, Batch: 414: Training Loss: 0.02110881358385086, Validation Loss: 0.02416563220322132\n",
      "Epoch 18, Batch: 415: Training Loss: 0.02473369613289833, Validation Loss: 0.023677201941609383\n",
      "Epoch 18, Batch: 416: Training Loss: 0.01965361461043358, Validation Loss: 0.0226709246635437\n",
      "Epoch 18, Batch: 417: Training Loss: 0.019645925611257553, Validation Loss: 0.02533709816634655\n",
      "Epoch 18, Batch: 418: Training Loss: 0.021428899839520454, Validation Loss: 0.026238419115543365\n",
      "Epoch 18, Batch: 419: Training Loss: 0.018549833446741104, Validation Loss: 0.023143311962485313\n",
      "Epoch 18, Batch: 420: Training Loss: 0.024948976933956146, Validation Loss: 0.024253476411104202\n",
      "Epoch 18, Batch: 421: Training Loss: 0.026408640667796135, Validation Loss: 0.02424241043627262\n",
      "Epoch 18, Batch: 422: Training Loss: 0.02722904458642006, Validation Loss: 0.02312128245830536\n",
      "Epoch 18, Batch: 423: Training Loss: 0.02531503699719906, Validation Loss: 0.024735447019338608\n",
      "Epoch 18, Batch: 424: Training Loss: 0.02288242243230343, Validation Loss: 0.024744587019085884\n",
      "Epoch 18, Batch: 425: Training Loss: 0.022110531106591225, Validation Loss: 0.02563910372555256\n",
      "Epoch 18, Batch: 426: Training Loss: 0.02129930630326271, Validation Loss: 0.026484737172722816\n",
      "Epoch 18, Batch: 427: Training Loss: 0.021509524434804916, Validation Loss: 0.025023160502314568\n",
      "Epoch 18, Batch: 428: Training Loss: 0.021010475233197212, Validation Loss: 0.026600874960422516\n",
      "Epoch 18, Batch: 429: Training Loss: 0.019065406173467636, Validation Loss: 0.026410967111587524\n",
      "Epoch 18, Batch: 430: Training Loss: 0.020588407292962074, Validation Loss: 0.028368785977363586\n",
      "Epoch 18, Batch: 431: Training Loss: 0.019854404032230377, Validation Loss: 0.026533085852861404\n",
      "Epoch 18, Batch: 432: Training Loss: 0.027136798948049545, Validation Loss: 0.027318812906742096\n",
      "Epoch 18, Batch: 433: Training Loss: 0.020976394414901733, Validation Loss: 0.028656689450144768\n",
      "Epoch 18, Batch: 434: Training Loss: 0.02590540237724781, Validation Loss: 0.028723666444420815\n",
      "Epoch 18, Batch: 435: Training Loss: 0.021870318800210953, Validation Loss: 0.0270403940230608\n",
      "Epoch 18, Batch: 436: Training Loss: 0.02333233878016472, Validation Loss: 0.029568012803792953\n",
      "Epoch 18, Batch: 437: Training Loss: 0.022801758721470833, Validation Loss: 0.026453591883182526\n",
      "Epoch 18, Batch: 438: Training Loss: 0.02619076892733574, Validation Loss: 0.026246260851621628\n",
      "Epoch 18, Batch: 439: Training Loss: 0.019239485263824463, Validation Loss: 0.029064716771245003\n",
      "Epoch 18, Batch: 440: Training Loss: 0.02494022622704506, Validation Loss: 0.024796124547719955\n",
      "Epoch 18, Batch: 441: Training Loss: 0.0206318236887455, Validation Loss: 0.025585686787962914\n",
      "Epoch 18, Batch: 442: Training Loss: 0.025551898404955864, Validation Loss: 0.025869222357869148\n",
      "Epoch 18, Batch: 443: Training Loss: 0.02349371835589409, Validation Loss: 0.02485097572207451\n",
      "Epoch 18, Batch: 444: Training Loss: 0.02034452185034752, Validation Loss: 0.023305388167500496\n",
      "Epoch 18, Batch: 445: Training Loss: 0.019107691943645477, Validation Loss: 0.023190665990114212\n",
      "Epoch 18, Batch: 446: Training Loss: 0.02445550076663494, Validation Loss: 0.0222689937800169\n",
      "Epoch 18, Batch: 447: Training Loss: 0.020349066704511642, Validation Loss: 0.02554316259920597\n",
      "Epoch 18, Batch: 448: Training Loss: 0.02489997260272503, Validation Loss: 0.024977920576930046\n",
      "Epoch 18, Batch: 449: Training Loss: 0.026642898097634315, Validation Loss: 0.024472378194332123\n",
      "Epoch 18, Batch: 450: Training Loss: 0.020415538921952248, Validation Loss: 0.023890120908617973\n",
      "Epoch 18, Batch: 451: Training Loss: 0.025863541290163994, Validation Loss: 0.024419479072093964\n",
      "Epoch 18, Batch: 452: Training Loss: 0.025555504485964775, Validation Loss: 0.02553897723555565\n",
      "Epoch 18, Batch: 453: Training Loss: 0.02289438806474209, Validation Loss: 0.022566257044672966\n",
      "Epoch 18, Batch: 454: Training Loss: 0.023373350501060486, Validation Loss: 0.02322153002023697\n",
      "Epoch 18, Batch: 455: Training Loss: 0.022184943780303, Validation Loss: 0.02406761236488819\n",
      "Epoch 18, Batch: 456: Training Loss: 0.022003117948770523, Validation Loss: 0.025401409715414047\n",
      "Epoch 18, Batch: 457: Training Loss: 0.023078996688127518, Validation Loss: 0.024798152968287468\n",
      "Epoch 18, Batch: 458: Training Loss: 0.019216954708099365, Validation Loss: 0.022579608485102654\n",
      "Epoch 18, Batch: 459: Training Loss: 0.022248951718211174, Validation Loss: 0.026243586093187332\n",
      "Epoch 18, Batch: 460: Training Loss: 0.020561927929520607, Validation Loss: 0.02343006618320942\n",
      "Epoch 18, Batch: 461: Training Loss: 0.023002268746495247, Validation Loss: 0.02291220985352993\n",
      "Epoch 18, Batch: 462: Training Loss: 0.020560022443532944, Validation Loss: 0.023810673505067825\n",
      "Epoch 18, Batch: 463: Training Loss: 0.023613743484020233, Validation Loss: 0.022343486547470093\n",
      "Epoch 18, Batch: 464: Training Loss: 0.01958731934428215, Validation Loss: 0.024942178279161453\n",
      "Epoch 18, Batch: 465: Training Loss: 0.02214224636554718, Validation Loss: 0.02247825637459755\n",
      "Epoch 18, Batch: 466: Training Loss: 0.020673898980021477, Validation Loss: 0.02597789093852043\n",
      "Epoch 18, Batch: 467: Training Loss: 0.02564666047692299, Validation Loss: 0.02575540356338024\n",
      "Epoch 18, Batch: 468: Training Loss: 0.025273192673921585, Validation Loss: 0.02247605472803116\n",
      "Epoch 18, Batch: 469: Training Loss: 0.02482122741639614, Validation Loss: 0.02545303851366043\n",
      "Epoch 18, Batch: 470: Training Loss: 0.022361092269420624, Validation Loss: 0.026312708854675293\n",
      "Epoch 18, Batch: 471: Training Loss: 0.023989520967006683, Validation Loss: 0.02733989618718624\n",
      "Epoch 18, Batch: 472: Training Loss: 0.022828709334135056, Validation Loss: 0.0285596065223217\n",
      "Epoch 18, Batch: 473: Training Loss: 0.022746291011571884, Validation Loss: 0.025875037536025047\n",
      "Epoch 18, Batch: 474: Training Loss: 0.02174537628889084, Validation Loss: 0.026625366881489754\n",
      "Epoch 18, Batch: 475: Training Loss: 0.02520514652132988, Validation Loss: 0.02675454691052437\n",
      "Epoch 18, Batch: 476: Training Loss: 0.020688725635409355, Validation Loss: 0.028393786400556564\n",
      "Epoch 18, Batch: 477: Training Loss: 0.02395271509885788, Validation Loss: 0.027003806084394455\n",
      "Epoch 18, Batch: 478: Training Loss: 0.022303560748696327, Validation Loss: 0.0250399149954319\n",
      "Epoch 18, Batch: 479: Training Loss: 0.022445423528552055, Validation Loss: 0.02596854791045189\n",
      "Epoch 18, Batch: 480: Training Loss: 0.025796083733439445, Validation Loss: 0.026141298934817314\n",
      "Epoch 18, Batch: 481: Training Loss: 0.02114058844745159, Validation Loss: 0.026093315333127975\n",
      "Epoch 18, Batch: 482: Training Loss: 0.023871228098869324, Validation Loss: 0.024041639640927315\n",
      "Epoch 18, Batch: 483: Training Loss: 0.023334292694926262, Validation Loss: 0.02374424785375595\n",
      "Epoch 18, Batch: 484: Training Loss: 0.023469816893339157, Validation Loss: 0.023581037297844887\n",
      "Epoch 18, Batch: 485: Training Loss: 0.025024130940437317, Validation Loss: 0.02610453963279724\n",
      "Epoch 18, Batch: 486: Training Loss: 0.02919904701411724, Validation Loss: 0.024500036612153053\n",
      "Epoch 18, Batch: 487: Training Loss: 0.024956585839390755, Validation Loss: 0.02615731582045555\n",
      "Epoch 18, Batch: 488: Training Loss: 0.023273339495062828, Validation Loss: 0.02379288151860237\n",
      "Epoch 18, Batch: 489: Training Loss: 0.02713698334991932, Validation Loss: 0.024941550567746162\n",
      "Epoch 18, Batch: 490: Training Loss: 0.028258055448532104, Validation Loss: 0.02811339497566223\n",
      "Epoch 18, Batch: 491: Training Loss: 0.019831011071801186, Validation Loss: 0.024803180247545242\n",
      "Epoch 18, Batch: 492: Training Loss: 0.022750454023480415, Validation Loss: 0.02790248766541481\n",
      "Epoch 18, Batch: 493: Training Loss: 0.021923944354057312, Validation Loss: 0.02355484664440155\n",
      "Epoch 18, Batch: 494: Training Loss: 0.0200694240629673, Validation Loss: 0.02422870136797428\n",
      "Epoch 18, Batch: 495: Training Loss: 0.020695192739367485, Validation Loss: 0.024825677275657654\n",
      "Epoch 18, Batch: 496: Training Loss: 0.025051899254322052, Validation Loss: 0.023783322423696518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch: 497: Training Loss: 0.020961128175258636, Validation Loss: 0.025193549692630768\n",
      "Epoch 18, Batch: 498: Training Loss: 0.0274119284003973, Validation Loss: 0.024443266913294792\n",
      "Epoch 18, Batch: 499: Training Loss: 0.021489569917321205, Validation Loss: 0.02143128030002117\n",
      "Epoch 19, Batch: 0: Training Loss: 0.024016285315155983, Validation Loss: 0.025214361026883125\n",
      "Epoch 19, Batch: 1: Training Loss: 0.020303435623645782, Validation Loss: 0.02512415684759617\n",
      "Epoch 19, Batch: 2: Training Loss: 0.02519792877137661, Validation Loss: 0.025790197774767876\n",
      "Epoch 19, Batch: 3: Training Loss: 0.022034190595149994, Validation Loss: 0.026315156370401382\n",
      "Epoch 19, Batch: 4: Training Loss: 0.019361238926649094, Validation Loss: 0.02427874319255352\n",
      "Epoch 19, Batch: 5: Training Loss: 0.023567495867609978, Validation Loss: 0.024016989395022392\n",
      "Epoch 19, Batch: 6: Training Loss: 0.023742813616991043, Validation Loss: 0.026008347049355507\n",
      "Epoch 19, Batch: 7: Training Loss: 0.02108331024646759, Validation Loss: 0.024065660312771797\n",
      "Epoch 19, Batch: 8: Training Loss: 0.02109987661242485, Validation Loss: 0.026593420654535294\n",
      "Epoch 19, Batch: 9: Training Loss: 0.021064426749944687, Validation Loss: 0.025922639295458794\n",
      "Epoch 19, Batch: 10: Training Loss: 0.021479900926351547, Validation Loss: 0.024259652942419052\n",
      "Epoch 19, Batch: 11: Training Loss: 0.024807944893836975, Validation Loss: 0.02382148988544941\n",
      "Epoch 19, Batch: 12: Training Loss: 0.024930439889431, Validation Loss: 0.02243613265454769\n",
      "Epoch 19, Batch: 13: Training Loss: 0.024336175993084908, Validation Loss: 0.02156192623078823\n",
      "Epoch 19, Batch: 14: Training Loss: 0.026989104226231575, Validation Loss: 0.02187296561896801\n",
      "Epoch 19, Batch: 15: Training Loss: 0.02401743456721306, Validation Loss: 0.025417042896151543\n",
      "Epoch 19, Batch: 16: Training Loss: 0.026831161230802536, Validation Loss: 0.022418631240725517\n",
      "Epoch 19, Batch: 17: Training Loss: 0.021750841289758682, Validation Loss: 0.022957783192396164\n",
      "Epoch 19, Batch: 18: Training Loss: 0.022302381694316864, Validation Loss: 0.024398135021328926\n",
      "Epoch 19, Batch: 19: Training Loss: 0.01908409409224987, Validation Loss: 0.023535272106528282\n",
      "Epoch 19, Batch: 20: Training Loss: 0.024921484291553497, Validation Loss: 0.023410342633724213\n",
      "Epoch 19, Batch: 21: Training Loss: 0.01990007609128952, Validation Loss: 0.024437855929136276\n",
      "Epoch 19, Batch: 22: Training Loss: 0.022751398384571075, Validation Loss: 0.022741954773664474\n",
      "Epoch 19, Batch: 23: Training Loss: 0.02204405516386032, Validation Loss: 0.025838201865553856\n",
      "Epoch 19, Batch: 24: Training Loss: 0.021477004513144493, Validation Loss: 0.02409745566546917\n",
      "Epoch 19, Batch: 25: Training Loss: 0.02203960157930851, Validation Loss: 0.02297867089509964\n",
      "Epoch 19, Batch: 26: Training Loss: 0.020334189757704735, Validation Loss: 0.027390511706471443\n",
      "Epoch 19, Batch: 27: Training Loss: 0.02487787976861, Validation Loss: 0.024159973487257957\n",
      "Epoch 19, Batch: 28: Training Loss: 0.025180578231811523, Validation Loss: 0.024843621999025345\n",
      "Epoch 19, Batch: 29: Training Loss: 0.021991970017552376, Validation Loss: 0.02382509969174862\n",
      "Epoch 19, Batch: 30: Training Loss: 0.021195290610194206, Validation Loss: 0.025089355185627937\n",
      "Epoch 19, Batch: 31: Training Loss: 0.026164166629314423, Validation Loss: 0.02524629235267639\n",
      "Epoch 19, Batch: 32: Training Loss: 0.02557450905442238, Validation Loss: 0.025418760254979134\n",
      "Epoch 19, Batch: 33: Training Loss: 0.02219201624393463, Validation Loss: 0.029135141521692276\n",
      "Epoch 19, Batch: 34: Training Loss: 0.01874339021742344, Validation Loss: 0.025286870077252388\n",
      "Epoch 19, Batch: 35: Training Loss: 0.022800926119089127, Validation Loss: 0.026641108095645905\n",
      "Epoch 19, Batch: 36: Training Loss: 0.021700412034988403, Validation Loss: 0.023213695734739304\n",
      "Epoch 19, Batch: 37: Training Loss: 0.020190276205539703, Validation Loss: 0.026783451437950134\n",
      "Epoch 19, Batch: 38: Training Loss: 0.02297625131905079, Validation Loss: 0.02622285857796669\n",
      "Epoch 19, Batch: 39: Training Loss: 0.020449543371796608, Validation Loss: 0.02730601839721203\n",
      "Epoch 19, Batch: 40: Training Loss: 0.027393246069550514, Validation Loss: 0.024329418316483498\n",
      "Epoch 19, Batch: 41: Training Loss: 0.022869493812322617, Validation Loss: 0.02432514913380146\n",
      "Epoch 19, Batch: 42: Training Loss: 0.023160427808761597, Validation Loss: 0.027196699753403664\n",
      "Epoch 19, Batch: 43: Training Loss: 0.020223641768097878, Validation Loss: 0.02619113214313984\n",
      "Epoch 19, Batch: 44: Training Loss: 0.02062486857175827, Validation Loss: 0.02551855333149433\n",
      "Epoch 19, Batch: 45: Training Loss: 0.02276051789522171, Validation Loss: 0.026385175064206123\n",
      "Epoch 19, Batch: 46: Training Loss: 0.02071358636021614, Validation Loss: 0.024442899972200394\n",
      "Epoch 19, Batch: 47: Training Loss: 0.027068370953202248, Validation Loss: 0.02592569589614868\n",
      "Epoch 19, Batch: 48: Training Loss: 0.027288585901260376, Validation Loss: 0.026067545637488365\n",
      "Epoch 19, Batch: 49: Training Loss: 0.019609950482845306, Validation Loss: 0.02771744318306446\n",
      "Epoch 19, Batch: 50: Training Loss: 0.022547630593180656, Validation Loss: 0.028694815933704376\n",
      "Epoch 19, Batch: 51: Training Loss: 0.027451293542981148, Validation Loss: 0.02581753395497799\n",
      "Epoch 19, Batch: 52: Training Loss: 0.02178783528506756, Validation Loss: 0.026221906766295433\n",
      "Epoch 19, Batch: 53: Training Loss: 0.023027930408716202, Validation Loss: 0.027673441916704178\n",
      "Epoch 19, Batch: 54: Training Loss: 0.021923096850514412, Validation Loss: 0.028120070695877075\n",
      "Epoch 19, Batch: 55: Training Loss: 0.020211180672049522, Validation Loss: 0.025051778182387352\n",
      "Epoch 19, Batch: 56: Training Loss: 0.024512430652976036, Validation Loss: 0.027317123487591743\n",
      "Epoch 19, Batch: 57: Training Loss: 0.022783391177654266, Validation Loss: 0.024610593914985657\n",
      "Epoch 19, Batch: 58: Training Loss: 0.023722244426608086, Validation Loss: 0.023929663002490997\n",
      "Epoch 19, Batch: 59: Training Loss: 0.020529264584183693, Validation Loss: 0.026348192244768143\n",
      "Epoch 19, Batch: 60: Training Loss: 0.020966801792383194, Validation Loss: 0.027172936126589775\n",
      "Epoch 19, Batch: 61: Training Loss: 0.02735036425292492, Validation Loss: 0.02450806461274624\n",
      "Epoch 19, Batch: 62: Training Loss: 0.020604370161890984, Validation Loss: 0.027463916689157486\n",
      "Epoch 19, Batch: 63: Training Loss: 0.026456398889422417, Validation Loss: 0.024115005508065224\n",
      "Epoch 19, Batch: 64: Training Loss: 0.023706583306193352, Validation Loss: 0.025476662442088127\n",
      "Epoch 19, Batch: 65: Training Loss: 0.022899046540260315, Validation Loss: 0.024197572842240334\n",
      "Epoch 19, Batch: 66: Training Loss: 0.021981077268719673, Validation Loss: 0.024952607229351997\n",
      "Epoch 19, Batch: 67: Training Loss: 0.01983202062547207, Validation Loss: 0.024930739775300026\n",
      "Epoch 19, Batch: 68: Training Loss: 0.022331969812512398, Validation Loss: 0.027530191466212273\n",
      "Epoch 19, Batch: 69: Training Loss: 0.020929329097270966, Validation Loss: 0.025692453607916832\n",
      "Epoch 19, Batch: 70: Training Loss: 0.02215961180627346, Validation Loss: 0.024193907156586647\n",
      "Epoch 19, Batch: 71: Training Loss: 0.020896736532449722, Validation Loss: 0.02647409960627556\n",
      "Epoch 19, Batch: 72: Training Loss: 0.02350030280649662, Validation Loss: 0.025982655584812164\n",
      "Epoch 19, Batch: 73: Training Loss: 0.020724406465888023, Validation Loss: 0.025760462507605553\n",
      "Epoch 19, Batch: 74: Training Loss: 0.022114841267466545, Validation Loss: 0.02483595907688141\n",
      "Epoch 19, Batch: 75: Training Loss: 0.018455397337675095, Validation Loss: 0.025634123012423515\n",
      "Epoch 19, Batch: 76: Training Loss: 0.02301502227783203, Validation Loss: 0.02443464659154415\n",
      "Epoch 19, Batch: 77: Training Loss: 0.02553398162126541, Validation Loss: 0.02659725956618786\n",
      "Epoch 19, Batch: 78: Training Loss: 0.024985937401652336, Validation Loss: 0.02500607818365097\n",
      "Epoch 19, Batch: 79: Training Loss: 0.022917814552783966, Validation Loss: 0.025578195229172707\n",
      "Epoch 19, Batch: 80: Training Loss: 0.01735520362854004, Validation Loss: 0.024404354393482208\n",
      "Epoch 19, Batch: 81: Training Loss: 0.022983700037002563, Validation Loss: 0.025745289400219917\n",
      "Epoch 19, Batch: 82: Training Loss: 0.02539336122572422, Validation Loss: 0.024948393926024437\n",
      "Epoch 19, Batch: 83: Training Loss: 0.024275917559862137, Validation Loss: 0.024981224909424782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch: 84: Training Loss: 0.022820357233285904, Validation Loss: 0.021999381482601166\n",
      "Epoch 19, Batch: 85: Training Loss: 0.02264268323779106, Validation Loss: 0.024405375123023987\n",
      "Epoch 19, Batch: 86: Training Loss: 0.02657194621860981, Validation Loss: 0.025771405547857285\n",
      "Epoch 19, Batch: 87: Training Loss: 0.02814740128815174, Validation Loss: 0.02472742833197117\n",
      "Epoch 19, Batch: 88: Training Loss: 0.02619425393640995, Validation Loss: 0.028127344325184822\n",
      "Epoch 19, Batch: 89: Training Loss: 0.023948146030306816, Validation Loss: 0.026332885026931763\n",
      "Epoch 19, Batch: 90: Training Loss: 0.023946262896060944, Validation Loss: 0.02775646187365055\n",
      "Epoch 19, Batch: 91: Training Loss: 0.025031935423612595, Validation Loss: 0.027499660849571228\n",
      "Epoch 19, Batch: 92: Training Loss: 0.023562859743833542, Validation Loss: 0.02709374576807022\n",
      "Epoch 19, Batch: 93: Training Loss: 0.02219385653734207, Validation Loss: 0.02899821847677231\n",
      "Epoch 19, Batch: 94: Training Loss: 0.026176998391747475, Validation Loss: 0.02541411854326725\n",
      "Epoch 19, Batch: 95: Training Loss: 0.022080320864915848, Validation Loss: 0.026360880583524704\n",
      "Epoch 19, Batch: 96: Training Loss: 0.023950902745127678, Validation Loss: 0.026607073843479156\n",
      "Epoch 19, Batch: 97: Training Loss: 0.021512264385819435, Validation Loss: 0.025311963632702827\n",
      "Epoch 19, Batch: 98: Training Loss: 0.021481111645698547, Validation Loss: 0.026284920051693916\n",
      "Epoch 19, Batch: 99: Training Loss: 0.022889534011483192, Validation Loss: 0.026903575286269188\n",
      "Epoch 19, Batch: 100: Training Loss: 0.024917347356677055, Validation Loss: 0.0241115503013134\n",
      "Epoch 19, Batch: 101: Training Loss: 0.02215711958706379, Validation Loss: 0.023873483762145042\n",
      "Epoch 19, Batch: 102: Training Loss: 0.022310735657811165, Validation Loss: 0.024978434666991234\n",
      "Epoch 19, Batch: 103: Training Loss: 0.02486353926360607, Validation Loss: 0.02435966767370701\n",
      "Epoch 19, Batch: 104: Training Loss: 0.02157139964401722, Validation Loss: 0.025721900165081024\n",
      "Epoch 19, Batch: 105: Training Loss: 0.021711204200983047, Validation Loss: 0.026575690135359764\n",
      "Epoch 19, Batch: 106: Training Loss: 0.022897211834788322, Validation Loss: 0.02497681975364685\n",
      "Epoch 19, Batch: 107: Training Loss: 0.02660295180976391, Validation Loss: 0.024694841355085373\n",
      "Epoch 19, Batch: 108: Training Loss: 0.021867873147130013, Validation Loss: 0.026647401973605156\n",
      "Epoch 19, Batch: 109: Training Loss: 0.02613890916109085, Validation Loss: 0.025777623057365417\n",
      "Epoch 19, Batch: 110: Training Loss: 0.025928553193807602, Validation Loss: 0.024845875799655914\n",
      "Epoch 19, Batch: 111: Training Loss: 0.021815266460180283, Validation Loss: 0.026540474966168404\n",
      "Epoch 19, Batch: 112: Training Loss: 0.02323935739696026, Validation Loss: 0.024020884186029434\n",
      "Epoch 19, Batch: 113: Training Loss: 0.024674074724316597, Validation Loss: 0.024584297090768814\n",
      "Epoch 19, Batch: 114: Training Loss: 0.02256946451961994, Validation Loss: 0.02541879564523697\n",
      "Epoch 19, Batch: 115: Training Loss: 0.027841908857226372, Validation Loss: 0.026366611942648888\n",
      "Epoch 19, Batch: 116: Training Loss: 0.021252550184726715, Validation Loss: 0.02522900141775608\n",
      "Epoch 19, Batch: 117: Training Loss: 0.025288168340921402, Validation Loss: 0.025017691776156425\n",
      "Epoch 19, Batch: 118: Training Loss: 0.021662000566720963, Validation Loss: 0.026385242119431496\n",
      "Epoch 19, Batch: 119: Training Loss: 0.024809395894408226, Validation Loss: 0.023313136771321297\n",
      "Epoch 19, Batch: 120: Training Loss: 0.022853033617138863, Validation Loss: 0.025478169322013855\n",
      "Epoch 19, Batch: 121: Training Loss: 0.021279893815517426, Validation Loss: 0.028033236041665077\n",
      "Epoch 19, Batch: 122: Training Loss: 0.024359336122870445, Validation Loss: 0.023702288046479225\n",
      "Epoch 19, Batch: 123: Training Loss: 0.019851960241794586, Validation Loss: 0.025147275999188423\n",
      "Epoch 19, Batch: 124: Training Loss: 0.025042006745934486, Validation Loss: 0.024524135515093803\n",
      "Epoch 19, Batch: 125: Training Loss: 0.01905200630426407, Validation Loss: 0.025633102282881737\n",
      "Epoch 19, Batch: 126: Training Loss: 0.024098720401525497, Validation Loss: 0.02462291531264782\n",
      "Epoch 19, Batch: 127: Training Loss: 0.022406799718737602, Validation Loss: 0.02493661642074585\n",
      "Epoch 19, Batch: 128: Training Loss: 0.022540433332324028, Validation Loss: 0.026120401918888092\n",
      "Epoch 19, Batch: 129: Training Loss: 0.020510617643594742, Validation Loss: 0.026002049446105957\n",
      "Epoch 19, Batch: 130: Training Loss: 0.02324547991156578, Validation Loss: 0.02497812733054161\n",
      "Epoch 19, Batch: 131: Training Loss: 0.02376602776348591, Validation Loss: 0.024943167343735695\n",
      "Epoch 19, Batch: 132: Training Loss: 0.021615250036120415, Validation Loss: 0.024086948484182358\n",
      "Epoch 19, Batch: 133: Training Loss: 0.022643383592367172, Validation Loss: 0.02434961497783661\n",
      "Epoch 19, Batch: 134: Training Loss: 0.023988336324691772, Validation Loss: 0.02551962248980999\n",
      "Epoch 19, Batch: 135: Training Loss: 0.023452121764421463, Validation Loss: 0.02499992400407791\n",
      "Epoch 19, Batch: 136: Training Loss: 0.02439718507230282, Validation Loss: 0.024462878704071045\n",
      "Epoch 19, Batch: 137: Training Loss: 0.021833905950188637, Validation Loss: 0.0266470517963171\n",
      "Epoch 19, Batch: 138: Training Loss: 0.024725083261728287, Validation Loss: 0.023294389247894287\n",
      "Epoch 19, Batch: 139: Training Loss: 0.019625071436166763, Validation Loss: 0.025068463757634163\n",
      "Epoch 19, Batch: 140: Training Loss: 0.02556363120675087, Validation Loss: 0.026710255071520805\n",
      "Epoch 19, Batch: 141: Training Loss: 0.02387896180152893, Validation Loss: 0.024950437247753143\n",
      "Epoch 19, Batch: 142: Training Loss: 0.02521050162613392, Validation Loss: 0.02435910515487194\n",
      "Epoch 19, Batch: 143: Training Loss: 0.022374460473656654, Validation Loss: 0.025668203830718994\n",
      "Epoch 19, Batch: 144: Training Loss: 0.020946873351931572, Validation Loss: 0.022633010521531105\n",
      "Epoch 19, Batch: 145: Training Loss: 0.023062603548169136, Validation Loss: 0.024118691682815552\n",
      "Epoch 19, Batch: 146: Training Loss: 0.022707799449563026, Validation Loss: 0.02499222196638584\n",
      "Epoch 19, Batch: 147: Training Loss: 0.02085723541676998, Validation Loss: 0.02190740779042244\n",
      "Epoch 19, Batch: 148: Training Loss: 0.020452337339520454, Validation Loss: 0.025388894602656364\n",
      "Epoch 19, Batch: 149: Training Loss: 0.023827357217669487, Validation Loss: 0.024238353595137596\n",
      "Epoch 19, Batch: 150: Training Loss: 0.028129372745752335, Validation Loss: 0.025428423658013344\n",
      "Epoch 19, Batch: 151: Training Loss: 0.022898150607943535, Validation Loss: 0.02401202730834484\n",
      "Epoch 19, Batch: 152: Training Loss: 0.02614736557006836, Validation Loss: 0.02654922753572464\n",
      "Epoch 19, Batch: 153: Training Loss: 0.02325524389743805, Validation Loss: 0.028658319264650345\n",
      "Epoch 19, Batch: 154: Training Loss: 0.02531617507338524, Validation Loss: 0.02764112874865532\n",
      "Epoch 19, Batch: 155: Training Loss: 0.02504621632397175, Validation Loss: 0.02485002763569355\n",
      "Epoch 19, Batch: 156: Training Loss: 0.02096915803849697, Validation Loss: 0.02296489104628563\n",
      "Epoch 19, Batch: 157: Training Loss: 0.021359426900744438, Validation Loss: 0.026943178847432137\n",
      "Epoch 19, Batch: 158: Training Loss: 0.020540524274110794, Validation Loss: 0.026241682469844818\n",
      "Epoch 19, Batch: 159: Training Loss: 0.021837256848812103, Validation Loss: 0.02513090893626213\n",
      "Epoch 19, Batch: 160: Training Loss: 0.022260107100009918, Validation Loss: 0.027050595730543137\n",
      "Epoch 19, Batch: 161: Training Loss: 0.023018723353743553, Validation Loss: 0.025231458246707916\n",
      "Epoch 19, Batch: 162: Training Loss: 0.023651650175452232, Validation Loss: 0.023945726454257965\n",
      "Epoch 19, Batch: 163: Training Loss: 0.02057081274688244, Validation Loss: 0.025440623983740807\n",
      "Epoch 19, Batch: 164: Training Loss: 0.02114546112716198, Validation Loss: 0.0282644834369421\n",
      "Epoch 19, Batch: 165: Training Loss: 0.02627350203692913, Validation Loss: 0.026514749974012375\n",
      "Epoch 19, Batch: 166: Training Loss: 0.0202361848205328, Validation Loss: 0.027267800644040108\n",
      "Epoch 19, Batch: 167: Training Loss: 0.02158789336681366, Validation Loss: 0.024450773373246193\n",
      "Epoch 19, Batch: 168: Training Loss: 0.024571843445301056, Validation Loss: 0.02053900621831417\n",
      "Epoch 19, Batch: 169: Training Loss: 0.023888109251856804, Validation Loss: 0.022863782942295074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch: 170: Training Loss: 0.020405329763889313, Validation Loss: 0.024443496018648148\n",
      "Epoch 19, Batch: 171: Training Loss: 0.020293258130550385, Validation Loss: 0.023716196417808533\n",
      "Epoch 19, Batch: 172: Training Loss: 0.020035872235894203, Validation Loss: 0.023724360391497612\n",
      "Epoch 19, Batch: 173: Training Loss: 0.019016414880752563, Validation Loss: 0.02524648606777191\n",
      "Epoch 19, Batch: 174: Training Loss: 0.022511882707476616, Validation Loss: 0.02319790981709957\n",
      "Epoch 19, Batch: 175: Training Loss: 0.02552875317633152, Validation Loss: 0.022003162652254105\n",
      "Epoch 19, Batch: 176: Training Loss: 0.023928601294755936, Validation Loss: 0.02336377464234829\n",
      "Epoch 19, Batch: 177: Training Loss: 0.020169749855995178, Validation Loss: 0.025034071877598763\n",
      "Epoch 19, Batch: 178: Training Loss: 0.025059647858142853, Validation Loss: 0.025497201830148697\n",
      "Epoch 19, Batch: 179: Training Loss: 0.0224206130951643, Validation Loss: 0.02303829975426197\n",
      "Epoch 19, Batch: 180: Training Loss: 0.02148900181055069, Validation Loss: 0.023731039837002754\n",
      "Epoch 19, Batch: 181: Training Loss: 0.021902091801166534, Validation Loss: 0.023012012243270874\n",
      "Epoch 19, Batch: 182: Training Loss: 0.023482661694288254, Validation Loss: 0.026033053174614906\n",
      "Epoch 19, Batch: 183: Training Loss: 0.023703688755631447, Validation Loss: 0.024691957980394363\n",
      "Epoch 19, Batch: 184: Training Loss: 0.02283027209341526, Validation Loss: 0.027384251356124878\n",
      "Epoch 19, Batch: 185: Training Loss: 0.02367844618856907, Validation Loss: 0.024228759109973907\n",
      "Epoch 19, Batch: 186: Training Loss: 0.023141145706176758, Validation Loss: 0.023695386946201324\n",
      "Epoch 19, Batch: 187: Training Loss: 0.02133174240589142, Validation Loss: 0.02585831843316555\n",
      "Epoch 19, Batch: 188: Training Loss: 0.027166316285729408, Validation Loss: 0.024138683453202248\n",
      "Epoch 19, Batch: 189: Training Loss: 0.021609267219901085, Validation Loss: 0.025082776322960854\n",
      "Epoch 19, Batch: 190: Training Loss: 0.022462738677859306, Validation Loss: 0.023416617885231972\n",
      "Epoch 19, Batch: 191: Training Loss: 0.023163318634033203, Validation Loss: 0.022941259667277336\n",
      "Epoch 19, Batch: 192: Training Loss: 0.02434580773115158, Validation Loss: 0.025968534871935844\n",
      "Epoch 19, Batch: 193: Training Loss: 0.024001922458410263, Validation Loss: 0.02516152523458004\n",
      "Epoch 19, Batch: 194: Training Loss: 0.02020857483148575, Validation Loss: 0.02510223537683487\n",
      "Epoch 19, Batch: 195: Training Loss: 0.022284124046564102, Validation Loss: 0.023855499923229218\n",
      "Epoch 19, Batch: 196: Training Loss: 0.022310052067041397, Validation Loss: 0.02317836694419384\n",
      "Epoch 19, Batch: 197: Training Loss: 0.020585186779499054, Validation Loss: 0.023034226149320602\n",
      "Epoch 19, Batch: 198: Training Loss: 0.023591896519064903, Validation Loss: 0.024333305656909943\n",
      "Epoch 19, Batch: 199: Training Loss: 0.02200237289071083, Validation Loss: 0.022746697068214417\n",
      "Epoch 19, Batch: 200: Training Loss: 0.020285198464989662, Validation Loss: 0.023667115718126297\n",
      "Epoch 19, Batch: 201: Training Loss: 0.02493331767618656, Validation Loss: 0.023193666711449623\n",
      "Epoch 19, Batch: 202: Training Loss: 0.02205900102853775, Validation Loss: 0.02448931336402893\n",
      "Epoch 19, Batch: 203: Training Loss: 0.022716280072927475, Validation Loss: 0.022442957386374474\n",
      "Epoch 19, Batch: 204: Training Loss: 0.02098100073635578, Validation Loss: 0.025379320606589317\n",
      "Epoch 19, Batch: 205: Training Loss: 0.02702241577208042, Validation Loss: 0.02447778917849064\n",
      "Epoch 19, Batch: 206: Training Loss: 0.022877095267176628, Validation Loss: 0.024887021631002426\n",
      "Epoch 19, Batch: 207: Training Loss: 0.026932982727885246, Validation Loss: 0.026381107047200203\n",
      "Epoch 19, Batch: 208: Training Loss: 0.023679552599787712, Validation Loss: 0.022783977910876274\n",
      "Epoch 19, Batch: 209: Training Loss: 0.021935980767011642, Validation Loss: 0.022445527836680412\n",
      "Epoch 19, Batch: 210: Training Loss: 0.02563229762017727, Validation Loss: 0.023627780377864838\n",
      "Epoch 19, Batch: 211: Training Loss: 0.02406756579875946, Validation Loss: 0.023707447573542595\n",
      "Epoch 19, Batch: 212: Training Loss: 0.023523816838860512, Validation Loss: 0.023920830339193344\n",
      "Epoch 19, Batch: 213: Training Loss: 0.02317303791642189, Validation Loss: 0.024365615099668503\n",
      "Epoch 19, Batch: 214: Training Loss: 0.0191802941262722, Validation Loss: 0.023859845474362373\n",
      "Epoch 19, Batch: 215: Training Loss: 0.02079097367823124, Validation Loss: 0.022838376462459564\n",
      "Epoch 19, Batch: 216: Training Loss: 0.021512486040592194, Validation Loss: 0.023933175951242447\n",
      "Epoch 19, Batch: 217: Training Loss: 0.027344955131411552, Validation Loss: 0.025471027940511703\n",
      "Epoch 19, Batch: 218: Training Loss: 0.021317344158887863, Validation Loss: 0.02420130930840969\n",
      "Epoch 19, Batch: 219: Training Loss: 0.02249380387365818, Validation Loss: 0.0256597138941288\n",
      "Epoch 19, Batch: 220: Training Loss: 0.022393031045794487, Validation Loss: 0.02403985895216465\n",
      "Epoch 19, Batch: 221: Training Loss: 0.02067567966878414, Validation Loss: 0.02438097447156906\n",
      "Epoch 19, Batch: 222: Training Loss: 0.0241666529327631, Validation Loss: 0.024684840813279152\n",
      "Epoch 19, Batch: 223: Training Loss: 0.020287396386265755, Validation Loss: 0.02498641423881054\n",
      "Epoch 19, Batch: 224: Training Loss: 0.02224249392747879, Validation Loss: 0.025386987254023552\n",
      "Epoch 19, Batch: 225: Training Loss: 0.02204686403274536, Validation Loss: 0.02353820949792862\n",
      "Epoch 19, Batch: 226: Training Loss: 0.023461150005459785, Validation Loss: 0.02310670167207718\n",
      "Epoch 19, Batch: 227: Training Loss: 0.02320900931954384, Validation Loss: 0.02517765946686268\n",
      "Epoch 19, Batch: 228: Training Loss: 0.02185351401567459, Validation Loss: 0.024435605853796005\n",
      "Epoch 19, Batch: 229: Training Loss: 0.02455044351518154, Validation Loss: 0.02453167364001274\n",
      "Epoch 19, Batch: 230: Training Loss: 0.021406253799796104, Validation Loss: 0.02344265580177307\n",
      "Epoch 19, Batch: 231: Training Loss: 0.02448674850165844, Validation Loss: 0.02485886588692665\n",
      "Epoch 19, Batch: 232: Training Loss: 0.02454911731183529, Validation Loss: 0.02563634142279625\n",
      "Epoch 19, Batch: 233: Training Loss: 0.021665535867214203, Validation Loss: 0.024417366832494736\n",
      "Epoch 19, Batch: 234: Training Loss: 0.024071594700217247, Validation Loss: 0.024020696058869362\n",
      "Epoch 19, Batch: 235: Training Loss: 0.023275962099432945, Validation Loss: 0.023217998445034027\n",
      "Epoch 19, Batch: 236: Training Loss: 0.02376101166009903, Validation Loss: 0.024945752695202827\n",
      "Epoch 19, Batch: 237: Training Loss: 0.022982019931077957, Validation Loss: 0.025486035272479057\n",
      "Epoch 19, Batch: 238: Training Loss: 0.020245781168341637, Validation Loss: 0.024253949522972107\n",
      "Epoch 19, Batch: 239: Training Loss: 0.024165580049157143, Validation Loss: 0.025712639093399048\n",
      "Epoch 19, Batch: 240: Training Loss: 0.022288138046860695, Validation Loss: 0.02503969892859459\n",
      "Epoch 19, Batch: 241: Training Loss: 0.02345006726682186, Validation Loss: 0.026440488174557686\n",
      "Epoch 19, Batch: 242: Training Loss: 0.02400946617126465, Validation Loss: 0.02642454020678997\n",
      "Epoch 19, Batch: 243: Training Loss: 0.02408009208738804, Validation Loss: 0.025071891024708748\n",
      "Epoch 19, Batch: 244: Training Loss: 0.023828161880373955, Validation Loss: 0.024905670434236526\n",
      "Epoch 19, Batch: 245: Training Loss: 0.024381550028920174, Validation Loss: 0.024536503478884697\n",
      "Epoch 19, Batch: 246: Training Loss: 0.023349642753601074, Validation Loss: 0.025023965165019035\n",
      "Epoch 19, Batch: 247: Training Loss: 0.023189188912510872, Validation Loss: 0.025133972987532616\n",
      "Epoch 19, Batch: 248: Training Loss: 0.019104216247797012, Validation Loss: 0.025782261043787003\n",
      "Epoch 19, Batch: 249: Training Loss: 0.022145386785268784, Validation Loss: 0.023752901703119278\n",
      "Epoch 19, Batch: 250: Training Loss: 0.022051962092518806, Validation Loss: 0.024734899401664734\n",
      "Epoch 19, Batch: 251: Training Loss: 0.020348263904452324, Validation Loss: 0.02325841411948204\n",
      "Epoch 19, Batch: 252: Training Loss: 0.021198291331529617, Validation Loss: 0.02650565654039383\n",
      "Epoch 19, Batch: 253: Training Loss: 0.02743002586066723, Validation Loss: 0.023988235741853714\n",
      "Epoch 19, Batch: 254: Training Loss: 0.019758734852075577, Validation Loss: 0.023247767239809036\n",
      "Epoch 19, Batch: 255: Training Loss: 0.02516789548099041, Validation Loss: 0.027208112180233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch: 256: Training Loss: 0.0259616207331419, Validation Loss: 0.02467859908938408\n",
      "Epoch 19, Batch: 257: Training Loss: 0.020900804549455643, Validation Loss: 0.026119060814380646\n",
      "Epoch 19, Batch: 258: Training Loss: 0.0257843267172575, Validation Loss: 0.025448311120271683\n",
      "Epoch 19, Batch: 259: Training Loss: 0.022829586640000343, Validation Loss: 0.025521129369735718\n",
      "Epoch 19, Batch: 260: Training Loss: 0.026747271418571472, Validation Loss: 0.02637440711259842\n",
      "Epoch 19, Batch: 261: Training Loss: 0.022824328392744064, Validation Loss: 0.024393228814005852\n",
      "Epoch 19, Batch: 262: Training Loss: 0.022554699331521988, Validation Loss: 0.024493837729096413\n",
      "Epoch 19, Batch: 263: Training Loss: 0.02264397032558918, Validation Loss: 0.025787318125367165\n",
      "Epoch 19, Batch: 264: Training Loss: 0.024941831827163696, Validation Loss: 0.02374640479683876\n",
      "Epoch 19, Batch: 265: Training Loss: 0.022424306720495224, Validation Loss: 0.023763958364725113\n",
      "Epoch 19, Batch: 266: Training Loss: 0.01988460309803486, Validation Loss: 0.02356112189590931\n",
      "Epoch 19, Batch: 267: Training Loss: 0.023221353068947792, Validation Loss: 0.02463368885219097\n",
      "Epoch 19, Batch: 268: Training Loss: 0.022133328020572662, Validation Loss: 0.023809796199202538\n",
      "Epoch 19, Batch: 269: Training Loss: 0.024670548737049103, Validation Loss: 0.024029871448874474\n",
      "Epoch 19, Batch: 270: Training Loss: 0.022288184612989426, Validation Loss: 0.024449164047837257\n",
      "Epoch 19, Batch: 271: Training Loss: 0.020529618486762047, Validation Loss: 0.020448561757802963\n",
      "Epoch 19, Batch: 272: Training Loss: 0.022714460268616676, Validation Loss: 0.023473171517252922\n",
      "Epoch 19, Batch: 273: Training Loss: 0.02174900285899639, Validation Loss: 0.024394793435931206\n",
      "Epoch 19, Batch: 274: Training Loss: 0.024102939292788506, Validation Loss: 0.02248874120414257\n",
      "Epoch 19, Batch: 275: Training Loss: 0.022103887051343918, Validation Loss: 0.024538135156035423\n",
      "Epoch 19, Batch: 276: Training Loss: 0.020864276215434074, Validation Loss: 0.02467152290046215\n",
      "Epoch 19, Batch: 277: Training Loss: 0.020460519939661026, Validation Loss: 0.025196660310029984\n",
      "Epoch 19, Batch: 278: Training Loss: 0.023199517279863358, Validation Loss: 0.024164872244000435\n",
      "Epoch 19, Batch: 279: Training Loss: 0.02563288062810898, Validation Loss: 0.023455820977687836\n",
      "Epoch 19, Batch: 280: Training Loss: 0.019937315955758095, Validation Loss: 0.023247692734003067\n",
      "Epoch 19, Batch: 281: Training Loss: 0.021770784631371498, Validation Loss: 0.023853393271565437\n",
      "Epoch 19, Batch: 282: Training Loss: 0.019303804263472557, Validation Loss: 0.023319389671087265\n",
      "Epoch 19, Batch: 283: Training Loss: 0.022059286013245583, Validation Loss: 0.023368963971734047\n",
      "Epoch 19, Batch: 284: Training Loss: 0.022656824439764023, Validation Loss: 0.024169448763132095\n",
      "Epoch 19, Batch: 285: Training Loss: 0.022658737376332283, Validation Loss: 0.02654215879738331\n",
      "Epoch 19, Batch: 286: Training Loss: 0.02242254465818405, Validation Loss: 0.026498695835471153\n",
      "Epoch 19, Batch: 287: Training Loss: 0.022346267476677895, Validation Loss: 0.02441372722387314\n",
      "Epoch 19, Batch: 288: Training Loss: 0.024279974400997162, Validation Loss: 0.025240682065486908\n",
      "Epoch 19, Batch: 289: Training Loss: 0.022334642708301544, Validation Loss: 0.027680901810526848\n",
      "Epoch 19, Batch: 290: Training Loss: 0.025533828884363174, Validation Loss: 0.02424619533121586\n",
      "Epoch 19, Batch: 291: Training Loss: 0.01986902765929699, Validation Loss: 0.02589418552815914\n",
      "Epoch 19, Batch: 292: Training Loss: 0.025732140988111496, Validation Loss: 0.025150641798973083\n",
      "Epoch 19, Batch: 293: Training Loss: 0.02357364259660244, Validation Loss: 0.023455806076526642\n",
      "Epoch 19, Batch: 294: Training Loss: 0.021691394969820976, Validation Loss: 0.024988941848278046\n",
      "Epoch 19, Batch: 295: Training Loss: 0.02319931983947754, Validation Loss: 0.02386939898133278\n",
      "Epoch 19, Batch: 296: Training Loss: 0.02260177582502365, Validation Loss: 0.025573035702109337\n",
      "Epoch 19, Batch: 297: Training Loss: 0.020151417702436447, Validation Loss: 0.026724735274910927\n",
      "Epoch 19, Batch: 298: Training Loss: 0.02216399274766445, Validation Loss: 0.026094334200024605\n",
      "Epoch 19, Batch: 299: Training Loss: 0.019882673397660255, Validation Loss: 0.02525266632437706\n",
      "Epoch 19, Batch: 300: Training Loss: 0.02310846745967865, Validation Loss: 0.024777844548225403\n",
      "Epoch 19, Batch: 301: Training Loss: 0.020352615043520927, Validation Loss: 0.026143260300159454\n",
      "Epoch 19, Batch: 302: Training Loss: 0.02004793845117092, Validation Loss: 0.025766858831048012\n",
      "Epoch 19, Batch: 303: Training Loss: 0.020990977063775063, Validation Loss: 0.024358443915843964\n",
      "Epoch 19, Batch: 304: Training Loss: 0.021641097962856293, Validation Loss: 0.02445450983941555\n",
      "Epoch 19, Batch: 305: Training Loss: 0.01745741069316864, Validation Loss: 0.023283090442419052\n",
      "Epoch 19, Batch: 306: Training Loss: 0.022345010191202164, Validation Loss: 0.022286584600806236\n",
      "Epoch 19, Batch: 307: Training Loss: 0.018714256584644318, Validation Loss: 0.023841822519898415\n",
      "Epoch 19, Batch: 308: Training Loss: 0.021936491131782532, Validation Loss: 0.02291068062186241\n",
      "Epoch 19, Batch: 309: Training Loss: 0.020786229521036148, Validation Loss: 0.022568529471755028\n",
      "Epoch 19, Batch: 310: Training Loss: 0.0190648902207613, Validation Loss: 0.025081628933548927\n",
      "Epoch 19, Batch: 311: Training Loss: 0.021108532324433327, Validation Loss: 0.021749554201960564\n",
      "Epoch 19, Batch: 312: Training Loss: 0.02260489948093891, Validation Loss: 0.024016661569476128\n",
      "Epoch 19, Batch: 313: Training Loss: 0.018798593431711197, Validation Loss: 0.025064464658498764\n",
      "Epoch 19, Batch: 314: Training Loss: 0.02123914659023285, Validation Loss: 0.02517963945865631\n",
      "Epoch 19, Batch: 315: Training Loss: 0.02004741132259369, Validation Loss: 0.022502044215798378\n",
      "Epoch 19, Batch: 316: Training Loss: 0.02317526936531067, Validation Loss: 0.02357456460595131\n",
      "Epoch 19, Batch: 317: Training Loss: 0.021625107154250145, Validation Loss: 0.02410404570400715\n",
      "Epoch 19, Batch: 318: Training Loss: 0.02029537968337536, Validation Loss: 0.023595348000526428\n",
      "Epoch 19, Batch: 319: Training Loss: 0.021399347111582756, Validation Loss: 0.02220037207007408\n",
      "Epoch 19, Batch: 320: Training Loss: 0.023230236023664474, Validation Loss: 0.023124195635318756\n",
      "Epoch 19, Batch: 321: Training Loss: 0.021430790424346924, Validation Loss: 0.023448145017027855\n",
      "Epoch 19, Batch: 322: Training Loss: 0.020980410277843475, Validation Loss: 0.027559682726860046\n",
      "Epoch 19, Batch: 323: Training Loss: 0.02164919301867485, Validation Loss: 0.025317421182990074\n",
      "Epoch 19, Batch: 324: Training Loss: 0.026027686893939972, Validation Loss: 0.025656020268797874\n",
      "Epoch 19, Batch: 325: Training Loss: 0.021295832470059395, Validation Loss: 0.024182496592402458\n",
      "Epoch 19, Batch: 326: Training Loss: 0.028497396036982536, Validation Loss: 0.0255062747746706\n",
      "Epoch 19, Batch: 327: Training Loss: 0.021260850131511688, Validation Loss: 0.024124406278133392\n",
      "Epoch 19, Batch: 328: Training Loss: 0.02361125685274601, Validation Loss: 0.02517426386475563\n",
      "Epoch 19, Batch: 329: Training Loss: 0.025603143498301506, Validation Loss: 0.02455815114080906\n",
      "Epoch 19, Batch: 330: Training Loss: 0.02163829281926155, Validation Loss: 0.025012269616127014\n",
      "Epoch 19, Batch: 331: Training Loss: 0.02351011522114277, Validation Loss: 0.02557634748518467\n",
      "Epoch 19, Batch: 332: Training Loss: 0.020731821656227112, Validation Loss: 0.025894368067383766\n",
      "Epoch 19, Batch: 333: Training Loss: 0.02591792866587639, Validation Loss: 0.024729445576667786\n",
      "Epoch 19, Batch: 334: Training Loss: 0.01928422786295414, Validation Loss: 0.026601381599903107\n",
      "Epoch 19, Batch: 335: Training Loss: 0.02068484202027321, Validation Loss: 0.02373754419386387\n",
      "Epoch 19, Batch: 336: Training Loss: 0.02308257669210434, Validation Loss: 0.024677909910678864\n",
      "Epoch 19, Batch: 337: Training Loss: 0.021296070888638496, Validation Loss: 0.024210762232542038\n",
      "Epoch 19, Batch: 338: Training Loss: 0.02137148752808571, Validation Loss: 0.02508053369820118\n",
      "Epoch 19, Batch: 339: Training Loss: 0.02570180967450142, Validation Loss: 0.024887187406420708\n",
      "Epoch 19, Batch: 340: Training Loss: 0.022876005619764328, Validation Loss: 0.025312870740890503\n",
      "Epoch 19, Batch: 341: Training Loss: 0.019483372569084167, Validation Loss: 0.02542816288769245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch: 342: Training Loss: 0.024068014696240425, Validation Loss: 0.02426747977733612\n",
      "Epoch 19, Batch: 343: Training Loss: 0.02104458026587963, Validation Loss: 0.024856451898813248\n",
      "Epoch 19, Batch: 344: Training Loss: 0.021276559680700302, Validation Loss: 0.02309630997478962\n",
      "Epoch 19, Batch: 345: Training Loss: 0.02524706721305847, Validation Loss: 0.02546175941824913\n",
      "Epoch 19, Batch: 346: Training Loss: 0.022873979061841965, Validation Loss: 0.02421499788761139\n",
      "Epoch 19, Batch: 347: Training Loss: 0.022657591849565506, Validation Loss: 0.02478347346186638\n",
      "Epoch 19, Batch: 348: Training Loss: 0.02332814410328865, Validation Loss: 0.022530706599354744\n",
      "Epoch 19, Batch: 349: Training Loss: 0.024355489760637283, Validation Loss: 0.022292369976639748\n",
      "Epoch 19, Batch: 350: Training Loss: 0.018986206501722336, Validation Loss: 0.022417766973376274\n",
      "Epoch 19, Batch: 351: Training Loss: 0.02419733628630638, Validation Loss: 0.022978559136390686\n",
      "Epoch 19, Batch: 352: Training Loss: 0.02150023728609085, Validation Loss: 0.022991634905338287\n",
      "Epoch 19, Batch: 353: Training Loss: 0.021647872403264046, Validation Loss: 0.024498334154486656\n",
      "Epoch 19, Batch: 354: Training Loss: 0.02282801643013954, Validation Loss: 0.02389269694685936\n",
      "Epoch 19, Batch: 355: Training Loss: 0.02106017991900444, Validation Loss: 0.024276837706565857\n",
      "Epoch 19, Batch: 356: Training Loss: 0.0261495653539896, Validation Loss: 0.022615160793066025\n",
      "Epoch 19, Batch: 357: Training Loss: 0.02345341630280018, Validation Loss: 0.02355358935892582\n",
      "Epoch 19, Batch: 358: Training Loss: 0.023554503917694092, Validation Loss: 0.024393003433942795\n",
      "Epoch 19, Batch: 359: Training Loss: 0.022288361564278603, Validation Loss: 0.021707888692617416\n",
      "Epoch 19, Batch: 360: Training Loss: 0.02256641536951065, Validation Loss: 0.025291232392191887\n",
      "Epoch 19, Batch: 361: Training Loss: 0.019741006195545197, Validation Loss: 0.023287488147616386\n",
      "Epoch 19, Batch: 362: Training Loss: 0.02387990802526474, Validation Loss: 0.024321457371115685\n",
      "Epoch 19, Batch: 363: Training Loss: 0.02491653896868229, Validation Loss: 0.02369888871908188\n",
      "Epoch 19, Batch: 364: Training Loss: 0.018132327124476433, Validation Loss: 0.0220413226634264\n",
      "Epoch 19, Batch: 365: Training Loss: 0.02182958647608757, Validation Loss: 0.025402842089533806\n",
      "Epoch 19, Batch: 366: Training Loss: 0.022300835698843002, Validation Loss: 0.02260172739624977\n",
      "Epoch 19, Batch: 367: Training Loss: 0.018939465284347534, Validation Loss: 0.024112245067954063\n",
      "Epoch 19, Batch: 368: Training Loss: 0.02071734145283699, Validation Loss: 0.02280597575008869\n",
      "Epoch 19, Batch: 369: Training Loss: 0.021153083071112633, Validation Loss: 0.023417532444000244\n",
      "Epoch 19, Batch: 370: Training Loss: 0.019972218200564384, Validation Loss: 0.023563291877508163\n",
      "Epoch 19, Batch: 371: Training Loss: 0.02090431936085224, Validation Loss: 0.02381393499672413\n",
      "Epoch 19, Batch: 372: Training Loss: 0.020569562911987305, Validation Loss: 0.020681042224168777\n",
      "Epoch 19, Batch: 373: Training Loss: 0.02277873456478119, Validation Loss: 0.02266019769012928\n",
      "Epoch 19, Batch: 374: Training Loss: 0.020879611372947693, Validation Loss: 0.02201640047132969\n",
      "Epoch 19, Batch: 375: Training Loss: 0.025263862684369087, Validation Loss: 0.02517477050423622\n",
      "Epoch 19, Batch: 376: Training Loss: 0.021699506789445877, Validation Loss: 0.021638479083776474\n",
      "Epoch 19, Batch: 377: Training Loss: 0.023426000028848648, Validation Loss: 0.022736718878149986\n",
      "Epoch 19, Batch: 378: Training Loss: 0.020959332585334778, Validation Loss: 0.02472810074687004\n",
      "Epoch 19, Batch: 379: Training Loss: 0.021748729050159454, Validation Loss: 0.021871672943234444\n",
      "Epoch 19, Batch: 380: Training Loss: 0.024040071293711662, Validation Loss: 0.023419197648763657\n",
      "Epoch 19, Batch: 381: Training Loss: 0.02342469058930874, Validation Loss: 0.02503475919365883\n",
      "Epoch 19, Batch: 382: Training Loss: 0.020224876701831818, Validation Loss: 0.02249935083091259\n",
      "Epoch 19, Batch: 383: Training Loss: 0.02193683572113514, Validation Loss: 0.02327612228691578\n",
      "Epoch 19, Batch: 384: Training Loss: 0.020605599507689476, Validation Loss: 0.02523750439286232\n",
      "Epoch 19, Batch: 385: Training Loss: 0.02319704182446003, Validation Loss: 0.021888399496674538\n",
      "Epoch 19, Batch: 386: Training Loss: 0.01891830936074257, Validation Loss: 0.022875677794218063\n",
      "Epoch 19, Batch: 387: Training Loss: 0.022599248215556145, Validation Loss: 0.021504323929548264\n",
      "Epoch 19, Batch: 388: Training Loss: 0.01967785507440567, Validation Loss: 0.021322356536984444\n",
      "Epoch 19, Batch: 389: Training Loss: 0.020254964008927345, Validation Loss: 0.023373840376734734\n",
      "Epoch 19, Batch: 390: Training Loss: 0.020060714334249496, Validation Loss: 0.022104918956756592\n",
      "Epoch 19, Batch: 391: Training Loss: 0.023733563721179962, Validation Loss: 0.02321450226008892\n",
      "Epoch 19, Batch: 392: Training Loss: 0.018784193322062492, Validation Loss: 0.021837124601006508\n",
      "Epoch 19, Batch: 393: Training Loss: 0.02245938591659069, Validation Loss: 0.022259196266531944\n",
      "Epoch 19, Batch: 394: Training Loss: 0.020908784121274948, Validation Loss: 0.021198732778429985\n",
      "Epoch 19, Batch: 395: Training Loss: 0.022167980670928955, Validation Loss: 0.023212851956486702\n",
      "Epoch 19, Batch: 396: Training Loss: 0.023927005007863045, Validation Loss: 0.021650366485118866\n",
      "Epoch 19, Batch: 397: Training Loss: 0.024820595979690552, Validation Loss: 0.023414533585309982\n",
      "Epoch 19, Batch: 398: Training Loss: 0.01986115239560604, Validation Loss: 0.02515537664294243\n",
      "Epoch 19, Batch: 399: Training Loss: 0.02398722618818283, Validation Loss: 0.02289886586368084\n",
      "Epoch 19, Batch: 400: Training Loss: 0.02290576510131359, Validation Loss: 0.023128725588321686\n",
      "Epoch 19, Batch: 401: Training Loss: 0.024005165323615074, Validation Loss: 0.02538701519370079\n",
      "Epoch 19, Batch: 402: Training Loss: 0.02045074664056301, Validation Loss: 0.025791790336370468\n",
      "Epoch 19, Batch: 403: Training Loss: 0.0255587100982666, Validation Loss: 0.023553431034088135\n",
      "Epoch 19, Batch: 404: Training Loss: 0.020998330786824226, Validation Loss: 0.02263803593814373\n",
      "Epoch 19, Batch: 405: Training Loss: 0.02442050166428089, Validation Loss: 0.022713307291269302\n",
      "Epoch 19, Batch: 406: Training Loss: 0.023857036605477333, Validation Loss: 0.022539984434843063\n",
      "Epoch 19, Batch: 407: Training Loss: 0.018215876072645187, Validation Loss: 0.02086833119392395\n",
      "Epoch 19, Batch: 408: Training Loss: 0.020280741155147552, Validation Loss: 0.023269688710570335\n",
      "Epoch 19, Batch: 409: Training Loss: 0.022831451147794724, Validation Loss: 0.020956438034772873\n",
      "Epoch 19, Batch: 410: Training Loss: 0.021017299965023994, Validation Loss: 0.0240058284252882\n",
      "Epoch 19, Batch: 411: Training Loss: 0.020308757200837135, Validation Loss: 0.02164607122540474\n",
      "Epoch 19, Batch: 412: Training Loss: 0.022703666239976883, Validation Loss: 0.023255135864019394\n",
      "Epoch 19, Batch: 413: Training Loss: 0.022821050137281418, Validation Loss: 0.024022763594985008\n",
      "Epoch 19, Batch: 414: Training Loss: 0.022488825023174286, Validation Loss: 0.0229693204164505\n",
      "Epoch 19, Batch: 415: Training Loss: 0.023040715605020523, Validation Loss: 0.02173149771988392\n",
      "Epoch 19, Batch: 416: Training Loss: 0.023623855784535408, Validation Loss: 0.022017361596226692\n",
      "Epoch 19, Batch: 417: Training Loss: 0.020769352093338966, Validation Loss: 0.024006066843867302\n",
      "Epoch 19, Batch: 418: Training Loss: 0.02042704075574875, Validation Loss: 0.023807765915989876\n",
      "Epoch 19, Batch: 419: Training Loss: 0.020199261605739594, Validation Loss: 0.02427602931857109\n",
      "Epoch 19, Batch: 420: Training Loss: 0.02436419576406479, Validation Loss: 0.023009926080703735\n",
      "Epoch 19, Batch: 421: Training Loss: 0.022429367527365685, Validation Loss: 0.023428114131093025\n",
      "Epoch 19, Batch: 422: Training Loss: 0.024744031950831413, Validation Loss: 0.02324226126074791\n",
      "Epoch 19, Batch: 423: Training Loss: 0.024650713428854942, Validation Loss: 0.024704989045858383\n",
      "Epoch 19, Batch: 424: Training Loss: 0.022009719163179398, Validation Loss: 0.024244243279099464\n",
      "Epoch 19, Batch: 425: Training Loss: 0.020318681374192238, Validation Loss: 0.02724412828683853\n",
      "Epoch 19, Batch: 426: Training Loss: 0.021595213562250137, Validation Loss: 0.024415889754891396\n",
      "Epoch 19, Batch: 427: Training Loss: 0.02102971263229847, Validation Loss: 0.028400100767612457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch: 428: Training Loss: 0.024332882836461067, Validation Loss: 0.02734716236591339\n",
      "Epoch 19, Batch: 429: Training Loss: 0.023401636630296707, Validation Loss: 0.024076197296380997\n",
      "Epoch 19, Batch: 430: Training Loss: 0.02205444686114788, Validation Loss: 0.022484833374619484\n",
      "Epoch 19, Batch: 431: Training Loss: 0.018983006477355957, Validation Loss: 0.02261871099472046\n",
      "Epoch 19, Batch: 432: Training Loss: 0.024587715044617653, Validation Loss: 0.02577492967247963\n",
      "Epoch 19, Batch: 433: Training Loss: 0.023064006119966507, Validation Loss: 0.02241263911128044\n",
      "Epoch 19, Batch: 434: Training Loss: 0.024447942152619362, Validation Loss: 0.022826803848147392\n",
      "Epoch 19, Batch: 435: Training Loss: 0.02187100239098072, Validation Loss: 0.026304351165890694\n",
      "Epoch 19, Batch: 436: Training Loss: 0.023197095841169357, Validation Loss: 0.023683590814471245\n",
      "Epoch 19, Batch: 437: Training Loss: 0.01805793307721615, Validation Loss: 0.023242484778165817\n",
      "Epoch 19, Batch: 438: Training Loss: 0.026241233572363853, Validation Loss: 0.02403494156897068\n",
      "Epoch 19, Batch: 439: Training Loss: 0.018720509484410286, Validation Loss: 0.022873861715197563\n",
      "Epoch 19, Batch: 440: Training Loss: 0.02516854926943779, Validation Loss: 0.02438458427786827\n",
      "Epoch 19, Batch: 441: Training Loss: 0.020549418404698372, Validation Loss: 0.02196194976568222\n",
      "Epoch 19, Batch: 442: Training Loss: 0.026150690391659737, Validation Loss: 0.026926593855023384\n",
      "Epoch 19, Batch: 443: Training Loss: 0.021861175075173378, Validation Loss: 0.023761628195643425\n",
      "Epoch 19, Batch: 444: Training Loss: 0.021927451714873314, Validation Loss: 0.02391490526497364\n",
      "Epoch 19, Batch: 445: Training Loss: 0.02219729870557785, Validation Loss: 0.023970002308487892\n",
      "Epoch 19, Batch: 446: Training Loss: 0.026100821793079376, Validation Loss: 0.025924265384674072\n",
      "Epoch 19, Batch: 447: Training Loss: 0.02398422174155712, Validation Loss: 0.022598227486014366\n",
      "Epoch 19, Batch: 448: Training Loss: 0.023611856624484062, Validation Loss: 0.023165198042988777\n",
      "Epoch 19, Batch: 449: Training Loss: 0.02053266577422619, Validation Loss: 0.022292546927928925\n",
      "Epoch 19, Batch: 450: Training Loss: 0.022163351997733116, Validation Loss: 0.024431105703115463\n",
      "Epoch 19, Batch: 451: Training Loss: 0.02549903653562069, Validation Loss: 0.02394619584083557\n",
      "Epoch 19, Batch: 452: Training Loss: 0.02258768491446972, Validation Loss: 0.02340223640203476\n",
      "Epoch 19, Batch: 453: Training Loss: 0.023865289986133575, Validation Loss: 0.022823557257652283\n",
      "Epoch 19, Batch: 454: Training Loss: 0.021638723090291023, Validation Loss: 0.02216958813369274\n",
      "Epoch 19, Batch: 455: Training Loss: 0.020662739872932434, Validation Loss: 0.022089730948209763\n",
      "Epoch 19, Batch: 456: Training Loss: 0.01974649913609028, Validation Loss: 0.02296619303524494\n",
      "Epoch 19, Batch: 457: Training Loss: 0.020888052880764008, Validation Loss: 0.024470597505569458\n",
      "Epoch 19, Batch: 458: Training Loss: 0.017881032079458237, Validation Loss: 0.02376621402800083\n",
      "Epoch 19, Batch: 459: Training Loss: 0.01862758956849575, Validation Loss: 0.023279523476958275\n",
      "Epoch 19, Batch: 460: Training Loss: 0.021241089329123497, Validation Loss: 0.024242404848337173\n",
      "Epoch 19, Batch: 461: Training Loss: 0.021560508757829666, Validation Loss: 0.02393346093595028\n",
      "Epoch 19, Batch: 462: Training Loss: 0.016874661669135094, Validation Loss: 0.0249564740806818\n",
      "Epoch 19, Batch: 463: Training Loss: 0.020245328545570374, Validation Loss: 0.025491081178188324\n",
      "Epoch 19, Batch: 464: Training Loss: 0.02042929269373417, Validation Loss: 0.025775151327252388\n",
      "Epoch 19, Batch: 465: Training Loss: 0.02217048965394497, Validation Loss: 0.02425619773566723\n",
      "Epoch 19, Batch: 466: Training Loss: 0.019793134182691574, Validation Loss: 0.02235390618443489\n",
      "Epoch 19, Batch: 467: Training Loss: 0.026136746630072594, Validation Loss: 0.024277474731206894\n",
      "Epoch 19, Batch: 468: Training Loss: 0.02407218888401985, Validation Loss: 0.024760441854596138\n",
      "Epoch 19, Batch: 469: Training Loss: 0.024745812639594078, Validation Loss: 0.023626958951354027\n",
      "Epoch 19, Batch: 470: Training Loss: 0.01922658458352089, Validation Loss: 0.022548019886016846\n",
      "Epoch 19, Batch: 471: Training Loss: 0.02204366959631443, Validation Loss: 0.025635750964283943\n",
      "Epoch 19, Batch: 472: Training Loss: 0.023709846660494804, Validation Loss: 0.024685021489858627\n",
      "Epoch 19, Batch: 473: Training Loss: 0.02260693907737732, Validation Loss: 0.02591685578227043\n",
      "Epoch 19, Batch: 474: Training Loss: 0.01920439675450325, Validation Loss: 0.024091806262731552\n",
      "Epoch 19, Batch: 475: Training Loss: 0.02555968426167965, Validation Loss: 0.026048021391034126\n",
      "Epoch 19, Batch: 476: Training Loss: 0.019554495811462402, Validation Loss: 0.026140350848436356\n",
      "Epoch 19, Batch: 477: Training Loss: 0.025945963338017464, Validation Loss: 0.02309425361454487\n",
      "Epoch 19, Batch: 478: Training Loss: 0.02462681382894516, Validation Loss: 0.022874005138874054\n",
      "Epoch 19, Batch: 479: Training Loss: 0.021348509937524796, Validation Loss: 0.02478477731347084\n",
      "Epoch 19, Batch: 480: Training Loss: 0.022059723734855652, Validation Loss: 0.024081038311123848\n",
      "Epoch 19, Batch: 481: Training Loss: 0.021087927743792534, Validation Loss: 0.025240659713745117\n",
      "Epoch 19, Batch: 482: Training Loss: 0.02399579808115959, Validation Loss: 0.025776583701372147\n",
      "Epoch 19, Batch: 483: Training Loss: 0.020548604428768158, Validation Loss: 0.023817701265215874\n",
      "Epoch 19, Batch: 484: Training Loss: 0.02094331942498684, Validation Loss: 0.025468381121754646\n",
      "Epoch 19, Batch: 485: Training Loss: 0.023294106125831604, Validation Loss: 0.025863252580165863\n",
      "Epoch 19, Batch: 486: Training Loss: 0.029775267466902733, Validation Loss: 0.024575963616371155\n",
      "Epoch 19, Batch: 487: Training Loss: 0.02061341516673565, Validation Loss: 0.02569127082824707\n",
      "Epoch 19, Batch: 488: Training Loss: 0.020970696583390236, Validation Loss: 0.027103574946522713\n",
      "Epoch 19, Batch: 489: Training Loss: 0.024724112823605537, Validation Loss: 0.024850277230143547\n",
      "Epoch 19, Batch: 490: Training Loss: 0.02331625297665596, Validation Loss: 0.025549564510583878\n",
      "Epoch 19, Batch: 491: Training Loss: 0.0194416381418705, Validation Loss: 0.025227760896086693\n",
      "Epoch 19, Batch: 492: Training Loss: 0.022522982209920883, Validation Loss: 0.02116275019943714\n",
      "Epoch 19, Batch: 493: Training Loss: 0.021908925846219063, Validation Loss: 0.02250637114048004\n",
      "Epoch 19, Batch: 494: Training Loss: 0.021695522591471672, Validation Loss: 0.023265749216079712\n",
      "Epoch 19, Batch: 495: Training Loss: 0.020447084680199623, Validation Loss: 0.02434343472123146\n",
      "Epoch 19, Batch: 496: Training Loss: 0.01914159581065178, Validation Loss: 0.02428353577852249\n",
      "Epoch 19, Batch: 497: Training Loss: 0.020350394770503044, Validation Loss: 0.02479812316596508\n",
      "Epoch 19, Batch: 498: Training Loss: 0.0237272996455431, Validation Loss: 0.02426595613360405\n",
      "Epoch 19, Batch: 499: Training Loss: 0.021060612052679062, Validation Loss: 0.025687407702207565\n",
      "Epoch 20, Batch: 0: Training Loss: 0.020678535103797913, Validation Loss: 0.025293897837400436\n",
      "Epoch 20, Batch: 1: Training Loss: 0.020222418010234833, Validation Loss: 0.02566177025437355\n",
      "Epoch 20, Batch: 2: Training Loss: 0.02740471251308918, Validation Loss: 0.02657315880060196\n",
      "Epoch 20, Batch: 3: Training Loss: 0.02039002999663353, Validation Loss: 0.028331423178315163\n",
      "Epoch 20, Batch: 4: Training Loss: 0.019237758591771126, Validation Loss: 0.027814533561468124\n",
      "Epoch 20, Batch: 5: Training Loss: 0.021466370671987534, Validation Loss: 0.026813063770532608\n",
      "Epoch 20, Batch: 6: Training Loss: 0.022445928305387497, Validation Loss: 0.0253584161400795\n",
      "Epoch 20, Batch: 7: Training Loss: 0.021598003804683685, Validation Loss: 0.02514505572617054\n",
      "Epoch 20, Batch: 8: Training Loss: 0.021677864715456963, Validation Loss: 0.025126511231064796\n",
      "Epoch 20, Batch: 9: Training Loss: 0.021782930940389633, Validation Loss: 0.024578450247645378\n",
      "Epoch 20, Batch: 10: Training Loss: 0.022155437618494034, Validation Loss: 0.024911899119615555\n",
      "Epoch 20, Batch: 11: Training Loss: 0.025046169757843018, Validation Loss: 0.027019625529646873\n",
      "Epoch 20, Batch: 12: Training Loss: 0.02860577404499054, Validation Loss: 0.027595797553658485\n",
      "Epoch 20, Batch: 13: Training Loss: 0.025212906301021576, Validation Loss: 0.023425711318850517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 14: Training Loss: 0.020933980122208595, Validation Loss: 0.025195177644491196\n",
      "Epoch 20, Batch: 15: Training Loss: 0.022810475900769234, Validation Loss: 0.02187948301434517\n",
      "Epoch 20, Batch: 16: Training Loss: 0.02170805260539055, Validation Loss: 0.02421380765736103\n",
      "Epoch 20, Batch: 17: Training Loss: 0.021475229412317276, Validation Loss: 0.02354852668941021\n",
      "Epoch 20, Batch: 18: Training Loss: 0.020171621814370155, Validation Loss: 0.02479489892721176\n",
      "Epoch 20, Batch: 19: Training Loss: 0.023680416867136955, Validation Loss: 0.023591144010424614\n",
      "Epoch 20, Batch: 20: Training Loss: 0.02381858415901661, Validation Loss: 0.02355004847049713\n",
      "Epoch 20, Batch: 21: Training Loss: 0.022394031286239624, Validation Loss: 0.023413198068737984\n",
      "Epoch 20, Batch: 22: Training Loss: 0.02494150772690773, Validation Loss: 0.025218727067112923\n",
      "Epoch 20, Batch: 23: Training Loss: 0.02289509028196335, Validation Loss: 0.022525886073708534\n",
      "Epoch 20, Batch: 24: Training Loss: 0.020374465733766556, Validation Loss: 0.024532059207558632\n",
      "Epoch 20, Batch: 25: Training Loss: 0.022736681625247, Validation Loss: 0.025073641911149025\n",
      "Epoch 20, Batch: 26: Training Loss: 0.025310466066002846, Validation Loss: 0.026410842314362526\n",
      "Epoch 20, Batch: 27: Training Loss: 0.022306492552161217, Validation Loss: 0.026546260342001915\n",
      "Epoch 20, Batch: 28: Training Loss: 0.02768426202237606, Validation Loss: 0.027139456942677498\n",
      "Epoch 20, Batch: 29: Training Loss: 0.023231830447912216, Validation Loss: 0.025525685399770737\n",
      "Epoch 20, Batch: 30: Training Loss: 0.023819750174880028, Validation Loss: 0.026218829676508904\n",
      "Epoch 20, Batch: 31: Training Loss: 0.02697025053203106, Validation Loss: 0.026563050225377083\n",
      "Epoch 20, Batch: 32: Training Loss: 0.020102838054299355, Validation Loss: 0.028146175667643547\n",
      "Epoch 20, Batch: 33: Training Loss: 0.023500435054302216, Validation Loss: 0.027165943756699562\n",
      "Epoch 20, Batch: 34: Training Loss: 0.021886957809329033, Validation Loss: 0.028678501024842262\n",
      "Epoch 20, Batch: 35: Training Loss: 0.02330804243683815, Validation Loss: 0.027484185993671417\n",
      "Epoch 20, Batch: 36: Training Loss: 0.023894978687167168, Validation Loss: 0.02386585995554924\n",
      "Epoch 20, Batch: 37: Training Loss: 0.02233183942735195, Validation Loss: 0.026667755097150803\n",
      "Epoch 20, Batch: 38: Training Loss: 0.027474049478769302, Validation Loss: 0.02545645646750927\n",
      "Epoch 20, Batch: 39: Training Loss: 0.022840991616249084, Validation Loss: 0.023853803053498268\n",
      "Epoch 20, Batch: 40: Training Loss: 0.022386852651834488, Validation Loss: 0.025767961516976357\n",
      "Epoch 20, Batch: 41: Training Loss: 0.022949300706386566, Validation Loss: 0.025324245914816856\n",
      "Epoch 20, Batch: 42: Training Loss: 0.021710559725761414, Validation Loss: 0.025050459429621696\n",
      "Epoch 20, Batch: 43: Training Loss: 0.020042091608047485, Validation Loss: 0.025412222370505333\n",
      "Epoch 20, Batch: 44: Training Loss: 0.02589460276067257, Validation Loss: 0.02428354322910309\n",
      "Epoch 20, Batch: 45: Training Loss: 0.02215394750237465, Validation Loss: 0.023736659437417984\n",
      "Epoch 20, Batch: 46: Training Loss: 0.02569497562944889, Validation Loss: 0.022570302709937096\n",
      "Epoch 20, Batch: 47: Training Loss: 0.02425345592200756, Validation Loss: 0.02294524945318699\n",
      "Epoch 20, Batch: 48: Training Loss: 0.027525100857019424, Validation Loss: 0.022654054686427116\n",
      "Epoch 20, Batch: 49: Training Loss: 0.022682707756757736, Validation Loss: 0.024026894941926003\n",
      "Epoch 20, Batch: 50: Training Loss: 0.01908305287361145, Validation Loss: 0.02448359690606594\n",
      "Epoch 20, Batch: 51: Training Loss: 0.025557586923241615, Validation Loss: 0.02409777045249939\n",
      "Epoch 20, Batch: 52: Training Loss: 0.019176021218299866, Validation Loss: 0.02268097922205925\n",
      "Epoch 20, Batch: 53: Training Loss: 0.021121446043252945, Validation Loss: 0.024477772414684296\n",
      "Epoch 20, Batch: 54: Training Loss: 0.023613475263118744, Validation Loss: 0.024232905358076096\n",
      "Epoch 20, Batch: 55: Training Loss: 0.019315505400300026, Validation Loss: 0.024344269186258316\n",
      "Epoch 20, Batch: 56: Training Loss: 0.024815820157527924, Validation Loss: 0.02302822470664978\n",
      "Epoch 20, Batch: 57: Training Loss: 0.023619646206498146, Validation Loss: 0.021618105471134186\n",
      "Epoch 20, Batch: 58: Training Loss: 0.022217562422156334, Validation Loss: 0.019539538770914078\n",
      "Epoch 20, Batch: 59: Training Loss: 0.019739476963877678, Validation Loss: 0.022745739668607712\n",
      "Epoch 20, Batch: 60: Training Loss: 0.02112565189599991, Validation Loss: 0.023216014727950096\n",
      "Epoch 20, Batch: 61: Training Loss: 0.022913701832294464, Validation Loss: 0.023887349292635918\n",
      "Epoch 20, Batch: 62: Training Loss: 0.02318592555820942, Validation Loss: 0.022794431075453758\n",
      "Epoch 20, Batch: 63: Training Loss: 0.02163878083229065, Validation Loss: 0.022300487384200096\n",
      "Epoch 20, Batch: 64: Training Loss: 0.02334778942167759, Validation Loss: 0.021880483254790306\n",
      "Epoch 20, Batch: 65: Training Loss: 0.024544963613152504, Validation Loss: 0.024521032348275185\n",
      "Epoch 20, Batch: 66: Training Loss: 0.019684599712491035, Validation Loss: 0.02418505772948265\n",
      "Epoch 20, Batch: 67: Training Loss: 0.020397944375872612, Validation Loss: 0.022282356396317482\n",
      "Epoch 20, Batch: 68: Training Loss: 0.026161596179008484, Validation Loss: 0.02405700646340847\n",
      "Epoch 20, Batch: 69: Training Loss: 0.020668096840381622, Validation Loss: 0.025127416476607323\n",
      "Epoch 20, Batch: 70: Training Loss: 0.023224875330924988, Validation Loss: 0.02468099258840084\n",
      "Epoch 20, Batch: 71: Training Loss: 0.02024775743484497, Validation Loss: 0.02400447428226471\n",
      "Epoch 20, Batch: 72: Training Loss: 0.02249334566295147, Validation Loss: 0.022173183038830757\n",
      "Epoch 20, Batch: 73: Training Loss: 0.0206232201308012, Validation Loss: 0.02384231612086296\n",
      "Epoch 20, Batch: 74: Training Loss: 0.024067895486950874, Validation Loss: 0.023750606924295425\n",
      "Epoch 20, Batch: 75: Training Loss: 0.018813859671354294, Validation Loss: 0.023666882887482643\n",
      "Epoch 20, Batch: 76: Training Loss: 0.019500359892845154, Validation Loss: 0.022635333240032196\n",
      "Epoch 20, Batch: 77: Training Loss: 0.02587479166686535, Validation Loss: 0.023083364591002464\n",
      "Epoch 20, Batch: 78: Training Loss: 0.025851434096693993, Validation Loss: 0.021861517801880836\n",
      "Epoch 20, Batch: 79: Training Loss: 0.020157605409622192, Validation Loss: 0.023272933438420296\n",
      "Epoch 20, Batch: 80: Training Loss: 0.019644366577267647, Validation Loss: 0.022062841802835464\n",
      "Epoch 20, Batch: 81: Training Loss: 0.02207491174340248, Validation Loss: 0.02320646122097969\n",
      "Epoch 20, Batch: 82: Training Loss: 0.021682584658265114, Validation Loss: 0.022395417094230652\n",
      "Epoch 20, Batch: 83: Training Loss: 0.02016676962375641, Validation Loss: 0.021490424871444702\n",
      "Epoch 20, Batch: 84: Training Loss: 0.025301100686192513, Validation Loss: 0.023312443867325783\n",
      "Epoch 20, Batch: 85: Training Loss: 0.02475173957645893, Validation Loss: 0.02132202684879303\n",
      "Epoch 20, Batch: 86: Training Loss: 0.018812203779816628, Validation Loss: 0.022381285205483437\n",
      "Epoch 20, Batch: 87: Training Loss: 0.024017056450247765, Validation Loss: 0.02084093913435936\n",
      "Epoch 20, Batch: 88: Training Loss: 0.026872210204601288, Validation Loss: 0.021670253947377205\n",
      "Epoch 20, Batch: 89: Training Loss: 0.024565991014242172, Validation Loss: 0.023505646735429764\n",
      "Epoch 20, Batch: 90: Training Loss: 0.02612980455160141, Validation Loss: 0.02171783335506916\n",
      "Epoch 20, Batch: 91: Training Loss: 0.02208101935684681, Validation Loss: 0.022180043160915375\n",
      "Epoch 20, Batch: 92: Training Loss: 0.020913010463118553, Validation Loss: 0.020104628056287766\n",
      "Epoch 20, Batch: 93: Training Loss: 0.024313179776072502, Validation Loss: 0.02138236165046692\n",
      "Epoch 20, Batch: 94: Training Loss: 0.022613752633333206, Validation Loss: 0.020958397537469864\n",
      "Epoch 20, Batch: 95: Training Loss: 0.023255538195371628, Validation Loss: 0.023204781115055084\n",
      "Epoch 20, Batch: 96: Training Loss: 0.01924940198659897, Validation Loss: 0.02130987122654915\n",
      "Epoch 20, Batch: 97: Training Loss: 0.022646410390734673, Validation Loss: 0.02237885817885399\n",
      "Epoch 20, Batch: 98: Training Loss: 0.021615304052829742, Validation Loss: 0.022024817764759064\n",
      "Epoch 20, Batch: 99: Training Loss: 0.02303938753902912, Validation Loss: 0.024393539875745773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 100: Training Loss: 0.022908663377165794, Validation Loss: 0.022667935118079185\n",
      "Epoch 20, Batch: 101: Training Loss: 0.020796965807676315, Validation Loss: 0.024481620639562607\n",
      "Epoch 20, Batch: 102: Training Loss: 0.026095835492014885, Validation Loss: 0.023319272324442863\n",
      "Epoch 20, Batch: 103: Training Loss: 0.024586377665400505, Validation Loss: 0.026131870225071907\n",
      "Epoch 20, Batch: 104: Training Loss: 0.02051750011742115, Validation Loss: 0.023443331941962242\n",
      "Epoch 20, Batch: 105: Training Loss: 0.020159592851996422, Validation Loss: 0.02584780938923359\n",
      "Epoch 20, Batch: 106: Training Loss: 0.020449232310056686, Validation Loss: 0.025735240429639816\n",
      "Epoch 20, Batch: 107: Training Loss: 0.02346857637166977, Validation Loss: 0.026085376739501953\n",
      "Epoch 20, Batch: 108: Training Loss: 0.023526331409811974, Validation Loss: 0.025354625657200813\n",
      "Epoch 20, Batch: 109: Training Loss: 0.02604544907808304, Validation Loss: 0.026242529973387718\n",
      "Epoch 20, Batch: 110: Training Loss: 0.0250073429197073, Validation Loss: 0.02812955155968666\n",
      "Epoch 20, Batch: 111: Training Loss: 0.021347006782889366, Validation Loss: 0.028220288455486298\n",
      "Epoch 20, Batch: 112: Training Loss: 0.02123655565083027, Validation Loss: 0.02687705308198929\n",
      "Epoch 20, Batch: 113: Training Loss: 0.025285733863711357, Validation Loss: 0.0266221072524786\n",
      "Epoch 20, Batch: 114: Training Loss: 0.023806056007742882, Validation Loss: 0.024152865633368492\n",
      "Epoch 20, Batch: 115: Training Loss: 0.02411770634353161, Validation Loss: 0.02504955232143402\n",
      "Epoch 20, Batch: 116: Training Loss: 0.023045461624860764, Validation Loss: 0.028087761253118515\n",
      "Epoch 20, Batch: 117: Training Loss: 0.02306588552892208, Validation Loss: 0.022076526656746864\n",
      "Epoch 20, Batch: 118: Training Loss: 0.020101163536310196, Validation Loss: 0.023448338732123375\n",
      "Epoch 20, Batch: 119: Training Loss: 0.02342456579208374, Validation Loss: 0.024039622396230698\n",
      "Epoch 20, Batch: 120: Training Loss: 0.020167091861367226, Validation Loss: 0.023848874494433403\n",
      "Epoch 20, Batch: 121: Training Loss: 0.019240928813815117, Validation Loss: 0.022663069888949394\n",
      "Epoch 20, Batch: 122: Training Loss: 0.021161358803510666, Validation Loss: 0.02229398488998413\n",
      "Epoch 20, Batch: 123: Training Loss: 0.020974021404981613, Validation Loss: 0.023528030142188072\n",
      "Epoch 20, Batch: 124: Training Loss: 0.024689655750989914, Validation Loss: 0.02358204498887062\n",
      "Epoch 20, Batch: 125: Training Loss: 0.02227981947362423, Validation Loss: 0.021922539919614792\n",
      "Epoch 20, Batch: 126: Training Loss: 0.020366452634334564, Validation Loss: 0.02183261513710022\n",
      "Epoch 20, Batch: 127: Training Loss: 0.022121915593743324, Validation Loss: 0.021386917680501938\n",
      "Epoch 20, Batch: 128: Training Loss: 0.020339032635092735, Validation Loss: 0.02229972928762436\n",
      "Epoch 20, Batch: 129: Training Loss: 0.020173612982034683, Validation Loss: 0.02031031809747219\n",
      "Epoch 20, Batch: 130: Training Loss: 0.022517075762152672, Validation Loss: 0.0197783000767231\n",
      "Epoch 20, Batch: 131: Training Loss: 0.019898271188139915, Validation Loss: 0.02081192471086979\n",
      "Epoch 20, Batch: 132: Training Loss: 0.019733792170882225, Validation Loss: 0.022843921557068825\n",
      "Epoch 20, Batch: 133: Training Loss: 0.021571744233369827, Validation Loss: 0.0224482249468565\n",
      "Epoch 20, Batch: 134: Training Loss: 0.020608851686120033, Validation Loss: 0.021724898368120193\n",
      "Epoch 20, Batch: 135: Training Loss: 0.02178356610238552, Validation Loss: 0.02183600515127182\n",
      "Epoch 20, Batch: 136: Training Loss: 0.020782098174095154, Validation Loss: 0.019771836698055267\n",
      "Epoch 20, Batch: 137: Training Loss: 0.018382137641310692, Validation Loss: 0.020163383334875107\n",
      "Epoch 20, Batch: 138: Training Loss: 0.02192028798162937, Validation Loss: 0.021309588104486465\n",
      "Epoch 20, Batch: 139: Training Loss: 0.02109544910490513, Validation Loss: 0.024787701666355133\n",
      "Epoch 20, Batch: 140: Training Loss: 0.02330351248383522, Validation Loss: 0.023633291944861412\n",
      "Epoch 20, Batch: 141: Training Loss: 0.021126175299286842, Validation Loss: 0.02331206202507019\n",
      "Epoch 20, Batch: 142: Training Loss: 0.020702065899968147, Validation Loss: 0.023215090855956078\n",
      "Epoch 20, Batch: 143: Training Loss: 0.020747123286128044, Validation Loss: 0.024067338556051254\n",
      "Epoch 20, Batch: 144: Training Loss: 0.0211963914334774, Validation Loss: 0.023065123707056046\n",
      "Epoch 20, Batch: 145: Training Loss: 0.020140087231993675, Validation Loss: 0.022918498143553734\n",
      "Epoch 20, Batch: 146: Training Loss: 0.01903780922293663, Validation Loss: 0.02290995605289936\n",
      "Epoch 20, Batch: 147: Training Loss: 0.02162778377532959, Validation Loss: 0.022865574806928635\n",
      "Epoch 20, Batch: 148: Training Loss: 0.019752904772758484, Validation Loss: 0.02533602900803089\n",
      "Epoch 20, Batch: 149: Training Loss: 0.021502073854207993, Validation Loss: 0.023492025211453438\n",
      "Epoch 20, Batch: 150: Training Loss: 0.022700035944581032, Validation Loss: 0.02443457394838333\n",
      "Epoch 20, Batch: 151: Training Loss: 0.02337503246963024, Validation Loss: 0.0244089737534523\n",
      "Epoch 20, Batch: 152: Training Loss: 0.021897951140999794, Validation Loss: 0.02198841981589794\n",
      "Epoch 20, Batch: 153: Training Loss: 0.02343752607703209, Validation Loss: 0.023311717435717583\n",
      "Epoch 20, Batch: 154: Training Loss: 0.0235684085637331, Validation Loss: 0.022739160805940628\n",
      "Epoch 20, Batch: 155: Training Loss: 0.02634907327592373, Validation Loss: 0.024962104856967926\n",
      "Epoch 20, Batch: 156: Training Loss: 0.019755706191062927, Validation Loss: 0.021732652559876442\n",
      "Epoch 20, Batch: 157: Training Loss: 0.020695103332400322, Validation Loss: 0.02585398405790329\n",
      "Epoch 20, Batch: 158: Training Loss: 0.024062279611825943, Validation Loss: 0.023679545149207115\n",
      "Epoch 20, Batch: 159: Training Loss: 0.02184116654098034, Validation Loss: 0.023608217015862465\n",
      "Epoch 20, Batch: 160: Training Loss: 0.022016147151589394, Validation Loss: 0.024716440588235855\n",
      "Epoch 20, Batch: 161: Training Loss: 0.023287469521164894, Validation Loss: 0.022321203723549843\n",
      "Epoch 20, Batch: 162: Training Loss: 0.02218981273472309, Validation Loss: 0.024442363530397415\n",
      "Epoch 20, Batch: 163: Training Loss: 0.025167793035507202, Validation Loss: 0.024687163531780243\n",
      "Epoch 20, Batch: 164: Training Loss: 0.019224079325795174, Validation Loss: 0.022951608523726463\n",
      "Epoch 20, Batch: 165: Training Loss: 0.02187085524201393, Validation Loss: 0.023196104913949966\n",
      "Epoch 20, Batch: 166: Training Loss: 0.02071453630924225, Validation Loss: 0.02443048171699047\n",
      "Epoch 20, Batch: 167: Training Loss: 0.02076193504035473, Validation Loss: 0.023814745247364044\n",
      "Epoch 20, Batch: 168: Training Loss: 0.02495446987450123, Validation Loss: 0.023867733776569366\n",
      "Epoch 20, Batch: 169: Training Loss: 0.024534644559025764, Validation Loss: 0.022795185446739197\n",
      "Epoch 20, Batch: 170: Training Loss: 0.02076411433517933, Validation Loss: 0.022597309201955795\n",
      "Epoch 20, Batch: 171: Training Loss: 0.02216493897140026, Validation Loss: 0.022857876494526863\n",
      "Epoch 20, Batch: 172: Training Loss: 0.022247036918997765, Validation Loss: 0.023407042026519775\n",
      "Epoch 20, Batch: 173: Training Loss: 0.017834346741437912, Validation Loss: 0.022732160985469818\n",
      "Epoch 20, Batch: 174: Training Loss: 0.027787605300545692, Validation Loss: 0.02334626577794552\n",
      "Epoch 20, Batch: 175: Training Loss: 0.02368098683655262, Validation Loss: 0.024826359003782272\n",
      "Epoch 20, Batch: 176: Training Loss: 0.022948775440454483, Validation Loss: 0.02300962246954441\n",
      "Epoch 20, Batch: 177: Training Loss: 0.0203567985445261, Validation Loss: 0.023470770567655563\n",
      "Epoch 20, Batch: 178: Training Loss: 0.02855304256081581, Validation Loss: 0.025157034397125244\n",
      "Epoch 20, Batch: 179: Training Loss: 0.020353244617581367, Validation Loss: 0.023559628054499626\n",
      "Epoch 20, Batch: 180: Training Loss: 0.019663924351334572, Validation Loss: 0.02541145123541355\n",
      "Epoch 20, Batch: 181: Training Loss: 0.023703601211309433, Validation Loss: 0.028604744002223015\n",
      "Epoch 20, Batch: 182: Training Loss: 0.024539297446608543, Validation Loss: 0.025214675813913345\n",
      "Epoch 20, Batch: 183: Training Loss: 0.021310744807124138, Validation Loss: 0.0277680866420269\n",
      "Epoch 20, Batch: 184: Training Loss: 0.02402498759329319, Validation Loss: 0.02582274191081524\n",
      "Epoch 20, Batch: 185: Training Loss: 0.02594602108001709, Validation Loss: 0.02687082812190056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 186: Training Loss: 0.023232704028487206, Validation Loss: 0.024647805839776993\n",
      "Epoch 20, Batch: 187: Training Loss: 0.022947732359170914, Validation Loss: 0.024067698046565056\n",
      "Epoch 20, Batch: 188: Training Loss: 0.025797627866268158, Validation Loss: 0.024357810616493225\n",
      "Epoch 20, Batch: 189: Training Loss: 0.02270633354783058, Validation Loss: 0.024428443983197212\n",
      "Epoch 20, Batch: 190: Training Loss: 0.02403070777654648, Validation Loss: 0.02533060684800148\n",
      "Epoch 20, Batch: 191: Training Loss: 0.026197362691164017, Validation Loss: 0.027165262028574944\n",
      "Epoch 20, Batch: 192: Training Loss: 0.02357368916273117, Validation Loss: 0.023345453664660454\n",
      "Epoch 20, Batch: 193: Training Loss: 0.023940792307257652, Validation Loss: 0.025524010881781578\n",
      "Epoch 20, Batch: 194: Training Loss: 0.021328482776880264, Validation Loss: 0.026415688917040825\n",
      "Epoch 20, Batch: 195: Training Loss: 0.023565160110592842, Validation Loss: 0.026339063420891762\n",
      "Epoch 20, Batch: 196: Training Loss: 0.02257821522653103, Validation Loss: 0.025951212272047997\n",
      "Epoch 20, Batch: 197: Training Loss: 0.02012845315039158, Validation Loss: 0.025878112763166428\n",
      "Epoch 20, Batch: 198: Training Loss: 0.022779366001486778, Validation Loss: 0.024618729948997498\n",
      "Epoch 20, Batch: 199: Training Loss: 0.02420804277062416, Validation Loss: 0.026030689477920532\n",
      "Epoch 20, Batch: 200: Training Loss: 0.020302027463912964, Validation Loss: 0.02600804902613163\n",
      "Epoch 20, Batch: 201: Training Loss: 0.02794387750327587, Validation Loss: 0.027722936123609543\n",
      "Epoch 20, Batch: 202: Training Loss: 0.023889267817139626, Validation Loss: 0.02604064531624317\n",
      "Epoch 20, Batch: 203: Training Loss: 0.025678245350718498, Validation Loss: 0.02851773425936699\n",
      "Epoch 20, Batch: 204: Training Loss: 0.024452364072203636, Validation Loss: 0.026301266625523567\n",
      "Epoch 20, Batch: 205: Training Loss: 0.02698986791074276, Validation Loss: 0.027078673243522644\n",
      "Epoch 20, Batch: 206: Training Loss: 0.022361529991030693, Validation Loss: 0.024803830310702324\n",
      "Epoch 20, Batch: 207: Training Loss: 0.02679218165576458, Validation Loss: 0.026798013597726822\n",
      "Epoch 20, Batch: 208: Training Loss: 0.022520383819937706, Validation Loss: 0.025731880217790604\n",
      "Epoch 20, Batch: 209: Training Loss: 0.023356618359684944, Validation Loss: 0.025768842548131943\n",
      "Epoch 20, Batch: 210: Training Loss: 0.022659186273813248, Validation Loss: 0.025829870253801346\n",
      "Epoch 20, Batch: 211: Training Loss: 0.023207461461424828, Validation Loss: 0.027046309784054756\n",
      "Epoch 20, Batch: 212: Training Loss: 0.02094121463596821, Validation Loss: 0.025864025577902794\n",
      "Epoch 20, Batch: 213: Training Loss: 0.022997742518782616, Validation Loss: 0.026556827127933502\n",
      "Epoch 20, Batch: 214: Training Loss: 0.023514343425631523, Validation Loss: 0.026058804243803024\n",
      "Epoch 20, Batch: 215: Training Loss: 0.023159360513091087, Validation Loss: 0.02226816676557064\n",
      "Epoch 20, Batch: 216: Training Loss: 0.021758444607257843, Validation Loss: 0.021983588114380836\n",
      "Epoch 20, Batch: 217: Training Loss: 0.024405520409345627, Validation Loss: 0.02193652279675007\n",
      "Epoch 20, Batch: 218: Training Loss: 0.02026914618909359, Validation Loss: 0.025658564642071724\n",
      "Epoch 20, Batch: 219: Training Loss: 0.020273784175515175, Validation Loss: 0.024054601788520813\n",
      "Epoch 20, Batch: 220: Training Loss: 0.02179514802992344, Validation Loss: 0.02254389226436615\n",
      "Epoch 20, Batch: 221: Training Loss: 0.02474728412926197, Validation Loss: 0.024678967893123627\n",
      "Epoch 20, Batch: 222: Training Loss: 0.022583160549402237, Validation Loss: 0.023501209914684296\n",
      "Epoch 20, Batch: 223: Training Loss: 0.022171221673488617, Validation Loss: 0.020309144631028175\n",
      "Epoch 20, Batch: 224: Training Loss: 0.026795390993356705, Validation Loss: 0.02382945641875267\n",
      "Epoch 20, Batch: 225: Training Loss: 0.020865384489297867, Validation Loss: 0.025926552712917328\n",
      "Epoch 20, Batch: 226: Training Loss: 0.023715976625680923, Validation Loss: 0.02428765967488289\n",
      "Epoch 20, Batch: 227: Training Loss: 0.022961515933275223, Validation Loss: 0.022902904078364372\n",
      "Epoch 20, Batch: 228: Training Loss: 0.026636537164449692, Validation Loss: 0.025640569627285004\n",
      "Epoch 20, Batch: 229: Training Loss: 0.02479599416255951, Validation Loss: 0.024762820452451706\n",
      "Epoch 20, Batch: 230: Training Loss: 0.018676284700632095, Validation Loss: 0.025667227804660797\n",
      "Epoch 20, Batch: 231: Training Loss: 0.023854494094848633, Validation Loss: 0.024470465257763863\n",
      "Epoch 20, Batch: 232: Training Loss: 0.02410557121038437, Validation Loss: 0.026476413011550903\n",
      "Epoch 20, Batch: 233: Training Loss: 0.01852208748459816, Validation Loss: 0.027938537299633026\n",
      "Epoch 20, Batch: 234: Training Loss: 0.024335965514183044, Validation Loss: 0.02648623287677765\n",
      "Epoch 20, Batch: 235: Training Loss: 0.02270931750535965, Validation Loss: 0.02468891441822052\n",
      "Epoch 20, Batch: 236: Training Loss: 0.022818757221102715, Validation Loss: 0.026631971821188927\n",
      "Epoch 20, Batch: 237: Training Loss: 0.025663813576102257, Validation Loss: 0.02528831921517849\n",
      "Epoch 20, Batch: 238: Training Loss: 0.021864967420697212, Validation Loss: 0.024961572140455246\n",
      "Epoch 20, Batch: 239: Training Loss: 0.023282349109649658, Validation Loss: 0.02571111172437668\n",
      "Epoch 20, Batch: 240: Training Loss: 0.02560644969344139, Validation Loss: 0.02512063831090927\n",
      "Epoch 20, Batch: 241: Training Loss: 0.02196069434285164, Validation Loss: 0.026232071220874786\n",
      "Epoch 20, Batch: 242: Training Loss: 0.02315225824713707, Validation Loss: 0.02555040828883648\n",
      "Epoch 20, Batch: 243: Training Loss: 0.022419041022658348, Validation Loss: 0.026170291006565094\n",
      "Epoch 20, Batch: 244: Training Loss: 0.019806120544672012, Validation Loss: 0.026333648711442947\n",
      "Epoch 20, Batch: 245: Training Loss: 0.02479729801416397, Validation Loss: 0.026520144194364548\n",
      "Epoch 20, Batch: 246: Training Loss: 0.020748045295476913, Validation Loss: 0.02419181913137436\n",
      "Epoch 20, Batch: 247: Training Loss: 0.02348998188972473, Validation Loss: 0.024842189624905586\n",
      "Epoch 20, Batch: 248: Training Loss: 0.022750264033675194, Validation Loss: 0.025696566328406334\n",
      "Epoch 20, Batch: 249: Training Loss: 0.023614153265953064, Validation Loss: 0.02675735391676426\n",
      "Epoch 20, Batch: 250: Training Loss: 0.020194880664348602, Validation Loss: 0.02573179267346859\n",
      "Epoch 20, Batch: 251: Training Loss: 0.020876223221421242, Validation Loss: 0.02566283941268921\n",
      "Epoch 20, Batch: 252: Training Loss: 0.023405253887176514, Validation Loss: 0.02607916295528412\n",
      "Epoch 20, Batch: 253: Training Loss: 0.025005925446748734, Validation Loss: 0.02392977848649025\n",
      "Epoch 20, Batch: 254: Training Loss: 0.020728353410959244, Validation Loss: 0.02470303513109684\n",
      "Epoch 20, Batch: 255: Training Loss: 0.02173612266778946, Validation Loss: 0.024129625409841537\n",
      "Epoch 20, Batch: 256: Training Loss: 0.021202869713306427, Validation Loss: 0.02239586040377617\n",
      "Epoch 20, Batch: 257: Training Loss: 0.02291913330554962, Validation Loss: 0.02616957202553749\n",
      "Epoch 20, Batch: 258: Training Loss: 0.025214580819010735, Validation Loss: 0.024704137817025185\n",
      "Epoch 20, Batch: 259: Training Loss: 0.018611082807183266, Validation Loss: 0.02656029909849167\n",
      "Epoch 20, Batch: 260: Training Loss: 0.024367203935980797, Validation Loss: 0.024432286620140076\n",
      "Epoch 20, Batch: 261: Training Loss: 0.023981135338544846, Validation Loss: 0.02669515088200569\n",
      "Epoch 20, Batch: 262: Training Loss: 0.02070067822933197, Validation Loss: 0.026067189872264862\n",
      "Epoch 20, Batch: 263: Training Loss: 0.019372260197997093, Validation Loss: 0.024424482136964798\n",
      "Epoch 20, Batch: 264: Training Loss: 0.021611278876662254, Validation Loss: 0.02544633485376835\n",
      "Epoch 20, Batch: 265: Training Loss: 0.024914579465985298, Validation Loss: 0.02409706450998783\n",
      "Epoch 20, Batch: 266: Training Loss: 0.018338574096560478, Validation Loss: 0.025674978271126747\n",
      "Epoch 20, Batch: 267: Training Loss: 0.019887719303369522, Validation Loss: 0.02629001811146736\n",
      "Epoch 20, Batch: 268: Training Loss: 0.02119235321879387, Validation Loss: 0.026380669325590134\n",
      "Epoch 20, Batch: 269: Training Loss: 0.02187797985970974, Validation Loss: 0.02649785205721855\n",
      "Epoch 20, Batch: 270: Training Loss: 0.0228867270052433, Validation Loss: 0.024056846275925636\n",
      "Epoch 20, Batch: 271: Training Loss: 0.023255474865436554, Validation Loss: 0.02503773383796215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 272: Training Loss: 0.023472363129258156, Validation Loss: 0.02549947425723076\n",
      "Epoch 20, Batch: 273: Training Loss: 0.02271505817770958, Validation Loss: 0.023834839463233948\n",
      "Epoch 20, Batch: 274: Training Loss: 0.024239642545580864, Validation Loss: 0.02624879963696003\n",
      "Epoch 20, Batch: 275: Training Loss: 0.021366160362958908, Validation Loss: 0.02452562004327774\n",
      "Epoch 20, Batch: 276: Training Loss: 0.021435661241412163, Validation Loss: 0.023516004905104637\n",
      "Epoch 20, Batch: 277: Training Loss: 0.020224103704094887, Validation Loss: 0.022083699703216553\n",
      "Epoch 20, Batch: 278: Training Loss: 0.022595392540097237, Validation Loss: 0.02319631353020668\n",
      "Epoch 20, Batch: 279: Training Loss: 0.024994712322950363, Validation Loss: 0.022724563255906105\n",
      "Epoch 20, Batch: 280: Training Loss: 0.018433919176459312, Validation Loss: 0.025188801810145378\n",
      "Epoch 20, Batch: 281: Training Loss: 0.021569641306996346, Validation Loss: 0.0220950897783041\n",
      "Epoch 20, Batch: 282: Training Loss: 0.023107776418328285, Validation Loss: 0.023991532623767853\n",
      "Epoch 20, Batch: 283: Training Loss: 0.021101774647831917, Validation Loss: 0.022570734843611717\n",
      "Epoch 20, Batch: 284: Training Loss: 0.02055864967405796, Validation Loss: 0.02277853526175022\n",
      "Epoch 20, Batch: 285: Training Loss: 0.021360060200095177, Validation Loss: 0.02293924242258072\n",
      "Epoch 20, Batch: 286: Training Loss: 0.02027107961475849, Validation Loss: 0.02155284956097603\n",
      "Epoch 20, Batch: 287: Training Loss: 0.026315338909626007, Validation Loss: 0.02379523776471615\n",
      "Epoch 20, Batch: 288: Training Loss: 0.019803108647465706, Validation Loss: 0.02228829450905323\n",
      "Epoch 20, Batch: 289: Training Loss: 0.022281790152192116, Validation Loss: 0.024530479684472084\n",
      "Epoch 20, Batch: 290: Training Loss: 0.022781435400247574, Validation Loss: 0.02154730074107647\n",
      "Epoch 20, Batch: 291: Training Loss: 0.023227285593748093, Validation Loss: 0.021975407376885414\n",
      "Epoch 20, Batch: 292: Training Loss: 0.02553562819957733, Validation Loss: 0.02155885100364685\n",
      "Epoch 20, Batch: 293: Training Loss: 0.0224223081022501, Validation Loss: 0.021245842799544334\n",
      "Epoch 20, Batch: 294: Training Loss: 0.020031196996569633, Validation Loss: 0.0243209358304739\n",
      "Epoch 20, Batch: 295: Training Loss: 0.02036374807357788, Validation Loss: 0.023738445714116096\n",
      "Epoch 20, Batch: 296: Training Loss: 0.02232041209936142, Validation Loss: 0.021736865863204002\n",
      "Epoch 20, Batch: 297: Training Loss: 0.020759491249918938, Validation Loss: 0.022542541846632957\n",
      "Epoch 20, Batch: 298: Training Loss: 0.02289412170648575, Validation Loss: 0.020892374217510223\n",
      "Epoch 20, Batch: 299: Training Loss: 0.02073919214308262, Validation Loss: 0.02170940302312374\n",
      "Epoch 20, Batch: 300: Training Loss: 0.024358827620744705, Validation Loss: 0.02244502305984497\n",
      "Epoch 20, Batch: 301: Training Loss: 0.021927081048488617, Validation Loss: 0.02378223091363907\n",
      "Epoch 20, Batch: 302: Training Loss: 0.02043328993022442, Validation Loss: 0.02175612933933735\n",
      "Epoch 20, Batch: 303: Training Loss: 0.023076532408595085, Validation Loss: 0.023362109437584877\n",
      "Epoch 20, Batch: 304: Training Loss: 0.019282029941678047, Validation Loss: 0.02195611409842968\n",
      "Epoch 20, Batch: 305: Training Loss: 0.02000417746603489, Validation Loss: 0.021852849051356316\n",
      "Epoch 20, Batch: 306: Training Loss: 0.02241549640893936, Validation Loss: 0.021969221532344818\n",
      "Epoch 20, Batch: 307: Training Loss: 0.020807961001992226, Validation Loss: 0.023158134892582893\n",
      "Epoch 20, Batch: 308: Training Loss: 0.024036193266510963, Validation Loss: 0.02229573205113411\n",
      "Epoch 20, Batch: 309: Training Loss: 0.020503491163253784, Validation Loss: 0.02623800002038479\n",
      "Epoch 20, Batch: 310: Training Loss: 0.02555948868393898, Validation Loss: 0.021761275827884674\n",
      "Epoch 20, Batch: 311: Training Loss: 0.02216504141688347, Validation Loss: 0.0216833483427763\n",
      "Epoch 20, Batch: 312: Training Loss: 0.022416215389966965, Validation Loss: 0.021518856287002563\n",
      "Epoch 20, Batch: 313: Training Loss: 0.018999116495251656, Validation Loss: 0.021277092397212982\n",
      "Epoch 20, Batch: 314: Training Loss: 0.020433587953448296, Validation Loss: 0.02231484465301037\n",
      "Epoch 20, Batch: 315: Training Loss: 0.0200943686068058, Validation Loss: 0.02464948035776615\n",
      "Epoch 20, Batch: 316: Training Loss: 0.02245100401341915, Validation Loss: 0.02391432225704193\n",
      "Epoch 20, Batch: 317: Training Loss: 0.01934828795492649, Validation Loss: 0.024756088852882385\n",
      "Epoch 20, Batch: 318: Training Loss: 0.01926209218800068, Validation Loss: 0.02319108135998249\n",
      "Epoch 20, Batch: 319: Training Loss: 0.02123953215777874, Validation Loss: 0.02461828850209713\n",
      "Epoch 20, Batch: 320: Training Loss: 0.0214367862790823, Validation Loss: 0.02569280005991459\n",
      "Epoch 20, Batch: 321: Training Loss: 0.02254481241106987, Validation Loss: 0.025407439097762108\n",
      "Epoch 20, Batch: 322: Training Loss: 0.018876198679208755, Validation Loss: 0.026744406670331955\n",
      "Epoch 20, Batch: 323: Training Loss: 0.023283783346414566, Validation Loss: 0.024169055745005608\n",
      "Epoch 20, Batch: 324: Training Loss: 0.02420327626168728, Validation Loss: 0.025408895686268806\n",
      "Epoch 20, Batch: 325: Training Loss: 0.021416956558823586, Validation Loss: 0.02364061027765274\n",
      "Epoch 20, Batch: 326: Training Loss: 0.027787338942289352, Validation Loss: 0.02363268844783306\n",
      "Epoch 20, Batch: 327: Training Loss: 0.020387209951877594, Validation Loss: 0.024345792829990387\n",
      "Epoch 20, Batch: 328: Training Loss: 0.020432429388165474, Validation Loss: 0.022495269775390625\n",
      "Epoch 20, Batch: 329: Training Loss: 0.025470329448580742, Validation Loss: 0.02359486185014248\n",
      "Epoch 20, Batch: 330: Training Loss: 0.021566830575466156, Validation Loss: 0.022025400772690773\n",
      "Epoch 20, Batch: 331: Training Loss: 0.02243083529174328, Validation Loss: 0.023971959948539734\n",
      "Epoch 20, Batch: 332: Training Loss: 0.01995370164513588, Validation Loss: 0.021925758570432663\n",
      "Epoch 20, Batch: 333: Training Loss: 0.02499821037054062, Validation Loss: 0.022198086604475975\n",
      "Epoch 20, Batch: 334: Training Loss: 0.01963845267891884, Validation Loss: 0.022622888907790184\n",
      "Epoch 20, Batch: 335: Training Loss: 0.0223526693880558, Validation Loss: 0.023639168590307236\n",
      "Epoch 20, Batch: 336: Training Loss: 0.02016916498541832, Validation Loss: 0.022477412596344948\n",
      "Epoch 20, Batch: 337: Training Loss: 0.020122410729527473, Validation Loss: 0.024580519646406174\n",
      "Epoch 20, Batch: 338: Training Loss: 0.019177425652742386, Validation Loss: 0.022414635866880417\n",
      "Epoch 20, Batch: 339: Training Loss: 0.022683298215270042, Validation Loss: 0.026585884392261505\n",
      "Epoch 20, Batch: 340: Training Loss: 0.025406649336218834, Validation Loss: 0.024222349748015404\n",
      "Epoch 20, Batch: 341: Training Loss: 0.021100705489516258, Validation Loss: 0.023163650184869766\n",
      "Epoch 20, Batch: 342: Training Loss: 0.02215682901442051, Validation Loss: 0.021047014743089676\n",
      "Epoch 20, Batch: 343: Training Loss: 0.023003295063972473, Validation Loss: 0.022562358528375626\n",
      "Epoch 20, Batch: 344: Training Loss: 0.023017743602395058, Validation Loss: 0.021534858271479607\n",
      "Epoch 20, Batch: 345: Training Loss: 0.02625509724020958, Validation Loss: 0.02100246399641037\n",
      "Epoch 20, Batch: 346: Training Loss: 0.02298848144710064, Validation Loss: 0.01989756152033806\n",
      "Epoch 20, Batch: 347: Training Loss: 0.021487629041075706, Validation Loss: 0.0231257826089859\n",
      "Epoch 20, Batch: 348: Training Loss: 0.021001385524868965, Validation Loss: 0.023357175290584564\n",
      "Epoch 20, Batch: 349: Training Loss: 0.024815941229462624, Validation Loss: 0.0208563432097435\n",
      "Epoch 20, Batch: 350: Training Loss: 0.021383969113230705, Validation Loss: 0.021534664556384087\n",
      "Epoch 20, Batch: 351: Training Loss: 0.024699075147509575, Validation Loss: 0.02491927146911621\n",
      "Epoch 20, Batch: 352: Training Loss: 0.02247891202569008, Validation Loss: 0.023882435634732246\n",
      "Epoch 20, Batch: 353: Training Loss: 0.02565545216202736, Validation Loss: 0.024557698518037796\n",
      "Epoch 20, Batch: 354: Training Loss: 0.021952642127871513, Validation Loss: 0.024135887622833252\n",
      "Epoch 20, Batch: 355: Training Loss: 0.01838432066142559, Validation Loss: 0.023203996941447258\n",
      "Epoch 20, Batch: 356: Training Loss: 0.02351633459329605, Validation Loss: 0.02507961541414261\n",
      "Epoch 20, Batch: 357: Training Loss: 0.020218905061483383, Validation Loss: 0.024962108582258224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 358: Training Loss: 0.02310446836054325, Validation Loss: 0.025034572929143906\n",
      "Epoch 20, Batch: 359: Training Loss: 0.023013342171907425, Validation Loss: 0.02394016832113266\n",
      "Epoch 20, Batch: 360: Training Loss: 0.023484520614147186, Validation Loss: 0.024891141802072525\n",
      "Epoch 20, Batch: 361: Training Loss: 0.023076515644788742, Validation Loss: 0.02398310974240303\n",
      "Epoch 20, Batch: 362: Training Loss: 0.023679323494434357, Validation Loss: 0.024018779397010803\n",
      "Epoch 20, Batch: 363: Training Loss: 0.025624755769968033, Validation Loss: 0.02559916116297245\n",
      "Epoch 20, Batch: 364: Training Loss: 0.020887991413474083, Validation Loss: 0.02443738840520382\n",
      "Epoch 20, Batch: 365: Training Loss: 0.021035537123680115, Validation Loss: 0.021446097642183304\n",
      "Epoch 20, Batch: 366: Training Loss: 0.02087279222905636, Validation Loss: 0.022922145202755928\n",
      "Epoch 20, Batch: 367: Training Loss: 0.018926136195659637, Validation Loss: 0.024377994239330292\n",
      "Epoch 20, Batch: 368: Training Loss: 0.01994747668504715, Validation Loss: 0.024802131578326225\n",
      "Epoch 20, Batch: 369: Training Loss: 0.02517968788743019, Validation Loss: 0.024068558588624\n",
      "Epoch 20, Batch: 370: Training Loss: 0.022568300366401672, Validation Loss: 0.025604326277971268\n",
      "Epoch 20, Batch: 371: Training Loss: 0.020767098292708397, Validation Loss: 0.0254823025316\n",
      "Epoch 20, Batch: 372: Training Loss: 0.022181926295161247, Validation Loss: 0.022976232692599297\n",
      "Epoch 20, Batch: 373: Training Loss: 0.025293761864304543, Validation Loss: 0.024350283667445183\n",
      "Epoch 20, Batch: 374: Training Loss: 0.02161519043147564, Validation Loss: 0.023273998871445656\n",
      "Epoch 20, Batch: 375: Training Loss: 0.023010481148958206, Validation Loss: 0.02294977754354477\n",
      "Epoch 20, Batch: 376: Training Loss: 0.025876402854919434, Validation Loss: 0.022766532376408577\n",
      "Epoch 20, Batch: 377: Training Loss: 0.01836719736456871, Validation Loss: 0.022330978885293007\n",
      "Epoch 20, Batch: 378: Training Loss: 0.020721489563584328, Validation Loss: 0.021788060665130615\n",
      "Epoch 20, Batch: 379: Training Loss: 0.020578136667609215, Validation Loss: 0.022740190848708153\n",
      "Epoch 20, Batch: 380: Training Loss: 0.026035252958536148, Validation Loss: 0.027328189462423325\n",
      "Epoch 20, Batch: 381: Training Loss: 0.020542079582810402, Validation Loss: 0.02645334042608738\n",
      "Epoch 20, Batch: 382: Training Loss: 0.020420681685209274, Validation Loss: 0.025204863399267197\n",
      "Epoch 20, Batch: 383: Training Loss: 0.021421773359179497, Validation Loss: 0.02842036262154579\n",
      "Epoch 20, Batch: 384: Training Loss: 0.024129077792167664, Validation Loss: 0.027473339810967445\n",
      "Epoch 20, Batch: 385: Training Loss: 0.02288663014769554, Validation Loss: 0.027473047375679016\n",
      "Epoch 20, Batch: 386: Training Loss: 0.02042047306895256, Validation Loss: 0.02743484452366829\n",
      "Epoch 20, Batch: 387: Training Loss: 0.021673424169421196, Validation Loss: 0.027869002893567085\n",
      "Epoch 20, Batch: 388: Training Loss: 0.021115081384778023, Validation Loss: 0.025345098227262497\n",
      "Epoch 20, Batch: 389: Training Loss: 0.023482099175453186, Validation Loss: 0.025101395323872566\n",
      "Epoch 20, Batch: 390: Training Loss: 0.023162435740232468, Validation Loss: 0.025295088067650795\n",
      "Epoch 20, Batch: 391: Training Loss: 0.02349034883081913, Validation Loss: 0.02739153616130352\n",
      "Epoch 20, Batch: 392: Training Loss: 0.02033817395567894, Validation Loss: 0.02458181418478489\n",
      "Epoch 20, Batch: 393: Training Loss: 0.02293633669614792, Validation Loss: 0.024562668055295944\n",
      "Epoch 20, Batch: 394: Training Loss: 0.022635655477643013, Validation Loss: 0.02788451686501503\n",
      "Epoch 20, Batch: 395: Training Loss: 0.021325720474123955, Validation Loss: 0.024320265278220177\n",
      "Epoch 20, Batch: 396: Training Loss: 0.02561086416244507, Validation Loss: 0.024953598156571388\n",
      "Epoch 20, Batch: 397: Training Loss: 0.027595113962888718, Validation Loss: 0.025084763765335083\n",
      "Epoch 20, Batch: 398: Training Loss: 0.0216180682182312, Validation Loss: 0.02242812141776085\n",
      "Epoch 20, Batch: 399: Training Loss: 0.02136385627090931, Validation Loss: 0.025372961536049843\n",
      "Epoch 20, Batch: 400: Training Loss: 0.02280043624341488, Validation Loss: 0.022487899288535118\n",
      "Epoch 20, Batch: 401: Training Loss: 0.02502567507326603, Validation Loss: 0.023808322846889496\n",
      "Epoch 20, Batch: 402: Training Loss: 0.020764801651239395, Validation Loss: 0.024936683475971222\n",
      "Epoch 20, Batch: 403: Training Loss: 0.024449409916996956, Validation Loss: 0.021375207230448723\n",
      "Epoch 20, Batch: 404: Training Loss: 0.02050226926803589, Validation Loss: 0.026193439960479736\n",
      "Epoch 20, Batch: 405: Training Loss: 0.023727137595415115, Validation Loss: 0.02505425736308098\n",
      "Epoch 20, Batch: 406: Training Loss: 0.02135174348950386, Validation Loss: 0.027229318395256996\n",
      "Epoch 20, Batch: 407: Training Loss: 0.022572800517082214, Validation Loss: 0.023955482989549637\n",
      "Epoch 20, Batch: 408: Training Loss: 0.017918216064572334, Validation Loss: 0.02842325158417225\n",
      "Epoch 20, Batch: 409: Training Loss: 0.02258005551993847, Validation Loss: 0.02585250325500965\n",
      "Epoch 20, Batch: 410: Training Loss: 0.021422069519758224, Validation Loss: 0.025250066071748734\n",
      "Epoch 20, Batch: 411: Training Loss: 0.017483705654740334, Validation Loss: 0.023781143128871918\n",
      "Epoch 20, Batch: 412: Training Loss: 0.02416554093360901, Validation Loss: 0.026085950434207916\n",
      "Epoch 20, Batch: 413: Training Loss: 0.025016609579324722, Validation Loss: 0.02484280802309513\n",
      "Epoch 20, Batch: 414: Training Loss: 0.02383756823837757, Validation Loss: 0.02376413159072399\n",
      "Epoch 20, Batch: 415: Training Loss: 0.021044204011559486, Validation Loss: 0.023030700162053108\n",
      "Epoch 20, Batch: 416: Training Loss: 0.018741827458143234, Validation Loss: 0.024040689691901207\n",
      "Epoch 20, Batch: 417: Training Loss: 0.022105198353528976, Validation Loss: 0.024582618847489357\n",
      "Epoch 20, Batch: 418: Training Loss: 0.020808786153793335, Validation Loss: 0.024253753945231438\n",
      "Epoch 20, Batch: 419: Training Loss: 0.019288038834929466, Validation Loss: 0.02411503903567791\n",
      "Epoch 20, Batch: 420: Training Loss: 0.020713381469249725, Validation Loss: 0.022790726274251938\n",
      "Epoch 20, Batch: 421: Training Loss: 0.02047000080347061, Validation Loss: 0.02317710779607296\n",
      "Epoch 20, Batch: 422: Training Loss: 0.028681157156825066, Validation Loss: 0.02535305544734001\n",
      "Epoch 20, Batch: 423: Training Loss: 0.021224791184067726, Validation Loss: 0.022478610277175903\n",
      "Epoch 20, Batch: 424: Training Loss: 0.021717749536037445, Validation Loss: 0.022798577323555946\n",
      "Epoch 20, Batch: 425: Training Loss: 0.02437315508723259, Validation Loss: 0.026440119370818138\n",
      "Epoch 20, Batch: 426: Training Loss: 0.020326951518654823, Validation Loss: 0.027387388050556183\n",
      "Epoch 20, Batch: 427: Training Loss: 0.024139605462551117, Validation Loss: 0.025459466502070427\n",
      "Epoch 20, Batch: 428: Training Loss: 0.021181635558605194, Validation Loss: 0.02526611089706421\n",
      "Epoch 20, Batch: 429: Training Loss: 0.020043237134814262, Validation Loss: 0.026585504412651062\n",
      "Epoch 20, Batch: 430: Training Loss: 0.020328059792518616, Validation Loss: 0.023314351215958595\n",
      "Epoch 20, Batch: 431: Training Loss: 0.020520050078630447, Validation Loss: 0.02501157484948635\n",
      "Epoch 20, Batch: 432: Training Loss: 0.02544478140771389, Validation Loss: 0.02345167100429535\n",
      "Epoch 20, Batch: 433: Training Loss: 0.021342063322663307, Validation Loss: 0.02589101344347\n",
      "Epoch 20, Batch: 434: Training Loss: 0.019613031297922134, Validation Loss: 0.026588689535856247\n",
      "Epoch 20, Batch: 435: Training Loss: 0.022436100989580154, Validation Loss: 0.02556830272078514\n",
      "Epoch 20, Batch: 436: Training Loss: 0.019560890272259712, Validation Loss: 0.024158397689461708\n",
      "Epoch 20, Batch: 437: Training Loss: 0.01836933009326458, Validation Loss: 0.02393939346075058\n",
      "Epoch 20, Batch: 438: Training Loss: 0.02226456068456173, Validation Loss: 0.02349565178155899\n",
      "Epoch 20, Batch: 439: Training Loss: 0.019727693870663643, Validation Loss: 0.024181367829442024\n",
      "Epoch 20, Batch: 440: Training Loss: 0.02208664081990719, Validation Loss: 0.024703796952962875\n",
      "Epoch 20, Batch: 441: Training Loss: 0.022636529058218002, Validation Loss: 0.02264820784330368\n",
      "Epoch 20, Batch: 442: Training Loss: 0.021384920924901962, Validation Loss: 0.02337103709578514\n",
      "Epoch 20, Batch: 443: Training Loss: 0.020247941836714745, Validation Loss: 0.0239634457975626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch: 444: Training Loss: 0.020194929093122482, Validation Loss: 0.025352593511343002\n",
      "Epoch 20, Batch: 445: Training Loss: 0.020855901762843132, Validation Loss: 0.022710872814059258\n",
      "Epoch 20, Batch: 446: Training Loss: 0.025475190952420235, Validation Loss: 0.021843209862709045\n",
      "Epoch 20, Batch: 447: Training Loss: 0.02298038639128208, Validation Loss: 0.022461146116256714\n",
      "Epoch 20, Batch: 448: Training Loss: 0.02273632027208805, Validation Loss: 0.022853391245007515\n",
      "Epoch 20, Batch: 449: Training Loss: 0.021141236647963524, Validation Loss: 0.025638731196522713\n",
      "Epoch 20, Batch: 450: Training Loss: 0.021771902218461037, Validation Loss: 0.026092154905200005\n",
      "Epoch 20, Batch: 451: Training Loss: 0.02521696500480175, Validation Loss: 0.027069207280874252\n",
      "Epoch 20, Batch: 452: Training Loss: 0.024930542334914207, Validation Loss: 0.025738921016454697\n",
      "Epoch 20, Batch: 453: Training Loss: 0.02482365444302559, Validation Loss: 0.026253128424286842\n",
      "Epoch 20, Batch: 454: Training Loss: 0.022182902321219444, Validation Loss: 0.024653149768710136\n",
      "Epoch 20, Batch: 455: Training Loss: 0.020213458687067032, Validation Loss: 0.023998359218239784\n",
      "Epoch 20, Batch: 456: Training Loss: 0.026099085807800293, Validation Loss: 0.029483109712600708\n",
      "Epoch 20, Batch: 457: Training Loss: 0.022514058277010918, Validation Loss: 0.02635016478598118\n",
      "Epoch 20, Batch: 458: Training Loss: 0.01974065788090229, Validation Loss: 0.025370746850967407\n",
      "Epoch 20, Batch: 459: Training Loss: 0.020912397652864456, Validation Loss: 0.024407507851719856\n",
      "Epoch 20, Batch: 460: Training Loss: 0.023732857778668404, Validation Loss: 0.024073436856269836\n",
      "Epoch 20, Batch: 461: Training Loss: 0.024829106405377388, Validation Loss: 0.02534722164273262\n",
      "Epoch 20, Batch: 462: Training Loss: 0.020615562796592712, Validation Loss: 0.027183188125491142\n",
      "Epoch 20, Batch: 463: Training Loss: 0.0214603953063488, Validation Loss: 0.026252081617712975\n",
      "Epoch 20, Batch: 464: Training Loss: 0.02168097533285618, Validation Loss: 0.02253255434334278\n",
      "Epoch 20, Batch: 465: Training Loss: 0.022233420982956886, Validation Loss: 0.02646735869348049\n",
      "Epoch 20, Batch: 466: Training Loss: 0.020326562225818634, Validation Loss: 0.024164576083421707\n",
      "Epoch 20, Batch: 467: Training Loss: 0.02699727565050125, Validation Loss: 0.02455841563642025\n",
      "Epoch 20, Batch: 468: Training Loss: 0.022339891642332077, Validation Loss: 0.025696009397506714\n",
      "Epoch 20, Batch: 469: Training Loss: 0.02278728038072586, Validation Loss: 0.02352202497422695\n",
      "Epoch 20, Batch: 470: Training Loss: 0.02082253247499466, Validation Loss: 0.024275461211800575\n",
      "Epoch 20, Batch: 471: Training Loss: 0.02244672365486622, Validation Loss: 0.023785611614584923\n",
      "Epoch 20, Batch: 472: Training Loss: 0.02001611515879631, Validation Loss: 0.023652350530028343\n",
      "Epoch 20, Batch: 473: Training Loss: 0.021498730406165123, Validation Loss: 0.025163182988762856\n",
      "Epoch 20, Batch: 474: Training Loss: 0.020148256793618202, Validation Loss: 0.02266375906765461\n",
      "Epoch 20, Batch: 475: Training Loss: 0.01968802697956562, Validation Loss: 0.022974586114287376\n",
      "Epoch 20, Batch: 476: Training Loss: 0.023487083613872528, Validation Loss: 0.02473030798137188\n",
      "Epoch 20, Batch: 477: Training Loss: 0.023403652012348175, Validation Loss: 0.02208689972758293\n",
      "Epoch 20, Batch: 478: Training Loss: 0.019179927185177803, Validation Loss: 0.023281339555978775\n",
      "Epoch 20, Batch: 479: Training Loss: 0.019893836230039597, Validation Loss: 0.02375013940036297\n",
      "Epoch 20, Batch: 480: Training Loss: 0.027470842003822327, Validation Loss: 0.022960783913731575\n",
      "Epoch 20, Batch: 481: Training Loss: 0.020415356382727623, Validation Loss: 0.021369680762290955\n",
      "Epoch 20, Batch: 482: Training Loss: 0.022154944017529488, Validation Loss: 0.023312004283070564\n",
      "Epoch 20, Batch: 483: Training Loss: 0.021429793909192085, Validation Loss: 0.021556422114372253\n",
      "Epoch 20, Batch: 484: Training Loss: 0.019824106246232986, Validation Loss: 0.023558789864182472\n",
      "Epoch 20, Batch: 485: Training Loss: 0.021905943751335144, Validation Loss: 0.02354009635746479\n",
      "Epoch 20, Batch: 486: Training Loss: 0.0217384472489357, Validation Loss: 0.023702047765254974\n",
      "Epoch 20, Batch: 487: Training Loss: 0.01924150623381138, Validation Loss: 0.024225590750575066\n",
      "Epoch 20, Batch: 488: Training Loss: 0.022666724398732185, Validation Loss: 0.02629655785858631\n",
      "Epoch 20, Batch: 489: Training Loss: 0.027297230437397957, Validation Loss: 0.024344488978385925\n",
      "Epoch 20, Batch: 490: Training Loss: 0.023481551557779312, Validation Loss: 0.021473366767168045\n",
      "Epoch 20, Batch: 491: Training Loss: 0.016625551506876945, Validation Loss: 0.02336406521499157\n",
      "Epoch 20, Batch: 492: Training Loss: 0.021987486630678177, Validation Loss: 0.02280866540968418\n",
      "Epoch 20, Batch: 493: Training Loss: 0.02074597217142582, Validation Loss: 0.025043144822120667\n",
      "Epoch 20, Batch: 494: Training Loss: 0.023303570225834846, Validation Loss: 0.02364278957247734\n",
      "Epoch 20, Batch: 495: Training Loss: 0.02307772822678089, Validation Loss: 0.02617727965116501\n",
      "Epoch 20, Batch: 496: Training Loss: 0.02207656018435955, Validation Loss: 0.022787848487496376\n",
      "Epoch 20, Batch: 497: Training Loss: 0.022220082581043243, Validation Loss: 0.024066748097538948\n",
      "Epoch 20, Batch: 498: Training Loss: 0.022299472242593765, Validation Loss: 0.023608174175024033\n",
      "Epoch 20, Batch: 499: Training Loss: 0.021559350192546844, Validation Loss: 0.024816986173391342\n",
      "Epoch 21, Batch: 0: Training Loss: 0.02343851886689663, Validation Loss: 0.02220149151980877\n",
      "Epoch 21, Batch: 1: Training Loss: 0.021237798035144806, Validation Loss: 0.02269243821501732\n",
      "Epoch 21, Batch: 2: Training Loss: 0.024329913780093193, Validation Loss: 0.02402050979435444\n",
      "Epoch 21, Batch: 3: Training Loss: 0.01907334104180336, Validation Loss: 0.021950285881757736\n",
      "Epoch 21, Batch: 4: Training Loss: 0.02101464383304119, Validation Loss: 0.02336583100259304\n",
      "Epoch 21, Batch: 5: Training Loss: 0.019914187490940094, Validation Loss: 0.025677047669887543\n",
      "Epoch 21, Batch: 6: Training Loss: 0.02087343856692314, Validation Loss: 0.0237285066395998\n",
      "Epoch 21, Batch: 7: Training Loss: 0.023720378056168556, Validation Loss: 0.022676818072795868\n",
      "Epoch 21, Batch: 8: Training Loss: 0.02033163048326969, Validation Loss: 0.023488780483603477\n",
      "Epoch 21, Batch: 9: Training Loss: 0.02191179431974888, Validation Loss: 0.02688661403954029\n",
      "Epoch 21, Batch: 10: Training Loss: 0.021945452317595482, Validation Loss: 0.025422394275665283\n",
      "Epoch 21, Batch: 11: Training Loss: 0.02305012010037899, Validation Loss: 0.025549529120326042\n",
      "Epoch 21, Batch: 12: Training Loss: 0.027194524183869362, Validation Loss: 0.026324164122343063\n",
      "Epoch 21, Batch: 13: Training Loss: 0.026674918830394745, Validation Loss: 0.026464570313692093\n",
      "Epoch 21, Batch: 14: Training Loss: 0.025089792907238007, Validation Loss: 0.026507651433348656\n",
      "Epoch 21, Batch: 15: Training Loss: 0.024504996836185455, Validation Loss: 0.024766122922301292\n",
      "Epoch 21, Batch: 16: Training Loss: 0.027236824855208397, Validation Loss: 0.026697492226958275\n",
      "Epoch 21, Batch: 17: Training Loss: 0.020606379956007004, Validation Loss: 0.028150711208581924\n",
      "Epoch 21, Batch: 18: Training Loss: 0.02350221946835518, Validation Loss: 0.028387710452079773\n",
      "Epoch 21, Batch: 19: Training Loss: 0.019839616492390633, Validation Loss: 0.02619796246290207\n",
      "Epoch 21, Batch: 20: Training Loss: 0.02339853160083294, Validation Loss: 0.027590017765760422\n",
      "Epoch 21, Batch: 21: Training Loss: 0.02021125517785549, Validation Loss: 0.024561384692788124\n",
      "Epoch 21, Batch: 22: Training Loss: 0.021032780408859253, Validation Loss: 0.02531408704817295\n",
      "Epoch 21, Batch: 23: Training Loss: 0.02499421127140522, Validation Loss: 0.0262614618986845\n",
      "Epoch 21, Batch: 24: Training Loss: 0.018992632627487183, Validation Loss: 0.024647776037454605\n",
      "Epoch 21, Batch: 25: Training Loss: 0.022070888429880142, Validation Loss: 0.02490922436118126\n",
      "Epoch 21, Batch: 26: Training Loss: 0.023759597912430763, Validation Loss: 0.02454151213169098\n",
      "Epoch 21, Batch: 27: Training Loss: 0.017189254984259605, Validation Loss: 0.02484869584441185\n",
      "Epoch 21, Batch: 28: Training Loss: 0.023732824251055717, Validation Loss: 0.023315975442528725\n",
      "Epoch 21, Batch: 29: Training Loss: 0.02273884229362011, Validation Loss: 0.024047300219535828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 30: Training Loss: 0.02183419279754162, Validation Loss: 0.02569151297211647\n",
      "Epoch 21, Batch: 31: Training Loss: 0.022970862686634064, Validation Loss: 0.025984259322285652\n",
      "Epoch 21, Batch: 32: Training Loss: 0.02224430814385414, Validation Loss: 0.025294598191976547\n",
      "Epoch 21, Batch: 33: Training Loss: 0.019121889024972916, Validation Loss: 0.02460852637887001\n",
      "Epoch 21, Batch: 34: Training Loss: 0.021261267364025116, Validation Loss: 0.023565730080008507\n",
      "Epoch 21, Batch: 35: Training Loss: 0.0211675725877285, Validation Loss: 0.023304376751184464\n",
      "Epoch 21, Batch: 36: Training Loss: 0.02130242995917797, Validation Loss: 0.02414901927113533\n",
      "Epoch 21, Batch: 37: Training Loss: 0.020289640873670578, Validation Loss: 0.024824969470500946\n",
      "Epoch 21, Batch: 38: Training Loss: 0.02376142330467701, Validation Loss: 0.02672252058982849\n",
      "Epoch 21, Batch: 39: Training Loss: 0.02551528811454773, Validation Loss: 0.02553814835846424\n",
      "Epoch 21, Batch: 40: Training Loss: 0.025061914697289467, Validation Loss: 0.026936477050185204\n",
      "Epoch 21, Batch: 41: Training Loss: 0.02049517259001732, Validation Loss: 0.026143115013837814\n",
      "Epoch 21, Batch: 42: Training Loss: 0.02084723860025406, Validation Loss: 0.025617580860853195\n",
      "Epoch 21, Batch: 43: Training Loss: 0.01943856105208397, Validation Loss: 0.02495376206934452\n",
      "Epoch 21, Batch: 44: Training Loss: 0.022816045209765434, Validation Loss: 0.026060450822114944\n",
      "Epoch 21, Batch: 45: Training Loss: 0.020620102062821388, Validation Loss: 0.024278519675135612\n",
      "Epoch 21, Batch: 46: Training Loss: 0.023628588765859604, Validation Loss: 0.023717757314443588\n",
      "Epoch 21, Batch: 47: Training Loss: 0.0236369576305151, Validation Loss: 0.025447895750403404\n",
      "Epoch 21, Batch: 48: Training Loss: 0.02396390214562416, Validation Loss: 0.025577643886208534\n",
      "Epoch 21, Batch: 49: Training Loss: 0.025081036612391472, Validation Loss: 0.027986865490674973\n",
      "Epoch 21, Batch: 50: Training Loss: 0.022368496283888817, Validation Loss: 0.024949811398983\n",
      "Epoch 21, Batch: 51: Training Loss: 0.02649535983800888, Validation Loss: 0.025263840332627296\n",
      "Epoch 21, Batch: 52: Training Loss: 0.022754540666937828, Validation Loss: 0.02765779197216034\n",
      "Epoch 21, Batch: 53: Training Loss: 0.019491983577609062, Validation Loss: 0.025507289916276932\n",
      "Epoch 21, Batch: 54: Training Loss: 0.020136715844273567, Validation Loss: 0.024302540346980095\n",
      "Epoch 21, Batch: 55: Training Loss: 0.01840035244822502, Validation Loss: 0.02782481163740158\n",
      "Epoch 21, Batch: 56: Training Loss: 0.025317445397377014, Validation Loss: 0.024719901382923126\n",
      "Epoch 21, Batch: 57: Training Loss: 0.020843492820858955, Validation Loss: 0.02588742971420288\n",
      "Epoch 21, Batch: 58: Training Loss: 0.021345186978578568, Validation Loss: 0.023870086297392845\n",
      "Epoch 21, Batch: 59: Training Loss: 0.01870143599808216, Validation Loss: 0.027558395639061928\n",
      "Epoch 21, Batch: 60: Training Loss: 0.01960483007133007, Validation Loss: 0.02899547666311264\n",
      "Epoch 21, Batch: 61: Training Loss: 0.024938354268670082, Validation Loss: 0.025636665523052216\n",
      "Epoch 21, Batch: 62: Training Loss: 0.025311864912509918, Validation Loss: 0.024374911561608315\n",
      "Epoch 21, Batch: 63: Training Loss: 0.026414083316922188, Validation Loss: 0.026966528967022896\n",
      "Epoch 21, Batch: 64: Training Loss: 0.022534020245075226, Validation Loss: 0.02389751374721527\n",
      "Epoch 21, Batch: 65: Training Loss: 0.026540102437138557, Validation Loss: 0.023822180926799774\n",
      "Epoch 21, Batch: 66: Training Loss: 0.019578315317630768, Validation Loss: 0.02545647881925106\n",
      "Epoch 21, Batch: 67: Training Loss: 0.02091558836400509, Validation Loss: 0.02564275451004505\n",
      "Epoch 21, Batch: 68: Training Loss: 0.020488280802965164, Validation Loss: 0.02291286550462246\n",
      "Epoch 21, Batch: 69: Training Loss: 0.022938888520002365, Validation Loss: 0.02496846206486225\n",
      "Epoch 21, Batch: 70: Training Loss: 0.025715405121445656, Validation Loss: 0.025403156876564026\n",
      "Epoch 21, Batch: 71: Training Loss: 0.019541729241609573, Validation Loss: 0.023750970140099525\n",
      "Epoch 21, Batch: 72: Training Loss: 0.022067731246352196, Validation Loss: 0.024499330669641495\n",
      "Epoch 21, Batch: 73: Training Loss: 0.021662767976522446, Validation Loss: 0.022743355482816696\n",
      "Epoch 21, Batch: 74: Training Loss: 0.0194059107452631, Validation Loss: 0.024595562368631363\n",
      "Epoch 21, Batch: 75: Training Loss: 0.02158455364406109, Validation Loss: 0.02594796195626259\n",
      "Epoch 21, Batch: 76: Training Loss: 0.024056797847151756, Validation Loss: 0.026085413992404938\n",
      "Epoch 21, Batch: 77: Training Loss: 0.02645915001630783, Validation Loss: 0.023479627445340157\n",
      "Epoch 21, Batch: 78: Training Loss: 0.024864325299859047, Validation Loss: 0.023615704849362373\n",
      "Epoch 21, Batch: 79: Training Loss: 0.024506591260433197, Validation Loss: 0.02346513792872429\n",
      "Epoch 21, Batch: 80: Training Loss: 0.020576413720846176, Validation Loss: 0.023569811135530472\n",
      "Epoch 21, Batch: 81: Training Loss: 0.019992658868432045, Validation Loss: 0.02408977970480919\n",
      "Epoch 21, Batch: 82: Training Loss: 0.02394094131886959, Validation Loss: 0.026426054537296295\n",
      "Epoch 21, Batch: 83: Training Loss: 0.023416223004460335, Validation Loss: 0.024039600044488907\n",
      "Epoch 21, Batch: 84: Training Loss: 0.020439596846699715, Validation Loss: 0.024262605234980583\n",
      "Epoch 21, Batch: 85: Training Loss: 0.01874558813869953, Validation Loss: 0.023675886914134026\n",
      "Epoch 21, Batch: 86: Training Loss: 0.025342879816889763, Validation Loss: 0.025382807478308678\n",
      "Epoch 21, Batch: 87: Training Loss: 0.029946764931082726, Validation Loss: 0.02485043928027153\n",
      "Epoch 21, Batch: 88: Training Loss: 0.02397298999130726, Validation Loss: 0.024633413180708885\n",
      "Epoch 21, Batch: 89: Training Loss: 0.027071580290794373, Validation Loss: 0.025090206414461136\n",
      "Epoch 21, Batch: 90: Training Loss: 0.02159212902188301, Validation Loss: 0.025269988924264908\n",
      "Epoch 21, Batch: 91: Training Loss: 0.024007301777601242, Validation Loss: 0.025575440376996994\n",
      "Epoch 21, Batch: 92: Training Loss: 0.021811949089169502, Validation Loss: 0.02408367022871971\n",
      "Epoch 21, Batch: 93: Training Loss: 0.021865028887987137, Validation Loss: 0.02571982517838478\n",
      "Epoch 21, Batch: 94: Training Loss: 0.025388333946466446, Validation Loss: 0.028675438836216927\n",
      "Epoch 21, Batch: 95: Training Loss: 0.019456323236227036, Validation Loss: 0.02653760276734829\n",
      "Epoch 21, Batch: 96: Training Loss: 0.0256965272128582, Validation Loss: 0.026308393105864525\n",
      "Epoch 21, Batch: 97: Training Loss: 0.0245734341442585, Validation Loss: 0.025189107283949852\n",
      "Epoch 21, Batch: 98: Training Loss: 0.021324539557099342, Validation Loss: 0.02720898762345314\n",
      "Epoch 21, Batch: 99: Training Loss: 0.024815941229462624, Validation Loss: 0.027298033237457275\n",
      "Epoch 21, Batch: 100: Training Loss: 0.02626633085310459, Validation Loss: 0.026100939139723778\n",
      "Epoch 21, Batch: 101: Training Loss: 0.023101434111595154, Validation Loss: 0.02597583457827568\n",
      "Epoch 21, Batch: 102: Training Loss: 0.02303791604936123, Validation Loss: 0.02409176528453827\n",
      "Epoch 21, Batch: 103: Training Loss: 0.022002803161740303, Validation Loss: 0.02228221483528614\n",
      "Epoch 21, Batch: 104: Training Loss: 0.02155463397502899, Validation Loss: 0.024066859856247902\n",
      "Epoch 21, Batch: 105: Training Loss: 0.021301530301570892, Validation Loss: 0.022618401795625687\n",
      "Epoch 21, Batch: 106: Training Loss: 0.02323494292795658, Validation Loss: 0.02384093962609768\n",
      "Epoch 21, Batch: 107: Training Loss: 0.021656658500432968, Validation Loss: 0.023301076143980026\n",
      "Epoch 21, Batch: 108: Training Loss: 0.02015780098736286, Validation Loss: 0.024151356890797615\n",
      "Epoch 21, Batch: 109: Training Loss: 0.02317645773291588, Validation Loss: 0.02245660312473774\n",
      "Epoch 21, Batch: 110: Training Loss: 0.025249043479561806, Validation Loss: 0.02380627952516079\n",
      "Epoch 21, Batch: 111: Training Loss: 0.022258836776018143, Validation Loss: 0.022154705598950386\n",
      "Epoch 21, Batch: 112: Training Loss: 0.021067213267087936, Validation Loss: 0.02294917404651642\n",
      "Epoch 21, Batch: 113: Training Loss: 0.023020684719085693, Validation Loss: 0.0215814970433712\n",
      "Epoch 21, Batch: 114: Training Loss: 0.022345663979649544, Validation Loss: 0.023735813796520233\n",
      "Epoch 21, Batch: 115: Training Loss: 0.02617768384516239, Validation Loss: 0.02332649566233158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 116: Training Loss: 0.022785337641835213, Validation Loss: 0.02209283411502838\n",
      "Epoch 21, Batch: 117: Training Loss: 0.023272870108485222, Validation Loss: 0.024885646998882294\n",
      "Epoch 21, Batch: 118: Training Loss: 0.022353846579790115, Validation Loss: 0.02319234423339367\n",
      "Epoch 21, Batch: 119: Training Loss: 0.026152994483709335, Validation Loss: 0.021813420578837395\n",
      "Epoch 21, Batch: 120: Training Loss: 0.023016827180981636, Validation Loss: 0.021991750225424767\n",
      "Epoch 21, Batch: 121: Training Loss: 0.020844021812081337, Validation Loss: 0.024507394060492516\n",
      "Epoch 21, Batch: 122: Training Loss: 0.022211037576198578, Validation Loss: 0.02315959334373474\n",
      "Epoch 21, Batch: 123: Training Loss: 0.021498311311006546, Validation Loss: 0.024204503744840622\n",
      "Epoch 21, Batch: 124: Training Loss: 0.022303448989987373, Validation Loss: 0.02565520815551281\n",
      "Epoch 21, Batch: 125: Training Loss: 0.018745921552181244, Validation Loss: 0.02358672395348549\n",
      "Epoch 21, Batch: 126: Training Loss: 0.02230524644255638, Validation Loss: 0.023381715640425682\n",
      "Epoch 21, Batch: 127: Training Loss: 0.022308001294732094, Validation Loss: 0.023657100275158882\n",
      "Epoch 21, Batch: 128: Training Loss: 0.022142071276903152, Validation Loss: 0.024996014311909676\n",
      "Epoch 21, Batch: 129: Training Loss: 0.018535898998379707, Validation Loss: 0.023119624704122543\n",
      "Epoch 21, Batch: 130: Training Loss: 0.022283731028437614, Validation Loss: 0.023831134662032127\n",
      "Epoch 21, Batch: 131: Training Loss: 0.021895701065659523, Validation Loss: 0.02398538962006569\n",
      "Epoch 21, Batch: 132: Training Loss: 0.024030281230807304, Validation Loss: 0.027190368622541428\n",
      "Epoch 21, Batch: 133: Training Loss: 0.021962542086839676, Validation Loss: 0.02551664039492607\n",
      "Epoch 21, Batch: 134: Training Loss: 0.02409922145307064, Validation Loss: 0.025718016549944878\n",
      "Epoch 21, Batch: 135: Training Loss: 0.021295659244060516, Validation Loss: 0.026758797466754913\n",
      "Epoch 21, Batch: 136: Training Loss: 0.020390404388308525, Validation Loss: 0.022734882310032845\n",
      "Epoch 21, Batch: 137: Training Loss: 0.019358189776539803, Validation Loss: 0.02381098084151745\n",
      "Epoch 21, Batch: 138: Training Loss: 0.021395845338702202, Validation Loss: 0.024216368794441223\n",
      "Epoch 21, Batch: 139: Training Loss: 0.019719881936907768, Validation Loss: 0.024890685454010963\n",
      "Epoch 21, Batch: 140: Training Loss: 0.02745227888226509, Validation Loss: 0.02506699413061142\n",
      "Epoch 21, Batch: 141: Training Loss: 0.02220531366765499, Validation Loss: 0.023870855569839478\n",
      "Epoch 21, Batch: 142: Training Loss: 0.021678611636161804, Validation Loss: 0.024358509108424187\n",
      "Epoch 21, Batch: 143: Training Loss: 0.02263132855296135, Validation Loss: 0.02362925373017788\n",
      "Epoch 21, Batch: 144: Training Loss: 0.017807302996516228, Validation Loss: 0.024133814498782158\n",
      "Epoch 21, Batch: 145: Training Loss: 0.019625261425971985, Validation Loss: 0.025571849197149277\n",
      "Epoch 21, Batch: 146: Training Loss: 0.022054051980376244, Validation Loss: 0.023881861940026283\n",
      "Epoch 21, Batch: 147: Training Loss: 0.01964430883526802, Validation Loss: 0.025123238563537598\n",
      "Epoch 21, Batch: 148: Training Loss: 0.023120539262890816, Validation Loss: 0.025894176214933395\n",
      "Epoch 21, Batch: 149: Training Loss: 0.02299293503165245, Validation Loss: 0.023444723337888718\n",
      "Epoch 21, Batch: 150: Training Loss: 0.023781869560480118, Validation Loss: 0.022926993668079376\n",
      "Epoch 21, Batch: 151: Training Loss: 0.02094707451760769, Validation Loss: 0.025319786742329597\n",
      "Epoch 21, Batch: 152: Training Loss: 0.02391919493675232, Validation Loss: 0.024894772097468376\n",
      "Epoch 21, Batch: 153: Training Loss: 0.02499321661889553, Validation Loss: 0.022380735725164413\n",
      "Epoch 21, Batch: 154: Training Loss: 0.023824719712138176, Validation Loss: 0.022169122472405434\n",
      "Epoch 21, Batch: 155: Training Loss: 0.026674587279558182, Validation Loss: 0.024698875844478607\n",
      "Epoch 21, Batch: 156: Training Loss: 0.020480003207921982, Validation Loss: 0.024222712963819504\n",
      "Epoch 21, Batch: 157: Training Loss: 0.020907217636704445, Validation Loss: 0.02502012625336647\n",
      "Epoch 21, Batch: 158: Training Loss: 0.019171781837940216, Validation Loss: 0.022779295220971107\n",
      "Epoch 21, Batch: 159: Training Loss: 0.022357944399118423, Validation Loss: 0.02267000824213028\n",
      "Epoch 21, Batch: 160: Training Loss: 0.02070716582238674, Validation Loss: 0.023218350484967232\n",
      "Epoch 21, Batch: 161: Training Loss: 0.02356092818081379, Validation Loss: 0.023129306733608246\n",
      "Epoch 21, Batch: 162: Training Loss: 0.023390641435980797, Validation Loss: 0.02371862530708313\n",
      "Epoch 21, Batch: 163: Training Loss: 0.02342703379690647, Validation Loss: 0.02341778390109539\n",
      "Epoch 21, Batch: 164: Training Loss: 0.02291608788073063, Validation Loss: 0.02690768614411354\n",
      "Epoch 21, Batch: 165: Training Loss: 0.022755194455385208, Validation Loss: 0.025243882089853287\n",
      "Epoch 21, Batch: 166: Training Loss: 0.023289402946829796, Validation Loss: 0.023363087326288223\n",
      "Epoch 21, Batch: 167: Training Loss: 0.019977880641818047, Validation Loss: 0.026344263926148415\n",
      "Epoch 21, Batch: 168: Training Loss: 0.02259354293346405, Validation Loss: 0.02435024082660675\n",
      "Epoch 21, Batch: 169: Training Loss: 0.02436012774705887, Validation Loss: 0.024731367826461792\n",
      "Epoch 21, Batch: 170: Training Loss: 0.02064535953104496, Validation Loss: 0.02705858275294304\n",
      "Epoch 21, Batch: 171: Training Loss: 0.018727311864495277, Validation Loss: 0.025110486894845963\n",
      "Epoch 21, Batch: 172: Training Loss: 0.021825561299920082, Validation Loss: 0.0266927070915699\n",
      "Epoch 21, Batch: 173: Training Loss: 0.01933874748647213, Validation Loss: 0.02619875967502594\n",
      "Epoch 21, Batch: 174: Training Loss: 0.02596859820187092, Validation Loss: 0.0259841475635767\n",
      "Epoch 21, Batch: 175: Training Loss: 0.021737409755587578, Validation Loss: 0.02627837471663952\n",
      "Epoch 21, Batch: 176: Training Loss: 0.026983806863427162, Validation Loss: 0.02659868262708187\n",
      "Epoch 21, Batch: 177: Training Loss: 0.024517547339200974, Validation Loss: 0.024876253679394722\n",
      "Epoch 21, Batch: 178: Training Loss: 0.02471998706459999, Validation Loss: 0.0251932293176651\n",
      "Epoch 21, Batch: 179: Training Loss: 0.019323213025927544, Validation Loss: 0.02604607678949833\n",
      "Epoch 21, Batch: 180: Training Loss: 0.023863304406404495, Validation Loss: 0.02435210719704628\n",
      "Epoch 21, Batch: 181: Training Loss: 0.020962104201316833, Validation Loss: 0.02436208352446556\n",
      "Epoch 21, Batch: 182: Training Loss: 0.022573454305529594, Validation Loss: 0.02635999023914337\n",
      "Epoch 21, Batch: 183: Training Loss: 0.02574080228805542, Validation Loss: 0.0250800009816885\n",
      "Epoch 21, Batch: 184: Training Loss: 0.024464884772896767, Validation Loss: 0.025972189381718636\n",
      "Epoch 21, Batch: 185: Training Loss: 0.021688047796487808, Validation Loss: 0.02685678005218506\n",
      "Epoch 21, Batch: 186: Training Loss: 0.024893222376704216, Validation Loss: 0.028622440993785858\n",
      "Epoch 21, Batch: 187: Training Loss: 0.022537779062986374, Validation Loss: 0.024241309612989426\n",
      "Epoch 21, Batch: 188: Training Loss: 0.024033837020397186, Validation Loss: 0.026221297681331635\n",
      "Epoch 21, Batch: 189: Training Loss: 0.02446558326482773, Validation Loss: 0.0279445331543684\n",
      "Epoch 21, Batch: 190: Training Loss: 0.024018581956624985, Validation Loss: 0.024416428059339523\n",
      "Epoch 21, Batch: 191: Training Loss: 0.023431116715073586, Validation Loss: 0.025218214839696884\n",
      "Epoch 21, Batch: 192: Training Loss: 0.023924678564071655, Validation Loss: 0.02638232335448265\n",
      "Epoch 21, Batch: 193: Training Loss: 0.022284889593720436, Validation Loss: 0.025902099907398224\n",
      "Epoch 21, Batch: 194: Training Loss: 0.02121138758957386, Validation Loss: 0.02683519758284092\n",
      "Epoch 21, Batch: 195: Training Loss: 0.02072311006486416, Validation Loss: 0.02640959993004799\n",
      "Epoch 21, Batch: 196: Training Loss: 0.023450303822755814, Validation Loss: 0.027049344033002853\n",
      "Epoch 21, Batch: 197: Training Loss: 0.021263718605041504, Validation Loss: 0.025589780882000923\n",
      "Epoch 21, Batch: 198: Training Loss: 0.023110633715987206, Validation Loss: 0.025633545592427254\n",
      "Epoch 21, Batch: 199: Training Loss: 0.02281658537685871, Validation Loss: 0.027145544067025185\n",
      "Epoch 21, Batch: 200: Training Loss: 0.018857618793845177, Validation Loss: 0.02791069820523262\n",
      "Epoch 21, Batch: 201: Training Loss: 0.02544490620493889, Validation Loss: 0.026806455105543137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 202: Training Loss: 0.022178251296281815, Validation Loss: 0.026047982275485992\n",
      "Epoch 21, Batch: 203: Training Loss: 0.021947568282485008, Validation Loss: 0.026638254523277283\n",
      "Epoch 21, Batch: 204: Training Loss: 0.01927335001528263, Validation Loss: 0.026772694662213326\n",
      "Epoch 21, Batch: 205: Training Loss: 0.024937454611063004, Validation Loss: 0.025974981486797333\n",
      "Epoch 21, Batch: 206: Training Loss: 0.024424027651548386, Validation Loss: 0.024509143084287643\n",
      "Epoch 21, Batch: 207: Training Loss: 0.024815460667014122, Validation Loss: 0.024998996406793594\n",
      "Epoch 21, Batch: 208: Training Loss: 0.022840913385152817, Validation Loss: 0.025081656873226166\n",
      "Epoch 21, Batch: 209: Training Loss: 0.023563355207443237, Validation Loss: 0.024598030373454094\n",
      "Epoch 21, Batch: 210: Training Loss: 0.023725122213363647, Validation Loss: 0.025788666680455208\n",
      "Epoch 21, Batch: 211: Training Loss: 0.024966435506939888, Validation Loss: 0.024184735491871834\n",
      "Epoch 21, Batch: 212: Training Loss: 0.022612357512116432, Validation Loss: 0.024620061740279198\n",
      "Epoch 21, Batch: 213: Training Loss: 0.023000644519925117, Validation Loss: 0.023547304794192314\n",
      "Epoch 21, Batch: 214: Training Loss: 0.019615624099969864, Validation Loss: 0.02335178479552269\n",
      "Epoch 21, Batch: 215: Training Loss: 0.020316902548074722, Validation Loss: 0.02277088351547718\n",
      "Epoch 21, Batch: 216: Training Loss: 0.022918932139873505, Validation Loss: 0.028957508504390717\n",
      "Epoch 21, Batch: 217: Training Loss: 0.023430919274687767, Validation Loss: 0.025789212435483932\n",
      "Epoch 21, Batch: 218: Training Loss: 0.022143995389342308, Validation Loss: 0.02829539030790329\n",
      "Epoch 21, Batch: 219: Training Loss: 0.022640410810709, Validation Loss: 0.030242301523685455\n",
      "Epoch 21, Batch: 220: Training Loss: 0.026006991043686867, Validation Loss: 0.02738913707435131\n",
      "Epoch 21, Batch: 221: Training Loss: 0.022750942036509514, Validation Loss: 0.02872382663190365\n",
      "Epoch 21, Batch: 222: Training Loss: 0.024221200495958328, Validation Loss: 0.02633679285645485\n",
      "Epoch 21, Batch: 223: Training Loss: 0.022371070459485054, Validation Loss: 0.025157025083899498\n",
      "Epoch 21, Batch: 224: Training Loss: 0.027677683159708977, Validation Loss: 0.02692829817533493\n",
      "Epoch 21, Batch: 225: Training Loss: 0.024339642375707626, Validation Loss: 0.029352102428674698\n",
      "Epoch 21, Batch: 226: Training Loss: 0.021787816658616066, Validation Loss: 0.02412978559732437\n",
      "Epoch 21, Batch: 227: Training Loss: 0.02554699406027794, Validation Loss: 0.02659432403743267\n",
      "Epoch 21, Batch: 228: Training Loss: 0.02519872598350048, Validation Loss: 0.02537035010755062\n",
      "Epoch 21, Batch: 229: Training Loss: 0.02383858524262905, Validation Loss: 0.030228398740291595\n",
      "Epoch 21, Batch: 230: Training Loss: 0.023974912241101265, Validation Loss: 0.027883773669600487\n",
      "Epoch 21, Batch: 231: Training Loss: 0.025393614545464516, Validation Loss: 0.027608606964349747\n",
      "Epoch 21, Batch: 232: Training Loss: 0.028258122503757477, Validation Loss: 0.030194580554962158\n",
      "Epoch 21, Batch: 233: Training Loss: 0.024793829768896103, Validation Loss: 0.029526323080062866\n",
      "Epoch 21, Batch: 234: Training Loss: 0.026636188849806786, Validation Loss: 0.028162268921732903\n",
      "Epoch 21, Batch: 235: Training Loss: 0.025537196546792984, Validation Loss: 0.02791326493024826\n",
      "Epoch 21, Batch: 236: Training Loss: 0.02726292796432972, Validation Loss: 0.029545215889811516\n",
      "Epoch 21, Batch: 237: Training Loss: 0.02572321705520153, Validation Loss: 0.027744876220822334\n",
      "Epoch 21, Batch: 238: Training Loss: 0.024197859689593315, Validation Loss: 0.02754979208111763\n",
      "Epoch 21, Batch: 239: Training Loss: 0.02207854576408863, Validation Loss: 0.02955198660492897\n",
      "Epoch 21, Batch: 240: Training Loss: 0.02354528196156025, Validation Loss: 0.02690405584871769\n",
      "Epoch 21, Batch: 241: Training Loss: 0.024287469685077667, Validation Loss: 0.02924441732466221\n",
      "Epoch 21, Batch: 242: Training Loss: 0.02301478013396263, Validation Loss: 0.029589463025331497\n",
      "Epoch 21, Batch: 243: Training Loss: 0.023870952427387238, Validation Loss: 0.027893688529729843\n",
      "Epoch 21, Batch: 244: Training Loss: 0.023582156747579575, Validation Loss: 0.027051394805312157\n",
      "Epoch 21, Batch: 245: Training Loss: 0.028633898124098778, Validation Loss: 0.02937752939760685\n",
      "Epoch 21, Batch: 246: Training Loss: 0.02630099095404148, Validation Loss: 0.028960345312952995\n",
      "Epoch 21, Batch: 247: Training Loss: 0.02848607487976551, Validation Loss: 0.026648711413145065\n",
      "Epoch 21, Batch: 248: Training Loss: 0.02533302642405033, Validation Loss: 0.02501983381807804\n",
      "Epoch 21, Batch: 249: Training Loss: 0.021678872406482697, Validation Loss: 0.025237780064344406\n",
      "Epoch 21, Batch: 250: Training Loss: 0.026068074628710747, Validation Loss: 0.025607381016016006\n",
      "Epoch 21, Batch: 251: Training Loss: 0.022183485329151154, Validation Loss: 0.02355174347758293\n",
      "Epoch 21, Batch: 252: Training Loss: 0.02587495744228363, Validation Loss: 0.027190901339054108\n",
      "Epoch 21, Batch: 253: Training Loss: 0.02388864941895008, Validation Loss: 0.027092359960079193\n",
      "Epoch 21, Batch: 254: Training Loss: 0.019904008135199547, Validation Loss: 0.023319216445088387\n",
      "Epoch 21, Batch: 255: Training Loss: 0.026565685868263245, Validation Loss: 0.024326331913471222\n",
      "Epoch 21, Batch: 256: Training Loss: 0.025261642411351204, Validation Loss: 0.023050718009471893\n",
      "Epoch 21, Batch: 257: Training Loss: 0.029746409505605698, Validation Loss: 0.023705098778009415\n",
      "Epoch 21, Batch: 258: Training Loss: 0.023351728916168213, Validation Loss: 0.02364601194858551\n",
      "Epoch 21, Batch: 259: Training Loss: 0.018757205456495285, Validation Loss: 0.026160843670368195\n",
      "Epoch 21, Batch: 260: Training Loss: 0.02531149424612522, Validation Loss: 0.02651495859026909\n",
      "Epoch 21, Batch: 261: Training Loss: 0.02464870736002922, Validation Loss: 0.023551829159259796\n",
      "Epoch 21, Batch: 262: Training Loss: 0.02268695831298828, Validation Loss: 0.02495439536869526\n",
      "Epoch 21, Batch: 263: Training Loss: 0.022276317700743675, Validation Loss: 0.025524837896227837\n",
      "Epoch 21, Batch: 264: Training Loss: 0.020903095602989197, Validation Loss: 0.02440471574664116\n",
      "Epoch 21, Batch: 265: Training Loss: 0.02329437807202339, Validation Loss: 0.024499552324414253\n",
      "Epoch 21, Batch: 266: Training Loss: 0.022403450682759285, Validation Loss: 0.025885017588734627\n",
      "Epoch 21, Batch: 267: Training Loss: 0.023236948996782303, Validation Loss: 0.025623556226491928\n",
      "Epoch 21, Batch: 268: Training Loss: 0.022003496065735817, Validation Loss: 0.0267783235758543\n",
      "Epoch 21, Batch: 269: Training Loss: 0.022116769105196, Validation Loss: 0.027293704450130463\n",
      "Epoch 21, Batch: 270: Training Loss: 0.02456763945519924, Validation Loss: 0.026573050767183304\n",
      "Epoch 21, Batch: 271: Training Loss: 0.023619823157787323, Validation Loss: 0.02409222535789013\n",
      "Epoch 21, Batch: 272: Training Loss: 0.025173254311084747, Validation Loss: 0.0244993194937706\n",
      "Epoch 21, Batch: 273: Training Loss: 0.022805124521255493, Validation Loss: 0.0243365541100502\n",
      "Epoch 21, Batch: 274: Training Loss: 0.023050865158438683, Validation Loss: 0.025302855297923088\n",
      "Epoch 21, Batch: 275: Training Loss: 0.023331541568040848, Validation Loss: 0.025081003084778786\n",
      "Epoch 21, Batch: 276: Training Loss: 0.019403982907533646, Validation Loss: 0.023918868973851204\n",
      "Epoch 21, Batch: 277: Training Loss: 0.020317548885941505, Validation Loss: 0.025776714086532593\n",
      "Epoch 21, Batch: 278: Training Loss: 0.021166738122701645, Validation Loss: 0.02541220933198929\n",
      "Epoch 21, Batch: 279: Training Loss: 0.023163609206676483, Validation Loss: 0.023084381595253944\n",
      "Epoch 21, Batch: 280: Training Loss: 0.017752621322870255, Validation Loss: 0.02422945760190487\n",
      "Epoch 21, Batch: 281: Training Loss: 0.02185336872935295, Validation Loss: 0.025658676400780678\n",
      "Epoch 21, Batch: 282: Training Loss: 0.022668251767754555, Validation Loss: 0.02509990520775318\n",
      "Epoch 21, Batch: 283: Training Loss: 0.02377905696630478, Validation Loss: 0.025683222338557243\n",
      "Epoch 21, Batch: 284: Training Loss: 0.019786294549703598, Validation Loss: 0.025427669286727905\n",
      "Epoch 21, Batch: 285: Training Loss: 0.025099685415625572, Validation Loss: 0.024957051500678062\n",
      "Epoch 21, Batch: 286: Training Loss: 0.020519206300377846, Validation Loss: 0.026077795773744583\n",
      "Epoch 21, Batch: 287: Training Loss: 0.022755661979317665, Validation Loss: 0.027204791083931923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 288: Training Loss: 0.021362675353884697, Validation Loss: 0.024005722254514694\n",
      "Epoch 21, Batch: 289: Training Loss: 0.023388929665088654, Validation Loss: 0.023362403735518456\n",
      "Epoch 21, Batch: 290: Training Loss: 0.019471216946840286, Validation Loss: 0.024967478588223457\n",
      "Epoch 21, Batch: 291: Training Loss: 0.020981118083000183, Validation Loss: 0.025697436183691025\n",
      "Epoch 21, Batch: 292: Training Loss: 0.021994775161147118, Validation Loss: 0.024398036301136017\n",
      "Epoch 21, Batch: 293: Training Loss: 0.021513059735298157, Validation Loss: 0.024122728034853935\n",
      "Epoch 21, Batch: 294: Training Loss: 0.018605921417474747, Validation Loss: 0.024945557117462158\n",
      "Epoch 21, Batch: 295: Training Loss: 0.022620363160967827, Validation Loss: 0.026137175038456917\n",
      "Epoch 21, Batch: 296: Training Loss: 0.02366669476032257, Validation Loss: 0.02768140472471714\n",
      "Epoch 21, Batch: 297: Training Loss: 0.023754380643367767, Validation Loss: 0.025775421410799026\n",
      "Epoch 21, Batch: 298: Training Loss: 0.024525249376893044, Validation Loss: 0.026659900322556496\n",
      "Epoch 21, Batch: 299: Training Loss: 0.020622259005904198, Validation Loss: 0.026542821899056435\n",
      "Epoch 21, Batch: 300: Training Loss: 0.023494619876146317, Validation Loss: 0.02775978110730648\n",
      "Epoch 21, Batch: 301: Training Loss: 0.020368875935673714, Validation Loss: 0.02662048302590847\n",
      "Epoch 21, Batch: 302: Training Loss: 0.019087862223386765, Validation Loss: 0.026106344535946846\n",
      "Epoch 21, Batch: 303: Training Loss: 0.022959228605031967, Validation Loss: 0.026892581954598427\n",
      "Epoch 21, Batch: 304: Training Loss: 0.020383896306157112, Validation Loss: 0.022900085896253586\n",
      "Epoch 21, Batch: 305: Training Loss: 0.02030784823000431, Validation Loss: 0.02507806196808815\n",
      "Epoch 21, Batch: 306: Training Loss: 0.02083801105618477, Validation Loss: 0.02580035664141178\n",
      "Epoch 21, Batch: 307: Training Loss: 0.020217716693878174, Validation Loss: 0.026559850201010704\n",
      "Epoch 21, Batch: 308: Training Loss: 0.021446580067276955, Validation Loss: 0.026076054200530052\n",
      "Epoch 21, Batch: 309: Training Loss: 0.022754082456231117, Validation Loss: 0.027719397097826004\n",
      "Epoch 21, Batch: 310: Training Loss: 0.020621510222554207, Validation Loss: 0.02885511703789234\n",
      "Epoch 21, Batch: 311: Training Loss: 0.01844695582985878, Validation Loss: 0.02548893168568611\n",
      "Epoch 21, Batch: 312: Training Loss: 0.021910054609179497, Validation Loss: 0.02689514309167862\n",
      "Epoch 21, Batch: 313: Training Loss: 0.019056757912039757, Validation Loss: 0.027661312371492386\n",
      "Epoch 21, Batch: 314: Training Loss: 0.022173697128891945, Validation Loss: 0.025803783908486366\n",
      "Epoch 21, Batch: 315: Training Loss: 0.02161477692425251, Validation Loss: 0.02556736022233963\n",
      "Epoch 21, Batch: 316: Training Loss: 0.0214422345161438, Validation Loss: 0.026228776201605797\n",
      "Epoch 21, Batch: 317: Training Loss: 0.023574968799948692, Validation Loss: 0.028628017753362656\n",
      "Epoch 21, Batch: 318: Training Loss: 0.021939951926469803, Validation Loss: 0.026845190674066544\n",
      "Epoch 21, Batch: 319: Training Loss: 0.02119404822587967, Validation Loss: 0.027029965072870255\n",
      "Epoch 21, Batch: 320: Training Loss: 0.023816972970962524, Validation Loss: 0.027668336406350136\n",
      "Epoch 21, Batch: 321: Training Loss: 0.021098198369145393, Validation Loss: 0.026545412838459015\n",
      "Epoch 21, Batch: 322: Training Loss: 0.021269792690873146, Validation Loss: 0.024300847202539444\n",
      "Epoch 21, Batch: 323: Training Loss: 0.020686578005552292, Validation Loss: 0.024095235392451286\n",
      "Epoch 21, Batch: 324: Training Loss: 0.020459281280636787, Validation Loss: 0.025367209687829018\n",
      "Epoch 21, Batch: 325: Training Loss: 0.022681059315800667, Validation Loss: 0.02608874812722206\n",
      "Epoch 21, Batch: 326: Training Loss: 0.02740245871245861, Validation Loss: 0.023401794955134392\n",
      "Epoch 21, Batch: 327: Training Loss: 0.019271478056907654, Validation Loss: 0.027964789420366287\n",
      "Epoch 21, Batch: 328: Training Loss: 0.02180556021630764, Validation Loss: 0.025364013388752937\n",
      "Epoch 21, Batch: 329: Training Loss: 0.022099222987890244, Validation Loss: 0.02532031014561653\n",
      "Epoch 21, Batch: 330: Training Loss: 0.02051270753145218, Validation Loss: 0.026951635256409645\n",
      "Epoch 21, Batch: 331: Training Loss: 0.019513729959726334, Validation Loss: 0.027009300887584686\n",
      "Epoch 21, Batch: 332: Training Loss: 0.021928023546934128, Validation Loss: 0.02719300240278244\n",
      "Epoch 21, Batch: 333: Training Loss: 0.022845137864351273, Validation Loss: 0.023530997335910797\n",
      "Epoch 21, Batch: 334: Training Loss: 0.01976730488240719, Validation Loss: 0.023586051538586617\n",
      "Epoch 21, Batch: 335: Training Loss: 0.019858447834849358, Validation Loss: 0.027841318398714066\n",
      "Epoch 21, Batch: 336: Training Loss: 0.018063370138406754, Validation Loss: 0.026960119605064392\n",
      "Epoch 21, Batch: 337: Training Loss: 0.017749100923538208, Validation Loss: 0.02355625294148922\n",
      "Epoch 21, Batch: 338: Training Loss: 0.019608594477176666, Validation Loss: 0.02437727339565754\n",
      "Epoch 21, Batch: 339: Training Loss: 0.02196967601776123, Validation Loss: 0.026750680059194565\n",
      "Epoch 21, Batch: 340: Training Loss: 0.023436781018972397, Validation Loss: 0.027264978736639023\n",
      "Epoch 21, Batch: 341: Training Loss: 0.017698420211672783, Validation Loss: 0.023235976696014404\n",
      "Epoch 21, Batch: 342: Training Loss: 0.02201458439230919, Validation Loss: 0.025305138900876045\n",
      "Epoch 21, Batch: 343: Training Loss: 0.023301871493458748, Validation Loss: 0.02544523775577545\n",
      "Epoch 21, Batch: 344: Training Loss: 0.02357562817633152, Validation Loss: 0.022955097258090973\n",
      "Epoch 21, Batch: 345: Training Loss: 0.019752632826566696, Validation Loss: 0.02274215780198574\n",
      "Epoch 21, Batch: 346: Training Loss: 0.02032051794230938, Validation Loss: 0.024494528770446777\n",
      "Epoch 21, Batch: 347: Training Loss: 0.020186467096209526, Validation Loss: 0.02329288423061371\n",
      "Epoch 21, Batch: 348: Training Loss: 0.022138193249702454, Validation Loss: 0.02520255744457245\n",
      "Epoch 21, Batch: 349: Training Loss: 0.022195860743522644, Validation Loss: 0.023169681429862976\n",
      "Epoch 21, Batch: 350: Training Loss: 0.01960052363574505, Validation Loss: 0.02609381265938282\n",
      "Epoch 21, Batch: 351: Training Loss: 0.023824568837881088, Validation Loss: 0.02510077692568302\n",
      "Epoch 21, Batch: 352: Training Loss: 0.0247621051967144, Validation Loss: 0.02707490883767605\n",
      "Epoch 21, Batch: 353: Training Loss: 0.022401563823223114, Validation Loss: 0.02541050873696804\n",
      "Epoch 21, Batch: 354: Training Loss: 0.025120364502072334, Validation Loss: 0.026411639526486397\n",
      "Epoch 21, Batch: 355: Training Loss: 0.0203767791390419, Validation Loss: 0.025907590985298157\n",
      "Epoch 21, Batch: 356: Training Loss: 0.022409604862332344, Validation Loss: 0.023655511438846588\n",
      "Epoch 21, Batch: 357: Training Loss: 0.021733947098255157, Validation Loss: 0.023618310689926147\n",
      "Epoch 21, Batch: 358: Training Loss: 0.021247321739792824, Validation Loss: 0.022507254034280777\n",
      "Epoch 21, Batch: 359: Training Loss: 0.022679878398776054, Validation Loss: 0.021808987483382225\n",
      "Epoch 21, Batch: 360: Training Loss: 0.021562006324529648, Validation Loss: 0.01970064640045166\n",
      "Epoch 21, Batch: 361: Training Loss: 0.02007206156849861, Validation Loss: 0.023115189746022224\n",
      "Epoch 21, Batch: 362: Training Loss: 0.022126367315649986, Validation Loss: 0.022357257083058357\n",
      "Epoch 21, Batch: 363: Training Loss: 0.022691641002893448, Validation Loss: 0.024987809360027313\n",
      "Epoch 21, Batch: 364: Training Loss: 0.020760230720043182, Validation Loss: 0.021798191592097282\n",
      "Epoch 21, Batch: 365: Training Loss: 0.021982157602906227, Validation Loss: 0.02436123974621296\n",
      "Epoch 21, Batch: 366: Training Loss: 0.02506132982671261, Validation Loss: 0.024540439248085022\n",
      "Epoch 21, Batch: 367: Training Loss: 0.019789589568972588, Validation Loss: 0.024990331381559372\n",
      "Epoch 21, Batch: 368: Training Loss: 0.02166850119829178, Validation Loss: 0.02315695211291313\n",
      "Epoch 21, Batch: 369: Training Loss: 0.025152375921607018, Validation Loss: 0.024407539516687393\n",
      "Epoch 21, Batch: 370: Training Loss: 0.020737309008836746, Validation Loss: 0.021619081497192383\n",
      "Epoch 21, Batch: 371: Training Loss: 0.022033140063285828, Validation Loss: 0.02545970119535923\n",
      "Epoch 21, Batch: 372: Training Loss: 0.024401286616921425, Validation Loss: 0.022590797394514084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 373: Training Loss: 0.027864277362823486, Validation Loss: 0.023703163489699364\n",
      "Epoch 21, Batch: 374: Training Loss: 0.019840046763420105, Validation Loss: 0.023891745135188103\n",
      "Epoch 21, Batch: 375: Training Loss: 0.024179331958293915, Validation Loss: 0.022855347022414207\n",
      "Epoch 21, Batch: 376: Training Loss: 0.01886889711022377, Validation Loss: 0.023592093959450722\n",
      "Epoch 21, Batch: 377: Training Loss: 0.02280590496957302, Validation Loss: 0.023919912055134773\n",
      "Epoch 21, Batch: 378: Training Loss: 0.019041093066334724, Validation Loss: 0.022906560450792313\n",
      "Epoch 21, Batch: 379: Training Loss: 0.021115582436323166, Validation Loss: 0.021756533533334732\n",
      "Epoch 21, Batch: 380: Training Loss: 0.024040602147579193, Validation Loss: 0.02378094010055065\n",
      "Epoch 21, Batch: 381: Training Loss: 0.025025852024555206, Validation Loss: 0.02272127754986286\n",
      "Epoch 21, Batch: 382: Training Loss: 0.02410850301384926, Validation Loss: 0.02600572071969509\n",
      "Epoch 21, Batch: 383: Training Loss: 0.022301901131868362, Validation Loss: 0.023077556863427162\n",
      "Epoch 21, Batch: 384: Training Loss: 0.022957706823945045, Validation Loss: 0.020647870376706123\n",
      "Epoch 21, Batch: 385: Training Loss: 0.02117997035384178, Validation Loss: 0.023461315780878067\n",
      "Epoch 21, Batch: 386: Training Loss: 0.017862720414996147, Validation Loss: 0.022976208478212357\n",
      "Epoch 21, Batch: 387: Training Loss: 0.019763601943850517, Validation Loss: 0.02397836744785309\n",
      "Epoch 21, Batch: 388: Training Loss: 0.019767338410019875, Validation Loss: 0.021632712334394455\n",
      "Epoch 21, Batch: 389: Training Loss: 0.02219163067638874, Validation Loss: 0.02163158357143402\n",
      "Epoch 21, Batch: 390: Training Loss: 0.021275604143738747, Validation Loss: 0.023655174300074577\n",
      "Epoch 21, Batch: 391: Training Loss: 0.0244953241199255, Validation Loss: 0.02202688157558441\n",
      "Epoch 21, Batch: 392: Training Loss: 0.021705925464630127, Validation Loss: 0.02146674320101738\n",
      "Epoch 21, Batch: 393: Training Loss: 0.020825278013944626, Validation Loss: 0.021895425394177437\n",
      "Epoch 21, Batch: 394: Training Loss: 0.020086592063307762, Validation Loss: 0.023542843759059906\n",
      "Epoch 21, Batch: 395: Training Loss: 0.021952642127871513, Validation Loss: 0.023137710988521576\n",
      "Epoch 21, Batch: 396: Training Loss: 0.02234194055199623, Validation Loss: 0.024315064772963524\n",
      "Epoch 21, Batch: 397: Training Loss: 0.022006914019584656, Validation Loss: 0.024671407416462898\n",
      "Epoch 21, Batch: 398: Training Loss: 0.022286249324679375, Validation Loss: 0.023260541260242462\n",
      "Epoch 21, Batch: 399: Training Loss: 0.022107094526290894, Validation Loss: 0.02253648452460766\n",
      "Epoch 21, Batch: 400: Training Loss: 0.022431854158639908, Validation Loss: 0.02396494150161743\n",
      "Epoch 21, Batch: 401: Training Loss: 0.022561505436897278, Validation Loss: 0.023507820442318916\n",
      "Epoch 21, Batch: 402: Training Loss: 0.019832469522953033, Validation Loss: 0.023951519280672073\n",
      "Epoch 21, Batch: 403: Training Loss: 0.02131127193570137, Validation Loss: 0.02349752001464367\n",
      "Epoch 21, Batch: 404: Training Loss: 0.02064693532884121, Validation Loss: 0.025374995544552803\n",
      "Epoch 21, Batch: 405: Training Loss: 0.019926343113183975, Validation Loss: 0.025123324245214462\n",
      "Epoch 21, Batch: 406: Training Loss: 0.0227504912763834, Validation Loss: 0.02640439197421074\n",
      "Epoch 21, Batch: 407: Training Loss: 0.019078178331255913, Validation Loss: 0.023618370294570923\n",
      "Epoch 21, Batch: 408: Training Loss: 0.01731136254966259, Validation Loss: 0.021602967754006386\n",
      "Epoch 21, Batch: 409: Training Loss: 0.017427150160074234, Validation Loss: 0.023708345368504524\n",
      "Epoch 21, Batch: 410: Training Loss: 0.020582234486937523, Validation Loss: 0.023375069722533226\n",
      "Epoch 21, Batch: 411: Training Loss: 0.019401809200644493, Validation Loss: 0.024933764711022377\n",
      "Epoch 21, Batch: 412: Training Loss: 0.027526574209332466, Validation Loss: 0.021073345094919205\n",
      "Epoch 21, Batch: 413: Training Loss: 0.022816186770796776, Validation Loss: 0.023127026855945587\n",
      "Epoch 21, Batch: 414: Training Loss: 0.024984747171401978, Validation Loss: 0.02493358962237835\n",
      "Epoch 21, Batch: 415: Training Loss: 0.023259416222572327, Validation Loss: 0.022677041590213776\n",
      "Epoch 21, Batch: 416: Training Loss: 0.021292710676789284, Validation Loss: 0.02281564101576805\n",
      "Epoch 21, Batch: 417: Training Loss: 0.020971467718482018, Validation Loss: 0.022782055661082268\n",
      "Epoch 21, Batch: 418: Training Loss: 0.021266266703605652, Validation Loss: 0.022865869104862213\n",
      "Epoch 21, Batch: 419: Training Loss: 0.01975100487470627, Validation Loss: 0.02479749731719494\n",
      "Epoch 21, Batch: 420: Training Loss: 0.023795267567038536, Validation Loss: 0.02149149961769581\n",
      "Epoch 21, Batch: 421: Training Loss: 0.02303353138267994, Validation Loss: 0.02381821721792221\n",
      "Epoch 21, Batch: 422: Training Loss: 0.02318486198782921, Validation Loss: 0.02396644651889801\n",
      "Epoch 21, Batch: 423: Training Loss: 0.01962580531835556, Validation Loss: 0.022583426907658577\n",
      "Epoch 21, Batch: 424: Training Loss: 0.02444048412144184, Validation Loss: 0.023796427994966507\n",
      "Epoch 21, Batch: 425: Training Loss: 0.022056298330426216, Validation Loss: 0.02236836589872837\n",
      "Epoch 21, Batch: 426: Training Loss: 0.02132285013794899, Validation Loss: 0.02163831517100334\n",
      "Epoch 21, Batch: 427: Training Loss: 0.020580381155014038, Validation Loss: 0.022085828706622124\n",
      "Epoch 21, Batch: 428: Training Loss: 0.020546626299619675, Validation Loss: 0.023745769634842873\n",
      "Epoch 21, Batch: 429: Training Loss: 0.021005554124712944, Validation Loss: 0.026513244956731796\n",
      "Epoch 21, Batch: 430: Training Loss: 0.022074611857533455, Validation Loss: 0.024730566889047623\n",
      "Epoch 21, Batch: 431: Training Loss: 0.019369300454854965, Validation Loss: 0.024088621139526367\n",
      "Epoch 21, Batch: 432: Training Loss: 0.025045890361070633, Validation Loss: 0.025344399735331535\n",
      "Epoch 21, Batch: 433: Training Loss: 0.019894760102033615, Validation Loss: 0.023726550862193108\n",
      "Epoch 21, Batch: 434: Training Loss: 0.02503223717212677, Validation Loss: 0.023618511855602264\n",
      "Epoch 21, Batch: 435: Training Loss: 0.023914488032460213, Validation Loss: 0.02568846195936203\n",
      "Epoch 21, Batch: 436: Training Loss: 0.022211957722902298, Validation Loss: 0.0245316494256258\n",
      "Epoch 21, Batch: 437: Training Loss: 0.018613064661622047, Validation Loss: 0.0252502653747797\n",
      "Epoch 21, Batch: 438: Training Loss: 0.02007126808166504, Validation Loss: 0.024952294304966927\n",
      "Epoch 21, Batch: 439: Training Loss: 0.018988773226737976, Validation Loss: 0.026137538254261017\n",
      "Epoch 21, Batch: 440: Training Loss: 0.02592260017991066, Validation Loss: 0.02445322647690773\n",
      "Epoch 21, Batch: 441: Training Loss: 0.018759867176413536, Validation Loss: 0.02353566512465477\n",
      "Epoch 21, Batch: 442: Training Loss: 0.02295486442744732, Validation Loss: 0.023671023547649384\n",
      "Epoch 21, Batch: 443: Training Loss: 0.022159792482852936, Validation Loss: 0.025003965944051743\n",
      "Epoch 21, Batch: 444: Training Loss: 0.022089777514338493, Validation Loss: 0.024783950299024582\n",
      "Epoch 21, Batch: 445: Training Loss: 0.019951634109020233, Validation Loss: 0.021300023421645164\n",
      "Epoch 21, Batch: 446: Training Loss: 0.02851358987390995, Validation Loss: 0.025152288377285004\n",
      "Epoch 21, Batch: 447: Training Loss: 0.02345256693661213, Validation Loss: 0.027641376480460167\n",
      "Epoch 21, Batch: 448: Training Loss: 0.022559456527233124, Validation Loss: 0.023322097957134247\n",
      "Epoch 21, Batch: 449: Training Loss: 0.0211503803730011, Validation Loss: 0.023618485778570175\n",
      "Epoch 21, Batch: 450: Training Loss: 0.021916715428233147, Validation Loss: 0.02358311042189598\n",
      "Epoch 21, Batch: 451: Training Loss: 0.028950925916433334, Validation Loss: 0.023501800373196602\n",
      "Epoch 21, Batch: 452: Training Loss: 0.0241549015045166, Validation Loss: 0.025014515966176987\n",
      "Epoch 21, Batch: 453: Training Loss: 0.026302620768547058, Validation Loss: 0.025908466428518295\n",
      "Epoch 21, Batch: 454: Training Loss: 0.025144528597593307, Validation Loss: 0.026381678879261017\n",
      "Epoch 21, Batch: 455: Training Loss: 0.022966662421822548, Validation Loss: 0.026164909824728966\n",
      "Epoch 21, Batch: 456: Training Loss: 0.023850226774811745, Validation Loss: 0.027340726926922798\n",
      "Epoch 21, Batch: 457: Training Loss: 0.022322574630379677, Validation Loss: 0.024812648072838783\n",
      "Epoch 21, Batch: 458: Training Loss: 0.018143825232982635, Validation Loss: 0.02304239198565483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch: 459: Training Loss: 0.020948458462953568, Validation Loss: 0.021649515256285667\n",
      "Epoch 21, Batch: 460: Training Loss: 0.021705130115151405, Validation Loss: 0.024009427055716515\n",
      "Epoch 21, Batch: 461: Training Loss: 0.022006791085004807, Validation Loss: 0.02643967792391777\n",
      "Epoch 21, Batch: 462: Training Loss: 0.022282596677541733, Validation Loss: 0.024121781811118126\n",
      "Epoch 21, Batch: 463: Training Loss: 0.020973360165953636, Validation Loss: 0.022432979196310043\n",
      "Epoch 21, Batch: 464: Training Loss: 0.021082008257508278, Validation Loss: 0.02446100488305092\n",
      "Epoch 21, Batch: 465: Training Loss: 0.025024689733982086, Validation Loss: 0.023994959890842438\n",
      "Epoch 21, Batch: 466: Training Loss: 0.020172251388430595, Validation Loss: 0.02599092572927475\n",
      "Epoch 21, Batch: 467: Training Loss: 0.029667114838957787, Validation Loss: 0.024951010942459106\n",
      "Epoch 21, Batch: 468: Training Loss: 0.02579694241285324, Validation Loss: 0.025810081511735916\n",
      "Epoch 21, Batch: 469: Training Loss: 0.020802875980734825, Validation Loss: 0.024082276970148087\n",
      "Epoch 21, Batch: 470: Training Loss: 0.01862645335495472, Validation Loss: 0.02677117846906185\n",
      "Epoch 21, Batch: 471: Training Loss: 0.02408180572092533, Validation Loss: 0.025267520919442177\n",
      "Epoch 21, Batch: 472: Training Loss: 0.025585757568478584, Validation Loss: 0.024922210723161697\n",
      "Epoch 21, Batch: 473: Training Loss: 0.023942487314343452, Validation Loss: 0.027182206511497498\n",
      "Epoch 21, Batch: 474: Training Loss: 0.020567143335938454, Validation Loss: 0.0263883825391531\n",
      "Epoch 21, Batch: 475: Training Loss: 0.02417922392487526, Validation Loss: 0.027202622964978218\n",
      "Epoch 21, Batch: 476: Training Loss: 0.022386493161320686, Validation Loss: 0.026485513895750046\n",
      "Epoch 21, Batch: 477: Training Loss: 0.024451887235045433, Validation Loss: 0.028443677350878716\n",
      "Epoch 21, Batch: 478: Training Loss: 0.021535784006118774, Validation Loss: 0.02475578524172306\n",
      "Epoch 21, Batch: 479: Training Loss: 0.02384866774082184, Validation Loss: 0.023645563051104546\n",
      "Epoch 21, Batch: 480: Training Loss: 0.026164619252085686, Validation Loss: 0.02362920343875885\n",
      "Epoch 21, Batch: 481: Training Loss: 0.0217137448489666, Validation Loss: 0.023182496428489685\n",
      "Epoch 21, Batch: 482: Training Loss: 0.024133890867233276, Validation Loss: 0.02373851276934147\n",
      "Epoch 21, Batch: 483: Training Loss: 0.02095719240605831, Validation Loss: 0.023384317755699158\n",
      "Epoch 21, Batch: 484: Training Loss: 0.023031853139400482, Validation Loss: 0.022520121186971664\n",
      "Epoch 21, Batch: 485: Training Loss: 0.020877376198768616, Validation Loss: 0.023025168105959892\n",
      "Epoch 21, Batch: 486: Training Loss: 0.026645326986908913, Validation Loss: 0.024825621396303177\n",
      "Epoch 21, Batch: 487: Training Loss: 0.022678151726722717, Validation Loss: 0.02221551537513733\n",
      "Epoch 21, Batch: 488: Training Loss: 0.024175595492124557, Validation Loss: 0.023326624184846878\n",
      "Epoch 21, Batch: 489: Training Loss: 0.026163047179579735, Validation Loss: 0.025058135390281677\n",
      "Epoch 21, Batch: 490: Training Loss: 0.023285597562789917, Validation Loss: 0.021876748651266098\n",
      "Epoch 21, Batch: 491: Training Loss: 0.020617179572582245, Validation Loss: 0.02452310547232628\n",
      "Epoch 21, Batch: 492: Training Loss: 0.026667829602956772, Validation Loss: 0.02419929951429367\n",
      "Epoch 21, Batch: 493: Training Loss: 0.021272780373692513, Validation Loss: 0.02471768483519554\n",
      "Epoch 21, Batch: 494: Training Loss: 0.0234084352850914, Validation Loss: 0.028046865016222\n",
      "Epoch 21, Batch: 495: Training Loss: 0.023411603644490242, Validation Loss: 0.02702033333480358\n",
      "Epoch 21, Batch: 496: Training Loss: 0.023110656067728996, Validation Loss: 0.028288453817367554\n",
      "Epoch 21, Batch: 497: Training Loss: 0.023104198276996613, Validation Loss: 0.025580907240509987\n",
      "Epoch 21, Batch: 498: Training Loss: 0.024302026256918907, Validation Loss: 0.02797340229153633\n",
      "Epoch 21, Batch: 499: Training Loss: 0.02145938016474247, Validation Loss: 0.027602046728134155\n",
      "Epoch 22, Batch: 0: Training Loss: 0.019714828580617905, Validation Loss: 0.027697863057255745\n",
      "Epoch 22, Batch: 1: Training Loss: 0.02091105654835701, Validation Loss: 0.026399847120046616\n",
      "Epoch 22, Batch: 2: Training Loss: 0.028198348358273506, Validation Loss: 0.02762467972934246\n",
      "Epoch 22, Batch: 3: Training Loss: 0.023923641070723534, Validation Loss: 0.029852429404854774\n",
      "Epoch 22, Batch: 4: Training Loss: 0.01884373649954796, Validation Loss: 0.03083813190460205\n",
      "Epoch 22, Batch: 5: Training Loss: 0.025059647858142853, Validation Loss: 0.027805162593722343\n",
      "Epoch 22, Batch: 6: Training Loss: 0.021526198834180832, Validation Loss: 0.028703805059194565\n",
      "Epoch 22, Batch: 7: Training Loss: 0.022091228514909744, Validation Loss: 0.02763621136546135\n",
      "Epoch 22, Batch: 8: Training Loss: 0.025291187688708305, Validation Loss: 0.023883052170276642\n",
      "Epoch 22, Batch: 9: Training Loss: 0.019499793648719788, Validation Loss: 0.02887771837413311\n",
      "Epoch 22, Batch: 10: Training Loss: 0.01901974529027939, Validation Loss: 0.027509670704603195\n",
      "Epoch 22, Batch: 11: Training Loss: 0.021014869213104248, Validation Loss: 0.024318337440490723\n",
      "Epoch 22, Batch: 12: Training Loss: 0.0239486675709486, Validation Loss: 0.026013068854808807\n",
      "Epoch 22, Batch: 13: Training Loss: 0.025894256308674812, Validation Loss: 0.025876114144921303\n",
      "Epoch 22, Batch: 14: Training Loss: 0.022199256345629692, Validation Loss: 0.023165417835116386\n",
      "Epoch 22, Batch: 15: Training Loss: 0.025257451459765434, Validation Loss: 0.02307184413075447\n",
      "Epoch 22, Batch: 16: Training Loss: 0.027728917077183723, Validation Loss: 0.021594803780317307\n",
      "Epoch 22, Batch: 17: Training Loss: 0.02522432617843151, Validation Loss: 0.024543335661292076\n",
      "Epoch 22, Batch: 18: Training Loss: 0.021611260250210762, Validation Loss: 0.023563550785183907\n",
      "Epoch 22, Batch: 19: Training Loss: 0.02034774236381054, Validation Loss: 0.024349015206098557\n",
      "Epoch 22, Batch: 20: Training Loss: 0.021282106637954712, Validation Loss: 0.02140977419912815\n",
      "Epoch 22, Batch: 21: Training Loss: 0.02573245018720627, Validation Loss: 0.022194894030690193\n",
      "Epoch 22, Batch: 22: Training Loss: 0.02057613991200924, Validation Loss: 0.0222889706492424\n",
      "Epoch 22, Batch: 23: Training Loss: 0.023444900289177895, Validation Loss: 0.023142343387007713\n",
      "Epoch 22, Batch: 24: Training Loss: 0.021756328642368317, Validation Loss: 0.02276967093348503\n",
      "Epoch 22, Batch: 25: Training Loss: 0.02398398518562317, Validation Loss: 0.023349812254309654\n",
      "Epoch 22, Batch: 26: Training Loss: 0.021087396889925003, Validation Loss: 0.02238284796476364\n",
      "Epoch 22, Batch: 27: Training Loss: 0.02455754019320011, Validation Loss: 0.022733259946107864\n",
      "Epoch 22, Batch: 28: Training Loss: 0.023907186463475227, Validation Loss: 0.023130927234888077\n",
      "Saving new best model w/ loss: 0.01925022527575493\n",
      "Epoch 22, Batch: 29: Training Loss: 0.022392122074961662, Validation Loss: 0.01925022527575493\n",
      "Epoch 22, Batch: 30: Training Loss: 0.023520128801465034, Validation Loss: 0.020751317963004112\n",
      "Epoch 22, Batch: 31: Training Loss: 0.022056762129068375, Validation Loss: 0.021272271871566772\n",
      "Epoch 22, Batch: 32: Training Loss: 0.02352820336818695, Validation Loss: 0.022564413025975227\n",
      "Epoch 22, Batch: 33: Training Loss: 0.020065801218152046, Validation Loss: 0.020658986642956734\n",
      "Epoch 22, Batch: 34: Training Loss: 0.020215723663568497, Validation Loss: 0.022946765646338463\n",
      "Epoch 22, Batch: 35: Training Loss: 0.023935643956065178, Validation Loss: 0.02412029355764389\n",
      "Epoch 22, Batch: 36: Training Loss: 0.019496073946356773, Validation Loss: 0.02305314689874649\n",
      "Epoch 22, Batch: 37: Training Loss: 0.023052606731653214, Validation Loss: 0.022620301693677902\n",
      "Epoch 22, Batch: 38: Training Loss: 0.02212963066995144, Validation Loss: 0.02303621731698513\n",
      "Epoch 22, Batch: 39: Training Loss: 0.023285627365112305, Validation Loss: 0.02134780026972294\n",
      "Epoch 22, Batch: 40: Training Loss: 0.025709455832839012, Validation Loss: 0.022893961519002914\n",
      "Epoch 22, Batch: 41: Training Loss: 0.021435091271996498, Validation Loss: 0.021845873445272446\n",
      "Epoch 22, Batch: 42: Training Loss: 0.019863348454236984, Validation Loss: 0.021456502377986908\n",
      "Epoch 22, Batch: 43: Training Loss: 0.01875866949558258, Validation Loss: 0.02272953651845455\n",
      "Epoch 22, Batch: 44: Training Loss: 0.01928051747381687, Validation Loss: 0.02503669634461403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch: 45: Training Loss: 0.019848311319947243, Validation Loss: 0.021977916359901428\n",
      "Epoch 22, Batch: 46: Training Loss: 0.0250785443931818, Validation Loss: 0.02438715472817421\n",
      "Epoch 22, Batch: 47: Training Loss: 0.024923579767346382, Validation Loss: 0.02159944735467434\n",
      "Epoch 22, Batch: 48: Training Loss: 0.0229015052318573, Validation Loss: 0.024562401697039604\n",
      "Epoch 22, Batch: 49: Training Loss: 0.025974996387958527, Validation Loss: 0.02436104230582714\n",
      "Epoch 22, Batch: 50: Training Loss: 0.02211567759513855, Validation Loss: 0.022698109969496727\n",
      "Epoch 22, Batch: 51: Training Loss: 0.02556578442454338, Validation Loss: 0.023608461022377014\n",
      "Epoch 22, Batch: 52: Training Loss: 0.020831018686294556, Validation Loss: 0.022505592554807663\n",
      "Epoch 22, Batch: 53: Training Loss: 0.01933262124657631, Validation Loss: 0.02314482443034649\n",
      "Epoch 22, Batch: 54: Training Loss: 0.023164208978414536, Validation Loss: 0.021614855155348778\n",
      "Epoch 22, Batch: 55: Training Loss: 0.0199521966278553, Validation Loss: 0.025296317413449287\n",
      "Epoch 22, Batch: 56: Training Loss: 0.023552438244223595, Validation Loss: 0.0222704466432333\n",
      "Epoch 22, Batch: 57: Training Loss: 0.021591758355498314, Validation Loss: 0.020810827612876892\n",
      "Epoch 22, Batch: 58: Training Loss: 0.02400127239525318, Validation Loss: 0.020878223702311516\n",
      "Epoch 22, Batch: 59: Training Loss: 0.01770944893360138, Validation Loss: 0.023524288088083267\n",
      "Epoch 22, Batch: 60: Training Loss: 0.021388767287135124, Validation Loss: 0.0214848555624485\n",
      "Epoch 22, Batch: 61: Training Loss: 0.024898558855056763, Validation Loss: 0.022839171811938286\n",
      "Epoch 22, Batch: 62: Training Loss: 0.021129148080945015, Validation Loss: 0.022492608055472374\n",
      "Epoch 22, Batch: 63: Training Loss: 0.024729548022150993, Validation Loss: 0.022331759333610535\n",
      "Epoch 22, Batch: 64: Training Loss: 0.023866547271609306, Validation Loss: 0.02260606735944748\n",
      "Epoch 22, Batch: 65: Training Loss: 0.02257627062499523, Validation Loss: 0.02338753268122673\n",
      "Epoch 22, Batch: 66: Training Loss: 0.020288776606321335, Validation Loss: 0.02411065436899662\n",
      "Epoch 22, Batch: 67: Training Loss: 0.018039079383015633, Validation Loss: 0.024646295234560966\n",
      "Epoch 22, Batch: 68: Training Loss: 0.02005787193775177, Validation Loss: 0.023332813754677773\n",
      "Epoch 22, Batch: 69: Training Loss: 0.021545125171542168, Validation Loss: 0.02504882961511612\n",
      "Epoch 22, Batch: 70: Training Loss: 0.02470504865050316, Validation Loss: 0.025992048904299736\n",
      "Epoch 22, Batch: 71: Training Loss: 0.02387235127389431, Validation Loss: 0.02542959526181221\n",
      "Epoch 22, Batch: 72: Training Loss: 0.02538439817726612, Validation Loss: 0.025516726076602936\n",
      "Epoch 22, Batch: 73: Training Loss: 0.027616247534751892, Validation Loss: 0.02643405646085739\n",
      "Epoch 22, Batch: 74: Training Loss: 0.019983597099781036, Validation Loss: 0.025778893381357193\n",
      "Epoch 22, Batch: 75: Training Loss: 0.018303286284208298, Validation Loss: 0.02592708356678486\n",
      "Epoch 22, Batch: 76: Training Loss: 0.019416000694036484, Validation Loss: 0.026963913813233376\n",
      "Epoch 22, Batch: 77: Training Loss: 0.02488464117050171, Validation Loss: 0.025674689561128616\n",
      "Epoch 22, Batch: 78: Training Loss: 0.024119723588228226, Validation Loss: 0.02707672491669655\n",
      "Epoch 22, Batch: 79: Training Loss: 0.023685898631811142, Validation Loss: 0.025217726826667786\n",
      "Epoch 22, Batch: 80: Training Loss: 0.018900150433182716, Validation Loss: 0.02480482868850231\n",
      "Epoch 22, Batch: 81: Training Loss: 0.021917223930358887, Validation Loss: 0.025117002427577972\n",
      "Epoch 22, Batch: 82: Training Loss: 0.022872569039463997, Validation Loss: 0.026358917355537415\n",
      "Epoch 22, Batch: 83: Training Loss: 0.018579473719000816, Validation Loss: 0.02408025600016117\n",
      "Epoch 22, Batch: 84: Training Loss: 0.02620280161499977, Validation Loss: 0.026886606588959694\n",
      "Epoch 22, Batch: 85: Training Loss: 0.02028447389602661, Validation Loss: 0.026578042656183243\n",
      "Epoch 22, Batch: 86: Training Loss: 0.022103549912571907, Validation Loss: 0.025729896500706673\n",
      "Epoch 22, Batch: 87: Training Loss: 0.02622191794216633, Validation Loss: 0.027499545365571976\n",
      "Epoch 22, Batch: 88: Training Loss: 0.024830402806401253, Validation Loss: 0.026215283200144768\n",
      "Epoch 22, Batch: 89: Training Loss: 0.022884896025061607, Validation Loss: 0.025117995217442513\n",
      "Epoch 22, Batch: 90: Training Loss: 0.019190868362784386, Validation Loss: 0.025637028738856316\n",
      "Epoch 22, Batch: 91: Training Loss: 0.022879498079419136, Validation Loss: 0.02463573031127453\n",
      "Epoch 22, Batch: 92: Training Loss: 0.022096965461969376, Validation Loss: 0.02691682241857052\n",
      "Epoch 22, Batch: 93: Training Loss: 0.023788027465343475, Validation Loss: 0.02446400187909603\n",
      "Epoch 22, Batch: 94: Training Loss: 0.01952630840241909, Validation Loss: 0.024296244606375694\n",
      "Epoch 22, Batch: 95: Training Loss: 0.018895937129855156, Validation Loss: 0.024808816611766815\n",
      "Epoch 22, Batch: 96: Training Loss: 0.022745691239833832, Validation Loss: 0.02392915077507496\n",
      "Epoch 22, Batch: 97: Training Loss: 0.02204098366200924, Validation Loss: 0.024920670315623283\n",
      "Epoch 22, Batch: 98: Training Loss: 0.021772976964712143, Validation Loss: 0.023462073877453804\n",
      "Epoch 22, Batch: 99: Training Loss: 0.025975510478019714, Validation Loss: 0.022099625319242477\n",
      "Epoch 22, Batch: 100: Training Loss: 0.02665640413761139, Validation Loss: 0.024071792140603065\n",
      "Epoch 22, Batch: 101: Training Loss: 0.02055828459560871, Validation Loss: 0.02141304314136505\n",
      "Epoch 22, Batch: 102: Training Loss: 0.020773833617568016, Validation Loss: 0.023060893639922142\n",
      "Epoch 22, Batch: 103: Training Loss: 0.021813001483678818, Validation Loss: 0.022717082872986794\n",
      "Epoch 22, Batch: 104: Training Loss: 0.021830972284078598, Validation Loss: 0.024161312729120255\n",
      "Epoch 22, Batch: 105: Training Loss: 0.01955421268939972, Validation Loss: 0.02386757731437683\n",
      "Epoch 22, Batch: 106: Training Loss: 0.02159939333796501, Validation Loss: 0.02551986090838909\n",
      "Epoch 22, Batch: 107: Training Loss: 0.022082382813096046, Validation Loss: 0.022200830280780792\n",
      "Epoch 22, Batch: 108: Training Loss: 0.02080366387963295, Validation Loss: 0.024870490655303\n",
      "Epoch 22, Batch: 109: Training Loss: 0.023939058184623718, Validation Loss: 0.02825213596224785\n",
      "Epoch 22, Batch: 110: Training Loss: 0.023706702515482903, Validation Loss: 0.02279491536319256\n",
      "Epoch 22, Batch: 111: Training Loss: 0.021565251052379608, Validation Loss: 0.025962134823203087\n",
      "Epoch 22, Batch: 112: Training Loss: 0.021650683134794235, Validation Loss: 0.026563551276922226\n",
      "Epoch 22, Batch: 113: Training Loss: 0.02281644381582737, Validation Loss: 0.02713027596473694\n",
      "Epoch 22, Batch: 114: Training Loss: 0.022527338936924934, Validation Loss: 0.026073621585965157\n",
      "Epoch 22, Batch: 115: Training Loss: 0.024510817602276802, Validation Loss: 0.022731037810444832\n",
      "Epoch 22, Batch: 116: Training Loss: 0.02069675736129284, Validation Loss: 0.023364407941699028\n",
      "Epoch 22, Batch: 117: Training Loss: 0.022813178598880768, Validation Loss: 0.023974554613232613\n",
      "Epoch 22, Batch: 118: Training Loss: 0.023657629266381264, Validation Loss: 0.024606619030237198\n",
      "Epoch 22, Batch: 119: Training Loss: 0.02454853430390358, Validation Loss: 0.02376522310078144\n",
      "Epoch 22, Batch: 120: Training Loss: 0.022769462317228317, Validation Loss: 0.02349802479147911\n",
      "Epoch 22, Batch: 121: Training Loss: 0.022814124822616577, Validation Loss: 0.02519438974559307\n",
      "Epoch 22, Batch: 122: Training Loss: 0.023600716143846512, Validation Loss: 0.02510375902056694\n",
      "Epoch 22, Batch: 123: Training Loss: 0.022298913449048996, Validation Loss: 0.026153571903705597\n",
      "Epoch 22, Batch: 124: Training Loss: 0.023089664056897163, Validation Loss: 0.024277852848172188\n",
      "Epoch 22, Batch: 125: Training Loss: 0.019175950437784195, Validation Loss: 0.024359049275517464\n",
      "Epoch 22, Batch: 126: Training Loss: 0.0229940265417099, Validation Loss: 0.024938594549894333\n",
      "Epoch 22, Batch: 127: Training Loss: 0.026720041409134865, Validation Loss: 0.02685289829969406\n",
      "Epoch 22, Batch: 128: Training Loss: 0.02525406889617443, Validation Loss: 0.02523641847074032\n",
      "Epoch 22, Batch: 129: Training Loss: 0.02079225890338421, Validation Loss: 0.02569405734539032\n",
      "Epoch 22, Batch: 130: Training Loss: 0.027051998302340508, Validation Loss: 0.02707831747829914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch: 131: Training Loss: 0.023503649979829788, Validation Loss: 0.026882048696279526\n",
      "Epoch 22, Batch: 132: Training Loss: 0.024307742714881897, Validation Loss: 0.026663044467568398\n",
      "Epoch 22, Batch: 133: Training Loss: 0.023047493770718575, Validation Loss: 0.027652697637677193\n",
      "Epoch 22, Batch: 134: Training Loss: 0.022096872329711914, Validation Loss: 0.02530868723988533\n",
      "Epoch 22, Batch: 135: Training Loss: 0.021972667425870895, Validation Loss: 0.022153215482831\n",
      "Epoch 22, Batch: 136: Training Loss: 0.02378363348543644, Validation Loss: 0.022337021306157112\n",
      "Epoch 22, Batch: 137: Training Loss: 0.020621798932552338, Validation Loss: 0.02146627940237522\n",
      "Epoch 22, Batch: 138: Training Loss: 0.021753927692770958, Validation Loss: 0.023120522499084473\n",
      "Epoch 22, Batch: 139: Training Loss: 0.020732399076223373, Validation Loss: 0.024612192064523697\n",
      "Epoch 22, Batch: 140: Training Loss: 0.026639357209205627, Validation Loss: 0.026581062003970146\n",
      "Epoch 22, Batch: 141: Training Loss: 0.021079374477267265, Validation Loss: 0.025192320346832275\n",
      "Epoch 22, Batch: 142: Training Loss: 0.020525390282273293, Validation Loss: 0.02602016180753708\n",
      "Epoch 22, Batch: 143: Training Loss: 0.02055410109460354, Validation Loss: 0.02663535811007023\n",
      "Epoch 22, Batch: 144: Training Loss: 0.019788147881627083, Validation Loss: 0.0273634921759367\n",
      "Epoch 22, Batch: 145: Training Loss: 0.02072327770292759, Validation Loss: 0.024761712178587914\n",
      "Epoch 22, Batch: 146: Training Loss: 0.02551831677556038, Validation Loss: 0.02491433545947075\n",
      "Epoch 22, Batch: 147: Training Loss: 0.02049756422638893, Validation Loss: 0.024541843682527542\n",
      "Epoch 22, Batch: 148: Training Loss: 0.02373342029750347, Validation Loss: 0.026786506175994873\n",
      "Epoch 22, Batch: 149: Training Loss: 0.02231571078300476, Validation Loss: 0.026300029829144478\n",
      "Epoch 22, Batch: 150: Training Loss: 0.02664065547287464, Validation Loss: 0.026898648589849472\n",
      "Epoch 22, Batch: 151: Training Loss: 0.023840397596359253, Validation Loss: 0.0281889196485281\n",
      "Epoch 22, Batch: 152: Training Loss: 0.02417023293673992, Validation Loss: 0.02704784646630287\n",
      "Epoch 22, Batch: 153: Training Loss: 0.024219157174229622, Validation Loss: 0.025969108566641808\n",
      "Epoch 22, Batch: 154: Training Loss: 0.023907162249088287, Validation Loss: 0.024880724027752876\n",
      "Epoch 22, Batch: 155: Training Loss: 0.027385972440242767, Validation Loss: 0.02516128309071064\n",
      "Epoch 22, Batch: 156: Training Loss: 0.023891687393188477, Validation Loss: 0.02546175941824913\n",
      "Epoch 22, Batch: 157: Training Loss: 0.020236747339367867, Validation Loss: 0.024251146242022514\n",
      "Epoch 22, Batch: 158: Training Loss: 0.02203655242919922, Validation Loss: 0.02358628436923027\n",
      "Epoch 22, Batch: 159: Training Loss: 0.022358300164341927, Validation Loss: 0.023628652095794678\n",
      "Epoch 22, Batch: 160: Training Loss: 0.02375086396932602, Validation Loss: 0.02672424726188183\n",
      "Epoch 22, Batch: 161: Training Loss: 0.023153914138674736, Validation Loss: 0.02272491157054901\n",
      "Epoch 22, Batch: 162: Training Loss: 0.02561885118484497, Validation Loss: 0.024502653628587723\n",
      "Epoch 22, Batch: 163: Training Loss: 0.023194871842861176, Validation Loss: 0.024947868660092354\n",
      "Epoch 22, Batch: 164: Training Loss: 0.021697789430618286, Validation Loss: 0.025650184601545334\n",
      "Epoch 22, Batch: 165: Training Loss: 0.024702711030840874, Validation Loss: 0.02696910873055458\n",
      "Epoch 22, Batch: 166: Training Loss: 0.02365005388855934, Validation Loss: 0.026109203696250916\n",
      "Epoch 22, Batch: 167: Training Loss: 0.02311950922012329, Validation Loss: 0.02684124931693077\n",
      "Epoch 22, Batch: 168: Training Loss: 0.02587791532278061, Validation Loss: 0.027296343818306923\n",
      "Epoch 22, Batch: 169: Training Loss: 0.0255903173238039, Validation Loss: 0.025267407298088074\n",
      "Epoch 22, Batch: 170: Training Loss: 0.02430720441043377, Validation Loss: 0.027661964297294617\n",
      "Epoch 22, Batch: 171: Training Loss: 0.02004080079495907, Validation Loss: 0.024688363075256348\n",
      "Epoch 22, Batch: 172: Training Loss: 0.02366778254508972, Validation Loss: 0.02679130807518959\n",
      "Epoch 22, Batch: 173: Training Loss: 0.023921094834804535, Validation Loss: 0.026042306795716286\n",
      "Epoch 22, Batch: 174: Training Loss: 0.027973227202892303, Validation Loss: 0.02480091154575348\n",
      "Epoch 22, Batch: 175: Training Loss: 0.02581341192126274, Validation Loss: 0.026245443150401115\n",
      "Epoch 22, Batch: 176: Training Loss: 0.02534622512757778, Validation Loss: 0.026182645931839943\n",
      "Epoch 22, Batch: 177: Training Loss: 0.024208325892686844, Validation Loss: 0.02662433683872223\n",
      "Epoch 22, Batch: 178: Training Loss: 0.0262808408588171, Validation Loss: 0.02437753789126873\n",
      "Epoch 22, Batch: 179: Training Loss: 0.022006018087267876, Validation Loss: 0.026652470231056213\n",
      "Epoch 22, Batch: 180: Training Loss: 0.026465507224202156, Validation Loss: 0.024774136021733284\n",
      "Epoch 22, Batch: 181: Training Loss: 0.020992450416088104, Validation Loss: 0.025730114430189133\n",
      "Epoch 22, Batch: 182: Training Loss: 0.024091050028800964, Validation Loss: 0.02634674869477749\n",
      "Epoch 22, Batch: 183: Training Loss: 0.024563726037740707, Validation Loss: 0.02378861792385578\n",
      "Epoch 22, Batch: 184: Training Loss: 0.024133600294589996, Validation Loss: 0.024784065783023834\n",
      "Epoch 22, Batch: 185: Training Loss: 0.024699045345187187, Validation Loss: 0.025379030033946037\n",
      "Epoch 22, Batch: 186: Training Loss: 0.024838559329509735, Validation Loss: 0.02409026399254799\n",
      "Epoch 22, Batch: 187: Training Loss: 0.02068127878010273, Validation Loss: 0.025712018832564354\n",
      "Epoch 22, Batch: 188: Training Loss: 0.023870861157774925, Validation Loss: 0.02159840054810047\n",
      "Epoch 22, Batch: 189: Training Loss: 0.02323048748075962, Validation Loss: 0.025136269629001617\n",
      "Epoch 22, Batch: 190: Training Loss: 0.022808386012911797, Validation Loss: 0.022089241072535515\n",
      "Epoch 22, Batch: 191: Training Loss: 0.02386128529906273, Validation Loss: 0.027327464893460274\n",
      "Epoch 22, Batch: 192: Training Loss: 0.023057784885168076, Validation Loss: 0.023600630462169647\n",
      "Epoch 22, Batch: 193: Training Loss: 0.023796701803803444, Validation Loss: 0.02351504936814308\n",
      "Epoch 22, Batch: 194: Training Loss: 0.019318468868732452, Validation Loss: 0.02534673362970352\n",
      "Epoch 22, Batch: 195: Training Loss: 0.01959831826388836, Validation Loss: 0.027303390204906464\n",
      "Epoch 22, Batch: 196: Training Loss: 0.023062139749526978, Validation Loss: 0.025168132036924362\n",
      "Epoch 22, Batch: 197: Training Loss: 0.017793402075767517, Validation Loss: 0.024485057219862938\n",
      "Epoch 22, Batch: 198: Training Loss: 0.023254025727510452, Validation Loss: 0.025526056066155434\n",
      "Epoch 22, Batch: 199: Training Loss: 0.022290116176009178, Validation Loss: 0.021998560056090355\n",
      "Epoch 22, Batch: 200: Training Loss: 0.01938980631530285, Validation Loss: 0.023745378479361534\n",
      "Epoch 22, Batch: 201: Training Loss: 0.02739456295967102, Validation Loss: 0.02544703520834446\n",
      "Epoch 22, Batch: 202: Training Loss: 0.025572052225470543, Validation Loss: 0.020986786112189293\n",
      "Epoch 22, Batch: 203: Training Loss: 0.022910935804247856, Validation Loss: 0.0232993271201849\n",
      "Epoch 22, Batch: 204: Training Loss: 0.02399187535047531, Validation Loss: 0.02425559051334858\n",
      "Epoch 22, Batch: 205: Training Loss: 0.021403683349490166, Validation Loss: 0.024456582963466644\n",
      "Epoch 22, Batch: 206: Training Loss: 0.021538086235523224, Validation Loss: 0.023953190073370934\n",
      "Epoch 22, Batch: 207: Training Loss: 0.023796390742063522, Validation Loss: 0.024953827261924744\n",
      "Epoch 22, Batch: 208: Training Loss: 0.024773556739091873, Validation Loss: 0.023760570213198662\n",
      "Epoch 22, Batch: 209: Training Loss: 0.023115424439311028, Validation Loss: 0.028408266603946686\n",
      "Epoch 22, Batch: 210: Training Loss: 0.023898635059595108, Validation Loss: 0.026066308841109276\n",
      "Epoch 22, Batch: 211: Training Loss: 0.023157116025686264, Validation Loss: 0.0236032884567976\n",
      "Epoch 22, Batch: 212: Training Loss: 0.02485509403049946, Validation Loss: 0.02515105903148651\n",
      "Epoch 22, Batch: 213: Training Loss: 0.025372136384248734, Validation Loss: 0.026772944256663322\n",
      "Epoch 22, Batch: 214: Training Loss: 0.022072304040193558, Validation Loss: 0.027713628485798836\n",
      "Epoch 22, Batch: 215: Training Loss: 0.021046848967671394, Validation Loss: 0.02701732888817787\n",
      "Epoch 22, Batch: 216: Training Loss: 0.020067794248461723, Validation Loss: 0.027386639267206192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch: 217: Training Loss: 0.02273561805486679, Validation Loss: 0.026944875717163086\n",
      "Epoch 22, Batch: 218: Training Loss: 0.02288544923067093, Validation Loss: 0.02774050086736679\n",
      "Epoch 22, Batch: 219: Training Loss: 0.020494820550084114, Validation Loss: 0.027266180142760277\n",
      "Epoch 22, Batch: 220: Training Loss: 0.02038470469415188, Validation Loss: 0.0266382098197937\n",
      "Epoch 22, Batch: 221: Training Loss: 0.02159084752202034, Validation Loss: 0.025658585131168365\n",
      "Epoch 22, Batch: 222: Training Loss: 0.022251775488257408, Validation Loss: 0.025276625528931618\n",
      "Epoch 22, Batch: 223: Training Loss: 0.02328699640929699, Validation Loss: 0.02651437371969223\n",
      "Epoch 22, Batch: 224: Training Loss: 0.020366845652461052, Validation Loss: 0.025454819202423096\n",
      "Epoch 22, Batch: 225: Training Loss: 0.021607786417007446, Validation Loss: 0.025946341454982758\n",
      "Epoch 22, Batch: 226: Training Loss: 0.024068869650363922, Validation Loss: 0.025347594171762466\n",
      "Epoch 22, Batch: 227: Training Loss: 0.021120553836226463, Validation Loss: 0.02588781714439392\n",
      "Epoch 22, Batch: 228: Training Loss: 0.023944249376654625, Validation Loss: 0.02763015776872635\n",
      "Epoch 22, Batch: 229: Training Loss: 0.02411297708749771, Validation Loss: 0.026363318786025047\n",
      "Epoch 22, Batch: 230: Training Loss: 0.023187140002846718, Validation Loss: 0.027667757123708725\n",
      "Epoch 22, Batch: 231: Training Loss: 0.022503498941659927, Validation Loss: 0.02674361690878868\n",
      "Epoch 22, Batch: 232: Training Loss: 0.025662705302238464, Validation Loss: 0.027002833783626556\n",
      "Epoch 22, Batch: 233: Training Loss: 0.01901453360915184, Validation Loss: 0.026309669017791748\n",
      "Epoch 22, Batch: 234: Training Loss: 0.02399096079170704, Validation Loss: 0.026576856151223183\n",
      "Epoch 22, Batch: 235: Training Loss: 0.02287844941020012, Validation Loss: 0.026368679478764534\n",
      "Epoch 22, Batch: 236: Training Loss: 0.02610461786389351, Validation Loss: 0.02889939211308956\n",
      "Epoch 22, Batch: 237: Training Loss: 0.023612283170223236, Validation Loss: 0.029650982469320297\n",
      "Epoch 22, Batch: 238: Training Loss: 0.02461491897702217, Validation Loss: 0.02828715182840824\n",
      "Epoch 22, Batch: 239: Training Loss: 0.024219123646616936, Validation Loss: 0.02826170064508915\n",
      "Epoch 22, Batch: 240: Training Loss: 0.022568173706531525, Validation Loss: 0.027131520211696625\n",
      "Epoch 22, Batch: 241: Training Loss: 0.020547540858387947, Validation Loss: 0.0269420575350523\n",
      "Epoch 22, Batch: 242: Training Loss: 0.025647053495049477, Validation Loss: 0.027444737032055855\n",
      "Epoch 22, Batch: 243: Training Loss: 0.024027105420827866, Validation Loss: 0.024609409272670746\n",
      "Epoch 22, Batch: 244: Training Loss: 0.024760983884334564, Validation Loss: 0.023316698148846626\n",
      "Epoch 22, Batch: 245: Training Loss: 0.021529998630285263, Validation Loss: 0.0236344113945961\n",
      "Epoch 22, Batch: 246: Training Loss: 0.021523920819163322, Validation Loss: 0.02257719822227955\n",
      "Epoch 22, Batch: 247: Training Loss: 0.024555141106247902, Validation Loss: 0.024757979437708855\n",
      "Epoch 22, Batch: 248: Training Loss: 0.02193891443312168, Validation Loss: 0.02349262498319149\n",
      "Epoch 22, Batch: 249: Training Loss: 0.021988824009895325, Validation Loss: 0.02288931980729103\n",
      "Epoch 22, Batch: 250: Training Loss: 0.022947901859879494, Validation Loss: 0.023558007553219795\n",
      "Epoch 22, Batch: 251: Training Loss: 0.019007530063390732, Validation Loss: 0.02609936334192753\n",
      "Epoch 22, Batch: 252: Training Loss: 0.02559376135468483, Validation Loss: 0.023350562900304794\n",
      "Epoch 22, Batch: 253: Training Loss: 0.025674378499388695, Validation Loss: 0.02473682351410389\n",
      "Epoch 22, Batch: 254: Training Loss: 0.024155806750059128, Validation Loss: 0.022031880915164948\n",
      "Epoch 22, Batch: 255: Training Loss: 0.024747129529714584, Validation Loss: 0.02206798456609249\n",
      "Epoch 22, Batch: 256: Training Loss: 0.02238483726978302, Validation Loss: 0.020945196971297264\n",
      "Epoch 22, Batch: 257: Training Loss: 0.024544980376958847, Validation Loss: 0.024057770147919655\n",
      "Epoch 22, Batch: 258: Training Loss: 0.024034418165683746, Validation Loss: 0.023990506306290627\n",
      "Epoch 22, Batch: 259: Training Loss: 0.027067415416240692, Validation Loss: 0.025267641991376877\n",
      "Epoch 22, Batch: 260: Training Loss: 0.02704874984920025, Validation Loss: 0.02573256939649582\n",
      "Epoch 22, Batch: 261: Training Loss: 0.02628752589225769, Validation Loss: 0.025056274607777596\n",
      "Epoch 22, Batch: 262: Training Loss: 0.022497111931443214, Validation Loss: 0.026434026658535004\n",
      "Epoch 22, Batch: 263: Training Loss: 0.020665213465690613, Validation Loss: 0.0256474819034338\n",
      "Epoch 22, Batch: 264: Training Loss: 0.018921000882983208, Validation Loss: 0.023571589961647987\n",
      "Epoch 22, Batch: 265: Training Loss: 0.020678669214248657, Validation Loss: 0.023847967386245728\n",
      "Epoch 22, Batch: 266: Training Loss: 0.019152507185935974, Validation Loss: 0.027332697063684464\n",
      "Epoch 22, Batch: 267: Training Loss: 0.021931050345301628, Validation Loss: 0.02529558539390564\n",
      "Epoch 22, Batch: 268: Training Loss: 0.022040074691176414, Validation Loss: 0.022850727662444115\n",
      "Epoch 22, Batch: 269: Training Loss: 0.025332221761345863, Validation Loss: 0.022441400215029716\n",
      "Epoch 22, Batch: 270: Training Loss: 0.02258247882127762, Validation Loss: 0.02374519407749176\n",
      "Epoch 22, Batch: 271: Training Loss: 0.023150237277150154, Validation Loss: 0.02359810285270214\n",
      "Epoch 22, Batch: 272: Training Loss: 0.02249220199882984, Validation Loss: 0.023534609004855156\n",
      "Epoch 22, Batch: 273: Training Loss: 0.02403167448937893, Validation Loss: 0.02374435029923916\n",
      "Epoch 22, Batch: 274: Training Loss: 0.022093361243605614, Validation Loss: 0.023684347048401833\n",
      "Epoch 22, Batch: 275: Training Loss: 0.02178550511598587, Validation Loss: 0.022648487240076065\n",
      "Epoch 22, Batch: 276: Training Loss: 0.01995096355676651, Validation Loss: 0.024779722094535828\n",
      "Epoch 22, Batch: 277: Training Loss: 0.023116912692785263, Validation Loss: 0.0225837342441082\n",
      "Epoch 22, Batch: 278: Training Loss: 0.02103484608232975, Validation Loss: 0.02432354725897312\n",
      "Epoch 22, Batch: 279: Training Loss: 0.023338526487350464, Validation Loss: 0.02431453764438629\n",
      "Epoch 22, Batch: 280: Training Loss: 0.019738897681236267, Validation Loss: 0.024631813168525696\n",
      "Epoch 22, Batch: 281: Training Loss: 0.021067503839731216, Validation Loss: 0.027950096875429153\n",
      "Epoch 22, Batch: 282: Training Loss: 0.02454555593430996, Validation Loss: 0.022991830483078957\n",
      "Epoch 22, Batch: 283: Training Loss: 0.023006059229373932, Validation Loss: 0.02382051944732666\n",
      "Epoch 22, Batch: 284: Training Loss: 0.02423461340367794, Validation Loss: 0.02398788370192051\n",
      "Epoch 22, Batch: 285: Training Loss: 0.024801814928650856, Validation Loss: 0.023685123771429062\n",
      "Epoch 22, Batch: 286: Training Loss: 0.020812448114156723, Validation Loss: 0.02355015091598034\n",
      "Epoch 22, Batch: 287: Training Loss: 0.024170048534870148, Validation Loss: 0.02235567383468151\n",
      "Epoch 22, Batch: 288: Training Loss: 0.021884460002183914, Validation Loss: 0.02286786399781704\n",
      "Epoch 22, Batch: 289: Training Loss: 0.022290892899036407, Validation Loss: 0.024540932849049568\n",
      "Epoch 22, Batch: 290: Training Loss: 0.02134122885763645, Validation Loss: 0.022496297955513\n",
      "Epoch 22, Batch: 291: Training Loss: 0.021634649485349655, Validation Loss: 0.024189040064811707\n",
      "Epoch 22, Batch: 292: Training Loss: 0.022376442328095436, Validation Loss: 0.024485953152179718\n",
      "Epoch 22, Batch: 293: Training Loss: 0.02252211794257164, Validation Loss: 0.024197779595851898\n",
      "Epoch 22, Batch: 294: Training Loss: 0.0218206699937582, Validation Loss: 0.024811506271362305\n",
      "Epoch 22, Batch: 295: Training Loss: 0.023137511685490608, Validation Loss: 0.02546781860291958\n",
      "Epoch 22, Batch: 296: Training Loss: 0.020121272653341293, Validation Loss: 0.02376471646130085\n",
      "Epoch 22, Batch: 297: Training Loss: 0.020444070920348167, Validation Loss: 0.023990744724869728\n",
      "Epoch 22, Batch: 298: Training Loss: 0.02325211651623249, Validation Loss: 0.0247095488011837\n",
      "Epoch 22, Batch: 299: Training Loss: 0.02080182172358036, Validation Loss: 0.023901674896478653\n",
      "Epoch 22, Batch: 300: Training Loss: 0.024393828585743904, Validation Loss: 0.02656717225909233\n",
      "Epoch 22, Batch: 301: Training Loss: 0.01827653869986534, Validation Loss: 0.026260647922754288\n",
      "Epoch 22, Batch: 302: Training Loss: 0.021757492795586586, Validation Loss: 0.024793043732643127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch: 303: Training Loss: 0.020264551043510437, Validation Loss: 0.024016646668314934\n",
      "Epoch 22, Batch: 304: Training Loss: 0.018323661759495735, Validation Loss: 0.02357134036719799\n",
      "Epoch 22, Batch: 305: Training Loss: 0.022268908098340034, Validation Loss: 0.023792769759893417\n",
      "Epoch 22, Batch: 306: Training Loss: 0.01962948590517044, Validation Loss: 0.02596813626587391\n",
      "Epoch 22, Batch: 307: Training Loss: 0.021383997052907944, Validation Loss: 0.022204380482435226\n",
      "Epoch 22, Batch: 308: Training Loss: 0.020104633644223213, Validation Loss: 0.022469453513622284\n",
      "Epoch 22, Batch: 309: Training Loss: 0.02081494964659214, Validation Loss: 0.022754423320293427\n",
      "Epoch 22, Batch: 310: Training Loss: 0.020622223615646362, Validation Loss: 0.024116791784763336\n",
      "Epoch 22, Batch: 311: Training Loss: 0.019131198525428772, Validation Loss: 0.02357378415763378\n",
      "Epoch 22, Batch: 312: Training Loss: 0.0206404197961092, Validation Loss: 0.024450058117508888\n",
      "Epoch 22, Batch: 313: Training Loss: 0.020667284727096558, Validation Loss: 0.02361200749874115\n",
      "Epoch 22, Batch: 314: Training Loss: 0.021125249564647675, Validation Loss: 0.02364329993724823\n",
      "Epoch 22, Batch: 315: Training Loss: 0.017970949411392212, Validation Loss: 0.024343283846974373\n",
      "Epoch 22, Batch: 316: Training Loss: 0.018563423305749893, Validation Loss: 0.023604050278663635\n",
      "Epoch 22, Batch: 317: Training Loss: 0.022166309878230095, Validation Loss: 0.024475423619151115\n",
      "Epoch 22, Batch: 318: Training Loss: 0.0181099995970726, Validation Loss: 0.023911986500024796\n",
      "Epoch 22, Batch: 319: Training Loss: 0.019230199977755547, Validation Loss: 0.023746812716126442\n",
      "Epoch 22, Batch: 320: Training Loss: 0.023137882351875305, Validation Loss: 0.02451472543179989\n",
      "Epoch 22, Batch: 321: Training Loss: 0.019712332636117935, Validation Loss: 0.024667348712682724\n",
      "Epoch 22, Batch: 322: Training Loss: 0.018936866894364357, Validation Loss: 0.02359316125512123\n",
      "Epoch 22, Batch: 323: Training Loss: 0.021104678511619568, Validation Loss: 0.024725547060370445\n",
      "Epoch 22, Batch: 324: Training Loss: 0.025555338710546494, Validation Loss: 0.024540884420275688\n",
      "Epoch 22, Batch: 325: Training Loss: 0.021289212629199028, Validation Loss: 0.022740716114640236\n",
      "Epoch 22, Batch: 326: Training Loss: 0.028989015147089958, Validation Loss: 0.025145258754491806\n",
      "Epoch 22, Batch: 327: Training Loss: 0.019512267783284187, Validation Loss: 0.023531462997198105\n",
      "Epoch 22, Batch: 328: Training Loss: 0.021652966737747192, Validation Loss: 0.02346634306013584\n",
      "Epoch 22, Batch: 329: Training Loss: 0.023358523845672607, Validation Loss: 0.021186374127864838\n",
      "Epoch 22, Batch: 330: Training Loss: 0.021523332223296165, Validation Loss: 0.022474380210042\n",
      "Epoch 22, Batch: 331: Training Loss: 0.023395847529172897, Validation Loss: 0.021951964125037193\n",
      "Epoch 22, Batch: 332: Training Loss: 0.021062450483441353, Validation Loss: 0.02124272659420967\n",
      "Epoch 22, Batch: 333: Training Loss: 0.02392345480620861, Validation Loss: 0.023201707750558853\n",
      "Epoch 22, Batch: 334: Training Loss: 0.021831637248396873, Validation Loss: 0.023347418755292892\n",
      "Epoch 22, Batch: 335: Training Loss: 0.01894659362733364, Validation Loss: 0.024068305268883705\n",
      "Epoch 22, Batch: 336: Training Loss: 0.020559843629598618, Validation Loss: 0.02322559244930744\n",
      "Epoch 22, Batch: 337: Training Loss: 0.018408874049782753, Validation Loss: 0.022033683955669403\n",
      "Epoch 22, Batch: 338: Training Loss: 0.020661721006035805, Validation Loss: 0.02466428279876709\n",
      "Epoch 22, Batch: 339: Training Loss: 0.022669881582260132, Validation Loss: 0.023636184632778168\n",
      "Epoch 22, Batch: 340: Training Loss: 0.021508391946554184, Validation Loss: 0.023569852113723755\n",
      "Epoch 22, Batch: 341: Training Loss: 0.02023269236087799, Validation Loss: 0.024195469915866852\n",
      "Epoch 22, Batch: 342: Training Loss: 0.0223801638931036, Validation Loss: 0.02272254042327404\n",
      "Epoch 22, Batch: 343: Training Loss: 0.021745571866631508, Validation Loss: 0.02317267656326294\n",
      "Epoch 22, Batch: 344: Training Loss: 0.023935817182064056, Validation Loss: 0.023483293130993843\n",
      "Epoch 22, Batch: 345: Training Loss: 0.023508278653025627, Validation Loss: 0.024442946538329124\n",
      "Epoch 22, Batch: 346: Training Loss: 0.020115060731768608, Validation Loss: 0.024598125368356705\n",
      "Epoch 22, Batch: 347: Training Loss: 0.018600037321448326, Validation Loss: 0.023100771009922028\n",
      "Epoch 22, Batch: 348: Training Loss: 0.021165935322642326, Validation Loss: 0.02461896277964115\n",
      "Epoch 22, Batch: 349: Training Loss: 0.022833919152617455, Validation Loss: 0.024551326408982277\n",
      "Epoch 22, Batch: 350: Training Loss: 0.019773408770561218, Validation Loss: 0.025523845106363297\n",
      "Epoch 22, Batch: 351: Training Loss: 0.024974996224045753, Validation Loss: 0.026285916566848755\n",
      "Epoch 22, Batch: 352: Training Loss: 0.023663626983761787, Validation Loss: 0.02490917779505253\n",
      "Epoch 22, Batch: 353: Training Loss: 0.021708061918616295, Validation Loss: 0.024642454460263252\n",
      "Epoch 22, Batch: 354: Training Loss: 0.021698415279388428, Validation Loss: 0.02351916767656803\n",
      "Epoch 22, Batch: 355: Training Loss: 0.017666954547166824, Validation Loss: 0.0242756474763155\n",
      "Epoch 22, Batch: 356: Training Loss: 0.024509072303771973, Validation Loss: 0.02518489584326744\n",
      "Epoch 22, Batch: 357: Training Loss: 0.01708514429628849, Validation Loss: 0.022637469694018364\n",
      "Epoch 22, Batch: 358: Training Loss: 0.022292228415608406, Validation Loss: 0.02590375393629074\n",
      "Epoch 22, Batch: 359: Training Loss: 0.02003152295947075, Validation Loss: 0.025696374475955963\n",
      "Epoch 22, Batch: 360: Training Loss: 0.022294605150818825, Validation Loss: 0.021872935816645622\n",
      "Epoch 22, Batch: 361: Training Loss: 0.02089342102408409, Validation Loss: 0.022392556071281433\n",
      "Epoch 22, Batch: 362: Training Loss: 0.021761124953627586, Validation Loss: 0.024526817724108696\n",
      "Epoch 22, Batch: 363: Training Loss: 0.024022990837693214, Validation Loss: 0.023969722911715508\n",
      "Epoch 22, Batch: 364: Training Loss: 0.021180782467126846, Validation Loss: 0.022920189425349236\n",
      "Epoch 22, Batch: 365: Training Loss: 0.020294658839702606, Validation Loss: 0.022864971309900284\n",
      "Epoch 22, Batch: 366: Training Loss: 0.022214435040950775, Validation Loss: 0.025569071993231773\n",
      "Epoch 22, Batch: 367: Training Loss: 0.02047552913427353, Validation Loss: 0.02369990013539791\n",
      "Epoch 22, Batch: 368: Training Loss: 0.019148189574480057, Validation Loss: 0.022398972883820534\n",
      "Epoch 22, Batch: 369: Training Loss: 0.022868365049362183, Validation Loss: 0.022184351459145546\n",
      "Epoch 22, Batch: 370: Training Loss: 0.022198043763637543, Validation Loss: 0.0223819799721241\n",
      "Epoch 22, Batch: 371: Training Loss: 0.01618235930800438, Validation Loss: 0.021967124193906784\n",
      "Epoch 22, Batch: 372: Training Loss: 0.022746188566088676, Validation Loss: 0.021956779062747955\n",
      "Epoch 22, Batch: 373: Training Loss: 0.023845426738262177, Validation Loss: 0.024478180333971977\n",
      "Epoch 22, Batch: 374: Training Loss: 0.022446798160672188, Validation Loss: 0.02254842221736908\n",
      "Epoch 22, Batch: 375: Training Loss: 0.02129109390079975, Validation Loss: 0.021621035411953926\n",
      "Epoch 22, Batch: 376: Training Loss: 0.01946186274290085, Validation Loss: 0.02380836009979248\n",
      "Epoch 22, Batch: 377: Training Loss: 0.023912329226732254, Validation Loss: 0.02283209003508091\n",
      "Epoch 22, Batch: 378: Training Loss: 0.021426597610116005, Validation Loss: 0.02328447438776493\n",
      "Epoch 22, Batch: 379: Training Loss: 0.020752932876348495, Validation Loss: 0.021641958504915237\n",
      "Epoch 22, Batch: 380: Training Loss: 0.027191689237952232, Validation Loss: 0.02185109257698059\n",
      "Epoch 22, Batch: 381: Training Loss: 0.020421113818883896, Validation Loss: 0.021789919584989548\n",
      "Epoch 22, Batch: 382: Training Loss: 0.02081822045147419, Validation Loss: 0.02414262667298317\n",
      "Epoch 22, Batch: 383: Training Loss: 0.023673729971051216, Validation Loss: 0.0257121529430151\n",
      "Epoch 22, Batch: 384: Training Loss: 0.0218945499509573, Validation Loss: 0.02348383329808712\n",
      "Epoch 22, Batch: 385: Training Loss: 0.02431274764239788, Validation Loss: 0.022902626544237137\n",
      "Epoch 22, Batch: 386: Training Loss: 0.02051340416073799, Validation Loss: 0.02246369980275631\n",
      "Epoch 22, Batch: 387: Training Loss: 0.023787204176187515, Validation Loss: 0.022729720920324326\n",
      "Epoch 22, Batch: 388: Training Loss: 0.01899123378098011, Validation Loss: 0.02180759608745575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch: 389: Training Loss: 0.02735956385731697, Validation Loss: 0.02315225452184677\n",
      "Epoch 22, Batch: 390: Training Loss: 0.01965726539492607, Validation Loss: 0.022471468895673752\n",
      "Epoch 22, Batch: 391: Training Loss: 0.02388177253305912, Validation Loss: 0.02557658776640892\n",
      "Epoch 22, Batch: 392: Training Loss: 0.02223070338368416, Validation Loss: 0.023080483078956604\n",
      "Epoch 22, Batch: 393: Training Loss: 0.020833896473050117, Validation Loss: 0.023022977635264397\n",
      "Epoch 22, Batch: 394: Training Loss: 0.021382151171565056, Validation Loss: 0.023844901472330093\n",
      "Epoch 22, Batch: 395: Training Loss: 0.02026396431028843, Validation Loss: 0.02384180575609207\n",
      "Epoch 22, Batch: 396: Training Loss: 0.020753614604473114, Validation Loss: 0.023757481947541237\n",
      "Epoch 22, Batch: 397: Training Loss: 0.022244947031140327, Validation Loss: 0.024039356037974358\n",
      "Epoch 22, Batch: 398: Training Loss: 0.021443262696266174, Validation Loss: 0.025571903213858604\n",
      "Epoch 22, Batch: 399: Training Loss: 0.023015351966023445, Validation Loss: 0.022999422624707222\n",
      "Epoch 22, Batch: 400: Training Loss: 0.024215877056121826, Validation Loss: 0.023394720628857613\n",
      "Epoch 22, Batch: 401: Training Loss: 0.0242115817964077, Validation Loss: 0.02265624888241291\n",
      "Epoch 22, Batch: 402: Training Loss: 0.022642571479082108, Validation Loss: 0.02297947369515896\n",
      "Epoch 22, Batch: 403: Training Loss: 0.02484622783958912, Validation Loss: 0.02385864406824112\n",
      "Epoch 22, Batch: 404: Training Loss: 0.02207975648343563, Validation Loss: 0.022972339764237404\n",
      "Epoch 22, Batch: 405: Training Loss: 0.02524024248123169, Validation Loss: 0.024278953671455383\n",
      "Epoch 22, Batch: 406: Training Loss: 0.021260906010866165, Validation Loss: 0.02280649170279503\n",
      "Epoch 22, Batch: 407: Training Loss: 0.02375706471502781, Validation Loss: 0.022177433595061302\n",
      "Epoch 22, Batch: 408: Training Loss: 0.02014959044754505, Validation Loss: 0.024078311398625374\n",
      "Epoch 22, Batch: 409: Training Loss: 0.02057684399187565, Validation Loss: 0.025353822857141495\n",
      "Epoch 22, Batch: 410: Training Loss: 0.022647475823760033, Validation Loss: 0.0203693974763155\n",
      "Epoch 22, Batch: 411: Training Loss: 0.01970232091844082, Validation Loss: 0.022569075226783752\n",
      "Epoch 22, Batch: 412: Training Loss: 0.02293219044804573, Validation Loss: 0.02199190855026245\n",
      "Epoch 22, Batch: 413: Training Loss: 0.021430058404803276, Validation Loss: 0.024066509678959846\n",
      "Epoch 22, Batch: 414: Training Loss: 0.020232541486620903, Validation Loss: 0.02542187087237835\n",
      "Epoch 22, Batch: 415: Training Loss: 0.024994006380438805, Validation Loss: 0.021705159917473793\n",
      "Epoch 22, Batch: 416: Training Loss: 0.022209441289305687, Validation Loss: 0.02595876343548298\n",
      "Epoch 22, Batch: 417: Training Loss: 0.024780478328466415, Validation Loss: 0.025819115340709686\n",
      "Epoch 22, Batch: 418: Training Loss: 0.01952945999801159, Validation Loss: 0.025156352669000626\n",
      "Epoch 22, Batch: 419: Training Loss: 0.018943866714835167, Validation Loss: 0.02501811645925045\n",
      "Epoch 22, Batch: 420: Training Loss: 0.024285992607474327, Validation Loss: 0.024283094331622124\n",
      "Epoch 22, Batch: 421: Training Loss: 0.023569032549858093, Validation Loss: 0.02456030249595642\n",
      "Epoch 22, Batch: 422: Training Loss: 0.024555932730436325, Validation Loss: 0.023856719955801964\n",
      "Epoch 22, Batch: 423: Training Loss: 0.023835957050323486, Validation Loss: 0.024473661556839943\n",
      "Epoch 22, Batch: 424: Training Loss: 0.02179771289229393, Validation Loss: 0.024413593113422394\n",
      "Epoch 22, Batch: 425: Training Loss: 0.021544959396123886, Validation Loss: 0.023955916985869408\n",
      "Epoch 22, Batch: 426: Training Loss: 0.018478794023394585, Validation Loss: 0.02272755466401577\n",
      "Epoch 22, Batch: 427: Training Loss: 0.022899439558386803, Validation Loss: 0.025221478193998337\n",
      "Epoch 22, Batch: 428: Training Loss: 0.022071128711104393, Validation Loss: 0.024767931550741196\n",
      "Epoch 22, Batch: 429: Training Loss: 0.022601483389735222, Validation Loss: 0.024916352704167366\n",
      "Epoch 22, Batch: 430: Training Loss: 0.02117450349032879, Validation Loss: 0.024905063211917877\n",
      "Epoch 22, Batch: 431: Training Loss: 0.020094184204936028, Validation Loss: 0.023983992636203766\n",
      "Epoch 22, Batch: 432: Training Loss: 0.02347937971353531, Validation Loss: 0.024044690653681755\n",
      "Epoch 22, Batch: 433: Training Loss: 0.02295396663248539, Validation Loss: 0.022221285849809647\n",
      "Epoch 22, Batch: 434: Training Loss: 0.021135631948709488, Validation Loss: 0.023388834670186043\n",
      "Epoch 22, Batch: 435: Training Loss: 0.023885315284132957, Validation Loss: 0.02251853421330452\n",
      "Epoch 22, Batch: 436: Training Loss: 0.021465139463543892, Validation Loss: 0.021884433925151825\n",
      "Epoch 22, Batch: 437: Training Loss: 0.021773550659418106, Validation Loss: 0.020780455321073532\n",
      "Epoch 22, Batch: 438: Training Loss: 0.023515580222010612, Validation Loss: 0.025492385029792786\n",
      "Epoch 22, Batch: 439: Training Loss: 0.017778536304831505, Validation Loss: 0.02182275429368019\n",
      "Epoch 22, Batch: 440: Training Loss: 0.02445349469780922, Validation Loss: 0.021651536226272583\n",
      "Epoch 22, Batch: 441: Training Loss: 0.01881963200867176, Validation Loss: 0.02015097253024578\n",
      "Epoch 22, Batch: 442: Training Loss: 0.02514958195388317, Validation Loss: 0.021937556564807892\n",
      "Epoch 22, Batch: 443: Training Loss: 0.021058009937405586, Validation Loss: 0.024312661960721016\n",
      "Epoch 22, Batch: 444: Training Loss: 0.022223781794309616, Validation Loss: 0.022540921345353127\n",
      "Epoch 22, Batch: 445: Training Loss: 0.018447082489728928, Validation Loss: 0.02491389773786068\n",
      "Epoch 22, Batch: 446: Training Loss: 0.02627069689333439, Validation Loss: 0.025329479947686195\n",
      "Epoch 22, Batch: 447: Training Loss: 0.02282331883907318, Validation Loss: 0.024151785299181938\n",
      "Epoch 22, Batch: 448: Training Loss: 0.02129492349922657, Validation Loss: 0.021781083196401596\n",
      "Epoch 22, Batch: 449: Training Loss: 0.0194955226033926, Validation Loss: 0.020340239629149437\n",
      "Epoch 22, Batch: 450: Training Loss: 0.01787499338388443, Validation Loss: 0.02377573773264885\n",
      "Epoch 22, Batch: 451: Training Loss: 0.02526991255581379, Validation Loss: 0.023982198908925056\n",
      "Epoch 22, Batch: 452: Training Loss: 0.022830752655863762, Validation Loss: 0.02409352734684944\n",
      "Epoch 22, Batch: 453: Training Loss: 0.02356809191405773, Validation Loss: 0.022530559450387955\n",
      "Epoch 22, Batch: 454: Training Loss: 0.022138655185699463, Validation Loss: 0.022138072177767754\n",
      "Epoch 22, Batch: 455: Training Loss: 0.018214082345366478, Validation Loss: 0.020111698657274246\n",
      "Epoch 22, Batch: 456: Training Loss: 0.020065702497959137, Validation Loss: 0.026398278772830963\n",
      "Epoch 22, Batch: 457: Training Loss: 0.022070351988077164, Validation Loss: 0.02337798848748207\n",
      "Epoch 22, Batch: 458: Training Loss: 0.02057655155658722, Validation Loss: 0.021586591377854347\n",
      "Epoch 22, Batch: 459: Training Loss: 0.01886388659477234, Validation Loss: 0.022019796073436737\n",
      "Epoch 22, Batch: 460: Training Loss: 0.02353798598051071, Validation Loss: 0.024105129763484\n",
      "Epoch 22, Batch: 461: Training Loss: 0.02059740200638771, Validation Loss: 0.021897872909903526\n",
      "Epoch 22, Batch: 462: Training Loss: 0.021005136892199516, Validation Loss: 0.02124171145260334\n",
      "Epoch 22, Batch: 463: Training Loss: 0.022359270602464676, Validation Loss: 0.020908934995532036\n",
      "Epoch 22, Batch: 464: Training Loss: 0.018889764323830605, Validation Loss: 0.022114405408501625\n",
      "Epoch 22, Batch: 465: Training Loss: 0.02238238789141178, Validation Loss: 0.022902872413396835\n",
      "Epoch 22, Batch: 466: Training Loss: 0.018074247986078262, Validation Loss: 0.02058272436261177\n",
      "Epoch 22, Batch: 467: Training Loss: 0.026941129937767982, Validation Loss: 0.021044643595814705\n",
      "Epoch 22, Batch: 468: Training Loss: 0.025117527693510056, Validation Loss: 0.021713359281420708\n",
      "Epoch 22, Batch: 469: Training Loss: 0.02141464874148369, Validation Loss: 0.021659428253769875\n",
      "Epoch 22, Batch: 470: Training Loss: 0.02050604112446308, Validation Loss: 0.021660301834344864\n",
      "Epoch 22, Batch: 471: Training Loss: 0.022107353433966637, Validation Loss: 0.02218758687376976\n",
      "Epoch 22, Batch: 472: Training Loss: 0.022218333557248116, Validation Loss: 0.02160109393298626\n",
      "Epoch 22, Batch: 473: Training Loss: 0.01996655948460102, Validation Loss: 0.02256818860769272\n",
      "Epoch 22, Batch: 474: Training Loss: 0.018762679770588875, Validation Loss: 0.02127583883702755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model w/ loss: 0.019198445603251457\n",
      "Epoch 22, Batch: 475: Training Loss: 0.02322951890528202, Validation Loss: 0.019198445603251457\n",
      "Epoch 22, Batch: 476: Training Loss: 0.021702758967876434, Validation Loss: 0.02187926135957241\n",
      "Epoch 22, Batch: 477: Training Loss: 0.022784583270549774, Validation Loss: 0.02130845934152603\n",
      "Epoch 22, Batch: 478: Training Loss: 0.02004268392920494, Validation Loss: 0.022180387750267982\n",
      "Epoch 22, Batch: 479: Training Loss: 0.02439003996551037, Validation Loss: 0.0226743221282959\n",
      "Epoch 22, Batch: 480: Training Loss: 0.024023842066526413, Validation Loss: 0.01996563747525215\n",
      "Epoch 22, Batch: 481: Training Loss: 0.02372746355831623, Validation Loss: 0.0219818614423275\n",
      "Epoch 22, Batch: 482: Training Loss: 0.023993637412786484, Validation Loss: 0.021508801728487015\n",
      "Epoch 22, Batch: 483: Training Loss: 0.023235337808728218, Validation Loss: 0.021231897175312042\n",
      "Epoch 22, Batch: 484: Training Loss: 0.021873746067285538, Validation Loss: 0.022537672892212868\n",
      "Epoch 22, Batch: 485: Training Loss: 0.022687893360853195, Validation Loss: 0.024151088669896126\n",
      "Epoch 22, Batch: 486: Training Loss: 0.024464281275868416, Validation Loss: 0.022604772821068764\n",
      "Epoch 22, Batch: 487: Training Loss: 0.023171845823526382, Validation Loss: 0.02103077620267868\n",
      "Epoch 22, Batch: 488: Training Loss: 0.02441694587469101, Validation Loss: 0.024946264922618866\n",
      "Epoch 22, Batch: 489: Training Loss: 0.02234991453588009, Validation Loss: 0.022138340398669243\n",
      "Epoch 22, Batch: 490: Training Loss: 0.024719860404729843, Validation Loss: 0.02211068756878376\n",
      "Epoch 22, Batch: 491: Training Loss: 0.019887709990143776, Validation Loss: 0.023340102285146713\n",
      "Epoch 22, Batch: 492: Training Loss: 0.024754496291279793, Validation Loss: 0.023948390036821365\n",
      "Epoch 22, Batch: 493: Training Loss: 0.024142291396856308, Validation Loss: 0.022546520456671715\n",
      "Epoch 22, Batch: 494: Training Loss: 0.020211227238178253, Validation Loss: 0.021602913737297058\n",
      "Epoch 22, Batch: 495: Training Loss: 0.017940310761332512, Validation Loss: 0.02180367149412632\n",
      "Epoch 22, Batch: 496: Training Loss: 0.019339239224791527, Validation Loss: 0.021591627970337868\n",
      "Epoch 22, Batch: 497: Training Loss: 0.018488410860300064, Validation Loss: 0.023590339347720146\n",
      "Epoch 22, Batch: 498: Training Loss: 0.018588928505778313, Validation Loss: 0.022249968722462654\n",
      "Epoch 22, Batch: 499: Training Loss: 0.01877422444522381, Validation Loss: 0.02341643162071705\n",
      "Epoch 23, Batch: 0: Training Loss: 0.019549213349819183, Validation Loss: 0.023080695420503616\n",
      "Epoch 23, Batch: 1: Training Loss: 0.02092023193836212, Validation Loss: 0.021097252145409584\n",
      "Epoch 23, Batch: 2: Training Loss: 0.02252422831952572, Validation Loss: 0.0224430151283741\n",
      "Epoch 23, Batch: 3: Training Loss: 0.020459482446312904, Validation Loss: 0.022166993468999863\n",
      "Epoch 23, Batch: 4: Training Loss: 0.018490348011255264, Validation Loss: 0.02226090244948864\n",
      "Epoch 23, Batch: 5: Training Loss: 0.020154237747192383, Validation Loss: 0.024718979373574257\n",
      "Epoch 23, Batch: 6: Training Loss: 0.019249936565756798, Validation Loss: 0.02238045632839203\n",
      "Epoch 23, Batch: 7: Training Loss: 0.021110275760293007, Validation Loss: 0.022300228476524353\n",
      "Epoch 23, Batch: 8: Training Loss: 0.018367446959018707, Validation Loss: 0.024144882336258888\n",
      "Epoch 23, Batch: 9: Training Loss: 0.01970241405069828, Validation Loss: 0.023220904171466827\n",
      "Epoch 23, Batch: 10: Training Loss: 0.022345907986164093, Validation Loss: 0.02256128564476967\n",
      "Epoch 23, Batch: 11: Training Loss: 0.024906914681196213, Validation Loss: 0.021886492148041725\n",
      "Epoch 23, Batch: 12: Training Loss: 0.025477750226855278, Validation Loss: 0.02386396937072277\n",
      "Epoch 23, Batch: 13: Training Loss: 0.025290021672844887, Validation Loss: 0.021041855216026306\n",
      "Epoch 23, Batch: 14: Training Loss: 0.020875366404652596, Validation Loss: 0.023264996707439423\n",
      "Epoch 23, Batch: 15: Training Loss: 0.02555590681731701, Validation Loss: 0.022602666169404984\n",
      "Epoch 23, Batch: 16: Training Loss: 0.023452581837773323, Validation Loss: 0.02241986058652401\n",
      "Epoch 23, Batch: 17: Training Loss: 0.020862242206931114, Validation Loss: 0.021754339337348938\n",
      "Epoch 23, Batch: 18: Training Loss: 0.02198140136897564, Validation Loss: 0.021832920610904694\n",
      "Epoch 23, Batch: 19: Training Loss: 0.02130073308944702, Validation Loss: 0.022848261520266533\n",
      "Epoch 23, Batch: 20: Training Loss: 0.023753900080919266, Validation Loss: 0.023135755211114883\n",
      "Epoch 23, Batch: 21: Training Loss: 0.022009478881955147, Validation Loss: 0.02314160391688347\n",
      "Epoch 23, Batch: 22: Training Loss: 0.021709291264414787, Validation Loss: 0.0245487242937088\n",
      "Epoch 23, Batch: 23: Training Loss: 0.023096727207303047, Validation Loss: 0.022333867847919464\n",
      "Epoch 23, Batch: 24: Training Loss: 0.01902926154434681, Validation Loss: 0.02403845079243183\n",
      "Epoch 23, Batch: 25: Training Loss: 0.019257163628935814, Validation Loss: 0.021611839532852173\n",
      "Epoch 23, Batch: 26: Training Loss: 0.022517645731568336, Validation Loss: 0.023093808442354202\n",
      "Epoch 23, Batch: 27: Training Loss: 0.01949726603925228, Validation Loss: 0.02179737389087677\n",
      "Epoch 23, Batch: 28: Training Loss: 0.021316349506378174, Validation Loss: 0.02398156002163887\n",
      "Epoch 23, Batch: 29: Training Loss: 0.023793211206793785, Validation Loss: 0.022752393037080765\n",
      "Epoch 23, Batch: 30: Training Loss: 0.020851189270615578, Validation Loss: 0.023205813020467758\n",
      "Epoch 23, Batch: 31: Training Loss: 0.02181084267795086, Validation Loss: 0.024839328601956367\n",
      "Epoch 23, Batch: 32: Training Loss: 0.02189331129193306, Validation Loss: 0.023141223937273026\n",
      "Epoch 23, Batch: 33: Training Loss: 0.01905437745153904, Validation Loss: 0.021137751638889313\n",
      "Epoch 23, Batch: 34: Training Loss: 0.019624540582299232, Validation Loss: 0.02312142215669155\n",
      "Epoch 23, Batch: 35: Training Loss: 0.019268177449703217, Validation Loss: 0.02480200119316578\n",
      "Epoch 23, Batch: 36: Training Loss: 0.021240083500742912, Validation Loss: 0.02701466530561447\n",
      "Epoch 23, Batch: 37: Training Loss: 0.02392391674220562, Validation Loss: 0.025028077885508537\n",
      "Epoch 23, Batch: 38: Training Loss: 0.020726902410387993, Validation Loss: 0.026995519176125526\n",
      "Epoch 23, Batch: 39: Training Loss: 0.021332139149308205, Validation Loss: 0.02533145062625408\n",
      "Epoch 23, Batch: 40: Training Loss: 0.024616664275527, Validation Loss: 0.026068175211548805\n",
      "Epoch 23, Batch: 41: Training Loss: 0.02045283280313015, Validation Loss: 0.025075331330299377\n",
      "Epoch 23, Batch: 42: Training Loss: 0.02208373136818409, Validation Loss: 0.026122041046619415\n",
      "Epoch 23, Batch: 43: Training Loss: 0.01914665289223194, Validation Loss: 0.02653057686984539\n",
      "Epoch 23, Batch: 44: Training Loss: 0.02090548165142536, Validation Loss: 0.024466872215270996\n",
      "Epoch 23, Batch: 45: Training Loss: 0.021915988996624947, Validation Loss: 0.023219166323542595\n",
      "Epoch 23, Batch: 46: Training Loss: 0.022797860205173492, Validation Loss: 0.02699139527976513\n",
      "Epoch 23, Batch: 47: Training Loss: 0.019971273839473724, Validation Loss: 0.02670302987098694\n",
      "Epoch 23, Batch: 48: Training Loss: 0.02270336262881756, Validation Loss: 0.024517836049199104\n",
      "Epoch 23, Batch: 49: Training Loss: 0.020737411454319954, Validation Loss: 0.02418414317071438\n",
      "Epoch 23, Batch: 50: Training Loss: 0.020999707281589508, Validation Loss: 0.027019411325454712\n",
      "Epoch 23, Batch: 51: Training Loss: 0.026145964860916138, Validation Loss: 0.02371109649538994\n",
      "Epoch 23, Batch: 52: Training Loss: 0.019943026825785637, Validation Loss: 0.025468217208981514\n",
      "Epoch 23, Batch: 53: Training Loss: 0.018827490508556366, Validation Loss: 0.023864757269620895\n",
      "Epoch 23, Batch: 54: Training Loss: 0.02393372170627117, Validation Loss: 0.02430935762822628\n",
      "Epoch 23, Batch: 55: Training Loss: 0.018233604729175568, Validation Loss: 0.024821732193231583\n",
      "Epoch 23, Batch: 56: Training Loss: 0.023123443126678467, Validation Loss: 0.02393980510532856\n",
      "Epoch 23, Batch: 57: Training Loss: 0.02206393890082836, Validation Loss: 0.02386610582470894\n",
      "Epoch 23, Batch: 58: Training Loss: 0.02146206796169281, Validation Loss: 0.022352268919348717\n",
      "Epoch 23, Batch: 59: Training Loss: 0.018677594140172005, Validation Loss: 0.02501922845840454\n",
      "Epoch 23, Batch: 60: Training Loss: 0.02146681398153305, Validation Loss: 0.024766942486166954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch: 61: Training Loss: 0.02274494059383869, Validation Loss: 0.02358219213783741\n",
      "Epoch 23, Batch: 62: Training Loss: 0.020864572376012802, Validation Loss: 0.020105740055441856\n",
      "Epoch 23, Batch: 63: Training Loss: 0.023929012939333916, Validation Loss: 0.025679724290966988\n",
      "Epoch 23, Batch: 64: Training Loss: 0.019787590950727463, Validation Loss: 0.022954681888222694\n",
      "Epoch 23, Batch: 65: Training Loss: 0.020902877673506737, Validation Loss: 0.024031076580286026\n",
      "Epoch 23, Batch: 66: Training Loss: 0.021386513486504555, Validation Loss: 0.023156266659498215\n",
      "Epoch 23, Batch: 67: Training Loss: 0.0222796518355608, Validation Loss: 0.024448739364743233\n",
      "Epoch 23, Batch: 68: Training Loss: 0.022106992080807686, Validation Loss: 0.024961695075035095\n",
      "Epoch 23, Batch: 69: Training Loss: 0.022133730351924896, Validation Loss: 0.023433368653059006\n",
      "Epoch 23, Batch: 70: Training Loss: 0.02191179245710373, Validation Loss: 0.024736834689974785\n",
      "Epoch 23, Batch: 71: Training Loss: 0.018840178847312927, Validation Loss: 0.023771747946739197\n",
      "Epoch 23, Batch: 72: Training Loss: 0.019339125603437424, Validation Loss: 0.02371050789952278\n",
      "Epoch 23, Batch: 73: Training Loss: 0.02279486134648323, Validation Loss: 0.024462612345814705\n",
      "Epoch 23, Batch: 74: Training Loss: 0.019127897918224335, Validation Loss: 0.02334357425570488\n",
      "Epoch 23, Batch: 75: Training Loss: 0.019368449226021767, Validation Loss: 0.02514616772532463\n",
      "Epoch 23, Batch: 76: Training Loss: 0.020318638533353806, Validation Loss: 0.024904325604438782\n",
      "Epoch 23, Batch: 77: Training Loss: 0.024280942976474762, Validation Loss: 0.023891407996416092\n",
      "Epoch 23, Batch: 78: Training Loss: 0.024245234206318855, Validation Loss: 0.023831216618418694\n",
      "Epoch 23, Batch: 79: Training Loss: 0.021747073158621788, Validation Loss: 0.02476799488067627\n",
      "Epoch 23, Batch: 80: Training Loss: 0.02096698246896267, Validation Loss: 0.02278011478483677\n",
      "Epoch 23, Batch: 81: Training Loss: 0.021570401266217232, Validation Loss: 0.021906640380620956\n",
      "Epoch 23, Batch: 82: Training Loss: 0.02195334993302822, Validation Loss: 0.02437623403966427\n",
      "Epoch 23, Batch: 83: Training Loss: 0.02222059667110443, Validation Loss: 0.02188948728144169\n",
      "Epoch 23, Batch: 84: Training Loss: 0.022365478798747063, Validation Loss: 0.02538824826478958\n",
      "Epoch 23, Batch: 85: Training Loss: 0.020465277135372162, Validation Loss: 0.021373433992266655\n",
      "Epoch 23, Batch: 86: Training Loss: 0.02373388037085533, Validation Loss: 0.02283179946243763\n",
      "Epoch 23, Batch: 87: Training Loss: 0.02278386801481247, Validation Loss: 0.023742416873574257\n",
      "Epoch 23, Batch: 88: Training Loss: 0.023108504712581635, Validation Loss: 0.023030631244182587\n",
      "Epoch 23, Batch: 89: Training Loss: 0.02418629638850689, Validation Loss: 0.021900389343500137\n",
      "Epoch 23, Batch: 90: Training Loss: 0.022248368710279465, Validation Loss: 0.022777078673243523\n",
      "Epoch 23, Batch: 91: Training Loss: 0.02288958802819252, Validation Loss: 0.0217380840331316\n",
      "Epoch 23, Batch: 92: Training Loss: 0.024110473692417145, Validation Loss: 0.023044180124998093\n",
      "Epoch 23, Batch: 93: Training Loss: 0.02396366558969021, Validation Loss: 0.02381092496216297\n",
      "Epoch 23, Batch: 94: Training Loss: 0.026067351922392845, Validation Loss: 0.022585703060030937\n",
      "Epoch 23, Batch: 95: Training Loss: 0.020986078307032585, Validation Loss: 0.027154255658388138\n",
      "Epoch 23, Batch: 96: Training Loss: 0.020936153829097748, Validation Loss: 0.02202553115785122\n",
      "Epoch 23, Batch: 97: Training Loss: 0.021639814600348473, Validation Loss: 0.023016782477498055\n",
      "Epoch 23, Batch: 98: Training Loss: 0.022160042077302933, Validation Loss: 0.02385508455336094\n",
      "Epoch 23, Batch: 99: Training Loss: 0.022609177976846695, Validation Loss: 0.023062897846102715\n",
      "Epoch 23, Batch: 100: Training Loss: 0.025504065677523613, Validation Loss: 0.022200876846909523\n",
      "Epoch 23, Batch: 101: Training Loss: 0.019653204828500748, Validation Loss: 0.021524837240576744\n",
      "Epoch 23, Batch: 102: Training Loss: 0.02246207930147648, Validation Loss: 0.022674130275845528\n",
      "Epoch 23, Batch: 103: Training Loss: 0.02350313775241375, Validation Loss: 0.023473424836993217\n",
      "Epoch 23, Batch: 104: Training Loss: 0.019658921286463737, Validation Loss: 0.024444710463285446\n",
      "Epoch 23, Batch: 105: Training Loss: 0.01833144947886467, Validation Loss: 0.024242432788014412\n",
      "Epoch 23, Batch: 106: Training Loss: 0.022790635004639626, Validation Loss: 0.02374855801463127\n",
      "Epoch 23, Batch: 107: Training Loss: 0.021702101454138756, Validation Loss: 0.024360956624150276\n",
      "Epoch 23, Batch: 108: Training Loss: 0.020800000056624413, Validation Loss: 0.02380540408194065\n",
      "Epoch 23, Batch: 109: Training Loss: 0.02177462726831436, Validation Loss: 0.021647151559591293\n",
      "Epoch 23, Batch: 110: Training Loss: 0.0223141647875309, Validation Loss: 0.023746469989418983\n",
      "Epoch 23, Batch: 111: Training Loss: 0.020630067214369774, Validation Loss: 0.023843839764595032\n",
      "Epoch 23, Batch: 112: Training Loss: 0.020447619259357452, Validation Loss: 0.024522190913558006\n",
      "Epoch 23, Batch: 113: Training Loss: 0.023961858823895454, Validation Loss: 0.023186901584267616\n",
      "Epoch 23, Batch: 114: Training Loss: 0.017135139554739, Validation Loss: 0.024182064458727837\n",
      "Epoch 23, Batch: 115: Training Loss: 0.022556915879249573, Validation Loss: 0.02249465323984623\n",
      "Epoch 23, Batch: 116: Training Loss: 0.020475788041949272, Validation Loss: 0.023986682295799255\n",
      "Epoch 23, Batch: 117: Training Loss: 0.020616918802261353, Validation Loss: 0.025741079822182655\n",
      "Epoch 23, Batch: 118: Training Loss: 0.019531769677996635, Validation Loss: 0.024302493780851364\n",
      "Epoch 23, Batch: 119: Training Loss: 0.022397106513381004, Validation Loss: 0.02227136120200157\n",
      "Epoch 23, Batch: 120: Training Loss: 0.019781701266765594, Validation Loss: 0.023673638701438904\n",
      "Epoch 23, Batch: 121: Training Loss: 0.021718164905905724, Validation Loss: 0.022867294028401375\n",
      "Epoch 23, Batch: 122: Training Loss: 0.020586790516972542, Validation Loss: 0.02272139862179756\n",
      "Epoch 23, Batch: 123: Training Loss: 0.019853239879012108, Validation Loss: 0.025025749579072\n",
      "Epoch 23, Batch: 124: Training Loss: 0.018575595691800117, Validation Loss: 0.021768279373645782\n",
      "Epoch 23, Batch: 125: Training Loss: 0.01935729943215847, Validation Loss: 0.02183074690401554\n",
      "Epoch 23, Batch: 126: Training Loss: 0.020927459001541138, Validation Loss: 0.02403167076408863\n",
      "Epoch 23, Batch: 127: Training Loss: 0.02282186597585678, Validation Loss: 0.024854475632309914\n",
      "Epoch 23, Batch: 128: Training Loss: 0.022322993725538254, Validation Loss: 0.023283986374735832\n",
      "Epoch 23, Batch: 129: Training Loss: 0.020043110474944115, Validation Loss: 0.020590156316757202\n",
      "Epoch 23, Batch: 130: Training Loss: 0.023196259513497353, Validation Loss: 0.022799266502261162\n",
      "Epoch 23, Batch: 131: Training Loss: 0.023390986025333405, Validation Loss: 0.023706935346126556\n",
      "Epoch 23, Batch: 132: Training Loss: 0.022036105394363403, Validation Loss: 0.022081146016716957\n",
      "Epoch 23, Batch: 133: Training Loss: 0.02087332122027874, Validation Loss: 0.020197823643684387\n",
      "Epoch 23, Batch: 134: Training Loss: 0.02240382321178913, Validation Loss: 0.024366682395339012\n",
      "Epoch 23, Batch: 135: Training Loss: 0.018626997247338295, Validation Loss: 0.02194463275372982\n",
      "Epoch 23, Batch: 136: Training Loss: 0.0198479276150465, Validation Loss: 0.022981446236371994\n",
      "Epoch 23, Batch: 137: Training Loss: 0.021820511668920517, Validation Loss: 0.020955603569746017\n",
      "Epoch 23, Batch: 138: Training Loss: 0.022004008293151855, Validation Loss: 0.02028302662074566\n",
      "Epoch 23, Batch: 139: Training Loss: 0.021481217816472054, Validation Loss: 0.02273797243833542\n",
      "Epoch 23, Batch: 140: Training Loss: 0.02519904635846615, Validation Loss: 0.021002471446990967\n",
      "Epoch 23, Batch: 141: Training Loss: 0.020185021683573723, Validation Loss: 0.020188352093100548\n",
      "Epoch 23, Batch: 142: Training Loss: 0.021354274824261665, Validation Loss: 0.02332402765750885\n",
      "Epoch 23, Batch: 143: Training Loss: 0.02108347974717617, Validation Loss: 0.022338230162858963\n",
      "Epoch 23, Batch: 144: Training Loss: 0.01908595860004425, Validation Loss: 0.02288373000919819\n",
      "Epoch 23, Batch: 145: Training Loss: 0.01943117007613182, Validation Loss: 0.023359045386314392\n",
      "Epoch 23, Batch: 146: Training Loss: 0.021190907806158066, Validation Loss: 0.02610229328274727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch: 147: Training Loss: 0.020709894597530365, Validation Loss: 0.023935573175549507\n",
      "Epoch 23, Batch: 148: Training Loss: 0.019208049401640892, Validation Loss: 0.023389140143990517\n",
      "Epoch 23, Batch: 149: Training Loss: 0.021545372903347015, Validation Loss: 0.024068566039204597\n",
      "Epoch 23, Batch: 150: Training Loss: 0.0240186620503664, Validation Loss: 0.024128399789333344\n",
      "Epoch 23, Batch: 151: Training Loss: 0.021425629034638405, Validation Loss: 0.02426910772919655\n",
      "Epoch 23, Batch: 152: Training Loss: 0.022513383999466896, Validation Loss: 0.02176644653081894\n",
      "Epoch 23, Batch: 153: Training Loss: 0.023843305185437202, Validation Loss: 0.025205785408616066\n",
      "Epoch 23, Batch: 154: Training Loss: 0.02067188359797001, Validation Loss: 0.02275860868394375\n",
      "Epoch 23, Batch: 155: Training Loss: 0.025992006063461304, Validation Loss: 0.023883981630206108\n",
      "Epoch 23, Batch: 156: Training Loss: 0.02096293494105339, Validation Loss: 0.02723134681582451\n",
      "Epoch 23, Batch: 157: Training Loss: 0.020624784752726555, Validation Loss: 0.023594442754983902\n",
      "Epoch 23, Batch: 158: Training Loss: 0.019373999908566475, Validation Loss: 0.024503318592905998\n",
      "Epoch 23, Batch: 159: Training Loss: 0.021767552942037582, Validation Loss: 0.024813193827867508\n",
      "Epoch 23, Batch: 160: Training Loss: 0.021653125062584877, Validation Loss: 0.023632824420928955\n",
      "Epoch 23, Batch: 161: Training Loss: 0.02017107605934143, Validation Loss: 0.023483915254473686\n",
      "Epoch 23, Batch: 162: Training Loss: 0.024811211973428726, Validation Loss: 0.023274138569831848\n",
      "Epoch 23, Batch: 163: Training Loss: 0.02631254494190216, Validation Loss: 0.022537855431437492\n",
      "Epoch 23, Batch: 164: Training Loss: 0.023812808096408844, Validation Loss: 0.024565724655985832\n",
      "Epoch 23, Batch: 165: Training Loss: 0.02136034332215786, Validation Loss: 0.02373625338077545\n",
      "Epoch 23, Batch: 166: Training Loss: 0.023702196776866913, Validation Loss: 0.02302837185561657\n",
      "Epoch 23, Batch: 167: Training Loss: 0.020936407148838043, Validation Loss: 0.02473069541156292\n",
      "Epoch 23, Batch: 168: Training Loss: 0.023386705666780472, Validation Loss: 0.02294146828353405\n",
      "Epoch 23, Batch: 169: Training Loss: 0.022446103394031525, Validation Loss: 0.02194841019809246\n",
      "Epoch 23, Batch: 170: Training Loss: 0.023676235228776932, Validation Loss: 0.024348944425582886\n",
      "Epoch 23, Batch: 171: Training Loss: 0.018512805923819542, Validation Loss: 0.02202562987804413\n",
      "Epoch 23, Batch: 172: Training Loss: 0.023695530369877815, Validation Loss: 0.022000206634402275\n",
      "Epoch 23, Batch: 173: Training Loss: 0.017471345141530037, Validation Loss: 0.021541306748986244\n",
      "Epoch 23, Batch: 174: Training Loss: 0.02334556356072426, Validation Loss: 0.0234537236392498\n",
      "Epoch 23, Batch: 175: Training Loss: 0.025899002328515053, Validation Loss: 0.02235526591539383\n",
      "Epoch 23, Batch: 176: Training Loss: 0.024082841351628304, Validation Loss: 0.024748463183641434\n",
      "Epoch 23, Batch: 177: Training Loss: 0.022696267813444138, Validation Loss: 0.02243603765964508\n",
      "Epoch 23, Batch: 178: Training Loss: 0.028331654146313667, Validation Loss: 0.022051887586712837\n",
      "Epoch 23, Batch: 179: Training Loss: 0.018304739147424698, Validation Loss: 0.02374916709959507\n",
      "Epoch 23, Batch: 180: Training Loss: 0.02409398928284645, Validation Loss: 0.02526029385626316\n",
      "Epoch 23, Batch: 181: Training Loss: 0.025159405544400215, Validation Loss: 0.022816963493824005\n",
      "Epoch 23, Batch: 182: Training Loss: 0.020852159708738327, Validation Loss: 0.02433629333972931\n",
      "Epoch 23, Batch: 183: Training Loss: 0.021527357399463654, Validation Loss: 0.023320099338889122\n",
      "Epoch 23, Batch: 184: Training Loss: 0.023250948637723923, Validation Loss: 0.02186828851699829\n",
      "Epoch 23, Batch: 185: Training Loss: 0.022210683673620224, Validation Loss: 0.02263130433857441\n",
      "Epoch 23, Batch: 186: Training Loss: 0.024459460750222206, Validation Loss: 0.02209637686610222\n",
      "Epoch 23, Batch: 187: Training Loss: 0.021395333111286163, Validation Loss: 0.023205183446407318\n",
      "Epoch 23, Batch: 188: Training Loss: 0.023970242589712143, Validation Loss: 0.026174157857894897\n",
      "Epoch 23, Batch: 189: Training Loss: 0.019891344010829926, Validation Loss: 0.025790441781282425\n",
      "Epoch 23, Batch: 190: Training Loss: 0.02327282354235649, Validation Loss: 0.02418268285691738\n",
      "Epoch 23, Batch: 191: Training Loss: 0.021571723744273186, Validation Loss: 0.02407803386449814\n",
      "Epoch 23, Batch: 192: Training Loss: 0.023631371557712555, Validation Loss: 0.025023723021149635\n",
      "Epoch 23, Batch: 193: Training Loss: 0.02068541757762432, Validation Loss: 0.024993272498250008\n",
      "Epoch 23, Batch: 194: Training Loss: 0.019813960418105125, Validation Loss: 0.023479310795664787\n",
      "Epoch 23, Batch: 195: Training Loss: 0.02154470421373844, Validation Loss: 0.023525036871433258\n",
      "Epoch 23, Batch: 196: Training Loss: 0.022838445380330086, Validation Loss: 0.023441897705197334\n",
      "Epoch 23, Batch: 197: Training Loss: 0.01829250529408455, Validation Loss: 0.02356361225247383\n",
      "Epoch 23, Batch: 198: Training Loss: 0.01968398690223694, Validation Loss: 0.024294178932905197\n",
      "Epoch 23, Batch: 199: Training Loss: 0.02133113518357277, Validation Loss: 0.022371958941221237\n",
      "Epoch 23, Batch: 200: Training Loss: 0.018633784726262093, Validation Loss: 0.023714985698461533\n",
      "Epoch 23, Batch: 201: Training Loss: 0.024013016372919083, Validation Loss: 0.02298792637884617\n",
      "Epoch 23, Batch: 202: Training Loss: 0.021109191700816154, Validation Loss: 0.023406002670526505\n",
      "Epoch 23, Batch: 203: Training Loss: 0.021170884370803833, Validation Loss: 0.022066805511713028\n",
      "Epoch 23, Batch: 204: Training Loss: 0.02164897508919239, Validation Loss: 0.023171570152044296\n",
      "Epoch 23, Batch: 205: Training Loss: 0.022692039608955383, Validation Loss: 0.02452828176319599\n",
      "Epoch 23, Batch: 206: Training Loss: 0.021573515608906746, Validation Loss: 0.021979670971632004\n",
      "Epoch 23, Batch: 207: Training Loss: 0.02379315160214901, Validation Loss: 0.022501809522509575\n",
      "Epoch 23, Batch: 208: Training Loss: 0.0220782533288002, Validation Loss: 0.021399501711130142\n",
      "Epoch 23, Batch: 209: Training Loss: 0.021649472415447235, Validation Loss: 0.02284173294901848\n",
      "Epoch 23, Batch: 210: Training Loss: 0.021374927833676338, Validation Loss: 0.02227175049483776\n",
      "Epoch 23, Batch: 211: Training Loss: 0.022776197642087936, Validation Loss: 0.021152637898921967\n",
      "Epoch 23, Batch: 212: Training Loss: 0.021935125812888145, Validation Loss: 0.022707924246788025\n",
      "Epoch 23, Batch: 213: Training Loss: 0.02350001595914364, Validation Loss: 0.022699009627103806\n",
      "Epoch 23, Batch: 214: Training Loss: 0.020069021731615067, Validation Loss: 0.021372444927692413\n",
      "Epoch 23, Batch: 215: Training Loss: 0.021145381033420563, Validation Loss: 0.023601893335580826\n",
      "Epoch 23, Batch: 216: Training Loss: 0.02031777985394001, Validation Loss: 0.022897815331816673\n",
      "Epoch 23, Batch: 217: Training Loss: 0.02360701933503151, Validation Loss: 0.022698858752846718\n",
      "Epoch 23, Batch: 218: Training Loss: 0.0213466826826334, Validation Loss: 0.021557386964559555\n",
      "Epoch 23, Batch: 219: Training Loss: 0.018333246931433678, Validation Loss: 0.02175012044608593\n",
      "Epoch 23, Batch: 220: Training Loss: 0.02036597765982151, Validation Loss: 0.02246706932783127\n",
      "Epoch 23, Batch: 221: Training Loss: 0.023768451064825058, Validation Loss: 0.02215961180627346\n",
      "Epoch 23, Batch: 222: Training Loss: 0.020754430443048477, Validation Loss: 0.02416130155324936\n",
      "Epoch 23, Batch: 223: Training Loss: 0.021136311814188957, Validation Loss: 0.022801635786890984\n",
      "Epoch 23, Batch: 224: Training Loss: 0.024212032556533813, Validation Loss: 0.02222830429673195\n",
      "Epoch 23, Batch: 225: Training Loss: 0.018806615844368935, Validation Loss: 0.022612860426306725\n",
      "Epoch 23, Batch: 226: Training Loss: 0.02287270501255989, Validation Loss: 0.024039853364229202\n",
      "Epoch 23, Batch: 227: Training Loss: 0.019937308505177498, Validation Loss: 0.022232139483094215\n",
      "Epoch 23, Batch: 228: Training Loss: 0.023559091612696648, Validation Loss: 0.020996179431676865\n",
      "Epoch 23, Batch: 229: Training Loss: 0.02441268600523472, Validation Loss: 0.024158501997590065\n",
      "Epoch 23, Batch: 230: Training Loss: 0.018561391159892082, Validation Loss: 0.021215954795479774\n",
      "Epoch 23, Batch: 231: Training Loss: 0.02302134782075882, Validation Loss: 0.022346822544932365\n",
      "Epoch 23, Batch: 232: Training Loss: 0.024915451183915138, Validation Loss: 0.02249499037861824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch: 233: Training Loss: 0.017268512398004532, Validation Loss: 0.026044880971312523\n",
      "Epoch 23, Batch: 234: Training Loss: 0.020902881398797035, Validation Loss: 0.023249659687280655\n",
      "Epoch 23, Batch: 235: Training Loss: 0.0202784426510334, Validation Loss: 0.023196851834654808\n",
      "Epoch 23, Batch: 236: Training Loss: 0.0213577002286911, Validation Loss: 0.021583186462521553\n",
      "Epoch 23, Batch: 237: Training Loss: 0.020656945183873177, Validation Loss: 0.023402046412229538\n",
      "Epoch 23, Batch: 238: Training Loss: 0.020886685699224472, Validation Loss: 0.02189014106988907\n",
      "Epoch 23, Batch: 239: Training Loss: 0.02224935032427311, Validation Loss: 0.022629093378782272\n",
      "Epoch 23, Batch: 240: Training Loss: 0.019979769363999367, Validation Loss: 0.02383056841790676\n",
      "Epoch 23, Batch: 241: Training Loss: 0.021925510838627815, Validation Loss: 0.022722648456692696\n",
      "Epoch 23, Batch: 242: Training Loss: 0.018807442858815193, Validation Loss: 0.021249672397971153\n",
      "Epoch 23, Batch: 243: Training Loss: 0.023841971531510353, Validation Loss: 0.020973308011889458\n",
      "Epoch 23, Batch: 244: Training Loss: 0.022218206897377968, Validation Loss: 0.021938374266028404\n",
      "Epoch 23, Batch: 245: Training Loss: 0.0225891824811697, Validation Loss: 0.022215064615011215\n",
      "Epoch 23, Batch: 246: Training Loss: 0.023194458335638046, Validation Loss: 0.022523505613207817\n",
      "Epoch 23, Batch: 247: Training Loss: 0.0225833710283041, Validation Loss: 0.022275758907198906\n",
      "Epoch 23, Batch: 248: Training Loss: 0.02240886352956295, Validation Loss: 0.024115724489092827\n",
      "Epoch 23, Batch: 249: Training Loss: 0.020822757855057716, Validation Loss: 0.022210143506526947\n",
      "Epoch 23, Batch: 250: Training Loss: 0.017749421298503876, Validation Loss: 0.02272147685289383\n",
      "Epoch 23, Batch: 251: Training Loss: 0.019956074655056, Validation Loss: 0.022540144622325897\n",
      "Epoch 23, Batch: 252: Training Loss: 0.021039757877588272, Validation Loss: 0.02425064891576767\n",
      "Epoch 23, Batch: 253: Training Loss: 0.02304413542151451, Validation Loss: 0.024317795410752296\n",
      "Epoch 23, Batch: 254: Training Loss: 0.0181453637778759, Validation Loss: 0.025120612233877182\n",
      "Epoch 23, Batch: 255: Training Loss: 0.023253455758094788, Validation Loss: 0.025409411638975143\n",
      "Epoch 23, Batch: 256: Training Loss: 0.022941287606954575, Validation Loss: 0.023656213656067848\n",
      "Epoch 23, Batch: 257: Training Loss: 0.02389717847108841, Validation Loss: 0.02290603332221508\n",
      "Epoch 23, Batch: 258: Training Loss: 0.024019746109843254, Validation Loss: 0.022104842588305473\n",
      "Epoch 23, Batch: 259: Training Loss: 0.023606345057487488, Validation Loss: 0.020861512050032616\n",
      "Epoch 23, Batch: 260: Training Loss: 0.02340846322476864, Validation Loss: 0.02207356132566929\n",
      "Epoch 23, Batch: 261: Training Loss: 0.023632382974028587, Validation Loss: 0.02675130032002926\n",
      "Epoch 23, Batch: 262: Training Loss: 0.01911129802465439, Validation Loss: 0.024182692170143127\n",
      "Epoch 23, Batch: 263: Training Loss: 0.019063666462898254, Validation Loss: 0.023038635030388832\n",
      "Epoch 23, Batch: 264: Training Loss: 0.017370132729411125, Validation Loss: 0.023324577137827873\n",
      "Epoch 23, Batch: 265: Training Loss: 0.018555887043476105, Validation Loss: 0.02479211613535881\n",
      "Epoch 23, Batch: 266: Training Loss: 0.02038494683802128, Validation Loss: 0.025115642696619034\n",
      "Epoch 23, Batch: 267: Training Loss: 0.021217243745923042, Validation Loss: 0.023352302610874176\n",
      "Epoch 23, Batch: 268: Training Loss: 0.01966021955013275, Validation Loss: 0.022780656814575195\n",
      "Epoch 23, Batch: 269: Training Loss: 0.020145677030086517, Validation Loss: 0.021730778738856316\n",
      "Epoch 23, Batch: 270: Training Loss: 0.017645161598920822, Validation Loss: 0.022984476760029793\n",
      "Epoch 23, Batch: 271: Training Loss: 0.020506488159298897, Validation Loss: 0.02340940199792385\n",
      "Epoch 23, Batch: 272: Training Loss: 0.020956164225935936, Validation Loss: 0.021978948265314102\n",
      "Epoch 23, Batch: 273: Training Loss: 0.021772461012005806, Validation Loss: 0.019675320014357567\n",
      "Epoch 23, Batch: 274: Training Loss: 0.02238534949719906, Validation Loss: 0.022789211943745613\n",
      "Epoch 23, Batch: 275: Training Loss: 0.020513104274868965, Validation Loss: 0.02298496849834919\n",
      "Epoch 23, Batch: 276: Training Loss: 0.02328982762992382, Validation Loss: 0.024019910022616386\n",
      "Epoch 23, Batch: 277: Training Loss: 0.0207777451723814, Validation Loss: 0.023919809609651566\n",
      "Epoch 23, Batch: 278: Training Loss: 0.02208123914897442, Validation Loss: 0.021264003589749336\n",
      "Epoch 23, Batch: 279: Training Loss: 0.022784624248743057, Validation Loss: 0.023238809779286385\n",
      "Epoch 23, Batch: 280: Training Loss: 0.018382029607892036, Validation Loss: 0.020040202885866165\n",
      "Epoch 23, Batch: 281: Training Loss: 0.01983688212931156, Validation Loss: 0.022797511890530586\n",
      "Epoch 23, Batch: 282: Training Loss: 0.021112050861120224, Validation Loss: 0.0221992377191782\n",
      "Epoch 23, Batch: 283: Training Loss: 0.02146994136273861, Validation Loss: 0.021748047322034836\n",
      "Epoch 23, Batch: 284: Training Loss: 0.01963081583380699, Validation Loss: 0.02174367755651474\n",
      "Epoch 23, Batch: 285: Training Loss: 0.021472496911883354, Validation Loss: 0.020054373890161514\n",
      "Epoch 23, Batch: 286: Training Loss: 0.021274195984005928, Validation Loss: 0.020688485354185104\n",
      "Epoch 23, Batch: 287: Training Loss: 0.022777900099754333, Validation Loss: 0.021890155971050262\n",
      "Epoch 23, Batch: 288: Training Loss: 0.018359674140810966, Validation Loss: 0.020195402204990387\n",
      "Epoch 23, Batch: 289: Training Loss: 0.02051178365945816, Validation Loss: 0.021900523453950882\n",
      "Epoch 23, Batch: 290: Training Loss: 0.02061839960515499, Validation Loss: 0.021301886066794395\n",
      "Epoch 23, Batch: 291: Training Loss: 0.021856987848877907, Validation Loss: 0.020789269357919693\n",
      "Epoch 23, Batch: 292: Training Loss: 0.02150855027139187, Validation Loss: 0.021279511973261833\n",
      "Epoch 23, Batch: 293: Training Loss: 0.02203386463224888, Validation Loss: 0.021675745025277138\n",
      "Epoch 23, Batch: 294: Training Loss: 0.018247045576572418, Validation Loss: 0.023045018315315247\n",
      "Saving new best model w/ loss: 0.019008053466677666\n",
      "Epoch 23, Batch: 295: Training Loss: 0.021289212629199028, Validation Loss: 0.019008053466677666\n",
      "Epoch 23, Batch: 296: Training Loss: 0.02228637970983982, Validation Loss: 0.02102193608880043\n",
      "Epoch 23, Batch: 297: Training Loss: 0.021972207352519035, Validation Loss: 0.02173592895269394\n",
      "Epoch 23, Batch: 298: Training Loss: 0.022340552881360054, Validation Loss: 0.02257225103676319\n",
      "Epoch 23, Batch: 299: Training Loss: 0.02080608904361725, Validation Loss: 0.019776122644543648\n",
      "Epoch 23, Batch: 300: Training Loss: 0.023956194519996643, Validation Loss: 0.02230139449238777\n",
      "Epoch 23, Batch: 301: Training Loss: 0.022054849192500114, Validation Loss: 0.0222308449447155\n",
      "Epoch 23, Batch: 302: Training Loss: 0.020483745262026787, Validation Loss: 0.02301745116710663\n",
      "Epoch 23, Batch: 303: Training Loss: 0.021511375904083252, Validation Loss: 0.022151285782456398\n",
      "Epoch 23, Batch: 304: Training Loss: 0.01767861284315586, Validation Loss: 0.022037893533706665\n",
      "Epoch 23, Batch: 305: Training Loss: 0.019911905750632286, Validation Loss: 0.020190594717860222\n",
      "Epoch 23, Batch: 306: Training Loss: 0.02218777686357498, Validation Loss: 0.021694716066122055\n",
      "Epoch 23, Batch: 307: Training Loss: 0.020205892622470856, Validation Loss: 0.021380677819252014\n",
      "Epoch 23, Batch: 308: Training Loss: 0.020525069907307625, Validation Loss: 0.020967897027730942\n",
      "Epoch 23, Batch: 309: Training Loss: 0.020827122032642365, Validation Loss: 0.020845279097557068\n",
      "Epoch 23, Batch: 310: Training Loss: 0.02300442010164261, Validation Loss: 0.021305937319993973\n",
      "Epoch 23, Batch: 311: Training Loss: 0.019493062049150467, Validation Loss: 0.020848287269473076\n",
      "Epoch 23, Batch: 312: Training Loss: 0.022107543423771858, Validation Loss: 0.02116703987121582\n",
      "Epoch 23, Batch: 313: Training Loss: 0.02221008948981762, Validation Loss: 0.020679570734500885\n",
      "Epoch 23, Batch: 314: Training Loss: 0.023775655776262283, Validation Loss: 0.022220848128199577\n",
      "Epoch 23, Batch: 315: Training Loss: 0.021124370396137238, Validation Loss: 0.02173127420246601\n",
      "Epoch 23, Batch: 316: Training Loss: 0.019949201494455338, Validation Loss: 0.020323200151324272\n",
      "Epoch 23, Batch: 317: Training Loss: 0.018424097448587418, Validation Loss: 0.023636292666196823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model w/ loss: 0.018765723332762718\n",
      "Epoch 23, Batch: 318: Training Loss: 0.021850604563951492, Validation Loss: 0.018765723332762718\n",
      "Epoch 23, Batch: 319: Training Loss: 0.022381378337740898, Validation Loss: 0.021633915603160858\n",
      "Epoch 23, Batch: 320: Training Loss: 0.023334311321377754, Validation Loss: 0.02025442384183407\n",
      "Epoch 23, Batch: 321: Training Loss: 0.018523860722780228, Validation Loss: 0.018938597291707993\n",
      "Epoch 23, Batch: 322: Training Loss: 0.019649909809231758, Validation Loss: 0.01989985816180706\n",
      "Epoch 23, Batch: 323: Training Loss: 0.02045069821178913, Validation Loss: 0.019631044939160347\n",
      "Epoch 23, Batch: 324: Training Loss: 0.02340962551534176, Validation Loss: 0.020544201135635376\n",
      "Epoch 23, Batch: 325: Training Loss: 0.019519075751304626, Validation Loss: 0.02108306996524334\n",
      "Epoch 23, Batch: 326: Training Loss: 0.023510035127401352, Validation Loss: 0.024716882035136223\n",
      "Epoch 23, Batch: 327: Training Loss: 0.019529277458786964, Validation Loss: 0.021450165659189224\n",
      "Epoch 23, Batch: 328: Training Loss: 0.020984916016459465, Validation Loss: 0.02274368889629841\n",
      "Epoch 23, Batch: 329: Training Loss: 0.01985616609454155, Validation Loss: 0.02181411162018776\n",
      "Epoch 23, Batch: 330: Training Loss: 0.018933109939098358, Validation Loss: 0.02162482775747776\n",
      "Epoch 23, Batch: 331: Training Loss: 0.023074986413121223, Validation Loss: 0.022272329777479172\n",
      "Epoch 23, Batch: 332: Training Loss: 0.018022049218416214, Validation Loss: 0.022792646661400795\n",
      "Epoch 23, Batch: 333: Training Loss: 0.02126470021903515, Validation Loss: 0.02265889383852482\n",
      "Epoch 23, Batch: 334: Training Loss: 0.020244626328349113, Validation Loss: 0.02055523544549942\n",
      "Epoch 23, Batch: 335: Training Loss: 0.019066981971263885, Validation Loss: 0.020618755370378494\n",
      "Epoch 23, Batch: 336: Training Loss: 0.018640397116541862, Validation Loss: 0.023279961198568344\n",
      "Epoch 23, Batch: 337: Training Loss: 0.023260558024048805, Validation Loss: 0.024913616478443146\n",
      "Epoch 23, Batch: 338: Training Loss: 0.018544912338256836, Validation Loss: 0.021473534405231476\n",
      "Epoch 23, Batch: 339: Training Loss: 0.019357243552803993, Validation Loss: 0.020336907356977463\n",
      "Epoch 23, Batch: 340: Training Loss: 0.02255018800497055, Validation Loss: 0.021393684670329094\n",
      "Epoch 23, Batch: 341: Training Loss: 0.018992505967617035, Validation Loss: 0.02104274556040764\n",
      "Epoch 23, Batch: 342: Training Loss: 0.020788412541151047, Validation Loss: 0.019747493788599968\n",
      "Epoch 23, Batch: 343: Training Loss: 0.020347807556390762, Validation Loss: 0.021809276193380356\n",
      "Epoch 23, Batch: 344: Training Loss: 0.0270055141299963, Validation Loss: 0.021272599697113037\n",
      "Epoch 23, Batch: 345: Training Loss: 0.020039154216647148, Validation Loss: 0.0230795219540596\n",
      "Epoch 23, Batch: 346: Training Loss: 0.020157407969236374, Validation Loss: 0.021593574434518814\n",
      "Epoch 23, Batch: 347: Training Loss: 0.018935205414891243, Validation Loss: 0.024340519681572914\n",
      "Epoch 23, Batch: 348: Training Loss: 0.02224971354007721, Validation Loss: 0.02360888570547104\n",
      "Epoch 23, Batch: 349: Training Loss: 0.020221492275595665, Validation Loss: 0.02318841777741909\n",
      "Epoch 23, Batch: 350: Training Loss: 0.018431581556797028, Validation Loss: 0.02052946202456951\n",
      "Epoch 23, Batch: 351: Training Loss: 0.024216173216700554, Validation Loss: 0.02131408266723156\n",
      "Epoch 23, Batch: 352: Training Loss: 0.022103862836956978, Validation Loss: 0.020564232021570206\n",
      "Epoch 23, Batch: 353: Training Loss: 0.02413490228354931, Validation Loss: 0.02072574384510517\n",
      "Epoch 23, Batch: 354: Training Loss: 0.02258918061852455, Validation Loss: 0.02210019901394844\n",
      "Epoch 23, Batch: 355: Training Loss: 0.018093012273311615, Validation Loss: 0.022973401471972466\n",
      "Epoch 23, Batch: 356: Training Loss: 0.020137064158916473, Validation Loss: 0.02347090095281601\n",
      "Epoch 23, Batch: 357: Training Loss: 0.02127578668296337, Validation Loss: 0.023900635540485382\n",
      "Epoch 23, Batch: 358: Training Loss: 0.024366091936826706, Validation Loss: 0.022101562470197678\n",
      "Epoch 23, Batch: 359: Training Loss: 0.022992363199591637, Validation Loss: 0.023177117109298706\n",
      "Epoch 23, Batch: 360: Training Loss: 0.023839661851525307, Validation Loss: 0.022563325241208076\n",
      "Epoch 23, Batch: 361: Training Loss: 0.020941026508808136, Validation Loss: 0.021850287914276123\n",
      "Epoch 23, Batch: 362: Training Loss: 0.021491115912795067, Validation Loss: 0.02327292039990425\n",
      "Epoch 23, Batch: 363: Training Loss: 0.02638925425708294, Validation Loss: 0.022713392972946167\n",
      "Epoch 23, Batch: 364: Training Loss: 0.023001592606306076, Validation Loss: 0.02311505377292633\n",
      "Epoch 23, Batch: 365: Training Loss: 0.0203990638256073, Validation Loss: 0.02274503745138645\n",
      "Epoch 23, Batch: 366: Training Loss: 0.0227512177079916, Validation Loss: 0.02027321793138981\n",
      "Epoch 23, Batch: 367: Training Loss: 0.019594790413975716, Validation Loss: 0.0207305159419775\n",
      "Epoch 23, Batch: 368: Training Loss: 0.01942583918571472, Validation Loss: 0.021844200789928436\n",
      "Epoch 23, Batch: 369: Training Loss: 0.02049069292843342, Validation Loss: 0.022988617420196533\n",
      "Epoch 23, Batch: 370: Training Loss: 0.023126665502786636, Validation Loss: 0.02207317389547825\n",
      "Epoch 23, Batch: 371: Training Loss: 0.02086961828172207, Validation Loss: 0.02244378626346588\n",
      "Epoch 23, Batch: 372: Training Loss: 0.020814338698983192, Validation Loss: 0.02065168134868145\n",
      "Epoch 23, Batch: 373: Training Loss: 0.02420179732143879, Validation Loss: 0.023494549095630646\n",
      "Epoch 23, Batch: 374: Training Loss: 0.018848517909646034, Validation Loss: 0.02105090394616127\n",
      "Saving new best model w/ loss: 0.018024306744337082\n",
      "Epoch 23, Batch: 375: Training Loss: 0.019565774127840996, Validation Loss: 0.018024306744337082\n",
      "Epoch 23, Batch: 376: Training Loss: 0.02023778110742569, Validation Loss: 0.020366204902529716\n",
      "Epoch 23, Batch: 377: Training Loss: 0.020668910816311836, Validation Loss: 0.020566705614328384\n",
      "Epoch 23, Batch: 378: Training Loss: 0.021161796525120735, Validation Loss: 0.021755117923021317\n",
      "Epoch 23, Batch: 379: Training Loss: 0.022155193611979485, Validation Loss: 0.022185269743204117\n",
      "Epoch 23, Batch: 380: Training Loss: 0.02231757529079914, Validation Loss: 0.0202166847884655\n",
      "Epoch 23, Batch: 381: Training Loss: 0.019002676010131836, Validation Loss: 0.022089509293437004\n",
      "Epoch 23, Batch: 382: Training Loss: 0.021251583471894264, Validation Loss: 0.022672753781080246\n",
      "Epoch 23, Batch: 383: Training Loss: 0.017932599410414696, Validation Loss: 0.021837353706359863\n",
      "Epoch 23, Batch: 384: Training Loss: 0.021425174549221992, Validation Loss: 0.02067386731505394\n",
      "Epoch 23, Batch: 385: Training Loss: 0.020727699622511864, Validation Loss: 0.019827816635370255\n",
      "Epoch 23, Batch: 386: Training Loss: 0.01866479218006134, Validation Loss: 0.02104770950973034\n",
      "Epoch 23, Batch: 387: Training Loss: 0.020290734246373177, Validation Loss: 0.02208307571709156\n",
      "Epoch 23, Batch: 388: Training Loss: 0.020091325044631958, Validation Loss: 0.020195061340928078\n",
      "Epoch 23, Batch: 389: Training Loss: 0.021829068660736084, Validation Loss: 0.02100815437734127\n",
      "Epoch 23, Batch: 390: Training Loss: 0.021779201924800873, Validation Loss: 0.020830994471907616\n",
      "Epoch 23, Batch: 391: Training Loss: 0.02377059869468212, Validation Loss: 0.021673448383808136\n",
      "Epoch 23, Batch: 392: Training Loss: 0.019240308552980423, Validation Loss: 0.022016221657395363\n",
      "Epoch 23, Batch: 393: Training Loss: 0.019946623593568802, Validation Loss: 0.021156728267669678\n",
      "Epoch 23, Batch: 394: Training Loss: 0.019285691902041435, Validation Loss: 0.019669655710458755\n",
      "Epoch 23, Batch: 395: Training Loss: 0.021239327266812325, Validation Loss: 0.021460682153701782\n",
      "Epoch 23, Batch: 396: Training Loss: 0.019955674186348915, Validation Loss: 0.020907698199152946\n",
      "Epoch 23, Batch: 397: Training Loss: 0.018795043230056763, Validation Loss: 0.021538134664297104\n",
      "Epoch 23, Batch: 398: Training Loss: 0.02097916044294834, Validation Loss: 0.022187361493706703\n",
      "Epoch 23, Batch: 399: Training Loss: 0.02125047706067562, Validation Loss: 0.021001791581511497\n",
      "Epoch 23, Batch: 400: Training Loss: 0.022036805748939514, Validation Loss: 0.021199900656938553\n",
      "Epoch 23, Batch: 401: Training Loss: 0.02192840911448002, Validation Loss: 0.02208181656897068\n",
      "Epoch 23, Batch: 402: Training Loss: 0.02077314257621765, Validation Loss: 0.024211788550019264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch: 403: Training Loss: 0.02341495081782341, Validation Loss: 0.020896773785352707\n",
      "Epoch 23, Batch: 404: Training Loss: 0.02088206447660923, Validation Loss: 0.021622920408844948\n",
      "Epoch 23, Batch: 405: Training Loss: 0.023518243804574013, Validation Loss: 0.02144148387014866\n",
      "Epoch 23, Batch: 406: Training Loss: 0.02135957032442093, Validation Loss: 0.022964879870414734\n",
      "Epoch 23, Batch: 407: Training Loss: 0.019477887079119682, Validation Loss: 0.021810773760080338\n",
      "Epoch 23, Batch: 408: Training Loss: 0.019597254693508148, Validation Loss: 0.02101653628051281\n",
      "Epoch 23, Batch: 409: Training Loss: 0.0220653023570776, Validation Loss: 0.02207617089152336\n",
      "Epoch 23, Batch: 410: Training Loss: 0.021400196477770805, Validation Loss: 0.021473877131938934\n",
      "Epoch 23, Batch: 411: Training Loss: 0.022199781611561775, Validation Loss: 0.023647701367735863\n",
      "Epoch 23, Batch: 412: Training Loss: 0.028291458263993263, Validation Loss: 0.025368839502334595\n",
      "Epoch 23, Batch: 413: Training Loss: 0.022621748968958855, Validation Loss: 0.022000160068273544\n",
      "Epoch 23, Batch: 414: Training Loss: 0.02280520275235176, Validation Loss: 0.02355530671775341\n",
      "Epoch 23, Batch: 415: Training Loss: 0.02538936771452427, Validation Loss: 0.02369055524468422\n",
      "Epoch 23, Batch: 416: Training Loss: 0.021222291514277458, Validation Loss: 0.021214496344327927\n",
      "Epoch 23, Batch: 417: Training Loss: 0.02065601386129856, Validation Loss: 0.021974904462695122\n",
      "Epoch 23, Batch: 418: Training Loss: 0.022552017122507095, Validation Loss: 0.023434070870280266\n",
      "Epoch 23, Batch: 419: Training Loss: 0.01915207877755165, Validation Loss: 0.022196494042873383\n",
      "Epoch 23, Batch: 420: Training Loss: 0.023199722170829773, Validation Loss: 0.021505054086446762\n",
      "Epoch 23, Batch: 421: Training Loss: 0.020331021398305893, Validation Loss: 0.020058423280715942\n",
      "Epoch 23, Batch: 422: Training Loss: 0.02458483912050724, Validation Loss: 0.02158873900771141\n",
      "Epoch 23, Batch: 423: Training Loss: 0.022173559293150902, Validation Loss: 0.022488771006464958\n",
      "Epoch 23, Batch: 424: Training Loss: 0.022019289433956146, Validation Loss: 0.02330319955945015\n",
      "Epoch 23, Batch: 425: Training Loss: 0.024043314158916473, Validation Loss: 0.023213179782032967\n",
      "Epoch 23, Batch: 426: Training Loss: 0.02335977926850319, Validation Loss: 0.022038405761122704\n",
      "Epoch 23, Batch: 427: Training Loss: 0.021689238026738167, Validation Loss: 0.022963693365454674\n",
      "Epoch 23, Batch: 428: Training Loss: 0.0202813521027565, Validation Loss: 0.02569657377898693\n",
      "Epoch 23, Batch: 429: Training Loss: 0.01941785030066967, Validation Loss: 0.021805066615343094\n",
      "Epoch 23, Batch: 430: Training Loss: 0.018612923100590706, Validation Loss: 0.019135741516947746\n",
      "Epoch 23, Batch: 431: Training Loss: 0.019288117066025734, Validation Loss: 0.021821698173880577\n",
      "Epoch 23, Batch: 432: Training Loss: 0.021313738077878952, Validation Loss: 0.02187236025929451\n",
      "Epoch 23, Batch: 433: Training Loss: 0.019841689616441727, Validation Loss: 0.021226875483989716\n",
      "Epoch 23, Batch: 434: Training Loss: 0.020078755915164948, Validation Loss: 0.023156534880399704\n",
      "Epoch 23, Batch: 435: Training Loss: 0.0215395987033844, Validation Loss: 0.022328823804855347\n",
      "Epoch 23, Batch: 436: Training Loss: 0.021186932921409607, Validation Loss: 0.02060079574584961\n",
      "Epoch 23, Batch: 437: Training Loss: 0.018277911469340324, Validation Loss: 0.0229632668197155\n",
      "Epoch 23, Batch: 438: Training Loss: 0.020319316536188126, Validation Loss: 0.020842470228672028\n",
      "Epoch 23, Batch: 439: Training Loss: 0.0173640176653862, Validation Loss: 0.020845195278525352\n",
      "Epoch 23, Batch: 440: Training Loss: 0.024747129529714584, Validation Loss: 0.022141730412840843\n",
      "Epoch 23, Batch: 441: Training Loss: 0.02103954367339611, Validation Loss: 0.022565219551324844\n",
      "Epoch 23, Batch: 442: Training Loss: 0.022077711299061775, Validation Loss: 0.021618474274873734\n",
      "Epoch 23, Batch: 443: Training Loss: 0.01878485269844532, Validation Loss: 0.022134125232696533\n",
      "Epoch 23, Batch: 444: Training Loss: 0.01849435642361641, Validation Loss: 0.022011006250977516\n",
      "Epoch 23, Batch: 445: Training Loss: 0.019882600754499435, Validation Loss: 0.023570872843265533\n",
      "Epoch 23, Batch: 446: Training Loss: 0.02210356667637825, Validation Loss: 0.02238859049975872\n",
      "Epoch 23, Batch: 447: Training Loss: 0.021934492513537407, Validation Loss: 0.021176883950829506\n",
      "Epoch 23, Batch: 448: Training Loss: 0.02142985165119171, Validation Loss: 0.02224024012684822\n",
      "Epoch 23, Batch: 449: Training Loss: 0.021877340972423553, Validation Loss: 0.023928780108690262\n",
      "Epoch 23, Batch: 450: Training Loss: 0.020099546760320663, Validation Loss: 0.021235916763544083\n",
      "Epoch 23, Batch: 451: Training Loss: 0.022338436916470528, Validation Loss: 0.021234732121229172\n",
      "Epoch 23, Batch: 452: Training Loss: 0.02020498365163803, Validation Loss: 0.021708788350224495\n",
      "Epoch 23, Batch: 453: Training Loss: 0.023432722315192223, Validation Loss: 0.02384997345507145\n",
      "Epoch 23, Batch: 454: Training Loss: 0.022570528090000153, Validation Loss: 0.022700859233736992\n",
      "Epoch 23, Batch: 455: Training Loss: 0.022516902536153793, Validation Loss: 0.021853206679224968\n",
      "Epoch 23, Batch: 456: Training Loss: 0.018666403368115425, Validation Loss: 0.022453397512435913\n",
      "Epoch 23, Batch: 457: Training Loss: 0.023531250655651093, Validation Loss: 0.021962115541100502\n",
      "Epoch 23, Batch: 458: Training Loss: 0.021660495549440384, Validation Loss: 0.022394463419914246\n",
      "Epoch 23, Batch: 459: Training Loss: 0.02070971019566059, Validation Loss: 0.020558776333928108\n",
      "Epoch 23, Batch: 460: Training Loss: 0.019373074173927307, Validation Loss: 0.01924731768667698\n",
      "Epoch 23, Batch: 461: Training Loss: 0.022651003673672676, Validation Loss: 0.021116510033607483\n",
      "Epoch 23, Batch: 462: Training Loss: 0.017583178356289864, Validation Loss: 0.020954638719558716\n",
      "Epoch 23, Batch: 463: Training Loss: 0.022360412403941154, Validation Loss: 0.021397583186626434\n",
      "Epoch 23, Batch: 464: Training Loss: 0.019956141710281372, Validation Loss: 0.022822527214884758\n",
      "Epoch 23, Batch: 465: Training Loss: 0.01953641138970852, Validation Loss: 0.01946144923567772\n",
      "Epoch 23, Batch: 466: Training Loss: 0.02001856081187725, Validation Loss: 0.022783612832427025\n",
      "Epoch 23, Batch: 467: Training Loss: 0.024444743990898132, Validation Loss: 0.022197358310222626\n",
      "Epoch 23, Batch: 468: Training Loss: 0.02233530394732952, Validation Loss: 0.020947908982634544\n",
      "Epoch 23, Batch: 469: Training Loss: 0.020707447081804276, Validation Loss: 0.02311793714761734\n",
      "Epoch 23, Batch: 470: Training Loss: 0.01880607195198536, Validation Loss: 0.023432627320289612\n",
      "Epoch 23, Batch: 471: Training Loss: 0.021154947578907013, Validation Loss: 0.02377353049814701\n",
      "Epoch 23, Batch: 472: Training Loss: 0.01956220716238022, Validation Loss: 0.021460389718413353\n",
      "Epoch 23, Batch: 473: Training Loss: 0.022217389196157455, Validation Loss: 0.019619902595877647\n",
      "Epoch 23, Batch: 474: Training Loss: 0.020732415840029716, Validation Loss: 0.02174314670264721\n",
      "Epoch 23, Batch: 475: Training Loss: 0.021947195753455162, Validation Loss: 0.022149769589304924\n",
      "Epoch 23, Batch: 476: Training Loss: 0.018603328615427017, Validation Loss: 0.022785188630223274\n",
      "Epoch 23, Batch: 477: Training Loss: 0.024775629863142967, Validation Loss: 0.0225025974214077\n",
      "Epoch 23, Batch: 478: Training Loss: 0.022152135148644447, Validation Loss: 0.021851785480976105\n",
      "Epoch 23, Batch: 479: Training Loss: 0.0204809308052063, Validation Loss: 0.02091180346906185\n",
      "Epoch 23, Batch: 480: Training Loss: 0.02321871928870678, Validation Loss: 0.022510897368192673\n",
      "Epoch 23, Batch: 481: Training Loss: 0.018066227436065674, Validation Loss: 0.021317949518561363\n",
      "Epoch 23, Batch: 482: Training Loss: 0.024813536554574966, Validation Loss: 0.02238890528678894\n",
      "Epoch 23, Batch: 483: Training Loss: 0.021433304995298386, Validation Loss: 0.020708341151475906\n",
      "Epoch 23, Batch: 484: Training Loss: 0.024184314534068108, Validation Loss: 0.02207571268081665\n",
      "Epoch 23, Batch: 485: Training Loss: 0.02436009794473648, Validation Loss: 0.021954737603664398\n",
      "Epoch 23, Batch: 486: Training Loss: 0.02563958242535591, Validation Loss: 0.024156199768185616\n",
      "Epoch 23, Batch: 487: Training Loss: 0.020133627578616142, Validation Loss: 0.022967705503106117\n",
      "Epoch 23, Batch: 488: Training Loss: 0.023834431543946266, Validation Loss: 0.021729452535510063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch: 489: Training Loss: 0.02588260918855667, Validation Loss: 0.02206435613334179\n",
      "Epoch 23, Batch: 490: Training Loss: 0.02542925998568535, Validation Loss: 0.021417735144495964\n",
      "Epoch 23, Batch: 491: Training Loss: 0.019765008240938187, Validation Loss: 0.019936420023441315\n",
      "Epoch 23, Batch: 492: Training Loss: 0.024373048916459084, Validation Loss: 0.020575985312461853\n",
      "Epoch 23, Batch: 493: Training Loss: 0.022377753630280495, Validation Loss: 0.021594922989606857\n",
      "Epoch 23, Batch: 494: Training Loss: 0.024433279410004616, Validation Loss: 0.020869867876172066\n",
      "Epoch 23, Batch: 495: Training Loss: 0.0192782673984766, Validation Loss: 0.02240137755870819\n",
      "Epoch 23, Batch: 496: Training Loss: 0.018435176461935043, Validation Loss: 0.022144749760627747\n",
      "Epoch 23, Batch: 497: Training Loss: 0.02124796248972416, Validation Loss: 0.02307090535759926\n",
      "Epoch 23, Batch: 498: Training Loss: 0.02324618771672249, Validation Loss: 0.021837487816810608\n",
      "Epoch 23, Batch: 499: Training Loss: 0.021047119051218033, Validation Loss: 0.020973138511180878\n",
      "Epoch 24, Batch: 0: Training Loss: 0.021437473595142365, Validation Loss: 0.023799123242497444\n",
      "Epoch 24, Batch: 1: Training Loss: 0.01899629645049572, Validation Loss: 0.024311400949954987\n",
      "Epoch 24, Batch: 2: Training Loss: 0.024127067998051643, Validation Loss: 0.022834526374936104\n",
      "Epoch 24, Batch: 3: Training Loss: 0.021241718903183937, Validation Loss: 0.023975346237421036\n",
      "Epoch 24, Batch: 4: Training Loss: 0.019469665363430977, Validation Loss: 0.02418431080877781\n",
      "Epoch 24, Batch: 5: Training Loss: 0.020951518788933754, Validation Loss: 0.021538490429520607\n",
      "Epoch 24, Batch: 6: Training Loss: 0.02258852869272232, Validation Loss: 0.020950408652424812\n",
      "Epoch 24, Batch: 7: Training Loss: 0.023367054760456085, Validation Loss: 0.022327223792672157\n",
      "Epoch 24, Batch: 8: Training Loss: 0.021647946909070015, Validation Loss: 0.02425951510667801\n",
      "Epoch 24, Batch: 9: Training Loss: 0.02394934929907322, Validation Loss: 0.02436838112771511\n",
      "Epoch 24, Batch: 10: Training Loss: 0.019026678055524826, Validation Loss: 0.021681418642401695\n",
      "Epoch 24, Batch: 11: Training Loss: 0.024152394384145737, Validation Loss: 0.023691315203905106\n",
      "Epoch 24, Batch: 12: Training Loss: 0.0215130764991045, Validation Loss: 0.022661542519927025\n",
      "Epoch 24, Batch: 13: Training Loss: 0.02465767040848732, Validation Loss: 0.022280685603618622\n",
      "Epoch 24, Batch: 14: Training Loss: 0.022766901180148125, Validation Loss: 0.02070644125342369\n",
      "Epoch 24, Batch: 15: Training Loss: 0.021859893575310707, Validation Loss: 0.02003917284309864\n",
      "Epoch 24, Batch: 16: Training Loss: 0.02173255756497383, Validation Loss: 0.02016816847026348\n",
      "Epoch 24, Batch: 17: Training Loss: 0.021239126101136208, Validation Loss: 0.020564695820212364\n",
      "Epoch 24, Batch: 18: Training Loss: 0.0193092692643404, Validation Loss: 0.021406035870313644\n",
      "Epoch 24, Batch: 19: Training Loss: 0.022070329636335373, Validation Loss: 0.02325105294585228\n",
      "Epoch 24, Batch: 20: Training Loss: 0.02085217647254467, Validation Loss: 0.023297976702451706\n",
      "Epoch 24, Batch: 21: Training Loss: 0.02103588730096817, Validation Loss: 0.024780642241239548\n",
      "Epoch 24, Batch: 22: Training Loss: 0.02215275913476944, Validation Loss: 0.022778520360589027\n",
      "Epoch 24, Batch: 23: Training Loss: 0.019477572292089462, Validation Loss: 0.022905170917510986\n",
      "Epoch 24, Batch: 24: Training Loss: 0.024430714547634125, Validation Loss: 0.02290191501379013\n",
      "Epoch 24, Batch: 25: Training Loss: 0.020005328580737114, Validation Loss: 0.023527806624770164\n",
      "Epoch 24, Batch: 26: Training Loss: 0.021404217928647995, Validation Loss: 0.022761637344956398\n",
      "Epoch 24, Batch: 27: Training Loss: 0.02074694260954857, Validation Loss: 0.021924883127212524\n",
      "Epoch 24, Batch: 28: Training Loss: 0.023085614666342735, Validation Loss: 0.023894235491752625\n",
      "Epoch 24, Batch: 29: Training Loss: 0.022719530388712883, Validation Loss: 0.02416340447962284\n",
      "Epoch 24, Batch: 30: Training Loss: 0.019877005368471146, Validation Loss: 0.02273835428059101\n",
      "Epoch 24, Batch: 31: Training Loss: 0.02242681384086609, Validation Loss: 0.02190335839986801\n",
      "Epoch 24, Batch: 32: Training Loss: 0.021307891234755516, Validation Loss: 0.020698416978120804\n",
      "Epoch 24, Batch: 33: Training Loss: 0.021038589999079704, Validation Loss: 0.02415122650563717\n",
      "Epoch 24, Batch: 34: Training Loss: 0.020616499707102776, Validation Loss: 0.02149772085249424\n",
      "Epoch 24, Batch: 35: Training Loss: 0.020590653643012047, Validation Loss: 0.02127034030854702\n",
      "Epoch 24, Batch: 36: Training Loss: 0.019777091220021248, Validation Loss: 0.02211209200322628\n",
      "Epoch 24, Batch: 37: Training Loss: 0.025733843445777893, Validation Loss: 0.023076854646205902\n",
      "Epoch 24, Batch: 38: Training Loss: 0.021857449784874916, Validation Loss: 0.022717956453561783\n",
      "Epoch 24, Batch: 39: Training Loss: 0.020147988572716713, Validation Loss: 0.023395607247948647\n",
      "Epoch 24, Batch: 40: Training Loss: 0.020399246364831924, Validation Loss: 0.023247985169291496\n",
      "Epoch 24, Batch: 41: Training Loss: 0.02191740833222866, Validation Loss: 0.022132856771349907\n",
      "Epoch 24, Batch: 42: Training Loss: 0.018435830250382423, Validation Loss: 0.024646751582622528\n",
      "Epoch 24, Batch: 43: Training Loss: 0.017737049609422684, Validation Loss: 0.02288946323096752\n",
      "Epoch 24, Batch: 44: Training Loss: 0.022303031757473946, Validation Loss: 0.022949788719415665\n",
      "Epoch 24, Batch: 45: Training Loss: 0.01855139061808586, Validation Loss: 0.021062174811959267\n",
      "Epoch 24, Batch: 46: Training Loss: 0.02389366552233696, Validation Loss: 0.024343546479940414\n",
      "Epoch 24, Batch: 47: Training Loss: 0.023218106478452682, Validation Loss: 0.020571330562233925\n",
      "Epoch 24, Batch: 48: Training Loss: 0.026762792840600014, Validation Loss: 0.023762477561831474\n",
      "Epoch 24, Batch: 49: Training Loss: 0.026175914332270622, Validation Loss: 0.02532016858458519\n",
      "Epoch 24, Batch: 50: Training Loss: 0.02037167362868786, Validation Loss: 0.024183550849556923\n",
      "Epoch 24, Batch: 51: Training Loss: 0.023521678522229195, Validation Loss: 0.02413373440504074\n",
      "Epoch 24, Batch: 52: Training Loss: 0.01976398192346096, Validation Loss: 0.021786848083138466\n",
      "Epoch 24, Batch: 53: Training Loss: 0.0190451517701149, Validation Loss: 0.023668471723794937\n",
      "Epoch 24, Batch: 54: Training Loss: 0.025680338963866234, Validation Loss: 0.023625006899237633\n",
      "Epoch 24, Batch: 55: Training Loss: 0.019018100574612617, Validation Loss: 0.024454912170767784\n",
      "Epoch 24, Batch: 56: Training Loss: 0.022349558770656586, Validation Loss: 0.021692238748073578\n",
      "Epoch 24, Batch: 57: Training Loss: 0.02088753506541252, Validation Loss: 0.021745959296822548\n",
      "Epoch 24, Batch: 58: Training Loss: 0.024879030883312225, Validation Loss: 0.023169895634055138\n",
      "Epoch 24, Batch: 59: Training Loss: 0.01818755269050598, Validation Loss: 0.020883524790406227\n",
      "Epoch 24, Batch: 60: Training Loss: 0.02116578444838524, Validation Loss: 0.021108342334628105\n",
      "Epoch 24, Batch: 61: Training Loss: 0.02035614848136902, Validation Loss: 0.02188980020582676\n",
      "Epoch 24, Batch: 62: Training Loss: 0.022361665964126587, Validation Loss: 0.02341851033270359\n",
      "Epoch 24, Batch: 63: Training Loss: 0.022864539176225662, Validation Loss: 0.021359236910939217\n",
      "Epoch 24, Batch: 64: Training Loss: 0.021756378933787346, Validation Loss: 0.0205885861068964\n",
      "Epoch 24, Batch: 65: Training Loss: 0.02167905867099762, Validation Loss: 0.023817671462893486\n",
      "Epoch 24, Batch: 66: Training Loss: 0.021814050152897835, Validation Loss: 0.022222187370061874\n",
      "Epoch 24, Batch: 67: Training Loss: 0.020745065063238144, Validation Loss: 0.022445078939199448\n",
      "Epoch 24, Batch: 68: Training Loss: 0.020466091111302376, Validation Loss: 0.021682018414139748\n",
      "Epoch 24, Batch: 69: Training Loss: 0.02094484679400921, Validation Loss: 0.020546384155750275\n",
      "Epoch 24, Batch: 70: Training Loss: 0.022757340222597122, Validation Loss: 0.022352656349539757\n",
      "Epoch 24, Batch: 71: Training Loss: 0.019038617610931396, Validation Loss: 0.022567590698599815\n",
      "Epoch 24, Batch: 72: Training Loss: 0.019961249083280563, Validation Loss: 0.019294561818242073\n",
      "Epoch 24, Batch: 73: Training Loss: 0.0218446496874094, Validation Loss: 0.021072188392281532\n",
      "Epoch 24, Batch: 74: Training Loss: 0.01897927187383175, Validation Loss: 0.020070096477866173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch: 75: Training Loss: 0.01734512485563755, Validation Loss: 0.022092748433351517\n",
      "Epoch 24, Batch: 76: Training Loss: 0.01922483555972576, Validation Loss: 0.020769303664565086\n",
      "Epoch 24, Batch: 77: Training Loss: 0.022271092981100082, Validation Loss: 0.02263071946799755\n",
      "Epoch 24, Batch: 78: Training Loss: 0.02286989986896515, Validation Loss: 0.019633091986179352\n",
      "Epoch 24, Batch: 79: Training Loss: 0.020723417401313782, Validation Loss: 0.024241482838988304\n",
      "Epoch 24, Batch: 80: Training Loss: 0.019936589524149895, Validation Loss: 0.02113262191414833\n",
      "Epoch 24, Batch: 81: Training Loss: 0.021328408271074295, Validation Loss: 0.0228574201464653\n",
      "Epoch 24, Batch: 82: Training Loss: 0.021015089005231857, Validation Loss: 0.021741516888141632\n",
      "Epoch 24, Batch: 83: Training Loss: 0.020158927887678146, Validation Loss: 0.022015344351530075\n",
      "Epoch 24, Batch: 84: Training Loss: 0.020068474113941193, Validation Loss: 0.022603671997785568\n",
      "Epoch 24, Batch: 85: Training Loss: 0.020909802988171577, Validation Loss: 0.02168930694460869\n",
      "Epoch 24, Batch: 86: Training Loss: 0.02288268879055977, Validation Loss: 0.02187102846801281\n",
      "Epoch 24, Batch: 87: Training Loss: 0.02234211377799511, Validation Loss: 0.023382795974612236\n",
      "Epoch 24, Batch: 88: Training Loss: 0.025085046887397766, Validation Loss: 0.02218315377831459\n",
      "Epoch 24, Batch: 89: Training Loss: 0.02584555745124817, Validation Loss: 0.022948630154132843\n",
      "Epoch 24, Batch: 90: Training Loss: 0.02402636408805847, Validation Loss: 0.02352599985897541\n",
      "Epoch 24, Batch: 91: Training Loss: 0.02332734316587448, Validation Loss: 0.021138740703463554\n",
      "Epoch 24, Batch: 92: Training Loss: 0.023844266310334206, Validation Loss: 0.022413548082113266\n",
      "Epoch 24, Batch: 93: Training Loss: 0.022694341838359833, Validation Loss: 0.022769233211874962\n",
      "Epoch 24, Batch: 94: Training Loss: 0.02406942844390869, Validation Loss: 0.02200373075902462\n",
      "Epoch 24, Batch: 95: Training Loss: 0.019102727994322777, Validation Loss: 0.022556157782673836\n",
      "Epoch 24, Batch: 96: Training Loss: 0.019740041345357895, Validation Loss: 0.020456716418266296\n",
      "Epoch 24, Batch: 97: Training Loss: 0.021908462047576904, Validation Loss: 0.020407114177942276\n",
      "Epoch 24, Batch: 98: Training Loss: 0.021202538162469864, Validation Loss: 0.02199835702776909\n",
      "Epoch 24, Batch: 99: Training Loss: 0.026647377759218216, Validation Loss: 0.02337304688990116\n",
      "Epoch 24, Batch: 100: Training Loss: 0.024341227486729622, Validation Loss: 0.023033518344163895\n",
      "Epoch 24, Batch: 101: Training Loss: 0.021923519670963287, Validation Loss: 0.023096341639757156\n",
      "Epoch 24, Batch: 102: Training Loss: 0.024558017030358315, Validation Loss: 0.02362644672393799\n",
      "Epoch 24, Batch: 103: Training Loss: 0.022873414680361748, Validation Loss: 0.024062080308794975\n",
      "Epoch 24, Batch: 104: Training Loss: 0.023424919694662094, Validation Loss: 0.023293014615774155\n",
      "Epoch 24, Batch: 105: Training Loss: 0.020534124225378036, Validation Loss: 0.022896120324730873\n",
      "Epoch 24, Batch: 106: Training Loss: 0.021912874653935432, Validation Loss: 0.02426101453602314\n",
      "Epoch 24, Batch: 107: Training Loss: 0.021734081208705902, Validation Loss: 0.022358225658535957\n",
      "Epoch 24, Batch: 108: Training Loss: 0.022203339263796806, Validation Loss: 0.02280593477189541\n",
      "Epoch 24, Batch: 109: Training Loss: 0.024940496310591698, Validation Loss: 0.022295119240880013\n",
      "Epoch 24, Batch: 110: Training Loss: 0.023733997717499733, Validation Loss: 0.024125387892127037\n",
      "Epoch 24, Batch: 111: Training Loss: 0.020899929106235504, Validation Loss: 0.024513639509677887\n",
      "Epoch 24, Batch: 112: Training Loss: 0.02147858403623104, Validation Loss: 0.02382362261414528\n",
      "Epoch 24, Batch: 113: Training Loss: 0.022826844826340675, Validation Loss: 0.024573424831032753\n",
      "Epoch 24, Batch: 114: Training Loss: 0.023750660941004753, Validation Loss: 0.02497255802154541\n",
      "Epoch 24, Batch: 115: Training Loss: 0.026923514902591705, Validation Loss: 0.027547476813197136\n",
      "Epoch 24, Batch: 116: Training Loss: 0.022199703380465508, Validation Loss: 0.024477669969201088\n",
      "Epoch 24, Batch: 117: Training Loss: 0.02038620598614216, Validation Loss: 0.023870186880230904\n",
      "Epoch 24, Batch: 118: Training Loss: 0.019764559343457222, Validation Loss: 0.024687407538294792\n",
      "Epoch 24, Batch: 119: Training Loss: 0.02124933712184429, Validation Loss: 0.023622071370482445\n",
      "Epoch 24, Batch: 120: Training Loss: 0.02175334468483925, Validation Loss: 0.02337183803319931\n",
      "Epoch 24, Batch: 121: Training Loss: 0.021834364160895348, Validation Loss: 0.023478152230381966\n",
      "Epoch 24, Batch: 122: Training Loss: 0.02346421591937542, Validation Loss: 0.02364766225218773\n",
      "Epoch 24, Batch: 123: Training Loss: 0.02520645409822464, Validation Loss: 0.024297572672367096\n",
      "Epoch 24, Batch: 124: Training Loss: 0.02363392524421215, Validation Loss: 0.02410341054201126\n",
      "Epoch 24, Batch: 125: Training Loss: 0.024388384073972702, Validation Loss: 0.02666553109884262\n",
      "Epoch 24, Batch: 126: Training Loss: 0.025311918929219246, Validation Loss: 0.024581678211688995\n",
      "Epoch 24, Batch: 127: Training Loss: 0.025483954697847366, Validation Loss: 0.022249385714530945\n",
      "Epoch 24, Batch: 128: Training Loss: 0.02081383392214775, Validation Loss: 0.023740805685520172\n",
      "Epoch 24, Batch: 129: Training Loss: 0.01840132288634777, Validation Loss: 0.023258626461029053\n",
      "Epoch 24, Batch: 130: Training Loss: 0.024346973747015, Validation Loss: 0.023112673312425613\n",
      "Epoch 24, Batch: 131: Training Loss: 0.02904493920505047, Validation Loss: 0.024253129959106445\n",
      "Epoch 24, Batch: 132: Training Loss: 0.02173227071762085, Validation Loss: 0.022734174504876137\n",
      "Epoch 24, Batch: 133: Training Loss: 0.02412273734807968, Validation Loss: 0.021501798182725906\n",
      "Epoch 24, Batch: 134: Training Loss: 0.023737404495477676, Validation Loss: 0.023129453882575035\n",
      "Epoch 24, Batch: 135: Training Loss: 0.02402873896062374, Validation Loss: 0.02194466069340706\n",
      "Epoch 24, Batch: 136: Training Loss: 0.021031644195318222, Validation Loss: 0.02213655412197113\n",
      "Epoch 24, Batch: 137: Training Loss: 0.01918383315205574, Validation Loss: 0.021184898912906647\n",
      "Epoch 24, Batch: 138: Training Loss: 0.020006973296403885, Validation Loss: 0.021334504708647728\n",
      "Epoch 24, Batch: 139: Training Loss: 0.020034272223711014, Validation Loss: 0.023001709952950478\n",
      "Epoch 24, Batch: 140: Training Loss: 0.025780683383345604, Validation Loss: 0.023142579942941666\n",
      "Epoch 24, Batch: 141: Training Loss: 0.021190600469708443, Validation Loss: 0.022359609603881836\n",
      "Epoch 24, Batch: 142: Training Loss: 0.020240381360054016, Validation Loss: 0.02323249913752079\n",
      "Epoch 24, Batch: 143: Training Loss: 0.020620673894882202, Validation Loss: 0.023294897750020027\n",
      "Epoch 24, Batch: 144: Training Loss: 0.019164644181728363, Validation Loss: 0.022741489112377167\n",
      "Epoch 24, Batch: 145: Training Loss: 0.021488996222615242, Validation Loss: 0.022974008694291115\n",
      "Epoch 24, Batch: 146: Training Loss: 0.022410821169614792, Validation Loss: 0.020208314061164856\n",
      "Epoch 24, Batch: 147: Training Loss: 0.01957976073026657, Validation Loss: 0.02189934067428112\n",
      "Epoch 24, Batch: 148: Training Loss: 0.02292371727526188, Validation Loss: 0.022004852071404457\n",
      "Epoch 24, Batch: 149: Training Loss: 0.024360492825508118, Validation Loss: 0.020598361268639565\n",
      "Epoch 24, Batch: 150: Training Loss: 0.026907682418823242, Validation Loss: 0.02098153531551361\n",
      "Epoch 24, Batch: 151: Training Loss: 0.020984062924981117, Validation Loss: 0.02059437520802021\n",
      "Epoch 24, Batch: 152: Training Loss: 0.02044454962015152, Validation Loss: 0.02186683379113674\n",
      "Epoch 24, Batch: 153: Training Loss: 0.01987905241549015, Validation Loss: 0.022799689322710037\n",
      "Epoch 24, Batch: 154: Training Loss: 0.02173922210931778, Validation Loss: 0.02096206322312355\n",
      "Epoch 24, Batch: 155: Training Loss: 0.026236508041620255, Validation Loss: 0.021131500601768494\n",
      "Epoch 24, Batch: 156: Training Loss: 0.016603469848632812, Validation Loss: 0.0213026013225317\n",
      "Epoch 24, Batch: 157: Training Loss: 0.019169362261891365, Validation Loss: 0.02021300606429577\n",
      "Epoch 24, Batch: 158: Training Loss: 0.022011781111359596, Validation Loss: 0.02130317874252796\n",
      "Epoch 24, Batch: 159: Training Loss: 0.023000769317150116, Validation Loss: 0.021250326186418533\n",
      "Epoch 24, Batch: 160: Training Loss: 0.02103409171104431, Validation Loss: 0.022424791008234024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch: 161: Training Loss: 0.024189110845327377, Validation Loss: 0.02131281979382038\n",
      "Epoch 24, Batch: 162: Training Loss: 0.02485402673482895, Validation Loss: 0.021259382367134094\n",
      "Epoch 24, Batch: 163: Training Loss: 0.020850440487265587, Validation Loss: 0.02129778079688549\n",
      "Epoch 24, Batch: 164: Training Loss: 0.021549042314291, Validation Loss: 0.021206093952059746\n",
      "Epoch 24, Batch: 165: Training Loss: 0.02281888946890831, Validation Loss: 0.020262500271201134\n",
      "Epoch 24, Batch: 166: Training Loss: 0.019526029005646706, Validation Loss: 0.021408025175333023\n",
      "Epoch 24, Batch: 167: Training Loss: 0.018422966822981834, Validation Loss: 0.021968258544802666\n",
      "Epoch 24, Batch: 168: Training Loss: 0.02129758894443512, Validation Loss: 0.023921620100736618\n",
      "Epoch 24, Batch: 169: Training Loss: 0.02331802248954773, Validation Loss: 0.024177834391593933\n",
      "Epoch 24, Batch: 170: Training Loss: 0.020742738619446754, Validation Loss: 0.02418656274676323\n",
      "Epoch 24, Batch: 171: Training Loss: 0.020581400021910667, Validation Loss: 0.022105148062109947\n",
      "Epoch 24, Batch: 172: Training Loss: 0.0203353613615036, Validation Loss: 0.024916579946875572\n",
      "Epoch 24, Batch: 173: Training Loss: 0.020832788199186325, Validation Loss: 0.022326460108160973\n",
      "Epoch 24, Batch: 174: Training Loss: 0.019854117184877396, Validation Loss: 0.023934053257107735\n",
      "Epoch 24, Batch: 175: Training Loss: 0.020312724635004997, Validation Loss: 0.025792773813009262\n",
      "Epoch 24, Batch: 176: Training Loss: 0.023555222898721695, Validation Loss: 0.022732902318239212\n",
      "Epoch 24, Batch: 177: Training Loss: 0.020054716616868973, Validation Loss: 0.022722609341144562\n",
      "Epoch 24, Batch: 178: Training Loss: 0.02474018931388855, Validation Loss: 0.021564694121479988\n",
      "Epoch 24, Batch: 179: Training Loss: 0.02136385627090931, Validation Loss: 0.02297714538872242\n",
      "Epoch 24, Batch: 180: Training Loss: 0.0246316809207201, Validation Loss: 0.02334626577794552\n",
      "Epoch 24, Batch: 181: Training Loss: 0.02024901658296585, Validation Loss: 0.023074885830283165\n",
      "Epoch 24, Batch: 182: Training Loss: 0.023939555510878563, Validation Loss: 0.024198215454816818\n",
      "Epoch 24, Batch: 183: Training Loss: 0.02043807879090309, Validation Loss: 0.02274908684194088\n",
      "Epoch 24, Batch: 184: Training Loss: 0.022081585600972176, Validation Loss: 0.021215174347162247\n",
      "Epoch 24, Batch: 185: Training Loss: 0.02323930896818638, Validation Loss: 0.021282613277435303\n",
      "Epoch 24, Batch: 186: Training Loss: 0.021977676078677177, Validation Loss: 0.020602673292160034\n",
      "Epoch 24, Batch: 187: Training Loss: 0.01936134323477745, Validation Loss: 0.021446090191602707\n",
      "Epoch 24, Batch: 188: Training Loss: 0.02205972746014595, Validation Loss: 0.021199790760874748\n",
      "Epoch 24, Batch: 189: Training Loss: 0.020907564088702202, Validation Loss: 0.021187830716371536\n",
      "Epoch 24, Batch: 190: Training Loss: 0.023657333105802536, Validation Loss: 0.022563284263014793\n",
      "Epoch 24, Batch: 191: Training Loss: 0.02299971878528595, Validation Loss: 0.02191876247525215\n",
      "Epoch 24, Batch: 192: Training Loss: 0.021137170493602753, Validation Loss: 0.021088555455207825\n",
      "Epoch 24, Batch: 193: Training Loss: 0.02084636501967907, Validation Loss: 0.021418455988168716\n",
      "Epoch 24, Batch: 194: Training Loss: 0.022163046523928642, Validation Loss: 0.021663978695869446\n",
      "Epoch 24, Batch: 195: Training Loss: 0.021305598318576813, Validation Loss: 0.02033369429409504\n",
      "Epoch 24, Batch: 196: Training Loss: 0.022475969046354294, Validation Loss: 0.022192472591996193\n",
      "Epoch 24, Batch: 197: Training Loss: 0.018918784335255623, Validation Loss: 0.023069534450769424\n",
      "Epoch 24, Batch: 198: Training Loss: 0.020267151296138763, Validation Loss: 0.025535235181450844\n",
      "Epoch 24, Batch: 199: Training Loss: 0.02076129987835884, Validation Loss: 0.024057680740952492\n",
      "Epoch 24, Batch: 200: Training Loss: 0.017574895173311234, Validation Loss: 0.025549305602908134\n",
      "Epoch 24, Batch: 201: Training Loss: 0.026289677247405052, Validation Loss: 0.02384175918996334\n",
      "Epoch 24, Batch: 202: Training Loss: 0.02278664894402027, Validation Loss: 0.0219093207269907\n",
      "Epoch 24, Batch: 203: Training Loss: 0.022435732185840607, Validation Loss: 0.02294231578707695\n",
      "Epoch 24, Batch: 204: Training Loss: 0.021057523787021637, Validation Loss: 0.022334257140755653\n",
      "Epoch 24, Batch: 205: Training Loss: 0.024005040526390076, Validation Loss: 0.02093628980219364\n",
      "Epoch 24, Batch: 206: Training Loss: 0.022040653973817825, Validation Loss: 0.021290291100740433\n",
      "Epoch 24, Batch: 207: Training Loss: 0.024144530296325684, Validation Loss: 0.02313903532922268\n",
      "Epoch 24, Batch: 208: Training Loss: 0.020176846534013748, Validation Loss: 0.02570575848221779\n",
      "Epoch 24, Batch: 209: Training Loss: 0.020616788417100906, Validation Loss: 0.023333681747317314\n",
      "Epoch 24, Batch: 210: Training Loss: 0.02325494773685932, Validation Loss: 0.023457985371351242\n",
      "Epoch 24, Batch: 211: Training Loss: 0.021479599177837372, Validation Loss: 0.022919073700904846\n",
      "Epoch 24, Batch: 212: Training Loss: 0.024182835593819618, Validation Loss: 0.024186518043279648\n",
      "Epoch 24, Batch: 213: Training Loss: 0.026292627677321434, Validation Loss: 0.0242904219776392\n",
      "Epoch 24, Batch: 214: Training Loss: 0.02183510735630989, Validation Loss: 0.024714099243283272\n",
      "Epoch 24, Batch: 215: Training Loss: 0.02109929360449314, Validation Loss: 0.02486223168671131\n",
      "Epoch 24, Batch: 216: Training Loss: 0.020204834640026093, Validation Loss: 0.023284129798412323\n",
      "Epoch 24, Batch: 217: Training Loss: 0.022560250014066696, Validation Loss: 0.023658456280827522\n",
      "Epoch 24, Batch: 218: Training Loss: 0.019473811611533165, Validation Loss: 0.0223716851323843\n",
      "Epoch 24, Batch: 219: Training Loss: 0.020818592980504036, Validation Loss: 0.02325991354882717\n",
      "Epoch 24, Batch: 220: Training Loss: 0.021500099450349808, Validation Loss: 0.025181572884321213\n",
      "Epoch 24, Batch: 221: Training Loss: 0.018582502380013466, Validation Loss: 0.02329966053366661\n",
      "Epoch 24, Batch: 222: Training Loss: 0.021557040512561798, Validation Loss: 0.020706450566649437\n",
      "Epoch 24, Batch: 223: Training Loss: 0.021168269217014313, Validation Loss: 0.02212938852608204\n",
      "Epoch 24, Batch: 224: Training Loss: 0.02251570113003254, Validation Loss: 0.02233656868338585\n",
      "Epoch 24, Batch: 225: Training Loss: 0.02177632972598076, Validation Loss: 0.022424470633268356\n",
      "Epoch 24, Batch: 226: Training Loss: 0.022202543914318085, Validation Loss: 0.02323395013809204\n",
      "Epoch 24, Batch: 227: Training Loss: 0.022588461637496948, Validation Loss: 0.024484284222126007\n",
      "Epoch 24, Batch: 228: Training Loss: 0.023035431280732155, Validation Loss: 0.026390844956040382\n",
      "Epoch 24, Batch: 229: Training Loss: 0.02495252713561058, Validation Loss: 0.022068722173571587\n",
      "Epoch 24, Batch: 230: Training Loss: 0.02204427309334278, Validation Loss: 0.02288019098341465\n",
      "Epoch 24, Batch: 231: Training Loss: 0.026842009276151657, Validation Loss: 0.024029875174164772\n",
      "Epoch 24, Batch: 232: Training Loss: 0.02281966619193554, Validation Loss: 0.02288823015987873\n",
      "Epoch 24, Batch: 233: Training Loss: 0.02070852369070053, Validation Loss: 0.024247195571660995\n",
      "Epoch 24, Batch: 234: Training Loss: 0.02361878752708435, Validation Loss: 0.025237374007701874\n",
      "Epoch 24, Batch: 235: Training Loss: 0.024724557995796204, Validation Loss: 0.023073310032486916\n",
      "Epoch 24, Batch: 236: Training Loss: 0.023749178275465965, Validation Loss: 0.022320326417684555\n",
      "Epoch 24, Batch: 237: Training Loss: 0.02005290798842907, Validation Loss: 0.021763762459158897\n",
      "Epoch 24, Batch: 238: Training Loss: 0.023898955434560776, Validation Loss: 0.023896878585219383\n",
      "Epoch 24, Batch: 239: Training Loss: 0.02002011425793171, Validation Loss: 0.024805942550301552\n",
      "Epoch 24, Batch: 240: Training Loss: 0.02363544888794422, Validation Loss: 0.02370472066104412\n",
      "Epoch 24, Batch: 241: Training Loss: 0.020770050585269928, Validation Loss: 0.025254352018237114\n",
      "Epoch 24, Batch: 242: Training Loss: 0.021056560799479485, Validation Loss: 0.026539817452430725\n",
      "Epoch 24, Batch: 243: Training Loss: 0.023072732612490654, Validation Loss: 0.02491343393921852\n",
      "Epoch 24, Batch: 244: Training Loss: 0.021337680518627167, Validation Loss: 0.024340182542800903\n",
      "Epoch 24, Batch: 245: Training Loss: 0.022191645577549934, Validation Loss: 0.024024179205298424\n",
      "Epoch 24, Batch: 246: Training Loss: 0.02031918801367283, Validation Loss: 0.02497928775846958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch: 247: Training Loss: 0.022422637790441513, Validation Loss: 0.023127596825361252\n",
      "Epoch 24, Batch: 248: Training Loss: 0.021002473309636116, Validation Loss: 0.025384236127138138\n",
      "Epoch 24, Batch: 249: Training Loss: 0.01839236170053482, Validation Loss: 0.021428605541586876\n",
      "Epoch 24, Batch: 250: Training Loss: 0.016835343092679977, Validation Loss: 0.024558639153838158\n",
      "Epoch 24, Batch: 251: Training Loss: 0.017333881929516792, Validation Loss: 0.024491997435688972\n",
      "Epoch 24, Batch: 252: Training Loss: 0.02619345299899578, Validation Loss: 0.025730352848768234\n",
      "Epoch 24, Batch: 253: Training Loss: 0.020178809762001038, Validation Loss: 0.024477394297719002\n",
      "Epoch 24, Batch: 254: Training Loss: 0.019671784713864326, Validation Loss: 0.024351326748728752\n",
      "Epoch 24, Batch: 255: Training Loss: 0.023786943405866623, Validation Loss: 0.024155626073479652\n",
      "Epoch 24, Batch: 256: Training Loss: 0.024936996400356293, Validation Loss: 0.021359074860811234\n",
      "Epoch 24, Batch: 257: Training Loss: 0.021652551367878914, Validation Loss: 0.0251112412661314\n",
      "Epoch 24, Batch: 258: Training Loss: 0.021385779604315758, Validation Loss: 0.024230364710092545\n",
      "Epoch 24, Batch: 259: Training Loss: 0.02165166288614273, Validation Loss: 0.022637898102402687\n",
      "Epoch 24, Batch: 260: Training Loss: 0.02362024411559105, Validation Loss: 0.023269275203347206\n",
      "Epoch 24, Batch: 261: Training Loss: 0.022534126415848732, Validation Loss: 0.023513400927186012\n",
      "Epoch 24, Batch: 262: Training Loss: 0.01960180699825287, Validation Loss: 0.025830646976828575\n",
      "Epoch 24, Batch: 263: Training Loss: 0.016255589202046394, Validation Loss: 0.025701597332954407\n",
      "Epoch 24, Batch: 264: Training Loss: 0.01724030077457428, Validation Loss: 0.024650534614920616\n",
      "Epoch 24, Batch: 265: Training Loss: 0.02295577898621559, Validation Loss: 0.024853598326444626\n",
      "Epoch 24, Batch: 266: Training Loss: 0.019564306363463402, Validation Loss: 0.024478236213326454\n",
      "Epoch 24, Batch: 267: Training Loss: 0.02057216502726078, Validation Loss: 0.02451890893280506\n",
      "Epoch 24, Batch: 268: Training Loss: 0.019235998392105103, Validation Loss: 0.024468058720231056\n",
      "Epoch 24, Batch: 269: Training Loss: 0.02046644687652588, Validation Loss: 0.02561640925705433\n",
      "Epoch 24, Batch: 270: Training Loss: 0.02039298228919506, Validation Loss: 0.024348927661776543\n",
      "Epoch 24, Batch: 271: Training Loss: 0.020557356998324394, Validation Loss: 0.023950116708874702\n",
      "Epoch 24, Batch: 272: Training Loss: 0.019908275455236435, Validation Loss: 0.020993255078792572\n",
      "Epoch 24, Batch: 273: Training Loss: 0.020546775311231613, Validation Loss: 0.02205239050090313\n",
      "Epoch 24, Batch: 274: Training Loss: 0.024071792140603065, Validation Loss: 0.02205614745616913\n",
      "Epoch 24, Batch: 275: Training Loss: 0.019263969734311104, Validation Loss: 0.023144762963056564\n",
      "Epoch 24, Batch: 276: Training Loss: 0.02354336529970169, Validation Loss: 0.022328902035951614\n",
      "Epoch 24, Batch: 277: Training Loss: 0.022238479927182198, Validation Loss: 0.022835079580545425\n",
      "Epoch 24, Batch: 278: Training Loss: 0.019325733184814453, Validation Loss: 0.024609623476862907\n",
      "Epoch 24, Batch: 279: Training Loss: 0.02425423450767994, Validation Loss: 0.025203963741660118\n",
      "Epoch 24, Batch: 280: Training Loss: 0.01743306964635849, Validation Loss: 0.024209799244999886\n",
      "Epoch 24, Batch: 281: Training Loss: 0.017968609929084778, Validation Loss: 0.026148362085223198\n",
      "Epoch 24, Batch: 282: Training Loss: 0.022662704810500145, Validation Loss: 0.023896822705864906\n",
      "Epoch 24, Batch: 283: Training Loss: 0.020491236820816994, Validation Loss: 0.02261604554951191\n",
      "Epoch 24, Batch: 284: Training Loss: 0.022927556186914444, Validation Loss: 0.023055722936987877\n",
      "Epoch 24, Batch: 285: Training Loss: 0.01977715454995632, Validation Loss: 0.022498758509755135\n",
      "Epoch 24, Batch: 286: Training Loss: 0.020183976739645004, Validation Loss: 0.024334605783224106\n",
      "Epoch 24, Batch: 287: Training Loss: 0.020267466083168983, Validation Loss: 0.026090135797858238\n",
      "Epoch 24, Batch: 288: Training Loss: 0.02296389639377594, Validation Loss: 0.022203082218766212\n",
      "Epoch 24, Batch: 289: Training Loss: 0.02588554471731186, Validation Loss: 0.025165773928165436\n",
      "Epoch 24, Batch: 290: Training Loss: 0.02199661172926426, Validation Loss: 0.026112807914614677\n",
      "Epoch 24, Batch: 291: Training Loss: 0.019804785028100014, Validation Loss: 0.023530825972557068\n",
      "Epoch 24, Batch: 292: Training Loss: 0.02434784360229969, Validation Loss: 0.02162354066967964\n",
      "Epoch 24, Batch: 293: Training Loss: 0.022238269448280334, Validation Loss: 0.021799327805638313\n",
      "Epoch 24, Batch: 294: Training Loss: 0.01907687820494175, Validation Loss: 0.0244826078414917\n",
      "Epoch 24, Batch: 295: Training Loss: 0.021724922582507133, Validation Loss: 0.02415778487920761\n",
      "Epoch 24, Batch: 296: Training Loss: 0.022243782877922058, Validation Loss: 0.024351876229047775\n",
      "Epoch 24, Batch: 297: Training Loss: 0.01926230452954769, Validation Loss: 0.022638898342847824\n",
      "Epoch 24, Batch: 298: Training Loss: 0.02246464602649212, Validation Loss: 0.025357723236083984\n",
      "Epoch 24, Batch: 299: Training Loss: 0.02403414435684681, Validation Loss: 0.024156929925084114\n",
      "Epoch 24, Batch: 300: Training Loss: 0.020833289250731468, Validation Loss: 0.026488572359085083\n",
      "Epoch 24, Batch: 301: Training Loss: 0.02182009629905224, Validation Loss: 0.02177138812839985\n",
      "Epoch 24, Batch: 302: Training Loss: 0.021150289103388786, Validation Loss: 0.02231956459581852\n",
      "Epoch 24, Batch: 303: Training Loss: 0.02196073904633522, Validation Loss: 0.02482377365231514\n",
      "Epoch 24, Batch: 304: Training Loss: 0.02115303836762905, Validation Loss: 0.022145533934235573\n",
      "Epoch 24, Batch: 305: Training Loss: 0.017728332430124283, Validation Loss: 0.025181209668517113\n",
      "Epoch 24, Batch: 306: Training Loss: 0.022680971771478653, Validation Loss: 0.02255813404917717\n",
      "Epoch 24, Batch: 307: Training Loss: 0.017863258719444275, Validation Loss: 0.020337140187621117\n",
      "Epoch 24, Batch: 308: Training Loss: 0.019737383350729942, Validation Loss: 0.023706458508968353\n",
      "Epoch 24, Batch: 309: Training Loss: 0.021636180579662323, Validation Loss: 0.02307921275496483\n",
      "Epoch 24, Batch: 310: Training Loss: 0.021579386666417122, Validation Loss: 0.02354692667722702\n",
      "Epoch 24, Batch: 311: Training Loss: 0.018979577347636223, Validation Loss: 0.021892160177230835\n",
      "Epoch 24, Batch: 312: Training Loss: 0.020520765334367752, Validation Loss: 0.020487438887357712\n",
      "Epoch 24, Batch: 313: Training Loss: 0.01953999325633049, Validation Loss: 0.022184716537594795\n",
      "Epoch 24, Batch: 314: Training Loss: 0.01991916447877884, Validation Loss: 0.024283530190587044\n",
      "Epoch 24, Batch: 315: Training Loss: 0.019194219261407852, Validation Loss: 0.022893959656357765\n",
      "Epoch 24, Batch: 316: Training Loss: 0.01868848130106926, Validation Loss: 0.02315242402255535\n",
      "Epoch 24, Batch: 317: Training Loss: 0.019579999148845673, Validation Loss: 0.02463492564857006\n",
      "Epoch 24, Batch: 318: Training Loss: 0.020388107746839523, Validation Loss: 0.02283988893032074\n",
      "Epoch 24, Batch: 319: Training Loss: 0.021208297461271286, Validation Loss: 0.024855369701981544\n",
      "Epoch 24, Batch: 320: Training Loss: 0.02106427773833275, Validation Loss: 0.023876098915934563\n",
      "Epoch 24, Batch: 321: Training Loss: 0.01944149285554886, Validation Loss: 0.022990364581346512\n",
      "Epoch 24, Batch: 322: Training Loss: 0.0199331846088171, Validation Loss: 0.022258849814534187\n",
      "Epoch 24, Batch: 323: Training Loss: 0.019823018461465836, Validation Loss: 0.02400091663002968\n",
      "Epoch 24, Batch: 324: Training Loss: 0.0237629022449255, Validation Loss: 0.024836160242557526\n",
      "Epoch 24, Batch: 325: Training Loss: 0.020496146753430367, Validation Loss: 0.023649662733078003\n",
      "Epoch 24, Batch: 326: Training Loss: 0.024787383154034615, Validation Loss: 0.023777563124895096\n",
      "Epoch 24, Batch: 327: Training Loss: 0.01813981682062149, Validation Loss: 0.021614402532577515\n",
      "Epoch 24, Batch: 328: Training Loss: 0.0216212160885334, Validation Loss: 0.025338348001241684\n",
      "Epoch 24, Batch: 329: Training Loss: 0.022115610539913177, Validation Loss: 0.024270586669445038\n",
      "Epoch 24, Batch: 330: Training Loss: 0.019518690183758736, Validation Loss: 0.02200302667915821\n",
      "Epoch 24, Batch: 331: Training Loss: 0.020948555320501328, Validation Loss: 0.023244446143507957\n",
      "Epoch 24, Batch: 332: Training Loss: 0.02047078125178814, Validation Loss: 0.022815698757767677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch: 333: Training Loss: 0.02103627845644951, Validation Loss: 0.02250661700963974\n",
      "Epoch 24, Batch: 334: Training Loss: 0.0215451642870903, Validation Loss: 0.023941129446029663\n",
      "Epoch 24, Batch: 335: Training Loss: 0.019873376935720444, Validation Loss: 0.025052400305867195\n",
      "Epoch 24, Batch: 336: Training Loss: 0.02243480086326599, Validation Loss: 0.024986233562231064\n",
      "Epoch 24, Batch: 337: Training Loss: 0.01788724772632122, Validation Loss: 0.023942263796925545\n",
      "Epoch 24, Batch: 338: Training Loss: 0.019420431926846504, Validation Loss: 0.023873573169112206\n",
      "Epoch 24, Batch: 339: Training Loss: 0.021879274398088455, Validation Loss: 0.022881507873535156\n",
      "Epoch 24, Batch: 340: Training Loss: 0.020640281960368156, Validation Loss: 0.02384485863149166\n",
      "Epoch 24, Batch: 341: Training Loss: 0.020785044878721237, Validation Loss: 0.023730142042040825\n",
      "Epoch 24, Batch: 342: Training Loss: 0.019375212490558624, Validation Loss: 0.025904778391122818\n",
      "Epoch 24, Batch: 343: Training Loss: 0.023910820484161377, Validation Loss: 0.024798665195703506\n",
      "Epoch 24, Batch: 344: Training Loss: 0.022748950868844986, Validation Loss: 0.023823246359825134\n",
      "Epoch 24, Batch: 345: Training Loss: 0.022666800767183304, Validation Loss: 0.023992974311113358\n",
      "Epoch 24, Batch: 346: Training Loss: 0.02214844711124897, Validation Loss: 0.023997684940695763\n",
      "Epoch 24, Batch: 347: Training Loss: 0.01834568940103054, Validation Loss: 0.024472951889038086\n",
      "Epoch 24, Batch: 348: Training Loss: 0.019173137843608856, Validation Loss: 0.022783886641263962\n",
      "Epoch 24, Batch: 349: Training Loss: 0.025542626157402992, Validation Loss: 0.021671727299690247\n",
      "Epoch 24, Batch: 350: Training Loss: 0.018513353541493416, Validation Loss: 0.02595609985291958\n",
      "Epoch 24, Batch: 351: Training Loss: 0.022210396826267242, Validation Loss: 0.024478016421198845\n",
      "Epoch 24, Batch: 352: Training Loss: 0.019783005118370056, Validation Loss: 0.025307828560471535\n",
      "Epoch 24, Batch: 353: Training Loss: 0.022398006170988083, Validation Loss: 0.02596879005432129\n",
      "Epoch 24, Batch: 354: Training Loss: 0.020690642297267914, Validation Loss: 0.02324928157031536\n",
      "Epoch 24, Batch: 355: Training Loss: 0.01660779118537903, Validation Loss: 0.026016732677817345\n",
      "Epoch 24, Batch: 356: Training Loss: 0.01971174217760563, Validation Loss: 0.02328670769929886\n",
      "Epoch 24, Batch: 357: Training Loss: 0.01780799590051174, Validation Loss: 0.024488050490617752\n",
      "Epoch 24, Batch: 358: Training Loss: 0.02289670519530773, Validation Loss: 0.026625515893101692\n",
      "Epoch 24, Batch: 359: Training Loss: 0.02055121213197708, Validation Loss: 0.025544319301843643\n",
      "Epoch 24, Batch: 360: Training Loss: 0.02408817782998085, Validation Loss: 0.022380465641617775\n",
      "Epoch 24, Batch: 361: Training Loss: 0.01983504183590412, Validation Loss: 0.023773077875375748\n",
      "Epoch 24, Batch: 362: Training Loss: 0.019623495638370514, Validation Loss: 0.020731830969452858\n",
      "Epoch 24, Batch: 363: Training Loss: 0.02847824990749359, Validation Loss: 0.024091176688671112\n",
      "Epoch 24, Batch: 364: Training Loss: 0.022887036204338074, Validation Loss: 0.023137615993618965\n",
      "Epoch 24, Batch: 365: Training Loss: 0.021031858399510384, Validation Loss: 0.02401529997587204\n",
      "Epoch 24, Batch: 366: Training Loss: 0.021774930879473686, Validation Loss: 0.023803364485502243\n",
      "Epoch 24, Batch: 367: Training Loss: 0.021305108442902565, Validation Loss: 0.025104226544499397\n",
      "Epoch 24, Batch: 368: Training Loss: 0.018522923812270164, Validation Loss: 0.02351311221718788\n",
      "Epoch 24, Batch: 369: Training Loss: 0.02070356160402298, Validation Loss: 0.023451676592230797\n",
      "Epoch 24, Batch: 370: Training Loss: 0.02187371626496315, Validation Loss: 0.026844151318073273\n",
      "Epoch 24, Batch: 371: Training Loss: 0.02077641524374485, Validation Loss: 0.024817250669002533\n",
      "Epoch 24, Batch: 372: Training Loss: 0.023243382573127747, Validation Loss: 0.023536907508969307\n",
      "Epoch 24, Batch: 373: Training Loss: 0.024137839674949646, Validation Loss: 0.024572748690843582\n",
      "Epoch 24, Batch: 374: Training Loss: 0.017360378056764603, Validation Loss: 0.02531738020479679\n",
      "Epoch 24, Batch: 375: Training Loss: 0.02328566461801529, Validation Loss: 0.0224077720195055\n",
      "Epoch 24, Batch: 376: Training Loss: 0.020502494648098946, Validation Loss: 0.022127415984869003\n",
      "Epoch 24, Batch: 377: Training Loss: 0.01949622854590416, Validation Loss: 0.026145700365304947\n",
      "Epoch 24, Batch: 378: Training Loss: 0.022330962121486664, Validation Loss: 0.02517278678715229\n",
      "Epoch 24, Batch: 379: Training Loss: 0.023832349106669426, Validation Loss: 0.023632949218153954\n",
      "Epoch 24, Batch: 380: Training Loss: 0.02270987257361412, Validation Loss: 0.0254487544298172\n",
      "Epoch 24, Batch: 381: Training Loss: 0.020412597805261612, Validation Loss: 0.025648245587944984\n",
      "Epoch 24, Batch: 382: Training Loss: 0.021957827731966972, Validation Loss: 0.023897387087345123\n",
      "Epoch 24, Batch: 383: Training Loss: 0.020523395389318466, Validation Loss: 0.026283835992217064\n",
      "Epoch 24, Batch: 384: Training Loss: 0.020191363990306854, Validation Loss: 0.026588108390569687\n",
      "Epoch 24, Batch: 385: Training Loss: 0.022075263783335686, Validation Loss: 0.026442866772413254\n",
      "Epoch 24, Batch: 386: Training Loss: 0.02034367248415947, Validation Loss: 0.024425456300377846\n",
      "Epoch 24, Batch: 387: Training Loss: 0.020205756649374962, Validation Loss: 0.024968376383185387\n",
      "Epoch 24, Batch: 388: Training Loss: 0.021228814497590065, Validation Loss: 0.026174508035182953\n",
      "Epoch 24, Batch: 389: Training Loss: 0.023262547329068184, Validation Loss: 0.023805368691682816\n",
      "Epoch 24, Batch: 390: Training Loss: 0.022963954135775566, Validation Loss: 0.023522742092609406\n",
      "Epoch 24, Batch: 391: Training Loss: 0.019822752103209496, Validation Loss: 0.027309944853186607\n",
      "Epoch 24, Batch: 392: Training Loss: 0.02281113900244236, Validation Loss: 0.025970837101340294\n",
      "Epoch 24, Batch: 393: Training Loss: 0.02363894321024418, Validation Loss: 0.0264253132045269\n",
      "Epoch 24, Batch: 394: Training Loss: 0.018843013793230057, Validation Loss: 0.02804701030254364\n",
      "Epoch 24, Batch: 395: Training Loss: 0.019021708518266678, Validation Loss: 0.02554408460855484\n",
      "Epoch 24, Batch: 396: Training Loss: 0.018348824232816696, Validation Loss: 0.0272831954061985\n",
      "Epoch 24, Batch: 397: Training Loss: 0.019908994436264038, Validation Loss: 0.02691126987338066\n",
      "Epoch 24, Batch: 398: Training Loss: 0.019711488857865334, Validation Loss: 0.026898594573140144\n",
      "Epoch 24, Batch: 399: Training Loss: 0.020528877153992653, Validation Loss: 0.02530481293797493\n",
      "Epoch 24, Batch: 400: Training Loss: 0.023480596020817757, Validation Loss: 0.024203652516007423\n",
      "Epoch 24, Batch: 401: Training Loss: 0.021826278418302536, Validation Loss: 0.02441032975912094\n",
      "Epoch 24, Batch: 402: Training Loss: 0.02317933551967144, Validation Loss: 0.027707139030098915\n",
      "Epoch 24, Batch: 403: Training Loss: 0.02245643548667431, Validation Loss: 0.02697543054819107\n",
      "Epoch 24, Batch: 404: Training Loss: 0.019590554758906364, Validation Loss: 0.025761039927601814\n",
      "Epoch 24, Batch: 405: Training Loss: 0.019846513867378235, Validation Loss: 0.02584613300859928\n",
      "Epoch 24, Batch: 406: Training Loss: 0.02144051529467106, Validation Loss: 0.028353342786431313\n",
      "Epoch 24, Batch: 407: Training Loss: 0.0167192742228508, Validation Loss: 0.0230565182864666\n",
      "Epoch 24, Batch: 408: Training Loss: 0.0173861775547266, Validation Loss: 0.02558576688170433\n",
      "Epoch 24, Batch: 409: Training Loss: 0.019396396353840828, Validation Loss: 0.025845983996987343\n",
      "Epoch 24, Batch: 410: Training Loss: 0.02251483127474785, Validation Loss: 0.0257201436907053\n",
      "Epoch 24, Batch: 411: Training Loss: 0.018367471173405647, Validation Loss: 0.02597462572157383\n",
      "Epoch 24, Batch: 412: Training Loss: 0.02377959154546261, Validation Loss: 0.02454603649675846\n",
      "Epoch 24, Batch: 413: Training Loss: 0.024831904098391533, Validation Loss: 0.02218024618923664\n",
      "Epoch 24, Batch: 414: Training Loss: 0.02106468752026558, Validation Loss: 0.024969346821308136\n",
      "Epoch 24, Batch: 415: Training Loss: 0.020470915362238884, Validation Loss: 0.025349313393235207\n",
      "Epoch 24, Batch: 416: Training Loss: 0.02203276939690113, Validation Loss: 0.024120112881064415\n",
      "Epoch 24, Batch: 417: Training Loss: 0.02054707147181034, Validation Loss: 0.023636868223547935\n",
      "Epoch 24, Batch: 418: Training Loss: 0.019701804965734482, Validation Loss: 0.024772074073553085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch: 419: Training Loss: 0.018684031441807747, Validation Loss: 0.02709577977657318\n",
      "Epoch 24, Batch: 420: Training Loss: 0.022325832396745682, Validation Loss: 0.02620644122362137\n",
      "Epoch 24, Batch: 421: Training Loss: 0.02390971966087818, Validation Loss: 0.02532135136425495\n",
      "Epoch 24, Batch: 422: Training Loss: 0.023968279361724854, Validation Loss: 0.024892481043934822\n",
      "Epoch 24, Batch: 423: Training Loss: 0.02436685375869274, Validation Loss: 0.02703571692109108\n",
      "Epoch 24, Batch: 424: Training Loss: 0.025353530421853065, Validation Loss: 0.02659417688846588\n",
      "Epoch 24, Batch: 425: Training Loss: 0.020778723061084747, Validation Loss: 0.026919955387711525\n",
      "Epoch 24, Batch: 426: Training Loss: 0.02245870977640152, Validation Loss: 0.026045726612210274\n",
      "Epoch 24, Batch: 427: Training Loss: 0.021632429212331772, Validation Loss: 0.026191459968686104\n",
      "Epoch 24, Batch: 428: Training Loss: 0.0203865859657526, Validation Loss: 0.022800516337156296\n",
      "Epoch 24, Batch: 429: Training Loss: 0.021240219473838806, Validation Loss: 0.022906295955181122\n",
      "Epoch 24, Batch: 430: Training Loss: 0.019906604662537575, Validation Loss: 0.024769362062215805\n",
      "Epoch 24, Batch: 431: Training Loss: 0.019673224538564682, Validation Loss: 0.02379094436764717\n",
      "Epoch 24, Batch: 432: Training Loss: 0.02354430966079235, Validation Loss: 0.02512378618121147\n",
      "Epoch 24, Batch: 433: Training Loss: 0.0172796119004488, Validation Loss: 0.025154532864689827\n",
      "Epoch 24, Batch: 434: Training Loss: 0.019132789224386215, Validation Loss: 0.023957472294569016\n",
      "Epoch 24, Batch: 435: Training Loss: 0.020884547382593155, Validation Loss: 0.02399863861501217\n",
      "Epoch 24, Batch: 436: Training Loss: 0.020281143486499786, Validation Loss: 0.023645363748073578\n",
      "Epoch 24, Batch: 437: Training Loss: 0.020162489265203476, Validation Loss: 0.02385803312063217\n",
      "Epoch 24, Batch: 438: Training Loss: 0.023179981857538223, Validation Loss: 0.025226477533578873\n",
      "Epoch 24, Batch: 439: Training Loss: 0.018066521733999252, Validation Loss: 0.024561166763305664\n",
      "Epoch 24, Batch: 440: Training Loss: 0.020517216995358467, Validation Loss: 0.023755917325615883\n",
      "Epoch 24, Batch: 441: Training Loss: 0.019541790708899498, Validation Loss: 0.02507263980805874\n",
      "Epoch 24, Batch: 442: Training Loss: 0.02053658291697502, Validation Loss: 0.022732997313141823\n",
      "Epoch 24, Batch: 443: Training Loss: 0.019297271966934204, Validation Loss: 0.025621114298701286\n",
      "Epoch 24, Batch: 444: Training Loss: 0.02114029973745346, Validation Loss: 0.02361363172531128\n",
      "Epoch 24, Batch: 445: Training Loss: 0.019220247864723206, Validation Loss: 0.02378835715353489\n",
      "Epoch 24, Batch: 446: Training Loss: 0.023060422390699387, Validation Loss: 0.023540185764431953\n",
      "Epoch 24, Batch: 447: Training Loss: 0.023179447278380394, Validation Loss: 0.025828277692198753\n",
      "Epoch 24, Batch: 448: Training Loss: 0.020953867584466934, Validation Loss: 0.02162759378552437\n",
      "Epoch 24, Batch: 449: Training Loss: 0.021846679970622063, Validation Loss: 0.02279796078801155\n",
      "Epoch 24, Batch: 450: Training Loss: 0.021484171971678734, Validation Loss: 0.023179413750767708\n",
      "Epoch 24, Batch: 451: Training Loss: 0.02570892497897148, Validation Loss: 0.0211046040058136\n",
      "Epoch 24, Batch: 452: Training Loss: 0.02249237149953842, Validation Loss: 0.021451834589242935\n",
      "Epoch 24, Batch: 453: Training Loss: 0.01983996108174324, Validation Loss: 0.02220139652490616\n",
      "Epoch 24, Batch: 454: Training Loss: 0.022704826667904854, Validation Loss: 0.023160208016633987\n",
      "Epoch 24, Batch: 455: Training Loss: 0.018718838691711426, Validation Loss: 0.021775908768177032\n",
      "Epoch 24, Batch: 456: Training Loss: 0.020215358585119247, Validation Loss: 0.02325737662613392\n",
      "Epoch 24, Batch: 457: Training Loss: 0.02405560202896595, Validation Loss: 0.022915011271834373\n",
      "Epoch 24, Batch: 458: Training Loss: 0.017920125275850296, Validation Loss: 0.022113746032118797\n",
      "Epoch 24, Batch: 459: Training Loss: 0.01896827667951584, Validation Loss: 0.02149222604930401\n",
      "Epoch 24, Batch: 460: Training Loss: 0.019602468237280846, Validation Loss: 0.021210946142673492\n",
      "Epoch 24, Batch: 461: Training Loss: 0.021327244117856026, Validation Loss: 0.021225327625870705\n",
      "Epoch 24, Batch: 462: Training Loss: 0.018431831151247025, Validation Loss: 0.023090962320566177\n",
      "Epoch 24, Batch: 463: Training Loss: 0.022176753729581833, Validation Loss: 0.0211296658962965\n",
      "Epoch 24, Batch: 464: Training Loss: 0.021275730803608894, Validation Loss: 0.02051783911883831\n",
      "Epoch 24, Batch: 465: Training Loss: 0.02282334864139557, Validation Loss: 0.021939881145954132\n",
      "Epoch 24, Batch: 466: Training Loss: 0.01996542327105999, Validation Loss: 0.02481156587600708\n",
      "Epoch 24, Batch: 467: Training Loss: 0.02729092910885811, Validation Loss: 0.023776952177286148\n",
      "Epoch 24, Batch: 468: Training Loss: 0.020498234778642654, Validation Loss: 0.02260560728609562\n",
      "Epoch 24, Batch: 469: Training Loss: 0.019647102802991867, Validation Loss: 0.023278841748833656\n",
      "Epoch 24, Batch: 470: Training Loss: 0.019953154027462006, Validation Loss: 0.020600030198693275\n",
      "Epoch 24, Batch: 471: Training Loss: 0.020601142197847366, Validation Loss: 0.022750262171030045\n",
      "Epoch 24, Batch: 472: Training Loss: 0.019720805808901787, Validation Loss: 0.023598292842507362\n",
      "Epoch 24, Batch: 473: Training Loss: 0.020949413999915123, Validation Loss: 0.023064810782670975\n",
      "Epoch 24, Batch: 474: Training Loss: 0.020518196746706963, Validation Loss: 0.02159835956990719\n",
      "Epoch 24, Batch: 475: Training Loss: 0.021594449877738953, Validation Loss: 0.02387465536594391\n",
      "Epoch 24, Batch: 476: Training Loss: 0.019653361290693283, Validation Loss: 0.022842608392238617\n",
      "Epoch 24, Batch: 477: Training Loss: 0.02526412531733513, Validation Loss: 0.02151603437960148\n",
      "Epoch 24, Batch: 478: Training Loss: 0.018893858417868614, Validation Loss: 0.022046498954296112\n",
      "Epoch 24, Batch: 479: Training Loss: 0.02101597562432289, Validation Loss: 0.023529483005404472\n",
      "Epoch 24, Batch: 480: Training Loss: 0.02222517691552639, Validation Loss: 0.0230485238134861\n",
      "Epoch 24, Batch: 481: Training Loss: 0.019258100539445877, Validation Loss: 0.02239110879600048\n",
      "Epoch 24, Batch: 482: Training Loss: 0.024791918694972992, Validation Loss: 0.022906644269824028\n",
      "Epoch 24, Batch: 483: Training Loss: 0.018761299550533295, Validation Loss: 0.02232433296740055\n",
      "Epoch 24, Batch: 484: Training Loss: 0.02089335210621357, Validation Loss: 0.02191457897424698\n",
      "Epoch 24, Batch: 485: Training Loss: 0.018720053136348724, Validation Loss: 0.021697837859392166\n",
      "Epoch 24, Batch: 486: Training Loss: 0.024115703999996185, Validation Loss: 0.01949584111571312\n",
      "Epoch 24, Batch: 487: Training Loss: 0.020427359268069267, Validation Loss: 0.02070845104753971\n",
      "Epoch 24, Batch: 488: Training Loss: 0.0202348455786705, Validation Loss: 0.022392800077795982\n",
      "Epoch 24, Batch: 489: Training Loss: 0.02549832873046398, Validation Loss: 0.021448250859975815\n",
      "Epoch 24, Batch: 490: Training Loss: 0.02203572355210781, Validation Loss: 0.023037133738398552\n",
      "Epoch 24, Batch: 491: Training Loss: 0.019937079399824142, Validation Loss: 0.021150780841708183\n",
      "Epoch 24, Batch: 492: Training Loss: 0.023774035274982452, Validation Loss: 0.02369401603937149\n",
      "Epoch 24, Batch: 493: Training Loss: 0.020970063284039497, Validation Loss: 0.02274174802005291\n",
      "Epoch 24, Batch: 494: Training Loss: 0.02278061956167221, Validation Loss: 0.021604102104902267\n",
      "Epoch 24, Batch: 495: Training Loss: 0.022222964093089104, Validation Loss: 0.02533036284148693\n",
      "Epoch 24, Batch: 496: Training Loss: 0.022497711703181267, Validation Loss: 0.021894900128245354\n",
      "Epoch 24, Batch: 497: Training Loss: 0.019327625632286072, Validation Loss: 0.02720540389418602\n",
      "Epoch 24, Batch: 498: Training Loss: 0.022788049653172493, Validation Loss: 0.022746803238987923\n",
      "Epoch 24, Batch: 499: Training Loss: 0.020998185500502586, Validation Loss: 0.02312311716377735\n",
      "Epoch 25, Batch: 0: Training Loss: 0.02047009766101837, Validation Loss: 0.02485840767621994\n",
      "Epoch 25, Batch: 1: Training Loss: 0.01856326498091221, Validation Loss: 0.026494920253753662\n",
      "Epoch 25, Batch: 2: Training Loss: 0.023561524227261543, Validation Loss: 0.024013860151171684\n",
      "Epoch 25, Batch: 3: Training Loss: 0.02092192880809307, Validation Loss: 0.023541975766420364\n",
      "Epoch 25, Batch: 4: Training Loss: 0.020540732890367508, Validation Loss: 0.02299652062356472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 5: Training Loss: 0.02045181393623352, Validation Loss: 0.02232450060546398\n",
      "Epoch 25, Batch: 6: Training Loss: 0.020724914968013763, Validation Loss: 0.025118645280599594\n",
      "Epoch 25, Batch: 7: Training Loss: 0.01925269328057766, Validation Loss: 0.021753929555416107\n",
      "Epoch 25, Batch: 8: Training Loss: 0.02083827368915081, Validation Loss: 0.023903287947177887\n",
      "Epoch 25, Batch: 9: Training Loss: 0.01847984455525875, Validation Loss: 0.020262645557522774\n",
      "Epoch 25, Batch: 10: Training Loss: 0.020406367257237434, Validation Loss: 0.019439751282334328\n",
      "Epoch 25, Batch: 11: Training Loss: 0.024806072935461998, Validation Loss: 0.023518186062574387\n",
      "Epoch 25, Batch: 12: Training Loss: 0.024107975885272026, Validation Loss: 0.021406343206763268\n",
      "Epoch 25, Batch: 13: Training Loss: 0.024614261463284492, Validation Loss: 0.02371673658490181\n",
      "Epoch 25, Batch: 14: Training Loss: 0.024956878274679184, Validation Loss: 0.024196891114115715\n",
      "Epoch 25, Batch: 15: Training Loss: 0.024743448942899704, Validation Loss: 0.023879148066043854\n",
      "Epoch 25, Batch: 16: Training Loss: 0.024326730519533157, Validation Loss: 0.022233353927731514\n",
      "Epoch 25, Batch: 17: Training Loss: 0.019472429528832436, Validation Loss: 0.023406757041811943\n",
      "Epoch 25, Batch: 18: Training Loss: 0.018730606883764267, Validation Loss: 0.02264796756207943\n",
      "Epoch 25, Batch: 19: Training Loss: 0.021576182916760445, Validation Loss: 0.023151632398366928\n",
      "Epoch 25, Batch: 20: Training Loss: 0.02260199561715126, Validation Loss: 0.025220928713679314\n",
      "Epoch 25, Batch: 21: Training Loss: 0.023058734834194183, Validation Loss: 0.023980792611837387\n",
      "Epoch 25, Batch: 22: Training Loss: 0.018949642777442932, Validation Loss: 0.02332330495119095\n",
      "Epoch 25, Batch: 23: Training Loss: 0.019814657047390938, Validation Loss: 0.024807443842291832\n",
      "Epoch 25, Batch: 24: Training Loss: 0.022065456956624985, Validation Loss: 0.02254590392112732\n",
      "Epoch 25, Batch: 25: Training Loss: 0.020759956911206245, Validation Loss: 0.022047683596611023\n",
      "Epoch 25, Batch: 26: Training Loss: 0.020639080554246902, Validation Loss: 0.022538812831044197\n",
      "Epoch 25, Batch: 27: Training Loss: 0.018200984224677086, Validation Loss: 0.022854000329971313\n",
      "Epoch 25, Batch: 28: Training Loss: 0.021127551794052124, Validation Loss: 0.022567447274923325\n",
      "Epoch 25, Batch: 29: Training Loss: 0.023443041369318962, Validation Loss: 0.021770326420664787\n",
      "Epoch 25, Batch: 30: Training Loss: 0.019388537853956223, Validation Loss: 0.021807324141263962\n",
      "Epoch 25, Batch: 31: Training Loss: 0.02403233014047146, Validation Loss: 0.021961158141493797\n",
      "Epoch 25, Batch: 32: Training Loss: 0.018786372616887093, Validation Loss: 0.02357608452439308\n",
      "Epoch 25, Batch: 33: Training Loss: 0.018026871606707573, Validation Loss: 0.023713599890470505\n",
      "Epoch 25, Batch: 34: Training Loss: 0.018319379538297653, Validation Loss: 0.02315223403275013\n",
      "Epoch 25, Batch: 35: Training Loss: 0.020677350461483, Validation Loss: 0.022934235632419586\n",
      "Epoch 25, Batch: 36: Training Loss: 0.021217159926891327, Validation Loss: 0.023121302947402\n",
      "Epoch 25, Batch: 37: Training Loss: 0.022620374336838722, Validation Loss: 0.02333180420100689\n",
      "Epoch 25, Batch: 38: Training Loss: 0.019791455939412117, Validation Loss: 0.022207748144865036\n",
      "Epoch 25, Batch: 39: Training Loss: 0.021655455231666565, Validation Loss: 0.024845797568559647\n",
      "Epoch 25, Batch: 40: Training Loss: 0.027416754513978958, Validation Loss: 0.023634007200598717\n",
      "Epoch 25, Batch: 41: Training Loss: 0.021343957632780075, Validation Loss: 0.023790888488292694\n",
      "Epoch 25, Batch: 42: Training Loss: 0.021777844056487083, Validation Loss: 0.020829999819397926\n",
      "Epoch 25, Batch: 43: Training Loss: 0.019024044275283813, Validation Loss: 0.023706242442131042\n",
      "Epoch 25, Batch: 44: Training Loss: 0.020060110837221146, Validation Loss: 0.023931151255965233\n",
      "Epoch 25, Batch: 45: Training Loss: 0.019872259348630905, Validation Loss: 0.021983938291668892\n",
      "Epoch 25, Batch: 46: Training Loss: 0.0245327427983284, Validation Loss: 0.02272207662463188\n",
      "Epoch 25, Batch: 47: Training Loss: 0.019780218601226807, Validation Loss: 0.021957414224743843\n",
      "Epoch 25, Batch: 48: Training Loss: 0.02541241981089115, Validation Loss: 0.02252211794257164\n",
      "Epoch 25, Batch: 49: Training Loss: 0.022592198103666306, Validation Loss: 0.02403130941092968\n",
      "Epoch 25, Batch: 50: Training Loss: 0.01770351640880108, Validation Loss: 0.024306975305080414\n",
      "Epoch 25, Batch: 51: Training Loss: 0.022172294557094574, Validation Loss: 0.024639181792736053\n",
      "Epoch 25, Batch: 52: Training Loss: 0.020275723189115524, Validation Loss: 0.026463421061635017\n",
      "Epoch 25, Batch: 53: Training Loss: 0.019238024950027466, Validation Loss: 0.025391612201929092\n",
      "Epoch 25, Batch: 54: Training Loss: 0.02219196781516075, Validation Loss: 0.02797861211001873\n",
      "Epoch 25, Batch: 55: Training Loss: 0.020616909489035606, Validation Loss: 0.024398107081651688\n",
      "Epoch 25, Batch: 56: Training Loss: 0.022151047363877296, Validation Loss: 0.024973619729280472\n",
      "Epoch 25, Batch: 57: Training Loss: 0.019830023869872093, Validation Loss: 0.024594677612185478\n",
      "Epoch 25, Batch: 58: Training Loss: 0.02407868020236492, Validation Loss: 0.025764010846614838\n",
      "Epoch 25, Batch: 59: Training Loss: 0.021733885630965233, Validation Loss: 0.023875372484326363\n",
      "Epoch 25, Batch: 60: Training Loss: 0.01986704394221306, Validation Loss: 0.023293863981962204\n",
      "Epoch 25, Batch: 61: Training Loss: 0.0234023779630661, Validation Loss: 0.02786516211926937\n",
      "Epoch 25, Batch: 62: Training Loss: 0.022495795041322708, Validation Loss: 0.02162444032728672\n",
      "Epoch 25, Batch: 63: Training Loss: 0.022237829864025116, Validation Loss: 0.023293498903512955\n",
      "Epoch 25, Batch: 64: Training Loss: 0.023864882066845894, Validation Loss: 0.02260235883295536\n",
      "Epoch 25, Batch: 65: Training Loss: 0.01935378648340702, Validation Loss: 0.024510828778147697\n",
      "Epoch 25, Batch: 66: Training Loss: 0.022113384678959846, Validation Loss: 0.025355692952871323\n",
      "Epoch 25, Batch: 67: Training Loss: 0.019532639533281326, Validation Loss: 0.02516196481883526\n",
      "Epoch 25, Batch: 68: Training Loss: 0.023255564272403717, Validation Loss: 0.025636011734604836\n",
      "Epoch 25, Batch: 69: Training Loss: 0.019625792279839516, Validation Loss: 0.02198137529194355\n",
      "Epoch 25, Batch: 70: Training Loss: 0.021617088466882706, Validation Loss: 0.02444097399711609\n",
      "Epoch 25, Batch: 71: Training Loss: 0.020868858322501183, Validation Loss: 0.02286047488451004\n",
      "Epoch 25, Batch: 72: Training Loss: 0.022701477631926537, Validation Loss: 0.02150537632405758\n",
      "Epoch 25, Batch: 73: Training Loss: 0.021076999604701996, Validation Loss: 0.02135598473250866\n",
      "Epoch 25, Batch: 74: Training Loss: 0.020436713472008705, Validation Loss: 0.022168010473251343\n",
      "Epoch 25, Batch: 75: Training Loss: 0.017710726708173752, Validation Loss: 0.02282135747373104\n",
      "Epoch 25, Batch: 76: Training Loss: 0.019010894000530243, Validation Loss: 0.023055054247379303\n",
      "Epoch 25, Batch: 77: Training Loss: 0.02216239832341671, Validation Loss: 0.021353434771299362\n",
      "Epoch 25, Batch: 78: Training Loss: 0.02461577020585537, Validation Loss: 0.021396595984697342\n",
      "Epoch 25, Batch: 79: Training Loss: 0.023184461519122124, Validation Loss: 0.022037209942936897\n",
      "Epoch 25, Batch: 80: Training Loss: 0.018281683325767517, Validation Loss: 0.022503001615405083\n",
      "Epoch 25, Batch: 81: Training Loss: 0.02132824994623661, Validation Loss: 0.019419318065047264\n",
      "Epoch 25, Batch: 82: Training Loss: 0.019864298403263092, Validation Loss: 0.02065090648829937\n",
      "Epoch 25, Batch: 83: Training Loss: 0.02300010249018669, Validation Loss: 0.02174125611782074\n",
      "Epoch 25, Batch: 84: Training Loss: 0.021520467475056648, Validation Loss: 0.023593662306666374\n",
      "Epoch 25, Batch: 85: Training Loss: 0.01968100666999817, Validation Loss: 0.019173096865415573\n",
      "Epoch 25, Batch: 86: Training Loss: 0.02251015603542328, Validation Loss: 0.02020159550011158\n",
      "Epoch 25, Batch: 87: Training Loss: 0.022967202588915825, Validation Loss: 0.022297926247119904\n",
      "Epoch 25, Batch: 88: Training Loss: 0.02381655015051365, Validation Loss: 0.02192157693207264\n",
      "Epoch 25, Batch: 89: Training Loss: 0.024414261803030968, Validation Loss: 0.02330618165433407\n",
      "Epoch 25, Batch: 90: Training Loss: 0.019138682633638382, Validation Loss: 0.025290999561548233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 91: Training Loss: 0.024189623072743416, Validation Loss: 0.02353779971599579\n",
      "Epoch 25, Batch: 92: Training Loss: 0.023113325238227844, Validation Loss: 0.025429541245102882\n",
      "Epoch 25, Batch: 93: Training Loss: 0.022191228345036507, Validation Loss: 0.024025626480579376\n",
      "Epoch 25, Batch: 94: Training Loss: 0.024096660315990448, Validation Loss: 0.023715173825621605\n",
      "Epoch 25, Batch: 95: Training Loss: 0.020184770226478577, Validation Loss: 0.022451117634773254\n",
      "Epoch 25, Batch: 96: Training Loss: 0.020658913999795914, Validation Loss: 0.023352505639195442\n",
      "Epoch 25, Batch: 97: Training Loss: 0.021237103268504143, Validation Loss: 0.02303784154355526\n",
      "Epoch 25, Batch: 98: Training Loss: 0.02364971861243248, Validation Loss: 0.021060792729258537\n",
      "Epoch 25, Batch: 99: Training Loss: 0.022435352206230164, Validation Loss: 0.020689837634563446\n",
      "Epoch 25, Batch: 100: Training Loss: 0.02461971528828144, Validation Loss: 0.021040940657258034\n",
      "Epoch 25, Batch: 101: Training Loss: 0.02084817923605442, Validation Loss: 0.023506175726652145\n",
      "Epoch 25, Batch: 102: Training Loss: 0.021216383203864098, Validation Loss: 0.02124103158712387\n",
      "Epoch 25, Batch: 103: Training Loss: 0.024774465709924698, Validation Loss: 0.020143263041973114\n",
      "Epoch 25, Batch: 104: Training Loss: 0.021164441481232643, Validation Loss: 0.02131585218012333\n",
      "Epoch 25, Batch: 105: Training Loss: 0.01665094867348671, Validation Loss: 0.020713187754154205\n",
      "Epoch 25, Batch: 106: Training Loss: 0.02208339050412178, Validation Loss: 0.022084608674049377\n",
      "Epoch 25, Batch: 107: Training Loss: 0.02168361097574234, Validation Loss: 0.024309303611516953\n",
      "Epoch 25, Batch: 108: Training Loss: 0.022148815914988518, Validation Loss: 0.02158275805413723\n",
      "Epoch 25, Batch: 109: Training Loss: 0.02379550412297249, Validation Loss: 0.020962854847311974\n",
      "Epoch 25, Batch: 110: Training Loss: 0.022520143538713455, Validation Loss: 0.0193836260586977\n",
      "Epoch 25, Batch: 111: Training Loss: 0.018835127353668213, Validation Loss: 0.020322348922491074\n",
      "Epoch 25, Batch: 112: Training Loss: 0.021496843546628952, Validation Loss: 0.02231229655444622\n",
      "Epoch 25, Batch: 113: Training Loss: 0.020916229113936424, Validation Loss: 0.02049839496612549\n",
      "Epoch 25, Batch: 114: Training Loss: 0.022098803892731667, Validation Loss: 0.02165667526423931\n",
      "Epoch 25, Batch: 115: Training Loss: 0.024439923465251923, Validation Loss: 0.02312074787914753\n",
      "Epoch 25, Batch: 116: Training Loss: 0.019938291981816292, Validation Loss: 0.024859709665179253\n",
      "Epoch 25, Batch: 117: Training Loss: 0.0203963965177536, Validation Loss: 0.024220513179898262\n",
      "Epoch 25, Batch: 118: Training Loss: 0.020994342863559723, Validation Loss: 0.024170905351638794\n",
      "Epoch 25, Batch: 119: Training Loss: 0.02384735830128193, Validation Loss: 0.023866597563028336\n",
      "Epoch 25, Batch: 120: Training Loss: 0.01774471439421177, Validation Loss: 0.024447984993457794\n",
      "Epoch 25, Batch: 121: Training Loss: 0.021525949239730835, Validation Loss: 0.022738471627235413\n",
      "Epoch 25, Batch: 122: Training Loss: 0.018776191398501396, Validation Loss: 0.021841874346137047\n",
      "Epoch 25, Batch: 123: Training Loss: 0.023302525281906128, Validation Loss: 0.022784598171710968\n",
      "Epoch 25, Batch: 124: Training Loss: 0.018025336787104607, Validation Loss: 0.022897763177752495\n",
      "Epoch 25, Batch: 125: Training Loss: 0.019883690401911736, Validation Loss: 0.023630129173398018\n",
      "Epoch 25, Batch: 126: Training Loss: 0.019046064466238022, Validation Loss: 0.02225084789097309\n",
      "Epoch 25, Batch: 127: Training Loss: 0.019391151145100594, Validation Loss: 0.023030979558825493\n",
      "Epoch 25, Batch: 128: Training Loss: 0.020831191912293434, Validation Loss: 0.023713205009698868\n",
      "Epoch 25, Batch: 129: Training Loss: 0.017401035875082016, Validation Loss: 0.024823496118187904\n",
      "Epoch 25, Batch: 130: Training Loss: 0.021162519231438637, Validation Loss: 0.02424664981663227\n",
      "Epoch 25, Batch: 131: Training Loss: 0.023709336295723915, Validation Loss: 0.024381965398788452\n",
      "Epoch 25, Batch: 132: Training Loss: 0.021376540884375572, Validation Loss: 0.023040931671857834\n",
      "Epoch 25, Batch: 133: Training Loss: 0.019641075283288956, Validation Loss: 0.026864539831876755\n",
      "Epoch 25, Batch: 134: Training Loss: 0.0196852870285511, Validation Loss: 0.027584441006183624\n",
      "Epoch 25, Batch: 135: Training Loss: 0.021283622831106186, Validation Loss: 0.024449422955513\n",
      "Epoch 25, Batch: 136: Training Loss: 0.02178715355694294, Validation Loss: 0.02670382149517536\n",
      "Epoch 25, Batch: 137: Training Loss: 0.0192228015512228, Validation Loss: 0.026093749329447746\n",
      "Epoch 25, Batch: 138: Training Loss: 0.019943270832300186, Validation Loss: 0.027477150782942772\n",
      "Epoch 25, Batch: 139: Training Loss: 0.020155033096671104, Validation Loss: 0.025588423013687134\n",
      "Epoch 25, Batch: 140: Training Loss: 0.027782922610640526, Validation Loss: 0.02563026361167431\n",
      "Epoch 25, Batch: 141: Training Loss: 0.02043641358613968, Validation Loss: 0.026536529883742332\n",
      "Epoch 25, Batch: 142: Training Loss: 0.02010229229927063, Validation Loss: 0.025960849598050117\n",
      "Epoch 25, Batch: 143: Training Loss: 0.022057702764868736, Validation Loss: 0.02495049312710762\n",
      "Epoch 25, Batch: 144: Training Loss: 0.020248351618647575, Validation Loss: 0.02467336319386959\n",
      "Epoch 25, Batch: 145: Training Loss: 0.022197747603058815, Validation Loss: 0.022989768534898758\n",
      "Epoch 25, Batch: 146: Training Loss: 0.024297716096043587, Validation Loss: 0.025705888867378235\n",
      "Epoch 25, Batch: 147: Training Loss: 0.02420635148882866, Validation Loss: 0.023191900923848152\n",
      "Epoch 25, Batch: 148: Training Loss: 0.0231754370033741, Validation Loss: 0.02282741852104664\n",
      "Epoch 25, Batch: 149: Training Loss: 0.018837308511137962, Validation Loss: 0.022279419004917145\n",
      "Epoch 25, Batch: 150: Training Loss: 0.023590998724102974, Validation Loss: 0.02393239550292492\n",
      "Epoch 25, Batch: 151: Training Loss: 0.023612558841705322, Validation Loss: 0.024508032947778702\n",
      "Epoch 25, Batch: 152: Training Loss: 0.026380980387330055, Validation Loss: 0.02672043815255165\n",
      "Epoch 25, Batch: 153: Training Loss: 0.022595299407839775, Validation Loss: 0.023744605481624603\n",
      "Epoch 25, Batch: 154: Training Loss: 0.023298876360058784, Validation Loss: 0.025234827771782875\n",
      "Epoch 25, Batch: 155: Training Loss: 0.02392369881272316, Validation Loss: 0.024502387270331383\n",
      "Epoch 25, Batch: 156: Training Loss: 0.022282088175415993, Validation Loss: 0.023940559476614\n",
      "Epoch 25, Batch: 157: Training Loss: 0.020568251609802246, Validation Loss: 0.025728661566972733\n",
      "Epoch 25, Batch: 158: Training Loss: 0.022984197363257408, Validation Loss: 0.02369217574596405\n",
      "Epoch 25, Batch: 159: Training Loss: 0.021145235747098923, Validation Loss: 0.024882793426513672\n",
      "Epoch 25, Batch: 160: Training Loss: 0.02104869671165943, Validation Loss: 0.023258745670318604\n",
      "Epoch 25, Batch: 161: Training Loss: 0.02062935009598732, Validation Loss: 0.02308044210076332\n",
      "Epoch 25, Batch: 162: Training Loss: 0.023710347712039948, Validation Loss: 0.026166873052716255\n",
      "Epoch 25, Batch: 163: Training Loss: 0.020638136193156242, Validation Loss: 0.024811748415231705\n",
      "Epoch 25, Batch: 164: Training Loss: 0.021456297487020493, Validation Loss: 0.0238777045160532\n",
      "Epoch 25, Batch: 165: Training Loss: 0.020770039409399033, Validation Loss: 0.026927724480628967\n",
      "Epoch 25, Batch: 166: Training Loss: 0.020438503473997116, Validation Loss: 0.02551567368209362\n",
      "Epoch 25, Batch: 167: Training Loss: 0.020387599244713783, Validation Loss: 0.024988532066345215\n",
      "Epoch 25, Batch: 168: Training Loss: 0.024867281317710876, Validation Loss: 0.023810483515262604\n",
      "Epoch 25, Batch: 169: Training Loss: 0.021224357187747955, Validation Loss: 0.022335359826683998\n",
      "Epoch 25, Batch: 170: Training Loss: 0.020874017849564552, Validation Loss: 0.02540092170238495\n",
      "Epoch 25, Batch: 171: Training Loss: 0.020774712786078453, Validation Loss: 0.022666724398732185\n",
      "Epoch 25, Batch: 172: Training Loss: 0.0218725074082613, Validation Loss: 0.025590868666768074\n",
      "Epoch 25, Batch: 173: Training Loss: 0.01856790855526924, Validation Loss: 0.023976173251867294\n",
      "Epoch 25, Batch: 174: Training Loss: 0.02025512233376503, Validation Loss: 0.02513972483575344\n",
      "Epoch 25, Batch: 175: Training Loss: 0.02074689045548439, Validation Loss: 0.023623142391443253\n",
      "Epoch 25, Batch: 176: Training Loss: 0.0217185840010643, Validation Loss: 0.022946931421756744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 177: Training Loss: 0.021314561367034912, Validation Loss: 0.020803866907954216\n",
      "Epoch 25, Batch: 178: Training Loss: 0.02307363972067833, Validation Loss: 0.022159254178404808\n",
      "Epoch 25, Batch: 179: Training Loss: 0.01974007673561573, Validation Loss: 0.024152306839823723\n",
      "Epoch 25, Batch: 180: Training Loss: 0.02141447924077511, Validation Loss: 0.021834945306181908\n",
      "Epoch 25, Batch: 181: Training Loss: 0.02325751818716526, Validation Loss: 0.022887563332915306\n",
      "Epoch 25, Batch: 182: Training Loss: 0.023715199902653694, Validation Loss: 0.02151588164269924\n",
      "Epoch 25, Batch: 183: Training Loss: 0.0216472540050745, Validation Loss: 0.022726906463503838\n",
      "Epoch 25, Batch: 184: Training Loss: 0.021601395681500435, Validation Loss: 0.02075502835214138\n",
      "Epoch 25, Batch: 185: Training Loss: 0.021602772176265717, Validation Loss: 0.01984921470284462\n",
      "Epoch 25, Batch: 186: Training Loss: 0.02294287644326687, Validation Loss: 0.020085180178284645\n",
      "Epoch 25, Batch: 187: Training Loss: 0.020924383774399757, Validation Loss: 0.022691873833537102\n",
      "Epoch 25, Batch: 188: Training Loss: 0.022077858448028564, Validation Loss: 0.02047586627304554\n",
      "Epoch 25, Batch: 189: Training Loss: 0.022105230018496513, Validation Loss: 0.021206341683864594\n",
      "Epoch 25, Batch: 190: Training Loss: 0.023281915113329887, Validation Loss: 0.021224960684776306\n",
      "Epoch 25, Batch: 191: Training Loss: 0.024141410365700722, Validation Loss: 0.021049948409199715\n",
      "Epoch 25, Batch: 192: Training Loss: 0.02196582593023777, Validation Loss: 0.02017153985798359\n",
      "Epoch 25, Batch: 193: Training Loss: 0.018233580514788628, Validation Loss: 0.022413447499275208\n",
      "Epoch 25, Batch: 194: Training Loss: 0.019238393753767014, Validation Loss: 0.020670440047979355\n",
      "Epoch 25, Batch: 195: Training Loss: 0.019953835755586624, Validation Loss: 0.020083460956811905\n",
      "Epoch 25, Batch: 196: Training Loss: 0.023132648319005966, Validation Loss: 0.023139789700508118\n",
      "Epoch 25, Batch: 197: Training Loss: 0.017959725111722946, Validation Loss: 0.019803540781140327\n",
      "Epoch 25, Batch: 198: Training Loss: 0.020648006349802017, Validation Loss: 0.02235206961631775\n",
      "Epoch 25, Batch: 199: Training Loss: 0.023101307451725006, Validation Loss: 0.022260619327425957\n",
      "Epoch 25, Batch: 200: Training Loss: 0.017353320494294167, Validation Loss: 0.02038116380572319\n",
      "Epoch 25, Batch: 201: Training Loss: 0.0221260953694582, Validation Loss: 0.021922634914517403\n",
      "Epoch 25, Batch: 202: Training Loss: 0.02162696234881878, Validation Loss: 0.02345368266105652\n",
      "Epoch 25, Batch: 203: Training Loss: 0.02180437743663788, Validation Loss: 0.021000146865844727\n",
      "Epoch 25, Batch: 204: Training Loss: 0.019531356170773506, Validation Loss: 0.0230365339666605\n",
      "Epoch 25, Batch: 205: Training Loss: 0.024765754118561745, Validation Loss: 0.022947125136852264\n",
      "Epoch 25, Batch: 206: Training Loss: 0.01883918233215809, Validation Loss: 0.023273803293704987\n",
      "Epoch 25, Batch: 207: Training Loss: 0.022847678512334824, Validation Loss: 0.023198973387479782\n",
      "Epoch 25, Batch: 208: Training Loss: 0.020849786698818207, Validation Loss: 0.020985541865229607\n",
      "Epoch 25, Batch: 209: Training Loss: 0.020166294649243355, Validation Loss: 0.020768197253346443\n",
      "Epoch 25, Batch: 210: Training Loss: 0.023211752995848656, Validation Loss: 0.02269640937447548\n",
      "Epoch 25, Batch: 211: Training Loss: 0.021764807403087616, Validation Loss: 0.021803777664899826\n",
      "Epoch 25, Batch: 212: Training Loss: 0.022845057770609856, Validation Loss: 0.02131027728319168\n",
      "Epoch 25, Batch: 213: Training Loss: 0.024800188839435577, Validation Loss: 0.02349599078297615\n",
      "Epoch 25, Batch: 214: Training Loss: 0.021168431267142296, Validation Loss: 0.021457619965076447\n",
      "Epoch 25, Batch: 215: Training Loss: 0.020695671439170837, Validation Loss: 0.024276016280055046\n",
      "Epoch 25, Batch: 216: Training Loss: 0.0218343585729599, Validation Loss: 0.024300822988152504\n",
      "Epoch 25, Batch: 217: Training Loss: 0.02119252271950245, Validation Loss: 0.023678850382566452\n",
      "Epoch 25, Batch: 218: Training Loss: 0.022878456860780716, Validation Loss: 0.023214826360344887\n",
      "Epoch 25, Batch: 219: Training Loss: 0.022356422618031502, Validation Loss: 0.023160235956311226\n",
      "Epoch 25, Batch: 220: Training Loss: 0.02238219603896141, Validation Loss: 0.023888491094112396\n",
      "Epoch 25, Batch: 221: Training Loss: 0.019611546769738197, Validation Loss: 0.021305903792381287\n",
      "Epoch 25, Batch: 222: Training Loss: 0.02337041310966015, Validation Loss: 0.02251310646533966\n",
      "Epoch 25, Batch: 223: Training Loss: 0.022473100572824478, Validation Loss: 0.023777887225151062\n",
      "Epoch 25, Batch: 224: Training Loss: 0.022651581093668938, Validation Loss: 0.023350438103079796\n",
      "Epoch 25, Batch: 225: Training Loss: 0.02078521065413952, Validation Loss: 0.02218369022011757\n",
      "Epoch 25, Batch: 226: Training Loss: 0.02004939317703247, Validation Loss: 0.02226553112268448\n",
      "Epoch 25, Batch: 227: Training Loss: 0.02172050066292286, Validation Loss: 0.02358574979007244\n",
      "Epoch 25, Batch: 228: Training Loss: 0.022546136751770973, Validation Loss: 0.021070918068289757\n",
      "Epoch 25, Batch: 229: Training Loss: 0.020380474627017975, Validation Loss: 0.024852627888321877\n",
      "Epoch 25, Batch: 230: Training Loss: 0.02209879644215107, Validation Loss: 0.023032324388623238\n",
      "Epoch 25, Batch: 231: Training Loss: 0.023702625185251236, Validation Loss: 0.023191343992948532\n",
      "Epoch 25, Batch: 232: Training Loss: 0.023474358022212982, Validation Loss: 0.022985296323895454\n",
      "Epoch 25, Batch: 233: Training Loss: 0.018148912116885185, Validation Loss: 0.023544836789369583\n",
      "Epoch 25, Batch: 234: Training Loss: 0.025448471307754517, Validation Loss: 0.02199476771056652\n",
      "Epoch 25, Batch: 235: Training Loss: 0.02440163865685463, Validation Loss: 0.0234084352850914\n",
      "Epoch 25, Batch: 236: Training Loss: 0.02314801886677742, Validation Loss: 0.024398505687713623\n",
      "Epoch 25, Batch: 237: Training Loss: 0.01996549218893051, Validation Loss: 0.023006921634078026\n",
      "Epoch 25, Batch: 238: Training Loss: 0.02127194218337536, Validation Loss: 0.020877420902252197\n",
      "Epoch 25, Batch: 239: Training Loss: 0.021109526976943016, Validation Loss: 0.023609602823853493\n",
      "Epoch 25, Batch: 240: Training Loss: 0.020839326083660126, Validation Loss: 0.025018921121954918\n",
      "Epoch 25, Batch: 241: Training Loss: 0.02105092816054821, Validation Loss: 0.024318937212228775\n",
      "Epoch 25, Batch: 242: Training Loss: 0.020833609625697136, Validation Loss: 0.022376375272870064\n",
      "Epoch 25, Batch: 243: Training Loss: 0.020567117258906364, Validation Loss: 0.02281245030462742\n",
      "Epoch 25, Batch: 244: Training Loss: 0.019608555361628532, Validation Loss: 0.021752024069428444\n",
      "Epoch 25, Batch: 245: Training Loss: 0.020182549953460693, Validation Loss: 0.02480355091392994\n",
      "Epoch 25, Batch: 246: Training Loss: 0.01751188188791275, Validation Loss: 0.022139862179756165\n",
      "Epoch 25, Batch: 247: Training Loss: 0.022134756669402122, Validation Loss: 0.02195839211344719\n",
      "Epoch 25, Batch: 248: Training Loss: 0.021576030179858208, Validation Loss: 0.02069859392940998\n",
      "Epoch 25, Batch: 249: Training Loss: 0.019887609407305717, Validation Loss: 0.023916885256767273\n",
      "Epoch 25, Batch: 250: Training Loss: 0.02029707096517086, Validation Loss: 0.023628078401088715\n",
      "Epoch 25, Batch: 251: Training Loss: 0.018994679674506187, Validation Loss: 0.02095133438706398\n",
      "Epoch 25, Batch: 252: Training Loss: 0.02316843532025814, Validation Loss: 0.024357404559850693\n",
      "Epoch 25, Batch: 253: Training Loss: 0.021731741726398468, Validation Loss: 0.02433961071074009\n",
      "Epoch 25, Batch: 254: Training Loss: 0.016516191884875298, Validation Loss: 0.023815549910068512\n",
      "Epoch 25, Batch: 255: Training Loss: 0.019366418942809105, Validation Loss: 0.021214844658970833\n",
      "Epoch 25, Batch: 256: Training Loss: 0.02224658615887165, Validation Loss: 0.02228434570133686\n",
      "Epoch 25, Batch: 257: Training Loss: 0.022452078759670258, Validation Loss: 0.023708192631602287\n",
      "Epoch 25, Batch: 258: Training Loss: 0.023225869983434677, Validation Loss: 0.022839630022644997\n",
      "Epoch 25, Batch: 259: Training Loss: 0.02056955173611641, Validation Loss: 0.021183742210268974\n",
      "Epoch 25, Batch: 260: Training Loss: 0.022733833640813828, Validation Loss: 0.01957804709672928\n",
      "Epoch 25, Batch: 261: Training Loss: 0.02203480154275894, Validation Loss: 0.02107638493180275\n",
      "Epoch 25, Batch: 262: Training Loss: 0.020618272945284843, Validation Loss: 0.022284965962171555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 263: Training Loss: 0.01959928311407566, Validation Loss: 0.023022187873721123\n",
      "Epoch 25, Batch: 264: Training Loss: 0.01814149133861065, Validation Loss: 0.022573094815015793\n",
      "Epoch 25, Batch: 265: Training Loss: 0.01923118159174919, Validation Loss: 0.022816721349954605\n",
      "Epoch 25, Batch: 266: Training Loss: 0.01735175959765911, Validation Loss: 0.02253667265176773\n",
      "Epoch 25, Batch: 267: Training Loss: 0.01834953762590885, Validation Loss: 0.022636860609054565\n",
      "Epoch 25, Batch: 268: Training Loss: 0.017893293872475624, Validation Loss: 0.021743299439549446\n",
      "Epoch 25, Batch: 269: Training Loss: 0.021038154140114784, Validation Loss: 0.02239501103758812\n",
      "Epoch 25, Batch: 270: Training Loss: 0.022196978330612183, Validation Loss: 0.023487752303481102\n",
      "Epoch 25, Batch: 271: Training Loss: 0.01835545524954796, Validation Loss: 0.02191450074315071\n",
      "Epoch 25, Batch: 272: Training Loss: 0.020311838015913963, Validation Loss: 0.02108868397772312\n",
      "Epoch 25, Batch: 273: Training Loss: 0.02040187269449234, Validation Loss: 0.02324344776570797\n",
      "Epoch 25, Batch: 274: Training Loss: 0.023710239678621292, Validation Loss: 0.021870577707886696\n",
      "Epoch 25, Batch: 275: Training Loss: 0.020557578653097153, Validation Loss: 0.023797202855348587\n",
      "Epoch 25, Batch: 276: Training Loss: 0.020624151453375816, Validation Loss: 0.024977482855319977\n",
      "Epoch 25, Batch: 277: Training Loss: 0.0208894032984972, Validation Loss: 0.02346789464354515\n",
      "Epoch 25, Batch: 278: Training Loss: 0.019254785031080246, Validation Loss: 0.023118048906326294\n",
      "Epoch 25, Batch: 279: Training Loss: 0.018982820212841034, Validation Loss: 0.022513309493660927\n",
      "Epoch 25, Batch: 280: Training Loss: 0.018976541236042976, Validation Loss: 0.02431398071348667\n",
      "Epoch 25, Batch: 281: Training Loss: 0.021597683429718018, Validation Loss: 0.023697586730122566\n",
      "Epoch 25, Batch: 282: Training Loss: 0.023314736783504486, Validation Loss: 0.021751046180725098\n",
      "Epoch 25, Batch: 283: Training Loss: 0.021612050011754036, Validation Loss: 0.022770680487155914\n",
      "Epoch 25, Batch: 284: Training Loss: 0.021425886079669, Validation Loss: 0.02002508006989956\n",
      "Epoch 25, Batch: 285: Training Loss: 0.020945724099874496, Validation Loss: 0.022875845432281494\n",
      "Epoch 25, Batch: 286: Training Loss: 0.020508892834186554, Validation Loss: 0.02304728887975216\n",
      "Epoch 25, Batch: 287: Training Loss: 0.01992456056177616, Validation Loss: 0.022958578541874886\n",
      "Epoch 25, Batch: 288: Training Loss: 0.021342558786273003, Validation Loss: 0.020901288837194443\n",
      "Epoch 25, Batch: 289: Training Loss: 0.021686960011720657, Validation Loss: 0.02575629949569702\n",
      "Epoch 25, Batch: 290: Training Loss: 0.022029800340533257, Validation Loss: 0.021285925060510635\n",
      "Epoch 25, Batch: 291: Training Loss: 0.021859267726540565, Validation Loss: 0.025125693529844284\n",
      "Epoch 25, Batch: 292: Training Loss: 0.021002592518925667, Validation Loss: 0.023655397817492485\n",
      "Epoch 25, Batch: 293: Training Loss: 0.022027544677257538, Validation Loss: 0.02356751635670662\n",
      "Epoch 25, Batch: 294: Training Loss: 0.021949412301182747, Validation Loss: 0.021765653043985367\n",
      "Epoch 25, Batch: 295: Training Loss: 0.02047726698219776, Validation Loss: 0.023080645129084587\n",
      "Epoch 25, Batch: 296: Training Loss: 0.01942349411547184, Validation Loss: 0.023454351350665092\n",
      "Epoch 25, Batch: 297: Training Loss: 0.021625055000185966, Validation Loss: 0.026044169440865517\n",
      "Epoch 25, Batch: 298: Training Loss: 0.021656224504113197, Validation Loss: 0.02356768772006035\n",
      "Epoch 25, Batch: 299: Training Loss: 0.019686119630932808, Validation Loss: 0.02308300882577896\n",
      "Epoch 25, Batch: 300: Training Loss: 0.02192247472703457, Validation Loss: 0.02267189882695675\n",
      "Epoch 25, Batch: 301: Training Loss: 0.020800655707716942, Validation Loss: 0.02352392114698887\n",
      "Epoch 25, Batch: 302: Training Loss: 0.018940310925245285, Validation Loss: 0.02217540703713894\n",
      "Epoch 25, Batch: 303: Training Loss: 0.019842049106955528, Validation Loss: 0.02208930440247059\n",
      "Epoch 25, Batch: 304: Training Loss: 0.01830361783504486, Validation Loss: 0.023162798956036568\n",
      "Epoch 25, Batch: 305: Training Loss: 0.01839965023100376, Validation Loss: 0.020273679867386818\n",
      "Epoch 25, Batch: 306: Training Loss: 0.020039137452840805, Validation Loss: 0.024456895887851715\n",
      "Epoch 25, Batch: 307: Training Loss: 0.01935574971139431, Validation Loss: 0.022795207798480988\n",
      "Epoch 25, Batch: 308: Training Loss: 0.02126685529947281, Validation Loss: 0.023161135613918304\n",
      "Epoch 25, Batch: 309: Training Loss: 0.021323323249816895, Validation Loss: 0.02145335078239441\n",
      "Epoch 25, Batch: 310: Training Loss: 0.021510343998670578, Validation Loss: 0.0256341602653265\n",
      "Epoch 25, Batch: 311: Training Loss: 0.018386196345090866, Validation Loss: 0.022146975621581078\n",
      "Epoch 25, Batch: 312: Training Loss: 0.02034604549407959, Validation Loss: 0.020977644249796867\n",
      "Epoch 25, Batch: 313: Training Loss: 0.01886509731411934, Validation Loss: 0.019340302795171738\n",
      "Epoch 25, Batch: 314: Training Loss: 0.020046433433890343, Validation Loss: 0.023464147001504898\n",
      "Epoch 25, Batch: 315: Training Loss: 0.020900573581457138, Validation Loss: 0.02113221026957035\n",
      "Epoch 25, Batch: 316: Training Loss: 0.017561374232172966, Validation Loss: 0.022835519164800644\n",
      "Epoch 25, Batch: 317: Training Loss: 0.02155858837068081, Validation Loss: 0.02149919606745243\n",
      "Epoch 25, Batch: 318: Training Loss: 0.020747797563672066, Validation Loss: 0.02305518090724945\n",
      "Epoch 25, Batch: 319: Training Loss: 0.021894995123147964, Validation Loss: 0.022117530927062035\n",
      "Epoch 25, Batch: 320: Training Loss: 0.021412014961242676, Validation Loss: 0.02170577272772789\n",
      "Epoch 25, Batch: 321: Training Loss: 0.02075132355093956, Validation Loss: 0.02207121066749096\n",
      "Epoch 25, Batch: 322: Training Loss: 0.02236384153366089, Validation Loss: 0.023009339347481728\n",
      "Epoch 25, Batch: 323: Training Loss: 0.018096061423420906, Validation Loss: 0.020669110119342804\n",
      "Epoch 25, Batch: 324: Training Loss: 0.02357228472828865, Validation Loss: 0.02391882985830307\n",
      "Epoch 25, Batch: 325: Training Loss: 0.018873516470193863, Validation Loss: 0.02326454594731331\n",
      "Epoch 25, Batch: 326: Training Loss: 0.022826826199889183, Validation Loss: 0.022625785320997238\n",
      "Epoch 25, Batch: 327: Training Loss: 0.020588118582963943, Validation Loss: 0.022228645160794258\n",
      "Epoch 25, Batch: 328: Training Loss: 0.023763418197631836, Validation Loss: 0.02451694756746292\n",
      "Epoch 25, Batch: 329: Training Loss: 0.023310266435146332, Validation Loss: 0.0221675056964159\n",
      "Epoch 25, Batch: 330: Training Loss: 0.022450098767876625, Validation Loss: 0.022612379863858223\n",
      "Epoch 25, Batch: 331: Training Loss: 0.021390730515122414, Validation Loss: 0.02606819011271\n",
      "Epoch 25, Batch: 332: Training Loss: 0.023914165794849396, Validation Loss: 0.025121796876192093\n",
      "Epoch 25, Batch: 333: Training Loss: 0.021711140871047974, Validation Loss: 0.025173785164952278\n",
      "Epoch 25, Batch: 334: Training Loss: 0.021947218105196953, Validation Loss: 0.021621141582727432\n",
      "Epoch 25, Batch: 335: Training Loss: 0.021920988336205482, Validation Loss: 0.022860396653413773\n",
      "Epoch 25, Batch: 336: Training Loss: 0.021581964567303658, Validation Loss: 0.028517644852399826\n",
      "Epoch 25, Batch: 337: Training Loss: 0.023010725155472755, Validation Loss: 0.024447714909911156\n",
      "Epoch 25, Batch: 338: Training Loss: 0.02374701015651226, Validation Loss: 0.022860344499349594\n",
      "Epoch 25, Batch: 339: Training Loss: 0.02064713090658188, Validation Loss: 0.02230401337146759\n",
      "Epoch 25, Batch: 340: Training Loss: 0.02345769852399826, Validation Loss: 0.021326938644051552\n",
      "Epoch 25, Batch: 341: Training Loss: 0.022632664069533348, Validation Loss: 0.024566931650042534\n",
      "Epoch 25, Batch: 342: Training Loss: 0.02019665762782097, Validation Loss: 0.02243143506348133\n",
      "Epoch 25, Batch: 343: Training Loss: 0.022722754627466202, Validation Loss: 0.024845469743013382\n",
      "Epoch 25, Batch: 344: Training Loss: 0.022726330906152725, Validation Loss: 0.022386612370610237\n",
      "Epoch 25, Batch: 345: Training Loss: 0.019607536494731903, Validation Loss: 0.024714792147278786\n",
      "Epoch 25, Batch: 346: Training Loss: 0.021502729505300522, Validation Loss: 0.02405865676701069\n",
      "Epoch 25, Batch: 347: Training Loss: 0.019139297306537628, Validation Loss: 0.02691750042140484\n",
      "Epoch 25, Batch: 348: Training Loss: 0.022412260994315147, Validation Loss: 0.023105334490537643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 349: Training Loss: 0.02294340915977955, Validation Loss: 0.025317372754216194\n",
      "Epoch 25, Batch: 350: Training Loss: 0.01766158640384674, Validation Loss: 0.02260695770382881\n",
      "Epoch 25, Batch: 351: Training Loss: 0.02547897957265377, Validation Loss: 0.02186932973563671\n",
      "Epoch 25, Batch: 352: Training Loss: 0.01851535402238369, Validation Loss: 0.023827124387025833\n",
      "Epoch 25, Batch: 353: Training Loss: 0.024813208729028702, Validation Loss: 0.023715395480394363\n",
      "Epoch 25, Batch: 354: Training Loss: 0.021083621308207512, Validation Loss: 0.02189767174422741\n",
      "Epoch 25, Batch: 355: Training Loss: 0.017675604671239853, Validation Loss: 0.024989303201436996\n",
      "Epoch 25, Batch: 356: Training Loss: 0.021022137254476547, Validation Loss: 0.02515355683863163\n",
      "Epoch 25, Batch: 357: Training Loss: 0.021009620279073715, Validation Loss: 0.02530893124639988\n",
      "Epoch 25, Batch: 358: Training Loss: 0.02215568535029888, Validation Loss: 0.02540997788310051\n",
      "Epoch 25, Batch: 359: Training Loss: 0.021725405007600784, Validation Loss: 0.026909373700618744\n",
      "Epoch 25, Batch: 360: Training Loss: 0.022143837064504623, Validation Loss: 0.026110734790563583\n",
      "Epoch 25, Batch: 361: Training Loss: 0.023879362270236015, Validation Loss: 0.025811312720179558\n",
      "Epoch 25, Batch: 362: Training Loss: 0.020182006061077118, Validation Loss: 0.02398017793893814\n",
      "Epoch 25, Batch: 363: Training Loss: 0.02427583932876587, Validation Loss: 0.02440948598086834\n",
      "Epoch 25, Batch: 364: Training Loss: 0.02088899165391922, Validation Loss: 0.022849833592772484\n",
      "Epoch 25, Batch: 365: Training Loss: 0.018919439986348152, Validation Loss: 0.024383965879678726\n",
      "Epoch 25, Batch: 366: Training Loss: 0.024282675236463547, Validation Loss: 0.02123318612575531\n",
      "Epoch 25, Batch: 367: Training Loss: 0.019095467403531075, Validation Loss: 0.021209580823779106\n",
      "Epoch 25, Batch: 368: Training Loss: 0.02137208916246891, Validation Loss: 0.023176128044724464\n",
      "Epoch 25, Batch: 369: Training Loss: 0.02229120396077633, Validation Loss: 0.0228867270052433\n",
      "Epoch 25, Batch: 370: Training Loss: 0.020724741742014885, Validation Loss: 0.023260565474629402\n",
      "Epoch 25, Batch: 371: Training Loss: 0.02118557132780552, Validation Loss: 0.021563543006777763\n",
      "Epoch 25, Batch: 372: Training Loss: 0.02061602473258972, Validation Loss: 0.023348277434706688\n",
      "Epoch 25, Batch: 373: Training Loss: 0.022741252556443214, Validation Loss: 0.02214222028851509\n",
      "Epoch 25, Batch: 374: Training Loss: 0.020052632316946983, Validation Loss: 0.0226583331823349\n",
      "Epoch 25, Batch: 375: Training Loss: 0.025112474337220192, Validation Loss: 0.019610421732068062\n",
      "Epoch 25, Batch: 376: Training Loss: 0.017525870352983475, Validation Loss: 0.02398429438471794\n",
      "Epoch 25, Batch: 377: Training Loss: 0.019068656489253044, Validation Loss: 0.02171674557030201\n",
      "Epoch 25, Batch: 378: Training Loss: 0.019560962915420532, Validation Loss: 0.02338814176619053\n",
      "Epoch 25, Batch: 379: Training Loss: 0.020254839211702347, Validation Loss: 0.02208966389298439\n",
      "Epoch 25, Batch: 380: Training Loss: 0.022642355412244797, Validation Loss: 0.02332514151930809\n",
      "Epoch 25, Batch: 381: Training Loss: 0.02109820768237114, Validation Loss: 0.024045677855610847\n",
      "Epoch 25, Batch: 382: Training Loss: 0.019449777901172638, Validation Loss: 0.021679062396287918\n",
      "Epoch 25, Batch: 383: Training Loss: 0.021426692605018616, Validation Loss: 0.023837188258767128\n",
      "Epoch 25, Batch: 384: Training Loss: 0.0206423569470644, Validation Loss: 0.023792389780282974\n",
      "Epoch 25, Batch: 385: Training Loss: 0.021812230348587036, Validation Loss: 0.02236967161297798\n",
      "Epoch 25, Batch: 386: Training Loss: 0.017147505655884743, Validation Loss: 0.02573832869529724\n",
      "Epoch 25, Batch: 387: Training Loss: 0.019012529402971268, Validation Loss: 0.022789422422647476\n",
      "Epoch 25, Batch: 388: Training Loss: 0.02140764519572258, Validation Loss: 0.024831656366586685\n",
      "Epoch 25, Batch: 389: Training Loss: 0.022402478381991386, Validation Loss: 0.022812359035015106\n",
      "Epoch 25, Batch: 390: Training Loss: 0.022044751793146133, Validation Loss: 0.02376178652048111\n",
      "Epoch 25, Batch: 391: Training Loss: 0.023424234241247177, Validation Loss: 0.02624620869755745\n",
      "Epoch 25, Batch: 392: Training Loss: 0.021519720554351807, Validation Loss: 0.02403247356414795\n",
      "Epoch 25, Batch: 393: Training Loss: 0.01968495175242424, Validation Loss: 0.027104120701551437\n",
      "Epoch 25, Batch: 394: Training Loss: 0.022027647122740746, Validation Loss: 0.02479618415236473\n",
      "Epoch 25, Batch: 395: Training Loss: 0.024105684831738472, Validation Loss: 0.02347927913069725\n",
      "Epoch 25, Batch: 396: Training Loss: 0.02455790340900421, Validation Loss: 0.02302437275648117\n",
      "Epoch 25, Batch: 397: Training Loss: 0.026692979037761688, Validation Loss: 0.02147495374083519\n",
      "Epoch 25, Batch: 398: Training Loss: 0.02247966267168522, Validation Loss: 0.021808739751577377\n",
      "Epoch 25, Batch: 399: Training Loss: 0.02240920253098011, Validation Loss: 0.02261565625667572\n",
      "Epoch 25, Batch: 400: Training Loss: 0.0228895116597414, Validation Loss: 0.02164461277425289\n",
      "Epoch 25, Batch: 401: Training Loss: 0.02549969032406807, Validation Loss: 0.022827448323369026\n",
      "Epoch 25, Batch: 402: Training Loss: 0.021914804354310036, Validation Loss: 0.021555187180638313\n",
      "Epoch 25, Batch: 403: Training Loss: 0.02388674020767212, Validation Loss: 0.022233949974179268\n",
      "Epoch 25, Batch: 404: Training Loss: 0.01891840621829033, Validation Loss: 0.02238723821938038\n",
      "Epoch 25, Batch: 405: Training Loss: 0.022939631715416908, Validation Loss: 0.0230527576059103\n",
      "Epoch 25, Batch: 406: Training Loss: 0.020983416587114334, Validation Loss: 0.02229313552379608\n",
      "Epoch 25, Batch: 407: Training Loss: 0.02223680540919304, Validation Loss: 0.022562149912118912\n",
      "Epoch 25, Batch: 408: Training Loss: 0.019160745665431023, Validation Loss: 0.021321306005120277\n",
      "Epoch 25, Batch: 409: Training Loss: 0.02186363749206066, Validation Loss: 0.024018878117203712\n",
      "Epoch 25, Batch: 410: Training Loss: 0.022855693474411964, Validation Loss: 0.02376684732735157\n",
      "Epoch 25, Batch: 411: Training Loss: 0.02056528441607952, Validation Loss: 0.021644171327352524\n",
      "Epoch 25, Batch: 412: Training Loss: 0.022033441811800003, Validation Loss: 0.022959822788834572\n",
      "Epoch 25, Batch: 413: Training Loss: 0.024933166801929474, Validation Loss: 0.02015143446624279\n",
      "Epoch 25, Batch: 414: Training Loss: 0.01940322108566761, Validation Loss: 0.0222676582634449\n",
      "Epoch 25, Batch: 415: Training Loss: 0.023575501516461372, Validation Loss: 0.021941939368844032\n",
      "Epoch 25, Batch: 416: Training Loss: 0.021334299817681313, Validation Loss: 0.022184429690241814\n",
      "Epoch 25, Batch: 417: Training Loss: 0.019092798233032227, Validation Loss: 0.023712243884801865\n",
      "Epoch 25, Batch: 418: Training Loss: 0.022620590403676033, Validation Loss: 0.023414483293890953\n",
      "Epoch 25, Batch: 419: Training Loss: 0.019708486273884773, Validation Loss: 0.022939564660191536\n",
      "Epoch 25, Batch: 420: Training Loss: 0.026617776602506638, Validation Loss: 0.024758150801062584\n",
      "Epoch 25, Batch: 421: Training Loss: 0.023210691288113594, Validation Loss: 0.024944785982370377\n",
      "Epoch 25, Batch: 422: Training Loss: 0.023259371519088745, Validation Loss: 0.026225712150335312\n",
      "Epoch 25, Batch: 423: Training Loss: 0.024332165718078613, Validation Loss: 0.024449899792671204\n",
      "Epoch 25, Batch: 424: Training Loss: 0.023886406794190407, Validation Loss: 0.024624379351735115\n",
      "Epoch 25, Batch: 425: Training Loss: 0.021935245022177696, Validation Loss: 0.02572077512741089\n",
      "Epoch 25, Batch: 426: Training Loss: 0.021674731746315956, Validation Loss: 0.025436658412218094\n",
      "Epoch 25, Batch: 427: Training Loss: 0.021655002608895302, Validation Loss: 0.02667500451207161\n",
      "Epoch 25, Batch: 428: Training Loss: 0.021125754341483116, Validation Loss: 0.025648199021816254\n",
      "Epoch 25, Batch: 429: Training Loss: 0.0208852831274271, Validation Loss: 0.027622869238257408\n",
      "Epoch 25, Batch: 430: Training Loss: 0.023297030478715897, Validation Loss: 0.027388043701648712\n",
      "Epoch 25, Batch: 431: Training Loss: 0.022305116057395935, Validation Loss: 0.02486739493906498\n",
      "Epoch 25, Batch: 432: Training Loss: 0.020735012367367744, Validation Loss: 0.024851486086845398\n",
      "Epoch 25, Batch: 433: Training Loss: 0.023102907463908195, Validation Loss: 0.024050183594226837\n",
      "Epoch 25, Batch: 434: Training Loss: 0.01874045841395855, Validation Loss: 0.021916240453720093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch: 435: Training Loss: 0.02070884220302105, Validation Loss: 0.019775716587901115\n",
      "Epoch 25, Batch: 436: Training Loss: 0.020804766565561295, Validation Loss: 0.02298639714717865\n",
      "Epoch 25, Batch: 437: Training Loss: 0.0194899532943964, Validation Loss: 0.021427985280752182\n",
      "Epoch 25, Batch: 438: Training Loss: 0.02225751243531704, Validation Loss: 0.02163960225880146\n",
      "Epoch 25, Batch: 439: Training Loss: 0.01815143972635269, Validation Loss: 0.023364178836345673\n",
      "Epoch 25, Batch: 440: Training Loss: 0.024921344593167305, Validation Loss: 0.02196430042386055\n",
      "Epoch 25, Batch: 441: Training Loss: 0.023312365636229515, Validation Loss: 0.025138942524790764\n",
      "Epoch 25, Batch: 442: Training Loss: 0.023183492943644524, Validation Loss: 0.024296021088957787\n",
      "Epoch 25, Batch: 443: Training Loss: 0.020503126084804535, Validation Loss: 0.023517487570643425\n",
      "Epoch 25, Batch: 444: Training Loss: 0.018840987235307693, Validation Loss: 0.024478333070874214\n",
      "Epoch 25, Batch: 445: Training Loss: 0.016812764108181, Validation Loss: 0.023346953094005585\n",
      "Epoch 25, Batch: 446: Training Loss: 0.024711590260267258, Validation Loss: 0.023305458948016167\n",
      "Epoch 25, Batch: 447: Training Loss: 0.021901598200201988, Validation Loss: 0.025119265541434288\n",
      "Epoch 25, Batch: 448: Training Loss: 0.021863531321287155, Validation Loss: 0.023606467992067337\n",
      "Epoch 25, Batch: 449: Training Loss: 0.021810518577694893, Validation Loss: 0.022253884002566338\n",
      "Epoch 25, Batch: 450: Training Loss: 0.01961803250014782, Validation Loss: 0.02269168756902218\n",
      "Epoch 25, Batch: 451: Training Loss: 0.023177066817879677, Validation Loss: 0.023139705881476402\n",
      "Epoch 25, Batch: 452: Training Loss: 0.022384755313396454, Validation Loss: 0.023308727890253067\n",
      "Epoch 25, Batch: 453: Training Loss: 0.021770499646663666, Validation Loss: 0.023548172786831856\n",
      "Epoch 25, Batch: 454: Training Loss: 0.020100438967347145, Validation Loss: 0.024619964882731438\n",
      "Epoch 25, Batch: 455: Training Loss: 0.017552513629198074, Validation Loss: 0.020569350570440292\n",
      "Epoch 25, Batch: 456: Training Loss: 0.020400051027536392, Validation Loss: 0.021467942744493484\n",
      "Epoch 25, Batch: 457: Training Loss: 0.022464651614427567, Validation Loss: 0.02354884333908558\n",
      "Epoch 25, Batch: 458: Training Loss: 0.01932998187839985, Validation Loss: 0.02379535883665085\n",
      "Epoch 25, Batch: 459: Training Loss: 0.020451463758945465, Validation Loss: 0.02333439514040947\n",
      "Epoch 25, Batch: 460: Training Loss: 0.022650571539998055, Validation Loss: 0.024318940937519073\n",
      "Epoch 25, Batch: 461: Training Loss: 0.020317235961556435, Validation Loss: 0.023135628551244736\n",
      "Epoch 25, Batch: 462: Training Loss: 0.018760742619633675, Validation Loss: 0.023245953023433685\n",
      "Epoch 25, Batch: 463: Training Loss: 0.02060636132955551, Validation Loss: 0.023779194802045822\n",
      "Epoch 25, Batch: 464: Training Loss: 0.019322777166962624, Validation Loss: 0.02241291105747223\n",
      "Epoch 25, Batch: 465: Training Loss: 0.02323102578520775, Validation Loss: 0.02299613505601883\n",
      "Epoch 25, Batch: 466: Training Loss: 0.020038146525621414, Validation Loss: 0.021929459646344185\n",
      "Epoch 25, Batch: 467: Training Loss: 0.028160421177744865, Validation Loss: 0.02310081012547016\n",
      "Epoch 25, Batch: 468: Training Loss: 0.024115435779094696, Validation Loss: 0.02304033562541008\n",
      "Epoch 25, Batch: 469: Training Loss: 0.022661669179797173, Validation Loss: 0.026005053892731667\n",
      "Epoch 25, Batch: 470: Training Loss: 0.01956038549542427, Validation Loss: 0.021385015919804573\n",
      "Epoch 25, Batch: 471: Training Loss: 0.02087758481502533, Validation Loss: 0.02253212034702301\n",
      "Epoch 25, Batch: 472: Training Loss: 0.020526090636849403, Validation Loss: 0.02184230275452137\n",
      "Epoch 25, Batch: 473: Training Loss: 0.02189815230667591, Validation Loss: 0.021288257092237473\n",
      "Epoch 25, Batch: 474: Training Loss: 0.021056491881608963, Validation Loss: 0.020111678168177605\n",
      "Epoch 25, Batch: 475: Training Loss: 0.023071425035595894, Validation Loss: 0.018790556117892265\n",
      "Epoch 25, Batch: 476: Training Loss: 0.020520418882369995, Validation Loss: 0.021577317267656326\n",
      "Epoch 25, Batch: 477: Training Loss: 0.025626452639698982, Validation Loss: 0.02366521582007408\n",
      "Epoch 25, Batch: 478: Training Loss: 0.020798372104763985, Validation Loss: 0.021921422332525253\n",
      "Epoch 25, Batch: 479: Training Loss: 0.019267680123448372, Validation Loss: 0.022844554856419563\n",
      "Epoch 25, Batch: 480: Training Loss: 0.022952694445848465, Validation Loss: 0.022487575188279152\n",
      "Epoch 25, Batch: 481: Training Loss: 0.02014831267297268, Validation Loss: 0.022552400827407837\n",
      "Epoch 25, Batch: 482: Training Loss: 0.024564165621995926, Validation Loss: 0.022887704893946648\n",
      "Epoch 25, Batch: 483: Training Loss: 0.02016393281519413, Validation Loss: 0.024328920990228653\n",
      "Epoch 25, Batch: 484: Training Loss: 0.018986264243721962, Validation Loss: 0.024155912920832634\n",
      "Epoch 25, Batch: 485: Training Loss: 0.019861707463860512, Validation Loss: 0.02315378375351429\n",
      "Epoch 25, Batch: 486: Training Loss: 0.02259412221610546, Validation Loss: 0.02161647379398346\n",
      "Epoch 25, Batch: 487: Training Loss: 0.01744159311056137, Validation Loss: 0.023788638412952423\n",
      "Epoch 25, Batch: 488: Training Loss: 0.019758615642786026, Validation Loss: 0.024763131514191628\n",
      "Epoch 25, Batch: 489: Training Loss: 0.023508209735155106, Validation Loss: 0.02441045269370079\n",
      "Epoch 25, Batch: 490: Training Loss: 0.025432586669921875, Validation Loss: 0.022207383066415787\n",
      "Epoch 25, Batch: 491: Training Loss: 0.01705152913928032, Validation Loss: 0.024362266063690186\n",
      "Epoch 25, Batch: 492: Training Loss: 0.021362945437431335, Validation Loss: 0.02292466349899769\n",
      "Epoch 25, Batch: 493: Training Loss: 0.02211748994886875, Validation Loss: 0.022369205951690674\n",
      "Epoch 25, Batch: 494: Training Loss: 0.023139208555221558, Validation Loss: 0.021466033533215523\n",
      "Epoch 25, Batch: 495: Training Loss: 0.02169761061668396, Validation Loss: 0.023139460012316704\n",
      "Epoch 25, Batch: 496: Training Loss: 0.01934085600078106, Validation Loss: 0.022780615836381912\n",
      "Epoch 25, Batch: 497: Training Loss: 0.018091412261128426, Validation Loss: 0.024328717961907387\n",
      "Epoch 25, Batch: 498: Training Loss: 0.01907677762210369, Validation Loss: 0.02233010157942772\n",
      "Epoch 25, Batch: 499: Training Loss: 0.017312893643975258, Validation Loss: 0.02149907499551773\n",
      "Epoch 26, Batch: 0: Training Loss: 0.021143505349755287, Validation Loss: 0.023367315530776978\n",
      "Epoch 26, Batch: 1: Training Loss: 0.018314888700842857, Validation Loss: 0.021892808377742767\n",
      "Epoch 26, Batch: 2: Training Loss: 0.022777007892727852, Validation Loss: 0.023973962292075157\n",
      "Epoch 26, Batch: 3: Training Loss: 0.0207993034273386, Validation Loss: 0.025190943852066994\n",
      "Epoch 26, Batch: 4: Training Loss: 0.017164070159196854, Validation Loss: 0.024653658270835876\n",
      "Epoch 26, Batch: 5: Training Loss: 0.017683835700154305, Validation Loss: 0.02223397232592106\n",
      "Epoch 26, Batch: 6: Training Loss: 0.018642233684659004, Validation Loss: 0.022090746089816093\n",
      "Epoch 26, Batch: 7: Training Loss: 0.018760763108730316, Validation Loss: 0.024279925972223282\n",
      "Epoch 26, Batch: 8: Training Loss: 0.01928313821554184, Validation Loss: 0.023727815598249435\n",
      "Epoch 26, Batch: 9: Training Loss: 0.020884081721305847, Validation Loss: 0.02289947308599949\n",
      "Epoch 26, Batch: 10: Training Loss: 0.019667133688926697, Validation Loss: 0.02378099597990513\n",
      "Epoch 26, Batch: 11: Training Loss: 0.022345520555973053, Validation Loss: 0.023197749629616737\n",
      "Epoch 26, Batch: 12: Training Loss: 0.02217426523566246, Validation Loss: 0.02493005432188511\n",
      "Epoch 26, Batch: 13: Training Loss: 0.024076126515865326, Validation Loss: 0.023207055404782295\n",
      "Epoch 26, Batch: 14: Training Loss: 0.02468358911573887, Validation Loss: 0.021572280675172806\n",
      "Epoch 26, Batch: 15: Training Loss: 0.019789284095168114, Validation Loss: 0.021943561732769012\n",
      "Epoch 26, Batch: 16: Training Loss: 0.023218464106321335, Validation Loss: 0.02138836681842804\n",
      "Epoch 26, Batch: 17: Training Loss: 0.018110446631908417, Validation Loss: 0.02250504121184349\n",
      "Epoch 26, Batch: 18: Training Loss: 0.020106716081500053, Validation Loss: 0.021736573427915573\n",
      "Epoch 26, Batch: 19: Training Loss: 0.023293573409318924, Validation Loss: 0.023180220276117325\n",
      "Epoch 26, Batch: 20: Training Loss: 0.021846599876880646, Validation Loss: 0.023327356204390526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 21: Training Loss: 0.019736073911190033, Validation Loss: 0.01997257210314274\n",
      "Epoch 26, Batch: 22: Training Loss: 0.02165517956018448, Validation Loss: 0.023649806156754494\n",
      "Epoch 26, Batch: 23: Training Loss: 0.020706860348582268, Validation Loss: 0.023070059716701508\n",
      "Epoch 26, Batch: 24: Training Loss: 0.021372364833950996, Validation Loss: 0.02418513596057892\n",
      "Epoch 26, Batch: 25: Training Loss: 0.018391389399766922, Validation Loss: 0.023080527782440186\n",
      "Epoch 26, Batch: 26: Training Loss: 0.019254162907600403, Validation Loss: 0.022814827039837837\n",
      "Epoch 26, Batch: 27: Training Loss: 0.020212097093462944, Validation Loss: 0.023097336292266846\n",
      "Epoch 26, Batch: 28: Training Loss: 0.02272501401603222, Validation Loss: 0.021500442177057266\n",
      "Epoch 26, Batch: 29: Training Loss: 0.021686676889657974, Validation Loss: 0.021365264430642128\n",
      "Epoch 26, Batch: 30: Training Loss: 0.01982177048921585, Validation Loss: 0.02538585476577282\n",
      "Epoch 26, Batch: 31: Training Loss: 0.024460364133119583, Validation Loss: 0.023658426478505135\n",
      "Epoch 26, Batch: 32: Training Loss: 0.021560033783316612, Validation Loss: 0.021755263209342957\n",
      "Epoch 26, Batch: 33: Training Loss: 0.02081012725830078, Validation Loss: 0.02376459538936615\n",
      "Epoch 26, Batch: 34: Training Loss: 0.01940823346376419, Validation Loss: 0.021634791046380997\n",
      "Epoch 26, Batch: 35: Training Loss: 0.019250670447945595, Validation Loss: 0.021793225780129433\n",
      "Epoch 26, Batch: 36: Training Loss: 0.02028505690395832, Validation Loss: 0.022131169214844704\n",
      "Epoch 26, Batch: 37: Training Loss: 0.019646530970931053, Validation Loss: 0.021706538274884224\n",
      "Epoch 26, Batch: 38: Training Loss: 0.021219322457909584, Validation Loss: 0.022598586976528168\n",
      "Epoch 26, Batch: 39: Training Loss: 0.02052213065326214, Validation Loss: 0.022039446979761124\n",
      "Epoch 26, Batch: 40: Training Loss: 0.020727643743157387, Validation Loss: 0.02334359474480152\n",
      "Epoch 26, Batch: 41: Training Loss: 0.020940057933330536, Validation Loss: 0.02046382613480091\n",
      "Epoch 26, Batch: 42: Training Loss: 0.02018267847597599, Validation Loss: 0.022342676296830177\n",
      "Epoch 26, Batch: 43: Training Loss: 0.01940273307263851, Validation Loss: 0.02285079099237919\n",
      "Epoch 26, Batch: 44: Training Loss: 0.02340736798942089, Validation Loss: 0.022681180387735367\n",
      "Epoch 26, Batch: 45: Training Loss: 0.020125165581703186, Validation Loss: 0.022996865212917328\n",
      "Epoch 26, Batch: 46: Training Loss: 0.024709703400731087, Validation Loss: 0.024192819371819496\n",
      "Epoch 26, Batch: 47: Training Loss: 0.01990636996924877, Validation Loss: 0.023158904165029526\n",
      "Epoch 26, Batch: 48: Training Loss: 0.021120276302099228, Validation Loss: 0.024270109832286835\n",
      "Epoch 26, Batch: 49: Training Loss: 0.02192831039428711, Validation Loss: 0.021829359233379364\n",
      "Epoch 26, Batch: 50: Training Loss: 0.017638592049479485, Validation Loss: 0.023882484063506126\n",
      "Epoch 26, Batch: 51: Training Loss: 0.02440829947590828, Validation Loss: 0.021758485585451126\n",
      "Epoch 26, Batch: 52: Training Loss: 0.020246507599949837, Validation Loss: 0.024551810696721077\n",
      "Epoch 26, Batch: 53: Training Loss: 0.02120041847229004, Validation Loss: 0.024594735354185104\n",
      "Epoch 26, Batch: 54: Training Loss: 0.02355094812810421, Validation Loss: 0.0243682824075222\n",
      "Epoch 26, Batch: 55: Training Loss: 0.02144070714712143, Validation Loss: 0.020957808941602707\n",
      "Epoch 26, Batch: 56: Training Loss: 0.024777216836810112, Validation Loss: 0.022383537143468857\n",
      "Epoch 26, Batch: 57: Training Loss: 0.018785934895277023, Validation Loss: 0.021351387724280357\n",
      "Epoch 26, Batch: 58: Training Loss: 0.021704386919736862, Validation Loss: 0.020652858540415764\n",
      "Epoch 26, Batch: 59: Training Loss: 0.017405923455953598, Validation Loss: 0.02288387529551983\n",
      "Epoch 26, Batch: 60: Training Loss: 0.018539104610681534, Validation Loss: 0.02430344745516777\n",
      "Epoch 26, Batch: 61: Training Loss: 0.01982680894434452, Validation Loss: 0.02314097434282303\n",
      "Epoch 26, Batch: 62: Training Loss: 0.019820895045995712, Validation Loss: 0.023144222795963287\n",
      "Epoch 26, Batch: 63: Training Loss: 0.022691205143928528, Validation Loss: 0.02145342342555523\n",
      "Epoch 26, Batch: 64: Training Loss: 0.02116468921303749, Validation Loss: 0.02194879576563835\n",
      "Epoch 26, Batch: 65: Training Loss: 0.025683419778943062, Validation Loss: 0.02251787669956684\n",
      "Epoch 26, Batch: 66: Training Loss: 0.019886596128344536, Validation Loss: 0.021914184093475342\n",
      "Epoch 26, Batch: 67: Training Loss: 0.019754378125071526, Validation Loss: 0.022503944113850594\n",
      "Epoch 26, Batch: 68: Training Loss: 0.022883346304297447, Validation Loss: 0.023091832175850868\n",
      "Epoch 26, Batch: 69: Training Loss: 0.020300759002566338, Validation Loss: 0.02211741916835308\n",
      "Epoch 26, Batch: 70: Training Loss: 0.021718602627515793, Validation Loss: 0.02433437667787075\n",
      "Epoch 26, Batch: 71: Training Loss: 0.018125589936971664, Validation Loss: 0.02215859480202198\n",
      "Epoch 26, Batch: 72: Training Loss: 0.018819961696863174, Validation Loss: 0.022252503782510757\n",
      "Epoch 26, Batch: 73: Training Loss: 0.02209709770977497, Validation Loss: 0.023384764790534973\n",
      "Epoch 26, Batch: 74: Training Loss: 0.01813533902168274, Validation Loss: 0.02319970354437828\n",
      "Epoch 26, Batch: 75: Training Loss: 0.018541300669312477, Validation Loss: 0.020746614784002304\n",
      "Epoch 26, Batch: 76: Training Loss: 0.018084339797496796, Validation Loss: 0.0238077100366354\n",
      "Epoch 26, Batch: 77: Training Loss: 0.022813554853200912, Validation Loss: 0.023260504007339478\n",
      "Epoch 26, Batch: 78: Training Loss: 0.02083229087293148, Validation Loss: 0.023800646886229515\n",
      "Epoch 26, Batch: 79: Training Loss: 0.019045749679207802, Validation Loss: 0.02163555845618248\n",
      "Epoch 26, Batch: 80: Training Loss: 0.01975243166089058, Validation Loss: 0.022983020171523094\n",
      "Epoch 26, Batch: 81: Training Loss: 0.018421849235892296, Validation Loss: 0.023643100634217262\n",
      "Epoch 26, Batch: 82: Training Loss: 0.022804614156484604, Validation Loss: 0.021273894235491753\n",
      "Epoch 26, Batch: 83: Training Loss: 0.019315481185913086, Validation Loss: 0.024505069479346275\n",
      "Epoch 26, Batch: 84: Training Loss: 0.021889252588152885, Validation Loss: 0.023084474727511406\n",
      "Epoch 26, Batch: 85: Training Loss: 0.019039204344153404, Validation Loss: 0.0228804349899292\n",
      "Epoch 26, Batch: 86: Training Loss: 0.021138543263077736, Validation Loss: 0.021466339007019997\n",
      "Epoch 26, Batch: 87: Training Loss: 0.022302482277154922, Validation Loss: 0.022691523656249046\n",
      "Epoch 26, Batch: 88: Training Loss: 0.02367342822253704, Validation Loss: 0.024034690111875534\n",
      "Epoch 26, Batch: 89: Training Loss: 0.022796455770730972, Validation Loss: 0.023172171786427498\n",
      "Epoch 26, Batch: 90: Training Loss: 0.020672259852290154, Validation Loss: 0.023501602932810783\n",
      "Epoch 26, Batch: 91: Training Loss: 0.02201799303293228, Validation Loss: 0.021797968074679375\n",
      "Epoch 26, Batch: 92: Training Loss: 0.021131999790668488, Validation Loss: 0.02192796766757965\n",
      "Epoch 26, Batch: 93: Training Loss: 0.021757060661911964, Validation Loss: 0.02153487130999565\n",
      "Epoch 26, Batch: 94: Training Loss: 0.02107171155512333, Validation Loss: 0.022970188409090042\n",
      "Epoch 26, Batch: 95: Training Loss: 0.019452407956123352, Validation Loss: 0.023847084492444992\n",
      "Epoch 26, Batch: 96: Training Loss: 0.021385924890637398, Validation Loss: 0.021167153492569923\n",
      "Epoch 26, Batch: 97: Training Loss: 0.019819051027297974, Validation Loss: 0.021772125735878944\n",
      "Epoch 26, Batch: 98: Training Loss: 0.021178986877202988, Validation Loss: 0.023860663175582886\n",
      "Epoch 26, Batch: 99: Training Loss: 0.022802084684371948, Validation Loss: 0.022452928125858307\n",
      "Epoch 26, Batch: 100: Training Loss: 0.021036691963672638, Validation Loss: 0.021319212391972542\n",
      "Epoch 26, Batch: 101: Training Loss: 0.018208622932434082, Validation Loss: 0.022792860865592957\n",
      "Epoch 26, Batch: 102: Training Loss: 0.01858038455247879, Validation Loss: 0.02035370096564293\n",
      "Epoch 26, Batch: 103: Training Loss: 0.019888265058398247, Validation Loss: 0.02193443663418293\n",
      "Epoch 26, Batch: 104: Training Loss: 0.019118834286928177, Validation Loss: 0.022437777370214462\n",
      "Epoch 26, Batch: 105: Training Loss: 0.017618931829929352, Validation Loss: 0.02328946813941002\n",
      "Epoch 26, Batch: 106: Training Loss: 0.02123912237584591, Validation Loss: 0.023569682613015175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 107: Training Loss: 0.02029871940612793, Validation Loss: 0.024518150836229324\n",
      "Epoch 26, Batch: 108: Training Loss: 0.019310323521494865, Validation Loss: 0.02291315607726574\n",
      "Epoch 26, Batch: 109: Training Loss: 0.02115115337073803, Validation Loss: 0.024213898926973343\n",
      "Epoch 26, Batch: 110: Training Loss: 0.021554009988904, Validation Loss: 0.022538233548402786\n",
      "Epoch 26, Batch: 111: Training Loss: 0.019559398293495178, Validation Loss: 0.022906048223376274\n",
      "Epoch 26, Batch: 112: Training Loss: 0.01709938235580921, Validation Loss: 0.02156899683177471\n",
      "Epoch 26, Batch: 113: Training Loss: 0.022440986707806587, Validation Loss: 0.021530531346797943\n",
      "Epoch 26, Batch: 114: Training Loss: 0.020339658483862877, Validation Loss: 0.02476898767054081\n",
      "Epoch 26, Batch: 115: Training Loss: 0.022978462278842926, Validation Loss: 0.022139588370919228\n",
      "Epoch 26, Batch: 116: Training Loss: 0.021137641742825508, Validation Loss: 0.021268878132104874\n",
      "Epoch 26, Batch: 117: Training Loss: 0.019141025841236115, Validation Loss: 0.02110515907406807\n",
      "Epoch 26, Batch: 118: Training Loss: 0.0194579865783453, Validation Loss: 0.024664048105478287\n",
      "Epoch 26, Batch: 119: Training Loss: 0.024279966950416565, Validation Loss: 0.02179536782205105\n",
      "Epoch 26, Batch: 120: Training Loss: 0.019305825233459473, Validation Loss: 0.022096676751971245\n",
      "Epoch 26, Batch: 121: Training Loss: 0.017621584236621857, Validation Loss: 0.020886100828647614\n",
      "Epoch 26, Batch: 122: Training Loss: 0.020275786519050598, Validation Loss: 0.021228807047009468\n",
      "Epoch 26, Batch: 123: Training Loss: 0.02003145031630993, Validation Loss: 0.020593207329511642\n",
      "Epoch 26, Batch: 124: Training Loss: 0.020466459915041924, Validation Loss: 0.022027865052223206\n",
      "Epoch 26, Batch: 125: Training Loss: 0.019001996144652367, Validation Loss: 0.021916447207331657\n",
      "Epoch 26, Batch: 126: Training Loss: 0.019310757517814636, Validation Loss: 0.022455567494034767\n",
      "Epoch 26, Batch: 127: Training Loss: 0.019821597263216972, Validation Loss: 0.021886348724365234\n",
      "Epoch 26, Batch: 128: Training Loss: 0.019863305613398552, Validation Loss: 0.02304060384631157\n",
      "Epoch 26, Batch: 129: Training Loss: 0.01991335116326809, Validation Loss: 0.022835444658994675\n",
      "Epoch 26, Batch: 130: Training Loss: 0.020894993096590042, Validation Loss: 0.024106347933411598\n",
      "Epoch 26, Batch: 131: Training Loss: 0.020274082198739052, Validation Loss: 0.02195449359714985\n",
      "Epoch 26, Batch: 132: Training Loss: 0.019689641892910004, Validation Loss: 0.023367375135421753\n",
      "Epoch 26, Batch: 133: Training Loss: 0.021078847348690033, Validation Loss: 0.022371424362063408\n",
      "Epoch 26, Batch: 134: Training Loss: 0.01947968266904354, Validation Loss: 0.02283512055873871\n",
      "Epoch 26, Batch: 135: Training Loss: 0.021304519847035408, Validation Loss: 0.022063422948122025\n",
      "Epoch 26, Batch: 136: Training Loss: 0.021272551268339157, Validation Loss: 0.020636560395359993\n",
      "Epoch 26, Batch: 137: Training Loss: 0.015160120092332363, Validation Loss: 0.022288724780082703\n",
      "Epoch 26, Batch: 138: Training Loss: 0.016472894698381424, Validation Loss: 0.022710507735610008\n",
      "Epoch 26, Batch: 139: Training Loss: 0.018210384994745255, Validation Loss: 0.02211694046854973\n",
      "Epoch 26, Batch: 140: Training Loss: 0.023346474394202232, Validation Loss: 0.02339893952012062\n",
      "Epoch 26, Batch: 141: Training Loss: 0.02323611080646515, Validation Loss: 0.023372549563646317\n",
      "Epoch 26, Batch: 142: Training Loss: 0.020072121173143387, Validation Loss: 0.022863132879137993\n",
      "Epoch 26, Batch: 143: Training Loss: 0.020616954192519188, Validation Loss: 0.02293327823281288\n",
      "Epoch 26, Batch: 144: Training Loss: 0.018189826980233192, Validation Loss: 0.024738645181059837\n",
      "Epoch 26, Batch: 145: Training Loss: 0.020499015226960182, Validation Loss: 0.022506024688482285\n",
      "Epoch 26, Batch: 146: Training Loss: 0.019647330045700073, Validation Loss: 0.021405944600701332\n",
      "Epoch 26, Batch: 147: Training Loss: 0.01997857354581356, Validation Loss: 0.022531235590577126\n",
      "Epoch 26, Batch: 148: Training Loss: 0.02088504657149315, Validation Loss: 0.02648036554455757\n",
      "Epoch 26, Batch: 149: Training Loss: 0.023179862648248672, Validation Loss: 0.023139718919992447\n",
      "Epoch 26, Batch: 150: Training Loss: 0.021366868168115616, Validation Loss: 0.0221351720392704\n",
      "Epoch 26, Batch: 151: Training Loss: 0.022631704807281494, Validation Loss: 0.022724201902747154\n",
      "Epoch 26, Batch: 152: Training Loss: 0.024080457165837288, Validation Loss: 0.0229289922863245\n",
      "Epoch 26, Batch: 153: Training Loss: 0.02220667526125908, Validation Loss: 0.022473478689789772\n",
      "Epoch 26, Batch: 154: Training Loss: 0.023232977837324142, Validation Loss: 0.02391822822391987\n",
      "Epoch 26, Batch: 155: Training Loss: 0.023797184228897095, Validation Loss: 0.023540087044239044\n",
      "Epoch 26, Batch: 156: Training Loss: 0.01807388849556446, Validation Loss: 0.023998577147722244\n",
      "Epoch 26, Batch: 157: Training Loss: 0.018497303128242493, Validation Loss: 0.020876221358776093\n",
      "Epoch 26, Batch: 158: Training Loss: 0.020443962886929512, Validation Loss: 0.022032877430319786\n",
      "Epoch 26, Batch: 159: Training Loss: 0.022922802716493607, Validation Loss: 0.020327530801296234\n",
      "Epoch 26, Batch: 160: Training Loss: 0.021876782178878784, Validation Loss: 0.02152899280190468\n",
      "Epoch 26, Batch: 161: Training Loss: 0.020827999338507652, Validation Loss: 0.02316628396511078\n",
      "Epoch 26, Batch: 162: Training Loss: 0.022260600700974464, Validation Loss: 0.019650263711810112\n",
      "Epoch 26, Batch: 163: Training Loss: 0.020812327042222023, Validation Loss: 0.020692646503448486\n",
      "Epoch 26, Batch: 164: Training Loss: 0.01905708573758602, Validation Loss: 0.021611804142594337\n",
      "Epoch 26, Batch: 165: Training Loss: 0.018886450678110123, Validation Loss: 0.022239523008465767\n",
      "Epoch 26, Batch: 166: Training Loss: 0.02095653861761093, Validation Loss: 0.020314406603574753\n",
      "Epoch 26, Batch: 167: Training Loss: 0.020485466346144676, Validation Loss: 0.0223084706813097\n",
      "Epoch 26, Batch: 168: Training Loss: 0.0214969702064991, Validation Loss: 0.020712533965706825\n",
      "Epoch 26, Batch: 169: Training Loss: 0.019429773092269897, Validation Loss: 0.02289617620408535\n",
      "Epoch 26, Batch: 170: Training Loss: 0.021465806290507317, Validation Loss: 0.02125588245689869\n",
      "Epoch 26, Batch: 171: Training Loss: 0.020435409620404243, Validation Loss: 0.01937553845345974\n",
      "Epoch 26, Batch: 172: Training Loss: 0.02015126496553421, Validation Loss: 0.02194243296980858\n",
      "Epoch 26, Batch: 173: Training Loss: 0.01761722005903721, Validation Loss: 0.021774940192699432\n",
      "Epoch 26, Batch: 174: Training Loss: 0.0207911878824234, Validation Loss: 0.023881906643509865\n",
      "Epoch 26, Batch: 175: Training Loss: 0.0214974507689476, Validation Loss: 0.021070413291454315\n",
      "Epoch 26, Batch: 176: Training Loss: 0.021351831033825874, Validation Loss: 0.02111959457397461\n",
      "Epoch 26, Batch: 177: Training Loss: 0.01856117695569992, Validation Loss: 0.0192125104367733\n",
      "Epoch 26, Batch: 178: Training Loss: 0.022832389920949936, Validation Loss: 0.019773948937654495\n",
      "Epoch 26, Batch: 179: Training Loss: 0.019917070865631104, Validation Loss: 0.021357329562306404\n",
      "Epoch 26, Batch: 180: Training Loss: 0.02378814108669758, Validation Loss: 0.02098478190600872\n",
      "Epoch 26, Batch: 181: Training Loss: 0.020961875095963478, Validation Loss: 0.02107941173017025\n",
      "Epoch 26, Batch: 182: Training Loss: 0.020384984090924263, Validation Loss: 0.023253047838807106\n",
      "Epoch 26, Batch: 183: Training Loss: 0.020751113072037697, Validation Loss: 0.02226065658032894\n",
      "Epoch 26, Batch: 184: Training Loss: 0.020044267177581787, Validation Loss: 0.02009158581495285\n",
      "Epoch 26, Batch: 185: Training Loss: 0.02114742621779442, Validation Loss: 0.022293485701084137\n",
      "Epoch 26, Batch: 186: Training Loss: 0.022137818858027458, Validation Loss: 0.022730762138962746\n",
      "Epoch 26, Batch: 187: Training Loss: 0.01908334717154503, Validation Loss: 0.022655673325061798\n",
      "Epoch 26, Batch: 188: Training Loss: 0.022473875433206558, Validation Loss: 0.021495692431926727\n",
      "Epoch 26, Batch: 189: Training Loss: 0.01715882681310177, Validation Loss: 0.02214103564620018\n",
      "Epoch 26, Batch: 190: Training Loss: 0.020874014124274254, Validation Loss: 0.023512478917837143\n",
      "Epoch 26, Batch: 191: Training Loss: 0.02032393403351307, Validation Loss: 0.02327999286353588\n",
      "Epoch 26, Batch: 192: Training Loss: 0.022217800840735435, Validation Loss: 0.02261889912188053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 193: Training Loss: 0.027398986741900444, Validation Loss: 0.023385025560855865\n",
      "Epoch 26, Batch: 194: Training Loss: 0.021336838603019714, Validation Loss: 0.023408982902765274\n",
      "Epoch 26, Batch: 195: Training Loss: 0.022830098867416382, Validation Loss: 0.021922525018453598\n",
      "Epoch 26, Batch: 196: Training Loss: 0.022960351780056953, Validation Loss: 0.0225495845079422\n",
      "Epoch 26, Batch: 197: Training Loss: 0.019332753494381905, Validation Loss: 0.022162452340126038\n",
      "Epoch 26, Batch: 198: Training Loss: 0.020794766023755074, Validation Loss: 0.022393841296434402\n",
      "Epoch 26, Batch: 199: Training Loss: 0.023212186992168427, Validation Loss: 0.02093072235584259\n",
      "Epoch 26, Batch: 200: Training Loss: 0.019373048096895218, Validation Loss: 0.01900847628712654\n",
      "Epoch 26, Batch: 201: Training Loss: 0.024549251422286034, Validation Loss: 0.02052881382405758\n",
      "Epoch 26, Batch: 202: Training Loss: 0.022226601839065552, Validation Loss: 0.02108587697148323\n",
      "Epoch 26, Batch: 203: Training Loss: 0.022545101121068, Validation Loss: 0.021467600017786026\n",
      "Epoch 26, Batch: 204: Training Loss: 0.019450020045042038, Validation Loss: 0.022191591560840607\n",
      "Epoch 26, Batch: 205: Training Loss: 0.025127291679382324, Validation Loss: 0.023971669375896454\n",
      "Epoch 26, Batch: 206: Training Loss: 0.017568513751029968, Validation Loss: 0.0213288813829422\n",
      "Epoch 26, Batch: 207: Training Loss: 0.02272028662264347, Validation Loss: 0.0225212424993515\n",
      "Epoch 26, Batch: 208: Training Loss: 0.02057063579559326, Validation Loss: 0.022970926016569138\n",
      "Epoch 26, Batch: 209: Training Loss: 0.0192951038479805, Validation Loss: 0.02331661246716976\n",
      "Epoch 26, Batch: 210: Training Loss: 0.02240910939872265, Validation Loss: 0.022724686190485954\n",
      "Epoch 26, Batch: 211: Training Loss: 0.02072688192129135, Validation Loss: 0.02380339615046978\n",
      "Epoch 26, Batch: 212: Training Loss: 0.021762201562523842, Validation Loss: 0.020360488444566727\n",
      "Epoch 26, Batch: 213: Training Loss: 0.02042343281209469, Validation Loss: 0.02224111370742321\n",
      "Epoch 26, Batch: 214: Training Loss: 0.018926028162240982, Validation Loss: 0.022339126095175743\n",
      "Epoch 26, Batch: 215: Training Loss: 0.021471509709954262, Validation Loss: 0.022947948426008224\n",
      "Epoch 26, Batch: 216: Training Loss: 0.018983637914061546, Validation Loss: 0.018573246896266937\n",
      "Epoch 26, Batch: 217: Training Loss: 0.02264517918229103, Validation Loss: 0.021178755909204483\n",
      "Epoch 26, Batch: 218: Training Loss: 0.018281111493706703, Validation Loss: 0.021527646109461784\n",
      "Epoch 26, Batch: 219: Training Loss: 0.020576652139425278, Validation Loss: 0.02019583247601986\n",
      "Epoch 26, Batch: 220: Training Loss: 0.021976104006171227, Validation Loss: 0.01951983943581581\n",
      "Epoch 26, Batch: 221: Training Loss: 0.019902680069208145, Validation Loss: 0.020806249231100082\n",
      "Epoch 26, Batch: 222: Training Loss: 0.020182425156235695, Validation Loss: 0.02093658782541752\n",
      "Epoch 26, Batch: 223: Training Loss: 0.017726143822073936, Validation Loss: 0.022145593538880348\n",
      "Epoch 26, Batch: 224: Training Loss: 0.022073106840252876, Validation Loss: 0.02082621492445469\n",
      "Epoch 26, Batch: 225: Training Loss: 0.022008320316672325, Validation Loss: 0.022327052429318428\n",
      "Epoch 26, Batch: 226: Training Loss: 0.021629195660352707, Validation Loss: 0.022338969632983208\n",
      "Epoch 26, Batch: 227: Training Loss: 0.022466415539383888, Validation Loss: 0.02178375981748104\n",
      "Epoch 26, Batch: 228: Training Loss: 0.020827217027544975, Validation Loss: 0.020236149430274963\n",
      "Epoch 26, Batch: 229: Training Loss: 0.022973276674747467, Validation Loss: 0.02118879370391369\n",
      "Epoch 26, Batch: 230: Training Loss: 0.019603371620178223, Validation Loss: 0.021668212488293648\n",
      "Epoch 26, Batch: 231: Training Loss: 0.023676665499806404, Validation Loss: 0.024075236171483994\n",
      "Epoch 26, Batch: 232: Training Loss: 0.024479946121573448, Validation Loss: 0.021530428901314735\n",
      "Epoch 26, Batch: 233: Training Loss: 0.020794227719306946, Validation Loss: 0.023765292018651962\n",
      "Epoch 26, Batch: 234: Training Loss: 0.02459387481212616, Validation Loss: 0.02337593585252762\n",
      "Epoch 26, Batch: 235: Training Loss: 0.0222800150513649, Validation Loss: 0.02193058840930462\n",
      "Epoch 26, Batch: 236: Training Loss: 0.024288617074489594, Validation Loss: 0.022295350208878517\n",
      "Epoch 26, Batch: 237: Training Loss: 0.021671853959560394, Validation Loss: 0.022639498114585876\n",
      "Epoch 26, Batch: 238: Training Loss: 0.022087113931775093, Validation Loss: 0.02388487197458744\n",
      "Epoch 26, Batch: 239: Training Loss: 0.020127631723880768, Validation Loss: 0.021528400480747223\n",
      "Epoch 26, Batch: 240: Training Loss: 0.02126818522810936, Validation Loss: 0.022822080180048943\n",
      "Epoch 26, Batch: 241: Training Loss: 0.0208028145134449, Validation Loss: 0.024832695722579956\n",
      "Epoch 26, Batch: 242: Training Loss: 0.021810512989759445, Validation Loss: 0.023847335949540138\n",
      "Epoch 26, Batch: 243: Training Loss: 0.018761903047561646, Validation Loss: 0.025574233382940292\n",
      "Epoch 26, Batch: 244: Training Loss: 0.02282365784049034, Validation Loss: 0.02560393698513508\n",
      "Epoch 26, Batch: 245: Training Loss: 0.025687210261821747, Validation Loss: 0.0242611076682806\n",
      "Epoch 26, Batch: 246: Training Loss: 0.02202637493610382, Validation Loss: 0.02635008469223976\n",
      "Epoch 26, Batch: 247: Training Loss: 0.02418835274875164, Validation Loss: 0.02680039405822754\n",
      "Epoch 26, Batch: 248: Training Loss: 0.021991191431879997, Validation Loss: 0.026012282818555832\n",
      "Epoch 26, Batch: 249: Training Loss: 0.02213851548731327, Validation Loss: 0.023272689431905746\n",
      "Epoch 26, Batch: 250: Training Loss: 0.020488163456320763, Validation Loss: 0.026107067242264748\n",
      "Epoch 26, Batch: 251: Training Loss: 0.02090209536254406, Validation Loss: 0.023652836680412292\n",
      "Epoch 26, Batch: 252: Training Loss: 0.021315740421414375, Validation Loss: 0.024645831435918808\n",
      "Epoch 26, Batch: 253: Training Loss: 0.02253771759569645, Validation Loss: 0.02378714084625244\n",
      "Epoch 26, Batch: 254: Training Loss: 0.018274515867233276, Validation Loss: 0.023016436025500298\n",
      "Epoch 26, Batch: 255: Training Loss: 0.020368756726384163, Validation Loss: 0.022245734930038452\n",
      "Epoch 26, Batch: 256: Training Loss: 0.019901074469089508, Validation Loss: 0.025493094697594643\n",
      "Epoch 26, Batch: 257: Training Loss: 0.02347845397889614, Validation Loss: 0.02626943029463291\n",
      "Epoch 26, Batch: 258: Training Loss: 0.025997882708907127, Validation Loss: 0.023715166375041008\n",
      "Epoch 26, Batch: 259: Training Loss: 0.02109522372484207, Validation Loss: 0.022511515766382217\n",
      "Epoch 26, Batch: 260: Training Loss: 0.02485537715256214, Validation Loss: 0.024066101759672165\n",
      "Epoch 26, Batch: 261: Training Loss: 0.024220427498221397, Validation Loss: 0.023214636370539665\n",
      "Epoch 26, Batch: 262: Training Loss: 0.019376277923583984, Validation Loss: 0.020524490624666214\n",
      "Epoch 26, Batch: 263: Training Loss: 0.019234921783208847, Validation Loss: 0.02282542549073696\n",
      "Epoch 26, Batch: 264: Training Loss: 0.019113991409540176, Validation Loss: 0.02329172194004059\n",
      "Epoch 26, Batch: 265: Training Loss: 0.021570343524217606, Validation Loss: 0.024049900472164154\n",
      "Epoch 26, Batch: 266: Training Loss: 0.02171526849269867, Validation Loss: 0.023137567564845085\n",
      "Epoch 26, Batch: 267: Training Loss: 0.019787197932600975, Validation Loss: 0.024792376905679703\n",
      "Epoch 26, Batch: 268: Training Loss: 0.01797308959066868, Validation Loss: 0.023481503129005432\n",
      "Epoch 26, Batch: 269: Training Loss: 0.024141397327184677, Validation Loss: 0.022482899948954582\n",
      "Epoch 26, Batch: 270: Training Loss: 0.02088155224919319, Validation Loss: 0.025463715195655823\n",
      "Epoch 26, Batch: 271: Training Loss: 0.018514087423682213, Validation Loss: 0.022400684654712677\n",
      "Epoch 26, Batch: 272: Training Loss: 0.018993016332387924, Validation Loss: 0.02338697575032711\n",
      "Epoch 26, Batch: 273: Training Loss: 0.022672127932310104, Validation Loss: 0.023471223190426826\n",
      "Epoch 26, Batch: 274: Training Loss: 0.022650938481092453, Validation Loss: 0.02241329848766327\n",
      "Epoch 26, Batch: 275: Training Loss: 0.02140018343925476, Validation Loss: 0.0215101670473814\n",
      "Epoch 26, Batch: 276: Training Loss: 0.021338367834687233, Validation Loss: 0.025377565994858742\n",
      "Epoch 26, Batch: 277: Training Loss: 0.017133280634880066, Validation Loss: 0.022622033953666687\n",
      "Epoch 26, Batch: 278: Training Loss: 0.0204539243131876, Validation Loss: 0.021906603127717972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 279: Training Loss: 0.021388916298747063, Validation Loss: 0.02341396175324917\n",
      "Epoch 26, Batch: 280: Training Loss: 0.02006954699754715, Validation Loss: 0.02302989922463894\n",
      "Epoch 26, Batch: 281: Training Loss: 0.018789108842611313, Validation Loss: 0.023324955254793167\n",
      "Epoch 26, Batch: 282: Training Loss: 0.021050358191132545, Validation Loss: 0.023555399850010872\n",
      "Epoch 26, Batch: 283: Training Loss: 0.020487181842327118, Validation Loss: 0.02370753325521946\n",
      "Epoch 26, Batch: 284: Training Loss: 0.024069445207715034, Validation Loss: 0.02336852252483368\n",
      "Epoch 26, Batch: 285: Training Loss: 0.020403051748871803, Validation Loss: 0.02347828820347786\n",
      "Epoch 26, Batch: 286: Training Loss: 0.019907254725694656, Validation Loss: 0.025525212287902832\n",
      "Epoch 26, Batch: 287: Training Loss: 0.01982737146317959, Validation Loss: 0.02345159277319908\n",
      "Epoch 26, Batch: 288: Training Loss: 0.017695141956210136, Validation Loss: 0.02750459872186184\n",
      "Epoch 26, Batch: 289: Training Loss: 0.023276610299944878, Validation Loss: 0.023717453703284264\n",
      "Epoch 26, Batch: 290: Training Loss: 0.02095278538763523, Validation Loss: 0.024463118985295296\n",
      "Epoch 26, Batch: 291: Training Loss: 0.022424425929784775, Validation Loss: 0.024152318015694618\n",
      "Epoch 26, Batch: 292: Training Loss: 0.021683961153030396, Validation Loss: 0.024015523493289948\n",
      "Epoch 26, Batch: 293: Training Loss: 0.01947588473558426, Validation Loss: 0.02611241675913334\n",
      "Epoch 26, Batch: 294: Training Loss: 0.022680629044771194, Validation Loss: 0.025669751688838005\n",
      "Epoch 26, Batch: 295: Training Loss: 0.023818636313080788, Validation Loss: 0.024131525307893753\n",
      "Epoch 26, Batch: 296: Training Loss: 0.018773989751935005, Validation Loss: 0.024371862411499023\n",
      "Epoch 26, Batch: 297: Training Loss: 0.021403605118393898, Validation Loss: 0.026203351095318794\n",
      "Epoch 26, Batch: 298: Training Loss: 0.02250628173351288, Validation Loss: 0.026193251833319664\n",
      "Epoch 26, Batch: 299: Training Loss: 0.02314060740172863, Validation Loss: 0.025875812396407127\n",
      "Epoch 26, Batch: 300: Training Loss: 0.019623948261141777, Validation Loss: 0.024727802723646164\n",
      "Epoch 26, Batch: 301: Training Loss: 0.020100807771086693, Validation Loss: 0.02374810166656971\n",
      "Epoch 26, Batch: 302: Training Loss: 0.02050262875854969, Validation Loss: 0.025002121925354004\n",
      "Epoch 26, Batch: 303: Training Loss: 0.02124420925974846, Validation Loss: 0.023802630603313446\n",
      "Epoch 26, Batch: 304: Training Loss: 0.019437797367572784, Validation Loss: 0.023821182548999786\n",
      "Epoch 26, Batch: 305: Training Loss: 0.018916502594947815, Validation Loss: 0.02298630215227604\n",
      "Epoch 26, Batch: 306: Training Loss: 0.02201380953192711, Validation Loss: 0.023544060066342354\n",
      "Epoch 26, Batch: 307: Training Loss: 0.01867889240384102, Validation Loss: 0.025280889123678207\n",
      "Epoch 26, Batch: 308: Training Loss: 0.02152511104941368, Validation Loss: 0.025232769548892975\n",
      "Epoch 26, Batch: 309: Training Loss: 0.02059861831367016, Validation Loss: 0.024927422404289246\n",
      "Epoch 26, Batch: 310: Training Loss: 0.02001894637942314, Validation Loss: 0.024691492319107056\n",
      "Epoch 26, Batch: 311: Training Loss: 0.021274250000715256, Validation Loss: 0.02327270433306694\n",
      "Epoch 26, Batch: 312: Training Loss: 0.023175688460469246, Validation Loss: 0.024115879088640213\n",
      "Epoch 26, Batch: 313: Training Loss: 0.02037064917385578, Validation Loss: 0.022352276369929314\n",
      "Epoch 26, Batch: 314: Training Loss: 0.024156449362635612, Validation Loss: 0.023435158655047417\n",
      "Epoch 26, Batch: 315: Training Loss: 0.022681638598442078, Validation Loss: 0.02206084132194519\n",
      "Epoch 26, Batch: 316: Training Loss: 0.021229693666100502, Validation Loss: 0.022461073473095894\n",
      "Epoch 26, Batch: 317: Training Loss: 0.019141528755426407, Validation Loss: 0.02274780347943306\n",
      "Epoch 26, Batch: 318: Training Loss: 0.02150220423936844, Validation Loss: 0.023251980543136597\n",
      "Epoch 26, Batch: 319: Training Loss: 0.02002994529902935, Validation Loss: 0.022486472502350807\n",
      "Epoch 26, Batch: 320: Training Loss: 0.02095215395092964, Validation Loss: 0.02171977423131466\n",
      "Epoch 26, Batch: 321: Training Loss: 0.019512007012963295, Validation Loss: 0.022963950410485268\n",
      "Epoch 26, Batch: 322: Training Loss: 0.020497236400842667, Validation Loss: 0.022138092666864395\n",
      "Epoch 26, Batch: 323: Training Loss: 0.020462172105908394, Validation Loss: 0.020953664556145668\n",
      "Epoch 26, Batch: 324: Training Loss: 0.0190590750426054, Validation Loss: 0.023110145702958107\n",
      "Epoch 26, Batch: 325: Training Loss: 0.018234608694911003, Validation Loss: 0.020716125145554543\n",
      "Epoch 26, Batch: 326: Training Loss: 0.023112671449780464, Validation Loss: 0.021035997197031975\n",
      "Epoch 26, Batch: 327: Training Loss: 0.019773004576563835, Validation Loss: 0.02337673492729664\n",
      "Epoch 26, Batch: 328: Training Loss: 0.02066493034362793, Validation Loss: 0.022891363129019737\n",
      "Epoch 26, Batch: 329: Training Loss: 0.019811104983091354, Validation Loss: 0.02106572315096855\n",
      "Epoch 26, Batch: 330: Training Loss: 0.02145674079656601, Validation Loss: 0.02333686128258705\n",
      "Epoch 26, Batch: 331: Training Loss: 0.018454402685165405, Validation Loss: 0.02242346666753292\n",
      "Epoch 26, Batch: 332: Training Loss: 0.017709171399474144, Validation Loss: 0.0213902797549963\n",
      "Epoch 26, Batch: 333: Training Loss: 0.020786408334970474, Validation Loss: 0.022839078679680824\n",
      "Epoch 26, Batch: 334: Training Loss: 0.01711040921509266, Validation Loss: 0.023356962949037552\n",
      "Epoch 26, Batch: 335: Training Loss: 0.022945601493120193, Validation Loss: 0.02445387654006481\n",
      "Epoch 26, Batch: 336: Training Loss: 0.017681386321783066, Validation Loss: 0.024382689967751503\n",
      "Epoch 26, Batch: 337: Training Loss: 0.018900591880083084, Validation Loss: 0.022550657391548157\n",
      "Epoch 26, Batch: 338: Training Loss: 0.019656362012028694, Validation Loss: 0.023283658549189568\n",
      "Epoch 26, Batch: 339: Training Loss: 0.0228382870554924, Validation Loss: 0.02670496515929699\n",
      "Epoch 26, Batch: 340: Training Loss: 0.020344097167253494, Validation Loss: 0.024389084428548813\n",
      "Epoch 26, Batch: 341: Training Loss: 0.020197976380586624, Validation Loss: 0.02156873792409897\n",
      "Epoch 26, Batch: 342: Training Loss: 0.021816153079271317, Validation Loss: 0.023104028776288033\n",
      "Epoch 26, Batch: 343: Training Loss: 0.02257818728685379, Validation Loss: 0.022443566471338272\n",
      "Epoch 26, Batch: 344: Training Loss: 0.02142752707004547, Validation Loss: 0.022951897233724594\n",
      "Epoch 26, Batch: 345: Training Loss: 0.023045992478728294, Validation Loss: 0.02211540937423706\n",
      "Epoch 26, Batch: 346: Training Loss: 0.024214183911681175, Validation Loss: 0.02287609875202179\n",
      "Epoch 26, Batch: 347: Training Loss: 0.019874434918165207, Validation Loss: 0.024097830057144165\n",
      "Epoch 26, Batch: 348: Training Loss: 0.022604480385780334, Validation Loss: 0.023968445137143135\n",
      "Epoch 26, Batch: 349: Training Loss: 0.022626250982284546, Validation Loss: 0.023867260664701462\n",
      "Epoch 26, Batch: 350: Training Loss: 0.01928587071597576, Validation Loss: 0.022834734991192818\n",
      "Epoch 26, Batch: 351: Training Loss: 0.022647172212600708, Validation Loss: 0.022177787497639656\n",
      "Epoch 26, Batch: 352: Training Loss: 0.021496647968888283, Validation Loss: 0.022079236805438995\n",
      "Epoch 26, Batch: 353: Training Loss: 0.021719884127378464, Validation Loss: 0.02331279031932354\n",
      "Epoch 26, Batch: 354: Training Loss: 0.021725278347730637, Validation Loss: 0.022894514724612236\n",
      "Epoch 26, Batch: 355: Training Loss: 0.01771775633096695, Validation Loss: 0.020329980179667473\n",
      "Epoch 26, Batch: 356: Training Loss: 0.02308243326842785, Validation Loss: 0.024460088461637497\n",
      "Epoch 26, Batch: 357: Training Loss: 0.019419433549046516, Validation Loss: 0.02237461321055889\n",
      "Epoch 26, Batch: 358: Training Loss: 0.022191165015101433, Validation Loss: 0.02179531380534172\n",
      "Epoch 26, Batch: 359: Training Loss: 0.02117539569735527, Validation Loss: 0.024295981973409653\n",
      "Epoch 26, Batch: 360: Training Loss: 0.023144954815506935, Validation Loss: 0.02123122662305832\n",
      "Epoch 26, Batch: 361: Training Loss: 0.021215585991740227, Validation Loss: 0.023356126621365547\n",
      "Epoch 26, Batch: 362: Training Loss: 0.018626734614372253, Validation Loss: 0.022952701896429062\n",
      "Epoch 26, Batch: 363: Training Loss: 0.02585502900183201, Validation Loss: 0.02176634408533573\n",
      "Epoch 26, Batch: 364: Training Loss: 0.019925599917769432, Validation Loss: 0.0221897903829813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 365: Training Loss: 0.021044116467237473, Validation Loss: 0.022473210468888283\n",
      "Epoch 26, Batch: 366: Training Loss: 0.022565603256225586, Validation Loss: 0.02402031049132347\n",
      "Epoch 26, Batch: 367: Training Loss: 0.01654093898832798, Validation Loss: 0.020548619329929352\n",
      "Epoch 26, Batch: 368: Training Loss: 0.019963916391134262, Validation Loss: 0.022218436002731323\n",
      "Epoch 26, Batch: 369: Training Loss: 0.020087487995624542, Validation Loss: 0.023135777562856674\n",
      "Epoch 26, Batch: 370: Training Loss: 0.021275486797094345, Validation Loss: 0.021034929901361465\n",
      "Epoch 26, Batch: 371: Training Loss: 0.02007606439292431, Validation Loss: 0.021284788846969604\n",
      "Epoch 26, Batch: 372: Training Loss: 0.020990679040551186, Validation Loss: 0.023219648748636246\n",
      "Epoch 26, Batch: 373: Training Loss: 0.02513551339507103, Validation Loss: 0.023115091025829315\n",
      "Epoch 26, Batch: 374: Training Loss: 0.019149981439113617, Validation Loss: 0.02191285230219364\n",
      "Epoch 26, Batch: 375: Training Loss: 0.02215506136417389, Validation Loss: 0.021365022286772728\n",
      "Epoch 26, Batch: 376: Training Loss: 0.02233639359474182, Validation Loss: 0.024952881038188934\n",
      "Epoch 26, Batch: 377: Training Loss: 0.01950235478579998, Validation Loss: 0.022413242608308792\n",
      "Epoch 26, Batch: 378: Training Loss: 0.019660770893096924, Validation Loss: 0.021790560334920883\n",
      "Epoch 26, Batch: 379: Training Loss: 0.018970785662531853, Validation Loss: 0.021649159491062164\n",
      "Epoch 26, Batch: 380: Training Loss: 0.02201494760811329, Validation Loss: 0.021323150023818016\n",
      "Epoch 26, Batch: 381: Training Loss: 0.02098369598388672, Validation Loss: 0.020242709666490555\n",
      "Epoch 26, Batch: 382: Training Loss: 0.02077130600810051, Validation Loss: 0.021969670429825783\n",
      "Epoch 26, Batch: 383: Training Loss: 0.01856851764023304, Validation Loss: 0.020000997930765152\n",
      "Epoch 26, Batch: 384: Training Loss: 0.020183954387903214, Validation Loss: 0.022706681862473488\n",
      "Epoch 26, Batch: 385: Training Loss: 0.021687177941203117, Validation Loss: 0.020895810797810555\n",
      "Epoch 26, Batch: 386: Training Loss: 0.015717390924692154, Validation Loss: 0.023163633421063423\n",
      "Epoch 26, Batch: 387: Training Loss: 0.025201672688126564, Validation Loss: 0.02168983779847622\n",
      "Epoch 26, Batch: 388: Training Loss: 0.026883648708462715, Validation Loss: 0.020115483552217484\n",
      "Epoch 26, Batch: 389: Training Loss: 0.01950974576175213, Validation Loss: 0.021119246259331703\n",
      "Epoch 26, Batch: 390: Training Loss: 0.019926857203245163, Validation Loss: 0.022442322224378586\n",
      "Epoch 26, Batch: 391: Training Loss: 0.026389434933662415, Validation Loss: 0.02244042605161667\n",
      "Epoch 26, Batch: 392: Training Loss: 0.01878967694938183, Validation Loss: 0.021400686353445053\n",
      "Epoch 26, Batch: 393: Training Loss: 0.020482933148741722, Validation Loss: 0.02055564522743225\n",
      "Epoch 26, Batch: 394: Training Loss: 0.020222846418619156, Validation Loss: 0.02119540050625801\n",
      "Epoch 26, Batch: 395: Training Loss: 0.02329794131219387, Validation Loss: 0.021938741207122803\n",
      "Epoch 26, Batch: 396: Training Loss: 0.02157597802579403, Validation Loss: 0.02069634199142456\n",
      "Epoch 26, Batch: 397: Training Loss: 0.02309204451739788, Validation Loss: 0.020152410492300987\n",
      "Epoch 26, Batch: 398: Training Loss: 0.021208712831139565, Validation Loss: 0.01976119726896286\n",
      "Epoch 26, Batch: 399: Training Loss: 0.02098916284739971, Validation Loss: 0.020783908665180206\n",
      "Epoch 26, Batch: 400: Training Loss: 0.02363775297999382, Validation Loss: 0.020425884053111076\n",
      "Epoch 26, Batch: 401: Training Loss: 0.02270462177693844, Validation Loss: 0.020456459373235703\n",
      "Epoch 26, Batch: 402: Training Loss: 0.020417114719748497, Validation Loss: 0.021015381440520287\n",
      "Epoch 26, Batch: 403: Training Loss: 0.02681354247033596, Validation Loss: 0.020147576928138733\n",
      "Epoch 26, Batch: 404: Training Loss: 0.017984168604016304, Validation Loss: 0.022379564121365547\n",
      "Epoch 26, Batch: 405: Training Loss: 0.021955281496047974, Validation Loss: 0.02331308275461197\n",
      "Epoch 26, Batch: 406: Training Loss: 0.01953919790685177, Validation Loss: 0.022570807486772537\n",
      "Epoch 26, Batch: 407: Training Loss: 0.01755218207836151, Validation Loss: 0.019835876300930977\n",
      "Epoch 26, Batch: 408: Training Loss: 0.02026117965579033, Validation Loss: 0.020690539851784706\n",
      "Epoch 26, Batch: 409: Training Loss: 0.019353346899151802, Validation Loss: 0.02347377873957157\n",
      "Epoch 26, Batch: 410: Training Loss: 0.019830193370580673, Validation Loss: 0.020803898572921753\n",
      "Epoch 26, Batch: 411: Training Loss: 0.01674613729119301, Validation Loss: 0.020395250990986824\n",
      "Epoch 26, Batch: 412: Training Loss: 0.020647786557674408, Validation Loss: 0.02215699292719364\n",
      "Epoch 26, Batch: 413: Training Loss: 0.023091593757271767, Validation Loss: 0.021602600812911987\n",
      "Epoch 26, Batch: 414: Training Loss: 0.019739458337426186, Validation Loss: 0.01895236037671566\n",
      "Epoch 26, Batch: 415: Training Loss: 0.022096438333392143, Validation Loss: 0.022274866700172424\n",
      "Epoch 26, Batch: 416: Training Loss: 0.01966327615082264, Validation Loss: 0.02051536552608013\n",
      "Epoch 26, Batch: 417: Training Loss: 0.022324008867144585, Validation Loss: 0.019328197464346886\n",
      "Epoch 26, Batch: 418: Training Loss: 0.01823526620864868, Validation Loss: 0.023576268926262856\n",
      "Epoch 26, Batch: 419: Training Loss: 0.02196037396788597, Validation Loss: 0.022723395377397537\n",
      "Epoch 26, Batch: 420: Training Loss: 0.023715302348136902, Validation Loss: 0.02128169685602188\n",
      "Epoch 26, Batch: 421: Training Loss: 0.020998407155275345, Validation Loss: 0.023324036970734596\n",
      "Epoch 26, Batch: 422: Training Loss: 0.021749619394540787, Validation Loss: 0.020923811942338943\n",
      "Epoch 26, Batch: 423: Training Loss: 0.022818706929683685, Validation Loss: 0.02287175878882408\n",
      "Epoch 26, Batch: 424: Training Loss: 0.023370204493403435, Validation Loss: 0.02273597940802574\n",
      "Epoch 26, Batch: 425: Training Loss: 0.02092532441020012, Validation Loss: 0.024017395451664925\n",
      "Epoch 26, Batch: 426: Training Loss: 0.01885065622627735, Validation Loss: 0.02329370565712452\n",
      "Epoch 26, Batch: 427: Training Loss: 0.020584912970662117, Validation Loss: 0.022882794961333275\n",
      "Epoch 26, Batch: 428: Training Loss: 0.020048754289746284, Validation Loss: 0.021599121391773224\n",
      "Epoch 26, Batch: 429: Training Loss: 0.01931651681661606, Validation Loss: 0.021365080028772354\n",
      "Epoch 26, Batch: 430: Training Loss: 0.01864788681268692, Validation Loss: 0.0230224821716547\n",
      "Epoch 26, Batch: 431: Training Loss: 0.018320487812161446, Validation Loss: 0.021397894248366356\n",
      "Epoch 26, Batch: 432: Training Loss: 0.020810900256037712, Validation Loss: 0.01970268040895462\n",
      "Epoch 26, Batch: 433: Training Loss: 0.01917138695716858, Validation Loss: 0.020061945542693138\n",
      "Epoch 26, Batch: 434: Training Loss: 0.01946154236793518, Validation Loss: 0.020224396139383316\n",
      "Epoch 26, Batch: 435: Training Loss: 0.021389277651906013, Validation Loss: 0.02118389680981636\n",
      "Epoch 26, Batch: 436: Training Loss: 0.02048674412071705, Validation Loss: 0.01967630535364151\n",
      "Epoch 26, Batch: 437: Training Loss: 0.01904553920030594, Validation Loss: 0.021765638142824173\n",
      "Epoch 26, Batch: 438: Training Loss: 0.02190360799431801, Validation Loss: 0.02037581242620945\n",
      "Epoch 26, Batch: 439: Training Loss: 0.01790272630751133, Validation Loss: 0.021494483575224876\n",
      "Epoch 26, Batch: 440: Training Loss: 0.022405777126550674, Validation Loss: 0.022493582218885422\n",
      "Epoch 26, Batch: 441: Training Loss: 0.020143406465649605, Validation Loss: 0.021798498928546906\n",
      "Epoch 26, Batch: 442: Training Loss: 0.022326074540615082, Validation Loss: 0.020791033282876015\n",
      "Epoch 26, Batch: 443: Training Loss: 0.02173502929508686, Validation Loss: 0.022233523428440094\n",
      "Epoch 26, Batch: 444: Training Loss: 0.01888439990580082, Validation Loss: 0.022048180922865868\n",
      "Epoch 26, Batch: 445: Training Loss: 0.018020741641521454, Validation Loss: 0.022239895537495613\n",
      "Epoch 26, Batch: 446: Training Loss: 0.023954978212714195, Validation Loss: 0.021202437579631805\n",
      "Epoch 26, Batch: 447: Training Loss: 0.020853262394666672, Validation Loss: 0.021965056657791138\n",
      "Epoch 26, Batch: 448: Training Loss: 0.01985042542219162, Validation Loss: 0.023926742374897003\n",
      "Epoch 26, Batch: 449: Training Loss: 0.02114104852080345, Validation Loss: 0.023532157763838768\n",
      "Epoch 26, Batch: 450: Training Loss: 0.0196794755756855, Validation Loss: 0.022687390446662903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch: 451: Training Loss: 0.021831165999174118, Validation Loss: 0.02204538881778717\n",
      "Epoch 26, Batch: 452: Training Loss: 0.021904874593019485, Validation Loss: 0.022684626281261444\n",
      "Epoch 26, Batch: 453: Training Loss: 0.021262425929307938, Validation Loss: 0.02471919171512127\n",
      "Epoch 26, Batch: 454: Training Loss: 0.020892759785056114, Validation Loss: 0.02281206101179123\n",
      "Epoch 26, Batch: 455: Training Loss: 0.02117159403860569, Validation Loss: 0.02456032857298851\n",
      "Epoch 26, Batch: 456: Training Loss: 0.020707551389932632, Validation Loss: 0.019852273166179657\n",
      "Epoch 26, Batch: 457: Training Loss: 0.022115610539913177, Validation Loss: 0.02234354242682457\n",
      "Epoch 26, Batch: 458: Training Loss: 0.015796104446053505, Validation Loss: 0.02214861288666725\n",
      "Epoch 26, Batch: 459: Training Loss: 0.02546333521604538, Validation Loss: 0.02254023216664791\n",
      "Epoch 26, Batch: 460: Training Loss: 0.022006889805197716, Validation Loss: 0.021481342613697052\n",
      "Epoch 26, Batch: 461: Training Loss: 0.02315152809023857, Validation Loss: 0.02138482592999935\n",
      "Epoch 26, Batch: 462: Training Loss: 0.01898365095257759, Validation Loss: 0.0215507410466671\n",
      "Epoch 26, Batch: 463: Training Loss: 0.023945273831486702, Validation Loss: 0.02258547581732273\n",
      "Epoch 26, Batch: 464: Training Loss: 0.0203304011374712, Validation Loss: 0.022664768621325493\n",
      "Epoch 26, Batch: 465: Training Loss: 0.023980863392353058, Validation Loss: 0.023721788078546524\n",
      "Epoch 26, Batch: 466: Training Loss: 0.02070155180990696, Validation Loss: 0.024115100502967834\n",
      "Epoch 26, Batch: 467: Training Loss: 0.029197154566645622, Validation Loss: 0.023518363013863564\n",
      "Epoch 26, Batch: 468: Training Loss: 0.025356385856866837, Validation Loss: 0.022567369043827057\n",
      "Epoch 26, Batch: 469: Training Loss: 0.021360045298933983, Validation Loss: 0.023698873817920685\n",
      "Epoch 26, Batch: 470: Training Loss: 0.020086269825696945, Validation Loss: 0.02206992171704769\n",
      "Epoch 26, Batch: 471: Training Loss: 0.0216364786028862, Validation Loss: 0.022472843527793884\n",
      "Epoch 26, Batch: 472: Training Loss: 0.020895758643746376, Validation Loss: 0.021983470767736435\n",
      "Epoch 26, Batch: 473: Training Loss: 0.023100517690181732, Validation Loss: 0.02225971780717373\n",
      "Epoch 26, Batch: 474: Training Loss: 0.018471824005246162, Validation Loss: 0.022144239395856857\n",
      "Epoch 26, Batch: 475: Training Loss: 0.022822843864560127, Validation Loss: 0.023293940350413322\n",
      "Epoch 26, Batch: 476: Training Loss: 0.01835780404508114, Validation Loss: 0.02466108277440071\n",
      "Epoch 26, Batch: 477: Training Loss: 0.024223286658525467, Validation Loss: 0.02533719688653946\n",
      "Epoch 26, Batch: 478: Training Loss: 0.017933133989572525, Validation Loss: 0.027068007737398148\n",
      "Epoch 26, Batch: 479: Training Loss: 0.022040439769625664, Validation Loss: 0.024642739444971085\n",
      "Epoch 26, Batch: 480: Training Loss: 0.024743225425481796, Validation Loss: 0.02548457868397236\n",
      "Epoch 26, Batch: 481: Training Loss: 0.01849227398633957, Validation Loss: 0.02346961572766304\n",
      "Epoch 26, Batch: 482: Training Loss: 0.02508961223065853, Validation Loss: 0.024372654035687447\n",
      "Epoch 26, Batch: 483: Training Loss: 0.018802335485816002, Validation Loss: 0.023700615391135216\n",
      "Epoch 26, Batch: 484: Training Loss: 0.017516490072011948, Validation Loss: 0.021074706688523293\n",
      "Epoch 26, Batch: 485: Training Loss: 0.020663339644670486, Validation Loss: 0.020867018029093742\n",
      "Epoch 26, Batch: 486: Training Loss: 0.023070378229022026, Validation Loss: 0.02221791073679924\n",
      "Epoch 26, Batch: 487: Training Loss: 0.017132196575403214, Validation Loss: 0.023237045854330063\n",
      "Epoch 26, Batch: 488: Training Loss: 0.020596878603100777, Validation Loss: 0.022634590044617653\n",
      "Epoch 26, Batch: 489: Training Loss: 0.024656573310494423, Validation Loss: 0.02244642749428749\n",
      "Epoch 26, Batch: 490: Training Loss: 0.02200043573975563, Validation Loss: 0.023889966309070587\n",
      "Epoch 26, Batch: 491: Training Loss: 0.017420688644051552, Validation Loss: 0.020438965409994125\n",
      "Epoch 26, Batch: 492: Training Loss: 0.0206450168043375, Validation Loss: 0.02256564237177372\n",
      "Epoch 26, Batch: 493: Training Loss: 0.020080920308828354, Validation Loss: 0.022709989920258522\n",
      "Epoch 26, Batch: 494: Training Loss: 0.02309790812432766, Validation Loss: 0.02075166255235672\n",
      "Epoch 26, Batch: 495: Training Loss: 0.017215322703123093, Validation Loss: 0.019312342628836632\n",
      "Epoch 26, Batch: 496: Training Loss: 0.02131955698132515, Validation Loss: 0.02057436667382717\n",
      "Epoch 26, Batch: 497: Training Loss: 0.02080037072300911, Validation Loss: 0.020695149898529053\n",
      "Epoch 26, Batch: 498: Training Loss: 0.022518711164593697, Validation Loss: 0.02072274312376976\n",
      "Epoch 26, Batch: 499: Training Loss: 0.019171344116330147, Validation Loss: 0.02086362987756729\n",
      "Epoch 27, Batch: 0: Training Loss: 0.02148331329226494, Validation Loss: 0.025303974747657776\n",
      "Epoch 27, Batch: 1: Training Loss: 0.01820610836148262, Validation Loss: 0.021075226366519928\n",
      "Epoch 27, Batch: 2: Training Loss: 0.021135060116648674, Validation Loss: 0.022351890802383423\n",
      "Epoch 27, Batch: 3: Training Loss: 0.019684622064232826, Validation Loss: 0.02366744354367256\n",
      "Epoch 27, Batch: 4: Training Loss: 0.02191253937780857, Validation Loss: 0.022635657340288162\n",
      "Epoch 27, Batch: 5: Training Loss: 0.022775139659643173, Validation Loss: 0.02246997319161892\n",
      "Epoch 27, Batch: 6: Training Loss: 0.018183639273047447, Validation Loss: 0.024148816242814064\n",
      "Epoch 27, Batch: 7: Training Loss: 0.020786164328455925, Validation Loss: 0.02420438826084137\n",
      "Epoch 27, Batch: 8: Training Loss: 0.021517569199204445, Validation Loss: 0.022775540128350258\n",
      "Epoch 27, Batch: 9: Training Loss: 0.021027639508247375, Validation Loss: 0.022395530715584755\n",
      "Epoch 27, Batch: 10: Training Loss: 0.018703151494264603, Validation Loss: 0.022150715813040733\n",
      "Epoch 27, Batch: 11: Training Loss: 0.02310319058597088, Validation Loss: 0.021004553884267807\n",
      "Epoch 27, Batch: 12: Training Loss: 0.024847988039255142, Validation Loss: 0.020226622000336647\n",
      "Epoch 27, Batch: 13: Training Loss: 0.026355087757110596, Validation Loss: 0.02063455618917942\n",
      "Epoch 27, Batch: 14: Training Loss: 0.02387339249253273, Validation Loss: 0.022279884666204453\n",
      "Epoch 27, Batch: 15: Training Loss: 0.02278972789645195, Validation Loss: 0.021090734750032425\n",
      "Epoch 27, Batch: 16: Training Loss: 0.021767793223261833, Validation Loss: 0.02265087328851223\n",
      "Epoch 27, Batch: 17: Training Loss: 0.01709849387407303, Validation Loss: 0.01923338882625103\n",
      "Epoch 27, Batch: 18: Training Loss: 0.021309593692421913, Validation Loss: 0.021873455494642258\n",
      "Epoch 27, Batch: 19: Training Loss: 0.022432981058955193, Validation Loss: 0.020194189622998238\n",
      "Epoch 27, Batch: 20: Training Loss: 0.019130051136016846, Validation Loss: 0.0212707556784153\n",
      "Epoch 27, Batch: 21: Training Loss: 0.019156359136104584, Validation Loss: 0.020977186039090157\n",
      "Epoch 27, Batch: 22: Training Loss: 0.02195311337709427, Validation Loss: 0.019300876185297966\n",
      "Epoch 27, Batch: 23: Training Loss: 0.02119629830121994, Validation Loss: 0.020422032102942467\n",
      "Epoch 27, Batch: 24: Training Loss: 0.02070138417184353, Validation Loss: 0.022271670401096344\n",
      "Epoch 27, Batch: 25: Training Loss: 0.0213958490639925, Validation Loss: 0.020992498844861984\n",
      "Epoch 27, Batch: 26: Training Loss: 0.018919331952929497, Validation Loss: 0.01977868378162384\n",
      "Epoch 27, Batch: 27: Training Loss: 0.021737515926361084, Validation Loss: 0.018371056765317917\n",
      "Epoch 27, Batch: 28: Training Loss: 0.022676434367895126, Validation Loss: 0.02136549912393093\n",
      "Epoch 27, Batch: 29: Training Loss: 0.02330353669822216, Validation Loss: 0.02263902872800827\n",
      "Epoch 27, Batch: 30: Training Loss: 0.024101771414279938, Validation Loss: 0.02084997296333313\n",
      "Epoch 27, Batch: 31: Training Loss: 0.024997619912028313, Validation Loss: 0.02153692953288555\n",
      "Epoch 27, Batch: 32: Training Loss: 0.0212236437946558, Validation Loss: 0.02146291919052601\n",
      "Epoch 27, Batch: 33: Training Loss: 0.020344048738479614, Validation Loss: 0.021437350660562515\n",
      "Epoch 27, Batch: 34: Training Loss: 0.018113819882273674, Validation Loss: 0.022583188489079475\n",
      "Epoch 27, Batch: 35: Training Loss: 0.018734591081738472, Validation Loss: 0.020200517028570175\n",
      "Epoch 27, Batch: 36: Training Loss: 0.018732372671365738, Validation Loss: 0.02120090276002884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 37: Training Loss: 0.019060667604207993, Validation Loss: 0.019073311239480972\n",
      "Epoch 27, Batch: 38: Training Loss: 0.022597894072532654, Validation Loss: 0.023309923708438873\n",
      "Epoch 27, Batch: 39: Training Loss: 0.021265309303998947, Validation Loss: 0.021579347550868988\n",
      "Epoch 27, Batch: 40: Training Loss: 0.025570666417479515, Validation Loss: 0.022859923541545868\n",
      "Epoch 27, Batch: 41: Training Loss: 0.01976192742586136, Validation Loss: 0.021343892440199852\n",
      "Epoch 27, Batch: 42: Training Loss: 0.0200541689991951, Validation Loss: 0.024806203320622444\n",
      "Epoch 27, Batch: 43: Training Loss: 0.019937416538596153, Validation Loss: 0.026019234210252762\n",
      "Epoch 27, Batch: 44: Training Loss: 0.020011499524116516, Validation Loss: 0.023702144622802734\n",
      "Epoch 27, Batch: 45: Training Loss: 0.020582830533385277, Validation Loss: 0.023925362154841423\n",
      "Epoch 27, Batch: 46: Training Loss: 0.02432096190750599, Validation Loss: 0.023756150156259537\n",
      "Epoch 27, Batch: 47: Training Loss: 0.02058924362063408, Validation Loss: 0.020434098318219185\n",
      "Epoch 27, Batch: 48: Training Loss: 0.022582020610570908, Validation Loss: 0.020808717235922813\n",
      "Epoch 27, Batch: 49: Training Loss: 0.02129173092544079, Validation Loss: 0.021224848926067352\n",
      "Epoch 27, Batch: 50: Training Loss: 0.021546630188822746, Validation Loss: 0.021701671183109283\n",
      "Epoch 27, Batch: 51: Training Loss: 0.02491912432014942, Validation Loss: 0.022858083248138428\n",
      "Epoch 27, Batch: 52: Training Loss: 0.019169652834534645, Validation Loss: 0.02357715182006359\n",
      "Epoch 27, Batch: 53: Training Loss: 0.02046790160238743, Validation Loss: 0.022695312276482582\n",
      "Epoch 27, Batch: 54: Training Loss: 0.020915614441037178, Validation Loss: 0.021641911938786507\n",
      "Epoch 27, Batch: 55: Training Loss: 0.020988913252949715, Validation Loss: 0.024904340505599976\n",
      "Epoch 27, Batch: 56: Training Loss: 0.024935780093073845, Validation Loss: 0.024588530883193016\n",
      "Epoch 27, Batch: 57: Training Loss: 0.021830815821886063, Validation Loss: 0.026636241003870964\n",
      "Epoch 27, Batch: 58: Training Loss: 0.02203361503779888, Validation Loss: 0.024050656706094742\n",
      "Epoch 27, Batch: 59: Training Loss: 0.017141737043857574, Validation Loss: 0.021771825850009918\n",
      "Epoch 27, Batch: 60: Training Loss: 0.018008820712566376, Validation Loss: 0.022114843130111694\n",
      "Epoch 27, Batch: 61: Training Loss: 0.02317417971789837, Validation Loss: 0.024034082889556885\n",
      "Epoch 27, Batch: 62: Training Loss: 0.022962618619203568, Validation Loss: 0.021860258653759956\n",
      "Epoch 27, Batch: 63: Training Loss: 0.019999444484710693, Validation Loss: 0.02155028097331524\n",
      "Epoch 27, Batch: 64: Training Loss: 0.022657610476017, Validation Loss: 0.020922524854540825\n",
      "Epoch 27, Batch: 65: Training Loss: 0.024011807516217232, Validation Loss: 0.022810451686382294\n",
      "Epoch 27, Batch: 66: Training Loss: 0.018599241971969604, Validation Loss: 0.01997441239655018\n",
      "Epoch 27, Batch: 67: Training Loss: 0.019724341109395027, Validation Loss: 0.01854836940765381\n",
      "Epoch 27, Batch: 68: Training Loss: 0.018982570618391037, Validation Loss: 0.02035929448902607\n",
      "Epoch 27, Batch: 69: Training Loss: 0.021605832502245903, Validation Loss: 0.0193173885345459\n",
      "Epoch 27, Batch: 70: Training Loss: 0.023722024634480476, Validation Loss: 0.02226254716515541\n",
      "Epoch 27, Batch: 71: Training Loss: 0.017074916511774063, Validation Loss: 0.01967475190758705\n",
      "Epoch 27, Batch: 72: Training Loss: 0.021172942593693733, Validation Loss: 0.018776684999465942\n",
      "Epoch 27, Batch: 73: Training Loss: 0.020882857963442802, Validation Loss: 0.01845308020710945\n",
      "Epoch 27, Batch: 74: Training Loss: 0.020789433270692825, Validation Loss: 0.019212963059544563\n",
      "Epoch 27, Batch: 75: Training Loss: 0.01881323754787445, Validation Loss: 0.019381053745746613\n",
      "Epoch 27, Batch: 76: Training Loss: 0.016793670132756233, Validation Loss: 0.021227765828371048\n",
      "Saving new best model w/ loss: 0.017877046018838882\n",
      "Epoch 27, Batch: 77: Training Loss: 0.022747524082660675, Validation Loss: 0.017877046018838882\n",
      "Epoch 27, Batch: 78: Training Loss: 0.021988822147250175, Validation Loss: 0.023177163675427437\n",
      "Epoch 27, Batch: 79: Training Loss: 0.020061489194631577, Validation Loss: 0.01980360597372055\n",
      "Epoch 27, Batch: 80: Training Loss: 0.01910831592977047, Validation Loss: 0.019913041964173317\n",
      "Epoch 27, Batch: 81: Training Loss: 0.023141924291849136, Validation Loss: 0.021448727697134018\n",
      "Epoch 27, Batch: 82: Training Loss: 0.021733000874519348, Validation Loss: 0.018828194588422775\n",
      "Epoch 27, Batch: 83: Training Loss: 0.02154534123837948, Validation Loss: 0.021220669150352478\n",
      "Epoch 27, Batch: 84: Training Loss: 0.022525370121002197, Validation Loss: 0.020694606006145477\n",
      "Epoch 27, Batch: 85: Training Loss: 0.018773602321743965, Validation Loss: 0.020885786041617393\n",
      "Epoch 27, Batch: 86: Training Loss: 0.022118566557765007, Validation Loss: 0.019712423905730247\n",
      "Epoch 27, Batch: 87: Training Loss: 0.023374315351247787, Validation Loss: 0.020372726023197174\n",
      "Epoch 27, Batch: 88: Training Loss: 0.020853806287050247, Validation Loss: 0.021448882296681404\n",
      "Epoch 27, Batch: 89: Training Loss: 0.020098533481359482, Validation Loss: 0.020336221903562546\n",
      "Epoch 27, Batch: 90: Training Loss: 0.019671371206641197, Validation Loss: 0.01979384385049343\n",
      "Epoch 27, Batch: 91: Training Loss: 0.02231811173260212, Validation Loss: 0.020149171352386475\n",
      "Epoch 27, Batch: 92: Training Loss: 0.021574582904577255, Validation Loss: 0.020576344802975655\n",
      "Epoch 27, Batch: 93: Training Loss: 0.024146512150764465, Validation Loss: 0.019040681421756744\n",
      "Epoch 27, Batch: 94: Training Loss: 0.021583670750260353, Validation Loss: 0.01977870985865593\n",
      "Epoch 27, Batch: 95: Training Loss: 0.020599570125341415, Validation Loss: 0.0192355215549469\n",
      "Epoch 27, Batch: 96: Training Loss: 0.018884029239416122, Validation Loss: 0.02078494057059288\n",
      "Epoch 27, Batch: 97: Training Loss: 0.022675860673189163, Validation Loss: 0.020383059978485107\n",
      "Epoch 27, Batch: 98: Training Loss: 0.020837394520640373, Validation Loss: 0.02160666510462761\n",
      "Epoch 27, Batch: 99: Training Loss: 0.021411269903182983, Validation Loss: 0.01918031834065914\n",
      "Epoch 27, Batch: 100: Training Loss: 0.023660439997911453, Validation Loss: 0.02155216783285141\n",
      "Epoch 27, Batch: 101: Training Loss: 0.019503198564052582, Validation Loss: 0.021344304084777832\n",
      "Epoch 27, Batch: 102: Training Loss: 0.02098494954407215, Validation Loss: 0.020163454115390778\n",
      "Epoch 27, Batch: 103: Training Loss: 0.021500108763575554, Validation Loss: 0.021661022678017616\n",
      "Epoch 27, Batch: 104: Training Loss: 0.017155490815639496, Validation Loss: 0.023502735421061516\n",
      "Epoch 27, Batch: 105: Training Loss: 0.019510896876454353, Validation Loss: 0.02167179249227047\n",
      "Epoch 27, Batch: 106: Training Loss: 0.021297378465533257, Validation Loss: 0.02294578216969967\n",
      "Epoch 27, Batch: 107: Training Loss: 0.021647844463586807, Validation Loss: 0.020123830065131187\n",
      "Epoch 27, Batch: 108: Training Loss: 0.017905395478010178, Validation Loss: 0.022911688312888145\n",
      "Epoch 27, Batch: 109: Training Loss: 0.022953422740101814, Validation Loss: 0.022476742044091225\n",
      "Epoch 27, Batch: 110: Training Loss: 0.02055208757519722, Validation Loss: 0.021904710680246353\n",
      "Epoch 27, Batch: 111: Training Loss: 0.01877480372786522, Validation Loss: 0.02018083818256855\n",
      "Epoch 27, Batch: 112: Training Loss: 0.01858622394502163, Validation Loss: 0.023295339196920395\n",
      "Epoch 27, Batch: 113: Training Loss: 0.022347288206219673, Validation Loss: 0.019157662987709045\n",
      "Epoch 27, Batch: 114: Training Loss: 0.0208439938724041, Validation Loss: 0.02139216661453247\n",
      "Epoch 27, Batch: 115: Training Loss: 0.02160804532468319, Validation Loss: 0.021970802918076515\n",
      "Epoch 27, Batch: 116: Training Loss: 0.02040211670100689, Validation Loss: 0.022285034880042076\n",
      "Epoch 27, Batch: 117: Training Loss: 0.021999569609761238, Validation Loss: 0.02109384536743164\n",
      "Epoch 27, Batch: 118: Training Loss: 0.020528586581349373, Validation Loss: 0.021554812788963318\n",
      "Epoch 27, Batch: 119: Training Loss: 0.020983319729566574, Validation Loss: 0.020410148426890373\n",
      "Saving new best model w/ loss: 0.017855320125818253\n",
      "Epoch 27, Batch: 120: Training Loss: 0.020829124376177788, Validation Loss: 0.017855320125818253\n",
      "Epoch 27, Batch: 121: Training Loss: 0.019858378916978836, Validation Loss: 0.019624166190624237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 122: Training Loss: 0.02043243870139122, Validation Loss: 0.022357052192091942\n",
      "Epoch 27, Batch: 123: Training Loss: 0.01774272508919239, Validation Loss: 0.020899862051010132\n",
      "Epoch 27, Batch: 124: Training Loss: 0.0210499856621027, Validation Loss: 0.02277218922972679\n",
      "Epoch 27, Batch: 125: Training Loss: 0.02053893357515335, Validation Loss: 0.021105455234646797\n",
      "Epoch 27, Batch: 126: Training Loss: 0.02003651112318039, Validation Loss: 0.0202067568898201\n",
      "Epoch 27, Batch: 127: Training Loss: 0.02072756551206112, Validation Loss: 0.018819577991962433\n",
      "Epoch 27, Batch: 128: Training Loss: 0.018180284649133682, Validation Loss: 0.020244218409061432\n",
      "Epoch 27, Batch: 129: Training Loss: 0.016573643311858177, Validation Loss: 0.019012970849871635\n",
      "Epoch 27, Batch: 130: Training Loss: 0.021189527586102486, Validation Loss: 0.021716637536883354\n",
      "Epoch 27, Batch: 131: Training Loss: 0.021817171946167946, Validation Loss: 0.0208441074937582\n",
      "Epoch 27, Batch: 132: Training Loss: 0.0209759920835495, Validation Loss: 0.022030474618077278\n",
      "Epoch 27, Batch: 133: Training Loss: 0.018489964306354523, Validation Loss: 0.022121062502264977\n",
      "Epoch 27, Batch: 134: Training Loss: 0.023003509268164635, Validation Loss: 0.022018011659383774\n",
      "Epoch 27, Batch: 135: Training Loss: 0.02155771292746067, Validation Loss: 0.021476153284311295\n",
      "Epoch 27, Batch: 136: Training Loss: 0.021714769303798676, Validation Loss: 0.02129283733665943\n",
      "Epoch 27, Batch: 137: Training Loss: 0.018495528027415276, Validation Loss: 0.022469278424978256\n",
      "Epoch 27, Batch: 138: Training Loss: 0.019498547539114952, Validation Loss: 0.021637048572301865\n",
      "Epoch 27, Batch: 139: Training Loss: 0.01955604739487171, Validation Loss: 0.023125123232603073\n",
      "Epoch 27, Batch: 140: Training Loss: 0.021173030138015747, Validation Loss: 0.020424125716090202\n",
      "Epoch 27, Batch: 141: Training Loss: 0.021094251424074173, Validation Loss: 0.021654190495610237\n",
      "Epoch 27, Batch: 142: Training Loss: 0.020293837413191795, Validation Loss: 0.02398763783276081\n",
      "Epoch 27, Batch: 143: Training Loss: 0.018502093851566315, Validation Loss: 0.022933809086680412\n",
      "Epoch 27, Batch: 144: Training Loss: 0.02063676528632641, Validation Loss: 0.02368554286658764\n",
      "Epoch 27, Batch: 145: Training Loss: 0.019598430022597313, Validation Loss: 0.023916220292448997\n",
      "Epoch 27, Batch: 146: Training Loss: 0.022515272721648216, Validation Loss: 0.02248167060315609\n",
      "Epoch 27, Batch: 147: Training Loss: 0.01989714428782463, Validation Loss: 0.02162155695259571\n",
      "Epoch 27, Batch: 148: Training Loss: 0.01876097545027733, Validation Loss: 0.02302922122180462\n",
      "Epoch 27, Batch: 149: Training Loss: 0.024387238547205925, Validation Loss: 0.022646818310022354\n",
      "Epoch 27, Batch: 150: Training Loss: 0.02570004016160965, Validation Loss: 0.024748971685767174\n",
      "Epoch 27, Batch: 151: Training Loss: 0.02383463643491268, Validation Loss: 0.021995896473526955\n",
      "Epoch 27, Batch: 152: Training Loss: 0.02273714356124401, Validation Loss: 0.0236036516726017\n",
      "Epoch 27, Batch: 153: Training Loss: 0.01987902633845806, Validation Loss: 0.021737856790423393\n",
      "Epoch 27, Batch: 154: Training Loss: 0.01835176721215248, Validation Loss: 0.019944308325648308\n",
      "Epoch 27, Batch: 155: Training Loss: 0.02417791821062565, Validation Loss: 0.02196471393108368\n",
      "Epoch 27, Batch: 156: Training Loss: 0.01769806630909443, Validation Loss: 0.021524930372834206\n",
      "Epoch 27, Batch: 157: Training Loss: 0.019543524831533432, Validation Loss: 0.021596252918243408\n",
      "Epoch 27, Batch: 158: Training Loss: 0.019597703590989113, Validation Loss: 0.024026421830058098\n",
      "Epoch 27, Batch: 159: Training Loss: 0.02081902138888836, Validation Loss: 0.021282251924276352\n",
      "Epoch 27, Batch: 160: Training Loss: 0.019377335906028748, Validation Loss: 0.022724376991391182\n",
      "Epoch 27, Batch: 161: Training Loss: 0.021572371944785118, Validation Loss: 0.022801348939538002\n",
      "Epoch 27, Batch: 162: Training Loss: 0.02297527901828289, Validation Loss: 0.021689947694540024\n",
      "Epoch 27, Batch: 163: Training Loss: 0.02095043659210205, Validation Loss: 0.020322829484939575\n",
      "Epoch 27, Batch: 164: Training Loss: 0.021109547466039658, Validation Loss: 0.02139092981815338\n",
      "Epoch 27, Batch: 165: Training Loss: 0.02400749921798706, Validation Loss: 0.020141571760177612\n",
      "Epoch 27, Batch: 166: Training Loss: 0.019773364067077637, Validation Loss: 0.022364797070622444\n",
      "Epoch 27, Batch: 167: Training Loss: 0.02052440121769905, Validation Loss: 0.021741241216659546\n",
      "Epoch 27, Batch: 168: Training Loss: 0.023691067472100258, Validation Loss: 0.024018390104174614\n",
      "Epoch 27, Batch: 169: Training Loss: 0.020836111158132553, Validation Loss: 0.021077755838632584\n",
      "Epoch 27, Batch: 170: Training Loss: 0.02013341523706913, Validation Loss: 0.019277013838291168\n",
      "Epoch 27, Batch: 171: Training Loss: 0.01841525360941887, Validation Loss: 0.0231392290443182\n",
      "Epoch 27, Batch: 172: Training Loss: 0.021771684288978577, Validation Loss: 0.021171366795897484\n",
      "Epoch 27, Batch: 173: Training Loss: 0.01800357736647129, Validation Loss: 0.023800639435648918\n",
      "Epoch 27, Batch: 174: Training Loss: 0.020400844514369965, Validation Loss: 0.023247025907039642\n",
      "Epoch 27, Batch: 175: Training Loss: 0.021356794983148575, Validation Loss: 0.024551400914788246\n",
      "Epoch 27, Batch: 176: Training Loss: 0.02432107925415039, Validation Loss: 0.02232958935201168\n",
      "Epoch 27, Batch: 177: Training Loss: 0.018260231241583824, Validation Loss: 0.022844575345516205\n",
      "Epoch 27, Batch: 178: Training Loss: 0.022861266508698463, Validation Loss: 0.0262452382594347\n",
      "Epoch 27, Batch: 179: Training Loss: 0.019707635045051575, Validation Loss: 0.026201866567134857\n",
      "Epoch 27, Batch: 180: Training Loss: 0.02277098409831524, Validation Loss: 0.02218809723854065\n",
      "Epoch 27, Batch: 181: Training Loss: 0.02316044084727764, Validation Loss: 0.022903436794877052\n",
      "Epoch 27, Batch: 182: Training Loss: 0.02364690974354744, Validation Loss: 0.02355029620230198\n",
      "Epoch 27, Batch: 183: Training Loss: 0.0218480434268713, Validation Loss: 0.0249362550675869\n",
      "Epoch 27, Batch: 184: Training Loss: 0.02145812101662159, Validation Loss: 0.02344481460750103\n",
      "Epoch 27, Batch: 185: Training Loss: 0.025933092460036278, Validation Loss: 0.022268854081630707\n",
      "Epoch 27, Batch: 186: Training Loss: 0.0205635204911232, Validation Loss: 0.023844709619879723\n",
      "Epoch 27, Batch: 187: Training Loss: 0.024035276845097542, Validation Loss: 0.024928255006670952\n",
      "Epoch 27, Batch: 188: Training Loss: 0.021369874477386475, Validation Loss: 0.023390481248497963\n",
      "Epoch 27, Batch: 189: Training Loss: 0.01962931826710701, Validation Loss: 0.021953700110316277\n",
      "Epoch 27, Batch: 190: Training Loss: 0.022588195279240608, Validation Loss: 0.022236937656998634\n",
      "Epoch 27, Batch: 191: Training Loss: 0.021649230271577835, Validation Loss: 0.021983982995152473\n",
      "Epoch 27, Batch: 192: Training Loss: 0.021892348304390907, Validation Loss: 0.025780659168958664\n",
      "Epoch 27, Batch: 193: Training Loss: 0.02243066392838955, Validation Loss: 0.024435056373476982\n",
      "Epoch 27, Batch: 194: Training Loss: 0.021272268146276474, Validation Loss: 0.02322925627231598\n",
      "Epoch 27, Batch: 195: Training Loss: 0.022988008335232735, Validation Loss: 0.022442897781729698\n",
      "Epoch 27, Batch: 196: Training Loss: 0.02531813457608223, Validation Loss: 0.023592472076416016\n",
      "Epoch 27, Batch: 197: Training Loss: 0.01825536973774433, Validation Loss: 0.023669345304369926\n",
      "Epoch 27, Batch: 198: Training Loss: 0.01980116032063961, Validation Loss: 0.023668896406888962\n",
      "Epoch 27, Batch: 199: Training Loss: 0.022257380187511444, Validation Loss: 0.022423535585403442\n",
      "Epoch 27, Batch: 200: Training Loss: 0.018103381618857384, Validation Loss: 0.023344483226537704\n",
      "Epoch 27, Batch: 201: Training Loss: 0.024359507486224174, Validation Loss: 0.022412670776247978\n",
      "Epoch 27, Batch: 202: Training Loss: 0.023271450772881508, Validation Loss: 0.022418322041630745\n",
      "Epoch 27, Batch: 203: Training Loss: 0.020765550434589386, Validation Loss: 0.021084971725940704\n",
      "Epoch 27, Batch: 204: Training Loss: 0.022086219862103462, Validation Loss: 0.021815484389662743\n",
      "Epoch 27, Batch: 205: Training Loss: 0.025716084986925125, Validation Loss: 0.022048471495509148\n",
      "Epoch 27, Batch: 206: Training Loss: 0.019942626357078552, Validation Loss: 0.022083155810832977\n",
      "Epoch 27, Batch: 207: Training Loss: 0.02094901353120804, Validation Loss: 0.020750906318426132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 208: Training Loss: 0.019282270222902298, Validation Loss: 0.024998661130666733\n",
      "Epoch 27, Batch: 209: Training Loss: 0.01997300609946251, Validation Loss: 0.02495863474905491\n",
      "Epoch 27, Batch: 210: Training Loss: 0.0200177188962698, Validation Loss: 0.02239975333213806\n",
      "Epoch 27, Batch: 211: Training Loss: 0.023080676794052124, Validation Loss: 0.02354557439684868\n",
      "Epoch 27, Batch: 212: Training Loss: 0.023359257727861404, Validation Loss: 0.021680118516087532\n",
      "Epoch 27, Batch: 213: Training Loss: 0.023062996566295624, Validation Loss: 0.02018583007156849\n",
      "Epoch 27, Batch: 214: Training Loss: 0.018092000856995583, Validation Loss: 0.02238021418452263\n",
      "Epoch 27, Batch: 215: Training Loss: 0.020610451698303223, Validation Loss: 0.023153357207775116\n",
      "Epoch 27, Batch: 216: Training Loss: 0.022133540362119675, Validation Loss: 0.022220605984330177\n",
      "Epoch 27, Batch: 217: Training Loss: 0.02161896415054798, Validation Loss: 0.022642094641923904\n",
      "Epoch 27, Batch: 218: Training Loss: 0.019627133384346962, Validation Loss: 0.01944696716964245\n",
      "Epoch 27, Batch: 219: Training Loss: 0.0202461127191782, Validation Loss: 0.02438514120876789\n",
      "Epoch 27, Batch: 220: Training Loss: 0.02053014375269413, Validation Loss: 0.021989736706018448\n",
      "Epoch 27, Batch: 221: Training Loss: 0.017469394952058792, Validation Loss: 0.022270331159234047\n",
      "Epoch 27, Batch: 222: Training Loss: 0.02077190764248371, Validation Loss: 0.02234542742371559\n",
      "Epoch 27, Batch: 223: Training Loss: 0.018819883465766907, Validation Loss: 0.022919563576579094\n",
      "Epoch 27, Batch: 224: Training Loss: 0.020561005920171738, Validation Loss: 0.020659662783145905\n",
      "Epoch 27, Batch: 225: Training Loss: 0.020345961675047874, Validation Loss: 0.022337643429636955\n",
      "Epoch 27, Batch: 226: Training Loss: 0.020461110398173332, Validation Loss: 0.020769719034433365\n",
      "Epoch 27, Batch: 227: Training Loss: 0.02010197378695011, Validation Loss: 0.023550106212496758\n",
      "Epoch 27, Batch: 228: Training Loss: 0.024074047803878784, Validation Loss: 0.023813197389245033\n",
      "Epoch 27, Batch: 229: Training Loss: 0.025545982643961906, Validation Loss: 0.022952871397137642\n",
      "Epoch 27, Batch: 230: Training Loss: 0.020838014781475067, Validation Loss: 0.020570317283272743\n",
      "Epoch 27, Batch: 231: Training Loss: 0.024397052824497223, Validation Loss: 0.02175120823085308\n",
      "Epoch 27, Batch: 232: Training Loss: 0.021707061678171158, Validation Loss: 0.022999875247478485\n",
      "Epoch 27, Batch: 233: Training Loss: 0.016233207657933235, Validation Loss: 0.022551611065864563\n",
      "Epoch 27, Batch: 234: Training Loss: 0.027526866644620895, Validation Loss: 0.020806215703487396\n",
      "Epoch 27, Batch: 235: Training Loss: 0.02488359808921814, Validation Loss: 0.022758422419428825\n",
      "Epoch 27, Batch: 236: Training Loss: 0.025050725787878036, Validation Loss: 0.021747324615716934\n",
      "Epoch 27, Batch: 237: Training Loss: 0.023181412369012833, Validation Loss: 0.022221265360713005\n",
      "Epoch 27, Batch: 238: Training Loss: 0.022888939827680588, Validation Loss: 0.020746158435940742\n",
      "Epoch 27, Batch: 239: Training Loss: 0.024160757660865784, Validation Loss: 0.021310972049832344\n",
      "Epoch 27, Batch: 240: Training Loss: 0.018133055418729782, Validation Loss: 0.024593466892838478\n",
      "Epoch 27, Batch: 241: Training Loss: 0.020253565162420273, Validation Loss: 0.0229507964104414\n",
      "Epoch 27, Batch: 242: Training Loss: 0.023098502308130264, Validation Loss: 0.022973934188485146\n",
      "Epoch 27, Batch: 243: Training Loss: 0.02475038170814514, Validation Loss: 0.02404080331325531\n",
      "Epoch 27, Batch: 244: Training Loss: 0.02173534408211708, Validation Loss: 0.025475814938545227\n",
      "Epoch 27, Batch: 245: Training Loss: 0.023839060217142105, Validation Loss: 0.02480037324130535\n",
      "Epoch 27, Batch: 246: Training Loss: 0.019844669848680496, Validation Loss: 0.024127811193466187\n",
      "Epoch 27, Batch: 247: Training Loss: 0.021122904494404793, Validation Loss: 0.025167176499962807\n",
      "Epoch 27, Batch: 248: Training Loss: 0.020528314635157585, Validation Loss: 0.026214180514216423\n",
      "Epoch 27, Batch: 249: Training Loss: 0.01849106140434742, Validation Loss: 0.026419637724757195\n",
      "Epoch 27, Batch: 250: Training Loss: 0.01996806263923645, Validation Loss: 0.026711415499448776\n",
      "Epoch 27, Batch: 251: Training Loss: 0.020350586622953415, Validation Loss: 0.022700268775224686\n",
      "Epoch 27, Batch: 252: Training Loss: 0.018585992977023125, Validation Loss: 0.02460194006562233\n",
      "Epoch 27, Batch: 253: Training Loss: 0.025378048419952393, Validation Loss: 0.023473121225833893\n",
      "Epoch 27, Batch: 254: Training Loss: 0.01931009814143181, Validation Loss: 0.024442624300718307\n",
      "Epoch 27, Batch: 255: Training Loss: 0.019491149112582207, Validation Loss: 0.02429179474711418\n",
      "Epoch 27, Batch: 256: Training Loss: 0.02371244877576828, Validation Loss: 0.024080682545900345\n",
      "Epoch 27, Batch: 257: Training Loss: 0.024796463549137115, Validation Loss: 0.024875182658433914\n",
      "Epoch 27, Batch: 258: Training Loss: 0.022176014259457588, Validation Loss: 0.025783803313970566\n",
      "Epoch 27, Batch: 259: Training Loss: 0.02299944870173931, Validation Loss: 0.02381257899105549\n",
      "Epoch 27, Batch: 260: Training Loss: 0.021500911563634872, Validation Loss: 0.0227830670773983\n",
      "Epoch 27, Batch: 261: Training Loss: 0.018952282145619392, Validation Loss: 0.024635909125208855\n",
      "Epoch 27, Batch: 262: Training Loss: 0.020509148016572, Validation Loss: 0.026021186262369156\n",
      "Epoch 27, Batch: 263: Training Loss: 0.017374295741319656, Validation Loss: 0.024876290932297707\n",
      "Epoch 27, Batch: 264: Training Loss: 0.018119661137461662, Validation Loss: 0.025744128972291946\n",
      "Epoch 27, Batch: 265: Training Loss: 0.018669987097382545, Validation Loss: 0.0272702444344759\n",
      "Epoch 27, Batch: 266: Training Loss: 0.0213440153747797, Validation Loss: 0.025022365152835846\n",
      "Epoch 27, Batch: 267: Training Loss: 0.01995151676237583, Validation Loss: 0.024961533024907112\n",
      "Epoch 27, Batch: 268: Training Loss: 0.019214916974306107, Validation Loss: 0.02523449994623661\n",
      "Epoch 27, Batch: 269: Training Loss: 0.019683167338371277, Validation Loss: 0.024621685966849327\n",
      "Epoch 27, Batch: 270: Training Loss: 0.020347077399492264, Validation Loss: 0.02323169633746147\n",
      "Epoch 27, Batch: 271: Training Loss: 0.01749134249985218, Validation Loss: 0.025632183998823166\n",
      "Epoch 27, Batch: 272: Training Loss: 0.019148606806993484, Validation Loss: 0.021612541750073433\n",
      "Epoch 27, Batch: 273: Training Loss: 0.018598759546875954, Validation Loss: 0.0256605613976717\n",
      "Epoch 27, Batch: 274: Training Loss: 0.018864931538701057, Validation Loss: 0.02387263998389244\n",
      "Epoch 27, Batch: 275: Training Loss: 0.022309904918074608, Validation Loss: 0.02343088760972023\n",
      "Epoch 27, Batch: 276: Training Loss: 0.02217133715748787, Validation Loss: 0.023887842893600464\n",
      "Epoch 27, Batch: 277: Training Loss: 0.022960688918828964, Validation Loss: 0.02593139000236988\n",
      "Epoch 27, Batch: 278: Training Loss: 0.01906936801970005, Validation Loss: 0.02361007034778595\n",
      "Epoch 27, Batch: 279: Training Loss: 0.02349391207098961, Validation Loss: 0.025561803951859474\n",
      "Epoch 27, Batch: 280: Training Loss: 0.01822444051504135, Validation Loss: 0.025220898911356926\n",
      "Epoch 27, Batch: 281: Training Loss: 0.021679982542991638, Validation Loss: 0.023269593715667725\n",
      "Epoch 27, Batch: 282: Training Loss: 0.02113300748169422, Validation Loss: 0.024186067283153534\n",
      "Epoch 27, Batch: 283: Training Loss: 0.0180344358086586, Validation Loss: 0.023704050108790398\n",
      "Epoch 27, Batch: 284: Training Loss: 0.020657286047935486, Validation Loss: 0.02344452403485775\n",
      "Epoch 27, Batch: 285: Training Loss: 0.019649390131235123, Validation Loss: 0.025135613977909088\n",
      "Epoch 27, Batch: 286: Training Loss: 0.019390489906072617, Validation Loss: 0.025479631498456\n",
      "Epoch 27, Batch: 287: Training Loss: 0.024028483778238297, Validation Loss: 0.023778662085533142\n",
      "Epoch 27, Batch: 288: Training Loss: 0.020416606217622757, Validation Loss: 0.021663064137101173\n",
      "Epoch 27, Batch: 289: Training Loss: 0.02247058041393757, Validation Loss: 0.022856147959828377\n",
      "Epoch 27, Batch: 290: Training Loss: 0.020905373618006706, Validation Loss: 0.023103369399905205\n",
      "Epoch 27, Batch: 291: Training Loss: 0.02255273051559925, Validation Loss: 0.02640959993004799\n",
      "Epoch 27, Batch: 292: Training Loss: 0.02011341042816639, Validation Loss: 0.02358841523528099\n",
      "Epoch 27, Batch: 293: Training Loss: 0.01829139143228531, Validation Loss: 0.024357331916689873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 294: Training Loss: 0.01857214979827404, Validation Loss: 0.02623513713479042\n",
      "Epoch 27, Batch: 295: Training Loss: 0.020632416009902954, Validation Loss: 0.025682514533400536\n",
      "Epoch 27, Batch: 296: Training Loss: 0.01837213523685932, Validation Loss: 0.023700609803199768\n",
      "Epoch 27, Batch: 297: Training Loss: 0.020143752917647362, Validation Loss: 0.022414976730942726\n",
      "Epoch 27, Batch: 298: Training Loss: 0.02066175267100334, Validation Loss: 0.02290216274559498\n",
      "Epoch 27, Batch: 299: Training Loss: 0.020097915083169937, Validation Loss: 0.02188684791326523\n",
      "Epoch 27, Batch: 300: Training Loss: 0.021637829020619392, Validation Loss: 0.023002363741397858\n",
      "Epoch 27, Batch: 301: Training Loss: 0.019593562930822372, Validation Loss: 0.021870078518986702\n",
      "Epoch 27, Batch: 302: Training Loss: 0.01853901892900467, Validation Loss: 0.02366713620722294\n",
      "Epoch 27, Batch: 303: Training Loss: 0.02025463432073593, Validation Loss: 0.022781943902373314\n",
      "Epoch 27, Batch: 304: Training Loss: 0.01857294701039791, Validation Loss: 0.021065453067421913\n",
      "Epoch 27, Batch: 305: Training Loss: 0.01611272059381008, Validation Loss: 0.024801217019557953\n",
      "Epoch 27, Batch: 306: Training Loss: 0.020727697759866714, Validation Loss: 0.02395631931722164\n",
      "Epoch 27, Batch: 307: Training Loss: 0.017955128103494644, Validation Loss: 0.024894271045923233\n",
      "Epoch 27, Batch: 308: Training Loss: 0.021633785218000412, Validation Loss: 0.02575632371008396\n",
      "Epoch 27, Batch: 309: Training Loss: 0.02147647738456726, Validation Loss: 0.023940522223711014\n",
      "Epoch 27, Batch: 310: Training Loss: 0.019737297669053078, Validation Loss: 0.0221334770321846\n",
      "Epoch 27, Batch: 311: Training Loss: 0.019884886220097542, Validation Loss: 0.02234657108783722\n",
      "Epoch 27, Batch: 312: Training Loss: 0.02239840291440487, Validation Loss: 0.02256336621940136\n",
      "Epoch 27, Batch: 313: Training Loss: 0.02062399499118328, Validation Loss: 0.023021474480628967\n",
      "Epoch 27, Batch: 314: Training Loss: 0.022353729233145714, Validation Loss: 0.024042470380663872\n",
      "Epoch 27, Batch: 315: Training Loss: 0.019754650071263313, Validation Loss: 0.02145666815340519\n",
      "Epoch 27, Batch: 316: Training Loss: 0.02049146592617035, Validation Loss: 0.023694831877946854\n",
      "Epoch 27, Batch: 317: Training Loss: 0.01847938820719719, Validation Loss: 0.023245615884661674\n",
      "Epoch 27, Batch: 318: Training Loss: 0.020502198487520218, Validation Loss: 0.023946259170770645\n",
      "Epoch 27, Batch: 319: Training Loss: 0.020465154200792313, Validation Loss: 0.024889037013053894\n",
      "Epoch 27, Batch: 320: Training Loss: 0.020682688802480698, Validation Loss: 0.024446526542305946\n",
      "Epoch 27, Batch: 321: Training Loss: 0.01886008307337761, Validation Loss: 0.024526448920369148\n",
      "Epoch 27, Batch: 322: Training Loss: 0.01796114444732666, Validation Loss: 0.0257711298763752\n",
      "Epoch 27, Batch: 323: Training Loss: 0.019957158714532852, Validation Loss: 0.024325933307409286\n",
      "Epoch 27, Batch: 324: Training Loss: 0.02300938218832016, Validation Loss: 0.023271143436431885\n",
      "Epoch 27, Batch: 325: Training Loss: 0.01730750873684883, Validation Loss: 0.0251738540828228\n",
      "Epoch 27, Batch: 326: Training Loss: 0.022464966401457787, Validation Loss: 0.022144779562950134\n",
      "Epoch 27, Batch: 327: Training Loss: 0.02020265907049179, Validation Loss: 0.02488008141517639\n",
      "Epoch 27, Batch: 328: Training Loss: 0.022129051387310028, Validation Loss: 0.02141530066728592\n",
      "Epoch 27, Batch: 329: Training Loss: 0.024015408009290695, Validation Loss: 0.02224527858197689\n",
      "Epoch 27, Batch: 330: Training Loss: 0.0204672422260046, Validation Loss: 0.025361530482769012\n",
      "Epoch 27, Batch: 331: Training Loss: 0.019064223393797874, Validation Loss: 0.023426184430718422\n",
      "Epoch 27, Batch: 332: Training Loss: 0.022239381447434425, Validation Loss: 0.025244923308491707\n",
      "Epoch 27, Batch: 333: Training Loss: 0.020196396857500076, Validation Loss: 0.023950722068548203\n",
      "Epoch 27, Batch: 334: Training Loss: 0.02073860727250576, Validation Loss: 0.02554032765328884\n",
      "Epoch 27, Batch: 335: Training Loss: 0.018912220373749733, Validation Loss: 0.024843847379088402\n",
      "Epoch 27, Batch: 336: Training Loss: 0.01907966285943985, Validation Loss: 0.025647208094596863\n",
      "Epoch 27, Batch: 337: Training Loss: 0.02123454213142395, Validation Loss: 0.024973435327410698\n",
      "Epoch 27, Batch: 338: Training Loss: 0.02012493461370468, Validation Loss: 0.023641280829906464\n",
      "Epoch 27, Batch: 339: Training Loss: 0.018662024289369583, Validation Loss: 0.023519769310951233\n",
      "Epoch 27, Batch: 340: Training Loss: 0.02004055678844452, Validation Loss: 0.022691452875733376\n",
      "Epoch 27, Batch: 341: Training Loss: 0.02114855870604515, Validation Loss: 0.02357380837202072\n",
      "Epoch 27, Batch: 342: Training Loss: 0.018218500539660454, Validation Loss: 0.02213364467024803\n",
      "Epoch 27, Batch: 343: Training Loss: 0.019489435479044914, Validation Loss: 0.024325445294380188\n",
      "Epoch 27, Batch: 344: Training Loss: 0.021429497748613358, Validation Loss: 0.023852763697504997\n",
      "Epoch 27, Batch: 345: Training Loss: 0.01990264654159546, Validation Loss: 0.02123294211924076\n",
      "Epoch 27, Batch: 346: Training Loss: 0.019575627520680428, Validation Loss: 0.023326965048909187\n",
      "Epoch 27, Batch: 347: Training Loss: 0.0195356085896492, Validation Loss: 0.01980920135974884\n",
      "Epoch 27, Batch: 348: Training Loss: 0.021747397258877754, Validation Loss: 0.023613588884472847\n",
      "Epoch 27, Batch: 349: Training Loss: 0.021694432944059372, Validation Loss: 0.0217543113976717\n",
      "Epoch 27, Batch: 350: Training Loss: 0.016522686928510666, Validation Loss: 0.02337890863418579\n",
      "Epoch 27, Batch: 351: Training Loss: 0.02117450162768364, Validation Loss: 0.023705540224909782\n",
      "Epoch 27, Batch: 352: Training Loss: 0.021111490204930305, Validation Loss: 0.021857967600226402\n",
      "Epoch 27, Batch: 353: Training Loss: 0.020222103223204613, Validation Loss: 0.02201833203434944\n",
      "Epoch 27, Batch: 354: Training Loss: 0.022829685360193253, Validation Loss: 0.023951251059770584\n",
      "Epoch 27, Batch: 355: Training Loss: 0.0167271438986063, Validation Loss: 0.025186792016029358\n",
      "Epoch 27, Batch: 356: Training Loss: 0.019082695245742798, Validation Loss: 0.02542734146118164\n",
      "Epoch 27, Batch: 357: Training Loss: 0.017504943534731865, Validation Loss: 0.02202928438782692\n",
      "Epoch 27, Batch: 358: Training Loss: 0.019092144444584846, Validation Loss: 0.021829215809702873\n",
      "Epoch 27, Batch: 359: Training Loss: 0.019763605669140816, Validation Loss: 0.02520112134516239\n",
      "Epoch 27, Batch: 360: Training Loss: 0.023283006623387337, Validation Loss: 0.02032766304910183\n",
      "Epoch 27, Batch: 361: Training Loss: 0.019691472873091698, Validation Loss: 0.02463924139738083\n",
      "Epoch 27, Batch: 362: Training Loss: 0.020787229761481285, Validation Loss: 0.024558601900935173\n",
      "Epoch 27, Batch: 363: Training Loss: 0.02540973573923111, Validation Loss: 0.023879796266555786\n",
      "Epoch 27, Batch: 364: Training Loss: 0.02067592553794384, Validation Loss: 0.023511093109846115\n",
      "Epoch 27, Batch: 365: Training Loss: 0.0206326674669981, Validation Loss: 0.02448311261832714\n",
      "Epoch 27, Batch: 366: Training Loss: 0.022570136934518814, Validation Loss: 0.024992546066641808\n",
      "Epoch 27, Batch: 367: Training Loss: 0.01958668790757656, Validation Loss: 0.02432839386165142\n",
      "Epoch 27, Batch: 368: Training Loss: 0.018513161689043045, Validation Loss: 0.0248844213783741\n",
      "Epoch 27, Batch: 369: Training Loss: 0.020588934421539307, Validation Loss: 0.02421841397881508\n",
      "Epoch 27, Batch: 370: Training Loss: 0.0194497499614954, Validation Loss: 0.026133600622415543\n",
      "Epoch 27, Batch: 371: Training Loss: 0.020652012899518013, Validation Loss: 0.024972908198833466\n",
      "Epoch 27, Batch: 372: Training Loss: 0.01810968481004238, Validation Loss: 0.02416064403951168\n",
      "Epoch 27, Batch: 373: Training Loss: 0.022356806322932243, Validation Loss: 0.02297970838844776\n",
      "Epoch 27, Batch: 374: Training Loss: 0.01975080743432045, Validation Loss: 0.024945415556430817\n",
      "Epoch 27, Batch: 375: Training Loss: 0.021103505045175552, Validation Loss: 0.023023907095193863\n",
      "Epoch 27, Batch: 376: Training Loss: 0.02051621489226818, Validation Loss: 0.025413567200303078\n",
      "Epoch 27, Batch: 377: Training Loss: 0.019401784986257553, Validation Loss: 0.022833161056041718\n",
      "Epoch 27, Batch: 378: Training Loss: 0.01955694705247879, Validation Loss: 0.023914460092782974\n",
      "Epoch 27, Batch: 379: Training Loss: 0.022292541339993477, Validation Loss: 0.023830465972423553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 380: Training Loss: 0.024106308817863464, Validation Loss: 0.02282611094415188\n",
      "Epoch 27, Batch: 381: Training Loss: 0.019196508452296257, Validation Loss: 0.02315276674926281\n",
      "Epoch 27, Batch: 382: Training Loss: 0.020624639466404915, Validation Loss: 0.023164259269833565\n",
      "Epoch 27, Batch: 383: Training Loss: 0.019073480740189552, Validation Loss: 0.022024104371666908\n",
      "Epoch 27, Batch: 384: Training Loss: 0.02116446942090988, Validation Loss: 0.026924384757876396\n",
      "Epoch 27, Batch: 385: Training Loss: 0.020592844113707542, Validation Loss: 0.025204448029398918\n",
      "Epoch 27, Batch: 386: Training Loss: 0.020971816033124924, Validation Loss: 0.02429392747581005\n",
      "Epoch 27, Batch: 387: Training Loss: 0.01920522004365921, Validation Loss: 0.02422838844358921\n",
      "Epoch 27, Batch: 388: Training Loss: 0.018773265182971954, Validation Loss: 0.021683646366000175\n",
      "Epoch 27, Batch: 389: Training Loss: 0.02066376432776451, Validation Loss: 0.024666568264365196\n",
      "Epoch 27, Batch: 390: Training Loss: 0.02101828344166279, Validation Loss: 0.023170247673988342\n",
      "Epoch 27, Batch: 391: Training Loss: 0.021643919870257378, Validation Loss: 0.023226095363497734\n",
      "Epoch 27, Batch: 392: Training Loss: 0.019358765333890915, Validation Loss: 0.02163771539926529\n",
      "Epoch 27, Batch: 393: Training Loss: 0.017716487869620323, Validation Loss: 0.02278519794344902\n",
      "Epoch 27, Batch: 394: Training Loss: 0.02156120538711548, Validation Loss: 0.022659586742520332\n",
      "Epoch 27, Batch: 395: Training Loss: 0.02092614397406578, Validation Loss: 0.024978771805763245\n",
      "Epoch 27, Batch: 396: Training Loss: 0.02074337750673294, Validation Loss: 0.02438650093972683\n",
      "Epoch 27, Batch: 397: Training Loss: 0.022402407601475716, Validation Loss: 0.021120622754096985\n",
      "Epoch 27, Batch: 398: Training Loss: 0.02104133926331997, Validation Loss: 0.02460404299199581\n",
      "Epoch 27, Batch: 399: Training Loss: 0.017422856763005257, Validation Loss: 0.022101687267422676\n",
      "Epoch 27, Batch: 400: Training Loss: 0.022458568215370178, Validation Loss: 0.02161942608654499\n",
      "Epoch 27, Batch: 401: Training Loss: 0.020266255363821983, Validation Loss: 0.022988930344581604\n",
      "Epoch 27, Batch: 402: Training Loss: 0.020940424874424934, Validation Loss: 0.024441860616207123\n",
      "Epoch 27, Batch: 403: Training Loss: 0.025660505518317223, Validation Loss: 0.02243742160499096\n",
      "Epoch 27, Batch: 404: Training Loss: 0.021728942170739174, Validation Loss: 0.022257031872868538\n",
      "Epoch 27, Batch: 405: Training Loss: 0.021763427183032036, Validation Loss: 0.024383775889873505\n",
      "Epoch 27, Batch: 406: Training Loss: 0.020754072815179825, Validation Loss: 0.024654878303408623\n",
      "Epoch 27, Batch: 407: Training Loss: 0.019641218706965446, Validation Loss: 0.02429584041237831\n",
      "Epoch 27, Batch: 408: Training Loss: 0.018668200820684433, Validation Loss: 0.02145826257765293\n",
      "Epoch 27, Batch: 409: Training Loss: 0.021967248991131783, Validation Loss: 0.021754393354058266\n",
      "Epoch 27, Batch: 410: Training Loss: 0.02285863272845745, Validation Loss: 0.023234080523252487\n",
      "Epoch 27, Batch: 411: Training Loss: 0.018108554184436798, Validation Loss: 0.022998139262199402\n",
      "Epoch 27, Batch: 412: Training Loss: 0.023409688845276833, Validation Loss: 0.024213144555687904\n",
      "Epoch 27, Batch: 413: Training Loss: 0.022518513724207878, Validation Loss: 0.023290518671274185\n",
      "Epoch 27, Batch: 414: Training Loss: 0.020995868369936943, Validation Loss: 0.020428674295544624\n",
      "Epoch 27, Batch: 415: Training Loss: 0.02461494505405426, Validation Loss: 0.021709587424993515\n",
      "Epoch 27, Batch: 416: Training Loss: 0.021489083766937256, Validation Loss: 0.021150019019842148\n",
      "Epoch 27, Batch: 417: Training Loss: 0.020004289224743843, Validation Loss: 0.02371826581656933\n",
      "Epoch 27, Batch: 418: Training Loss: 0.019398126751184464, Validation Loss: 0.02150752954185009\n",
      "Epoch 27, Batch: 419: Training Loss: 0.020026329904794693, Validation Loss: 0.023749832063913345\n",
      "Epoch 27, Batch: 420: Training Loss: 0.019489075988531113, Validation Loss: 0.0231732577085495\n",
      "Epoch 27, Batch: 421: Training Loss: 0.02040364220738411, Validation Loss: 0.02331048622727394\n",
      "Epoch 27, Batch: 422: Training Loss: 0.022598393261432648, Validation Loss: 0.02147563360631466\n",
      "Epoch 27, Batch: 423: Training Loss: 0.02401115745306015, Validation Loss: 0.022433098405599594\n",
      "Epoch 27, Batch: 424: Training Loss: 0.021004196256399155, Validation Loss: 0.024112151935696602\n",
      "Epoch 27, Batch: 425: Training Loss: 0.02075463905930519, Validation Loss: 0.023704014718532562\n",
      "Epoch 27, Batch: 426: Training Loss: 0.01869872212409973, Validation Loss: 0.022916126996278763\n",
      "Epoch 27, Batch: 427: Training Loss: 0.020341314375400543, Validation Loss: 0.022175664082169533\n",
      "Epoch 27, Batch: 428: Training Loss: 0.020410358905792236, Validation Loss: 0.021456526592373848\n",
      "Epoch 27, Batch: 429: Training Loss: 0.019907565787434578, Validation Loss: 0.023289868608117104\n",
      "Epoch 27, Batch: 430: Training Loss: 0.018334999680519104, Validation Loss: 0.02036633901298046\n",
      "Epoch 27, Batch: 431: Training Loss: 0.01957067847251892, Validation Loss: 0.022380871698260307\n",
      "Epoch 27, Batch: 432: Training Loss: 0.022497976198792458, Validation Loss: 0.021498750895261765\n",
      "Epoch 27, Batch: 433: Training Loss: 0.02121291309595108, Validation Loss: 0.021407591179013252\n",
      "Epoch 27, Batch: 434: Training Loss: 0.019115136936306953, Validation Loss: 0.023163290694355965\n",
      "Epoch 27, Batch: 435: Training Loss: 0.020910758525133133, Validation Loss: 0.01990593411028385\n",
      "Epoch 27, Batch: 436: Training Loss: 0.01921163499355316, Validation Loss: 0.020618746057152748\n",
      "Epoch 27, Batch: 437: Training Loss: 0.018614310771226883, Validation Loss: 0.021360639482736588\n",
      "Epoch 27, Batch: 438: Training Loss: 0.020461447536945343, Validation Loss: 0.022707756608724594\n",
      "Epoch 27, Batch: 439: Training Loss: 0.01625581830739975, Validation Loss: 0.019852887839078903\n",
      "Epoch 27, Batch: 440: Training Loss: 0.02041657269001007, Validation Loss: 0.019302308559417725\n",
      "Epoch 27, Batch: 441: Training Loss: 0.018757272511720657, Validation Loss: 0.019590014591813087\n",
      "Epoch 27, Batch: 442: Training Loss: 0.02153589576482773, Validation Loss: 0.02099468931555748\n",
      "Epoch 27, Batch: 443: Training Loss: 0.021133167669177055, Validation Loss: 0.020785439759492874\n",
      "Epoch 27, Batch: 444: Training Loss: 0.015462000854313374, Validation Loss: 0.02164294943213463\n",
      "Epoch 27, Batch: 445: Training Loss: 0.01843460649251938, Validation Loss: 0.018840517848730087\n",
      "Epoch 27, Batch: 446: Training Loss: 0.02188759297132492, Validation Loss: 0.020839523524045944\n",
      "Epoch 27, Batch: 447: Training Loss: 0.022874629124999046, Validation Loss: 0.018524913117289543\n",
      "Epoch 27, Batch: 448: Training Loss: 0.02113068476319313, Validation Loss: 0.022968636825680733\n",
      "Epoch 27, Batch: 449: Training Loss: 0.019495803862810135, Validation Loss: 0.02011816017329693\n",
      "Epoch 27, Batch: 450: Training Loss: 0.020045818760991096, Validation Loss: 0.0186855960637331\n",
      "Epoch 27, Batch: 451: Training Loss: 0.020760031417012215, Validation Loss: 0.023409493267536163\n",
      "Epoch 27, Batch: 452: Training Loss: 0.021279960870742798, Validation Loss: 0.021865608170628548\n",
      "Epoch 27, Batch: 453: Training Loss: 0.021093517541885376, Validation Loss: 0.02151108905673027\n",
      "Epoch 27, Batch: 454: Training Loss: 0.02253161557018757, Validation Loss: 0.022417308762669563\n",
      "Epoch 27, Batch: 455: Training Loss: 0.01906578801572323, Validation Loss: 0.0218027476221323\n",
      "Epoch 27, Batch: 456: Training Loss: 0.018239770084619522, Validation Loss: 0.021315041929483414\n",
      "Epoch 27, Batch: 457: Training Loss: 0.02063036523759365, Validation Loss: 0.022996628656983376\n",
      "Epoch 27, Batch: 458: Training Loss: 0.01690063253045082, Validation Loss: 0.024273311719298363\n",
      "Epoch 27, Batch: 459: Training Loss: 0.015961941331624985, Validation Loss: 0.02110574021935463\n",
      "Epoch 27, Batch: 460: Training Loss: 0.024150444194674492, Validation Loss: 0.02316378243267536\n",
      "Epoch 27, Batch: 461: Training Loss: 0.019353050738573074, Validation Loss: 0.022396473214030266\n",
      "Epoch 27, Batch: 462: Training Loss: 0.01919439062476158, Validation Loss: 0.024452758952975273\n",
      "Epoch 27, Batch: 463: Training Loss: 0.020170943811535835, Validation Loss: 0.021614978089928627\n",
      "Epoch 27, Batch: 464: Training Loss: 0.019410045817494392, Validation Loss: 0.02220279723405838\n",
      "Epoch 27, Batch: 465: Training Loss: 0.02076059952378273, Validation Loss: 0.022185975685715675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch: 466: Training Loss: 0.018543627113103867, Validation Loss: 0.023962918668985367\n",
      "Epoch 27, Batch: 467: Training Loss: 0.023366421461105347, Validation Loss: 0.023210953921079636\n",
      "Epoch 27, Batch: 468: Training Loss: 0.023293813690543175, Validation Loss: 0.021252116188406944\n",
      "Epoch 27, Batch: 469: Training Loss: 0.018322886899113655, Validation Loss: 0.022742072120308876\n",
      "Epoch 27, Batch: 470: Training Loss: 0.017099924385547638, Validation Loss: 0.02159615233540535\n",
      "Epoch 27, Batch: 471: Training Loss: 0.021774355322122574, Validation Loss: 0.024851471185684204\n",
      "Epoch 27, Batch: 472: Training Loss: 0.02084914967417717, Validation Loss: 0.02170390635728836\n",
      "Epoch 27, Batch: 473: Training Loss: 0.02080441452562809, Validation Loss: 0.023935401812195778\n",
      "Epoch 27, Batch: 474: Training Loss: 0.018102573230862617, Validation Loss: 0.022820869460701942\n",
      "Epoch 27, Batch: 475: Training Loss: 0.022068707272410393, Validation Loss: 0.022675732150673866\n",
      "Epoch 27, Batch: 476: Training Loss: 0.01871832087635994, Validation Loss: 0.02115955390036106\n",
      "Epoch 27, Batch: 477: Training Loss: 0.021694540977478027, Validation Loss: 0.02184968814253807\n",
      "Epoch 27, Batch: 478: Training Loss: 0.02020167000591755, Validation Loss: 0.023082856088876724\n",
      "Epoch 27, Batch: 479: Training Loss: 0.02188359946012497, Validation Loss: 0.022561026737093925\n",
      "Epoch 27, Batch: 480: Training Loss: 0.02094605565071106, Validation Loss: 0.021217895671725273\n",
      "Epoch 27, Batch: 481: Training Loss: 0.017949450761079788, Validation Loss: 0.021920768544077873\n",
      "Epoch 27, Batch: 482: Training Loss: 0.02259981445968151, Validation Loss: 0.022753462195396423\n",
      "Epoch 27, Batch: 483: Training Loss: 0.020548831671476364, Validation Loss: 0.021960485726594925\n",
      "Epoch 27, Batch: 484: Training Loss: 0.018197154626250267, Validation Loss: 0.024977639317512512\n",
      "Epoch 27, Batch: 485: Training Loss: 0.02081684209406376, Validation Loss: 0.021855739876627922\n",
      "Epoch 27, Batch: 486: Training Loss: 0.02145618200302124, Validation Loss: 0.022642938420176506\n",
      "Epoch 27, Batch: 487: Training Loss: 0.020471973344683647, Validation Loss: 0.02165963314473629\n",
      "Epoch 27, Batch: 488: Training Loss: 0.022249611094594002, Validation Loss: 0.022082610055804253\n",
      "Epoch 27, Batch: 489: Training Loss: 0.02077789045870304, Validation Loss: 0.023172836750745773\n",
      "Epoch 27, Batch: 490: Training Loss: 0.022031286731362343, Validation Loss: 0.021615613251924515\n",
      "Epoch 27, Batch: 491: Training Loss: 0.01672656461596489, Validation Loss: 0.022629136219620705\n",
      "Epoch 27, Batch: 492: Training Loss: 0.021615661680698395, Validation Loss: 0.021411258727312088\n",
      "Epoch 27, Batch: 493: Training Loss: 0.019372867420315742, Validation Loss: 0.0224674791097641\n",
      "Epoch 27, Batch: 494: Training Loss: 0.022507140412926674, Validation Loss: 0.021595455706119537\n",
      "Epoch 27, Batch: 495: Training Loss: 0.017558466643095016, Validation Loss: 0.02258027158677578\n",
      "Epoch 27, Batch: 496: Training Loss: 0.019750839099287987, Validation Loss: 0.021632157266139984\n",
      "Epoch 27, Batch: 497: Training Loss: 0.01742350310087204, Validation Loss: 0.020130928605794907\n",
      "Epoch 27, Batch: 498: Training Loss: 0.021129382774233818, Validation Loss: 0.021169692277908325\n",
      "Epoch 27, Batch: 499: Training Loss: 0.017542794346809387, Validation Loss: 0.024573223665356636\n",
      "Epoch 28, Batch: 0: Training Loss: 0.020837711170315742, Validation Loss: 0.021642927080392838\n",
      "Epoch 28, Batch: 1: Training Loss: 0.01707688719034195, Validation Loss: 0.023313233628869057\n",
      "Epoch 28, Batch: 2: Training Loss: 0.022475481033325195, Validation Loss: 0.022749094292521477\n",
      "Epoch 28, Batch: 3: Training Loss: 0.018469110131263733, Validation Loss: 0.023228719830513\n",
      "Epoch 28, Batch: 4: Training Loss: 0.01856200397014618, Validation Loss: 0.021901912987232208\n",
      "Epoch 28, Batch: 5: Training Loss: 0.01797124557197094, Validation Loss: 0.02303498424589634\n",
      "Epoch 28, Batch: 6: Training Loss: 0.01896662451326847, Validation Loss: 0.022949639707803726\n",
      "Epoch 28, Batch: 7: Training Loss: 0.019860222935676575, Validation Loss: 0.021699855104088783\n",
      "Epoch 28, Batch: 8: Training Loss: 0.02034534513950348, Validation Loss: 0.02191370166838169\n",
      "Epoch 28, Batch: 9: Training Loss: 0.01885984092950821, Validation Loss: 0.022110797464847565\n",
      "Epoch 28, Batch: 10: Training Loss: 0.019944101572036743, Validation Loss: 0.020539771765470505\n",
      "Epoch 28, Batch: 11: Training Loss: 0.020395496860146523, Validation Loss: 0.02151305042207241\n",
      "Epoch 28, Batch: 12: Training Loss: 0.020237116143107414, Validation Loss: 0.02279963344335556\n",
      "Epoch 28, Batch: 13: Training Loss: 0.025533078238368034, Validation Loss: 0.02317238226532936\n",
      "Epoch 28, Batch: 14: Training Loss: 0.02209244854748249, Validation Loss: 0.021727528423070908\n",
      "Epoch 28, Batch: 15: Training Loss: 0.022160910069942474, Validation Loss: 0.024979865178465843\n",
      "Epoch 28, Batch: 16: Training Loss: 0.022449372336268425, Validation Loss: 0.01980733312666416\n",
      "Epoch 28, Batch: 17: Training Loss: 0.019448459148406982, Validation Loss: 0.020438412204384804\n",
      "Epoch 28, Batch: 18: Training Loss: 0.020706918090581894, Validation Loss: 0.02037089131772518\n",
      "Epoch 28, Batch: 19: Training Loss: 0.01979353465139866, Validation Loss: 0.019811561331152916\n",
      "Epoch 28, Batch: 20: Training Loss: 0.020487617701292038, Validation Loss: 0.020476756617426872\n",
      "Epoch 28, Batch: 21: Training Loss: 0.018586749210953712, Validation Loss: 0.01986493170261383\n",
      "Epoch 28, Batch: 22: Training Loss: 0.019289173185825348, Validation Loss: 0.02204565517604351\n",
      "Epoch 28, Batch: 23: Training Loss: 0.020716307684779167, Validation Loss: 0.02142844721674919\n",
      "Epoch 28, Batch: 24: Training Loss: 0.02009018138051033, Validation Loss: 0.022296525537967682\n",
      "Epoch 28, Batch: 25: Training Loss: 0.01883719488978386, Validation Loss: 0.023375332355499268\n",
      "Epoch 28, Batch: 26: Training Loss: 0.02079303003847599, Validation Loss: 0.021062621846795082\n",
      "Epoch 28, Batch: 27: Training Loss: 0.01908719353377819, Validation Loss: 0.021165957674384117\n",
      "Epoch 28, Batch: 28: Training Loss: 0.021168816834688187, Validation Loss: 0.02092495933175087\n",
      "Epoch 28, Batch: 29: Training Loss: 0.021772630512714386, Validation Loss: 0.022253746166825294\n",
      "Epoch 28, Batch: 30: Training Loss: 0.020235328003764153, Validation Loss: 0.018610665574669838\n",
      "Epoch 28, Batch: 31: Training Loss: 0.022350257262587547, Validation Loss: 0.022397484630346298\n",
      "Epoch 28, Batch: 32: Training Loss: 0.020276866853237152, Validation Loss: 0.022515354678034782\n",
      "Epoch 28, Batch: 33: Training Loss: 0.02019701525568962, Validation Loss: 0.0219265203922987\n",
      "Epoch 28, Batch: 34: Training Loss: 0.016789644956588745, Validation Loss: 0.021751046180725098\n",
      "Epoch 28, Batch: 35: Training Loss: 0.02198413386940956, Validation Loss: 0.019878942519426346\n",
      "Epoch 28, Batch: 36: Training Loss: 0.020326726138591766, Validation Loss: 0.021205781027674675\n",
      "Epoch 28, Batch: 37: Training Loss: 0.01981518790125847, Validation Loss: 0.023577315732836723\n",
      "Epoch 28, Batch: 38: Training Loss: 0.021228885278105736, Validation Loss: 0.02142343670129776\n",
      "Epoch 28, Batch: 39: Training Loss: 0.01996484026312828, Validation Loss: 0.021200472488999367\n",
      "Epoch 28, Batch: 40: Training Loss: 0.02477751486003399, Validation Loss: 0.021328624337911606\n",
      "Epoch 28, Batch: 41: Training Loss: 0.020767057314515114, Validation Loss: 0.022419625893235207\n",
      "Epoch 28, Batch: 42: Training Loss: 0.02106957882642746, Validation Loss: 0.02422814629971981\n",
      "Epoch 28, Batch: 43: Training Loss: 0.01901310123503208, Validation Loss: 0.02157161571085453\n",
      "Epoch 28, Batch: 44: Training Loss: 0.0225606057792902, Validation Loss: 0.02431250363588333\n",
      "Epoch 28, Batch: 45: Training Loss: 0.019275343045592308, Validation Loss: 0.025429457426071167\n",
      "Epoch 28, Batch: 46: Training Loss: 0.021064989268779755, Validation Loss: 0.024350687861442566\n",
      "Epoch 28, Batch: 47: Training Loss: 0.021575162187218666, Validation Loss: 0.024610886350274086\n",
      "Epoch 28, Batch: 48: Training Loss: 0.023767560720443726, Validation Loss: 0.02272132597863674\n",
      "Epoch 28, Batch: 49: Training Loss: 0.020550629124045372, Validation Loss: 0.024919424206018448\n",
      "Epoch 28, Batch: 50: Training Loss: 0.020565461367368698, Validation Loss: 0.0239176657050848\n",
      "Epoch 28, Batch: 51: Training Loss: 0.02427399717271328, Validation Loss: 0.02499174326658249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 52: Training Loss: 0.020882055163383484, Validation Loss: 0.02580364979803562\n",
      "Epoch 28, Batch: 53: Training Loss: 0.018078504130244255, Validation Loss: 0.02521440200507641\n",
      "Epoch 28, Batch: 54: Training Loss: 0.02198503538966179, Validation Loss: 0.02369171380996704\n",
      "Epoch 28, Batch: 55: Training Loss: 0.021015740931034088, Validation Loss: 0.022791489958763123\n",
      "Epoch 28, Batch: 56: Training Loss: 0.022944536060094833, Validation Loss: 0.024370873346924782\n",
      "Epoch 28, Batch: 57: Training Loss: 0.019885623827576637, Validation Loss: 0.022441158071160316\n",
      "Epoch 28, Batch: 58: Training Loss: 0.020874222740530968, Validation Loss: 0.02527523785829544\n",
      "Epoch 28, Batch: 59: Training Loss: 0.017384305596351624, Validation Loss: 0.023130597546696663\n",
      "Epoch 28, Batch: 60: Training Loss: 0.01778971403837204, Validation Loss: 0.02202252298593521\n",
      "Epoch 28, Batch: 61: Training Loss: 0.02317129448056221, Validation Loss: 0.02232995443046093\n",
      "Epoch 28, Batch: 62: Training Loss: 0.019712233915925026, Validation Loss: 0.022570153698325157\n",
      "Epoch 28, Batch: 63: Training Loss: 0.020363083109259605, Validation Loss: 0.021202782168984413\n",
      "Epoch 28, Batch: 64: Training Loss: 0.01779000833630562, Validation Loss: 0.020971138030290604\n",
      "Epoch 28, Batch: 65: Training Loss: 0.020942797884345055, Validation Loss: 0.020438484847545624\n",
      "Epoch 28, Batch: 66: Training Loss: 0.02023441344499588, Validation Loss: 0.019828494638204575\n",
      "Epoch 28, Batch: 67: Training Loss: 0.019428160041570663, Validation Loss: 0.021534990519285202\n",
      "Epoch 28, Batch: 68: Training Loss: 0.020443705841898918, Validation Loss: 0.021296879276633263\n",
      "Epoch 28, Batch: 69: Training Loss: 0.01905636116862297, Validation Loss: 0.02067353017628193\n",
      "Epoch 28, Batch: 70: Training Loss: 0.02218801900744438, Validation Loss: 0.021783433854579926\n",
      "Epoch 28, Batch: 71: Training Loss: 0.02042282186448574, Validation Loss: 0.020445847883820534\n",
      "Epoch 28, Batch: 72: Training Loss: 0.016751883551478386, Validation Loss: 0.021190723404288292\n",
      "Epoch 28, Batch: 73: Training Loss: 0.021118881180882454, Validation Loss: 0.02105887420475483\n",
      "Epoch 28, Batch: 74: Training Loss: 0.018186291679739952, Validation Loss: 0.023344041779637337\n",
      "Epoch 28, Batch: 75: Training Loss: 0.0170220285654068, Validation Loss: 0.02010292559862137\n",
      "Epoch 28, Batch: 76: Training Loss: 0.017278095707297325, Validation Loss: 0.021601995453238487\n",
      "Epoch 28, Batch: 77: Training Loss: 0.023921668529510498, Validation Loss: 0.023998616263270378\n",
      "Epoch 28, Batch: 78: Training Loss: 0.020405244082212448, Validation Loss: 0.02151484787464142\n",
      "Epoch 28, Batch: 79: Training Loss: 0.020432375371456146, Validation Loss: 0.022837108001112938\n",
      "Epoch 28, Batch: 80: Training Loss: 0.01973811164498329, Validation Loss: 0.022757073864340782\n",
      "Epoch 28, Batch: 81: Training Loss: 0.020902510732412338, Validation Loss: 0.02230493538081646\n",
      "Epoch 28, Batch: 82: Training Loss: 0.02259388566017151, Validation Loss: 0.02301463857293129\n",
      "Epoch 28, Batch: 83: Training Loss: 0.023715898394584656, Validation Loss: 0.02010747417807579\n",
      "Epoch 28, Batch: 84: Training Loss: 0.022890830412507057, Validation Loss: 0.019558396190404892\n",
      "Epoch 28, Batch: 85: Training Loss: 0.016297703608870506, Validation Loss: 0.021457336843013763\n",
      "Epoch 28, Batch: 86: Training Loss: 0.020883681252598763, Validation Loss: 0.02247179113328457\n",
      "Epoch 28, Batch: 87: Training Loss: 0.024391423910856247, Validation Loss: 0.023654168471693993\n",
      "Epoch 28, Batch: 88: Training Loss: 0.020796190947294235, Validation Loss: 0.02322017215192318\n",
      "Epoch 28, Batch: 89: Training Loss: 0.023790445178747177, Validation Loss: 0.02150825411081314\n",
      "Epoch 28, Batch: 90: Training Loss: 0.019852343946695328, Validation Loss: 0.02276579849421978\n",
      "Epoch 28, Batch: 91: Training Loss: 0.022808484733104706, Validation Loss: 0.02279854193329811\n",
      "Epoch 28, Batch: 92: Training Loss: 0.02341831661760807, Validation Loss: 0.02199472300708294\n",
      "Epoch 28, Batch: 93: Training Loss: 0.022032644599676132, Validation Loss: 0.023179078474640846\n",
      "Epoch 28, Batch: 94: Training Loss: 0.020117470994591713, Validation Loss: 0.02240387722849846\n",
      "Epoch 28, Batch: 95: Training Loss: 0.015395063906908035, Validation Loss: 0.022210799157619476\n",
      "Epoch 28, Batch: 96: Training Loss: 0.01898832619190216, Validation Loss: 0.020939696580171585\n",
      "Epoch 28, Batch: 97: Training Loss: 0.01960056833922863, Validation Loss: 0.020850298926234245\n",
      "Epoch 28, Batch: 98: Training Loss: 0.02222980186343193, Validation Loss: 0.023236582055687904\n",
      "Epoch 28, Batch: 99: Training Loss: 0.022531993687152863, Validation Loss: 0.02150924690067768\n",
      "Epoch 28, Batch: 100: Training Loss: 0.0206647589802742, Validation Loss: 0.021863996982574463\n",
      "Epoch 28, Batch: 101: Training Loss: 0.01837628334760666, Validation Loss: 0.021253058686852455\n",
      "Epoch 28, Batch: 102: Training Loss: 0.0220799557864666, Validation Loss: 0.02163737080991268\n",
      "Epoch 28, Batch: 103: Training Loss: 0.01867731846868992, Validation Loss: 0.02094113640487194\n",
      "Epoch 28, Batch: 104: Training Loss: 0.016865883022546768, Validation Loss: 0.02233719825744629\n",
      "Epoch 28, Batch: 105: Training Loss: 0.016443392261862755, Validation Loss: 0.02322300523519516\n",
      "Epoch 28, Batch: 106: Training Loss: 0.02372979000210762, Validation Loss: 0.023952459916472435\n",
      "Epoch 28, Batch: 107: Training Loss: 0.020491162315011024, Validation Loss: 0.02211187034845352\n",
      "Epoch 28, Batch: 108: Training Loss: 0.01772475056350231, Validation Loss: 0.02445586584508419\n",
      "Epoch 28, Batch: 109: Training Loss: 0.02069244161248207, Validation Loss: 0.02181260846555233\n",
      "Epoch 28, Batch: 110: Training Loss: 0.02086537890136242, Validation Loss: 0.021569235250353813\n",
      "Epoch 28, Batch: 111: Training Loss: 0.018240539357066154, Validation Loss: 0.022778404876589775\n",
      "Epoch 28, Batch: 112: Training Loss: 0.017496172338724136, Validation Loss: 0.021760446950793266\n",
      "Epoch 28, Batch: 113: Training Loss: 0.01843593828380108, Validation Loss: 0.023075509816408157\n",
      "Epoch 28, Batch: 114: Training Loss: 0.021271662786602974, Validation Loss: 0.024841183796525\n",
      "Epoch 28, Batch: 115: Training Loss: 0.02463899925351143, Validation Loss: 0.02331717312335968\n",
      "Epoch 28, Batch: 116: Training Loss: 0.019495602697134018, Validation Loss: 0.02102075144648552\n",
      "Epoch 28, Batch: 117: Training Loss: 0.02022739127278328, Validation Loss: 0.02113610878586769\n",
      "Epoch 28, Batch: 118: Training Loss: 0.0226227268576622, Validation Loss: 0.021358704194426537\n",
      "Epoch 28, Batch: 119: Training Loss: 0.020597990602254868, Validation Loss: 0.021914051845669746\n",
      "Epoch 28, Batch: 120: Training Loss: 0.01876087673008442, Validation Loss: 0.02121146395802498\n",
      "Epoch 28, Batch: 121: Training Loss: 0.018705153837800026, Validation Loss: 0.02239505760371685\n",
      "Epoch 28, Batch: 122: Training Loss: 0.021468564867973328, Validation Loss: 0.021285157650709152\n",
      "Epoch 28, Batch: 123: Training Loss: 0.017552757635712624, Validation Loss: 0.020646439865231514\n",
      "Epoch 28, Batch: 124: Training Loss: 0.018553955480456352, Validation Loss: 0.018829865381121635\n",
      "Epoch 28, Batch: 125: Training Loss: 0.018361598253250122, Validation Loss: 0.020453613251447678\n",
      "Epoch 28, Batch: 126: Training Loss: 0.021413566544651985, Validation Loss: 0.021771777421236038\n",
      "Epoch 28, Batch: 127: Training Loss: 0.023121686652302742, Validation Loss: 0.022573331370949745\n",
      "Epoch 28, Batch: 128: Training Loss: 0.019211210310459137, Validation Loss: 0.021152611821889877\n",
      "Epoch 28, Batch: 129: Training Loss: 0.016055867075920105, Validation Loss: 0.023641880601644516\n",
      "Epoch 28, Batch: 130: Training Loss: 0.02319764345884323, Validation Loss: 0.02360023930668831\n",
      "Epoch 28, Batch: 131: Training Loss: 0.022568441927433014, Validation Loss: 0.02141641080379486\n",
      "Epoch 28, Batch: 132: Training Loss: 0.020489271730184555, Validation Loss: 0.021024297922849655\n",
      "Epoch 28, Batch: 133: Training Loss: 0.019060324877500534, Validation Loss: 0.020982956513762474\n",
      "Epoch 28, Batch: 134: Training Loss: 0.02304169163107872, Validation Loss: 0.01898164674639702\n",
      "Epoch 28, Batch: 135: Training Loss: 0.02120061218738556, Validation Loss: 0.02001357078552246\n",
      "Epoch 28, Batch: 136: Training Loss: 0.01863415539264679, Validation Loss: 0.02077769860625267\n",
      "Epoch 28, Batch: 137: Training Loss: 0.019149726256728172, Validation Loss: 0.02421490103006363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 138: Training Loss: 0.01690375618636608, Validation Loss: 0.020869478583335876\n",
      "Epoch 28, Batch: 139: Training Loss: 0.01845145784318447, Validation Loss: 0.022017616778612137\n",
      "Epoch 28, Batch: 140: Training Loss: 0.022485166788101196, Validation Loss: 0.019588463008403778\n",
      "Epoch 28, Batch: 141: Training Loss: 0.0201750248670578, Validation Loss: 0.023647334426641464\n",
      "Epoch 28, Batch: 142: Training Loss: 0.022023001685738564, Validation Loss: 0.02113812416791916\n",
      "Epoch 28, Batch: 143: Training Loss: 0.01691163145005703, Validation Loss: 0.02105598896741867\n",
      "Epoch 28, Batch: 144: Training Loss: 0.016090968623757362, Validation Loss: 0.01990445889532566\n",
      "Epoch 28, Batch: 145: Training Loss: 0.019603241235017776, Validation Loss: 0.021884392946958542\n",
      "Epoch 28, Batch: 146: Training Loss: 0.021996894851326942, Validation Loss: 0.020406175404787064\n",
      "Epoch 28, Batch: 147: Training Loss: 0.02075609378516674, Validation Loss: 0.01903001219034195\n",
      "Epoch 28, Batch: 148: Training Loss: 0.020135264843702316, Validation Loss: 0.01922174170613289\n",
      "Epoch 28, Batch: 149: Training Loss: 0.01998213492333889, Validation Loss: 0.019898606464266777\n",
      "Epoch 28, Batch: 150: Training Loss: 0.023113446310162544, Validation Loss: 0.021735066547989845\n",
      "Epoch 28, Batch: 151: Training Loss: 0.020883675664663315, Validation Loss: 0.02172270603477955\n",
      "Epoch 28, Batch: 152: Training Loss: 0.02236899547278881, Validation Loss: 0.020397890359163284\n",
      "Epoch 28, Batch: 153: Training Loss: 0.02016669325530529, Validation Loss: 0.019938549026846886\n",
      "Epoch 28, Batch: 154: Training Loss: 0.019963081926107407, Validation Loss: 0.021115025505423546\n",
      "Epoch 28, Batch: 155: Training Loss: 0.024000346660614014, Validation Loss: 0.019373001530766487\n",
      "Epoch 28, Batch: 156: Training Loss: 0.01836765557527542, Validation Loss: 0.021446997299790382\n",
      "Epoch 28, Batch: 157: Training Loss: 0.017512431368231773, Validation Loss: 0.019218383356928825\n",
      "Epoch 28, Batch: 158: Training Loss: 0.020176822319626808, Validation Loss: 0.019592072814702988\n",
      "Epoch 28, Batch: 159: Training Loss: 0.023177124559879303, Validation Loss: 0.020057925954461098\n",
      "Epoch 28, Batch: 160: Training Loss: 0.02061958611011505, Validation Loss: 0.020388467237353325\n",
      "Epoch 28, Batch: 161: Training Loss: 0.019055821001529694, Validation Loss: 0.02076570875942707\n",
      "Epoch 28, Batch: 162: Training Loss: 0.02318856492638588, Validation Loss: 0.01922118291258812\n",
      "Epoch 28, Batch: 163: Training Loss: 0.01980336755514145, Validation Loss: 0.022090721875429153\n",
      "Epoch 28, Batch: 164: Training Loss: 0.01998991146683693, Validation Loss: 0.02135573886334896\n",
      "Epoch 28, Batch: 165: Training Loss: 0.02257312461733818, Validation Loss: 0.022753644734621048\n",
      "Epoch 28, Batch: 166: Training Loss: 0.020531747490167618, Validation Loss: 0.0203399695456028\n",
      "Epoch 28, Batch: 167: Training Loss: 0.019283466041088104, Validation Loss: 0.01979842223227024\n",
      "Epoch 28, Batch: 168: Training Loss: 0.022009316831827164, Validation Loss: 0.022797634825110435\n",
      "Epoch 28, Batch: 169: Training Loss: 0.01971028931438923, Validation Loss: 0.021631496027112007\n",
      "Epoch 28, Batch: 170: Training Loss: 0.021081481128931046, Validation Loss: 0.019207963719964027\n",
      "Epoch 28, Batch: 171: Training Loss: 0.018630919978022575, Validation Loss: 0.021522123366594315\n",
      "Epoch 28, Batch: 172: Training Loss: 0.019706739112734795, Validation Loss: 0.021038521081209183\n",
      "Epoch 28, Batch: 173: Training Loss: 0.01789097487926483, Validation Loss: 0.020213281735777855\n",
      "Epoch 28, Batch: 174: Training Loss: 0.024162497371435165, Validation Loss: 0.02206171676516533\n",
      "Epoch 28, Batch: 175: Training Loss: 0.01775074005126953, Validation Loss: 0.019743962213397026\n",
      "Epoch 28, Batch: 176: Training Loss: 0.02161531336605549, Validation Loss: 0.02279655635356903\n",
      "Epoch 28, Batch: 177: Training Loss: 0.022104457020759583, Validation Loss: 0.02277001366019249\n",
      "Epoch 28, Batch: 178: Training Loss: 0.020862067118287086, Validation Loss: 0.025284839794039726\n",
      "Epoch 28, Batch: 179: Training Loss: 0.01986675336956978, Validation Loss: 0.02437215857207775\n",
      "Epoch 28, Batch: 180: Training Loss: 0.02720494009554386, Validation Loss: 0.025468735024333\n",
      "Epoch 28, Batch: 181: Training Loss: 0.020443079993128777, Validation Loss: 0.023532962426543236\n",
      "Epoch 28, Batch: 182: Training Loss: 0.023014161735773087, Validation Loss: 0.02306714653968811\n",
      "Epoch 28, Batch: 183: Training Loss: 0.019005214795470238, Validation Loss: 0.022991299629211426\n",
      "Epoch 28, Batch: 184: Training Loss: 0.021942783147096634, Validation Loss: 0.02340325154364109\n",
      "Epoch 28, Batch: 185: Training Loss: 0.024641644209623337, Validation Loss: 0.02215142734348774\n",
      "Epoch 28, Batch: 186: Training Loss: 0.02239811420440674, Validation Loss: 0.021173082292079926\n",
      "Epoch 28, Batch: 187: Training Loss: 0.020151741802692413, Validation Loss: 0.02185000479221344\n",
      "Epoch 28, Batch: 188: Training Loss: 0.020176760852336884, Validation Loss: 0.02318117953836918\n",
      "Epoch 28, Batch: 189: Training Loss: 0.01955084502696991, Validation Loss: 0.022854572162032127\n",
      "Epoch 28, Batch: 190: Training Loss: 0.02063373103737831, Validation Loss: 0.021691111847758293\n",
      "Epoch 28, Batch: 191: Training Loss: 0.023500820621848106, Validation Loss: 0.022074278444051743\n",
      "Epoch 28, Batch: 192: Training Loss: 0.022291872650384903, Validation Loss: 0.020240332931280136\n",
      "Epoch 28, Batch: 193: Training Loss: 0.021791940554976463, Validation Loss: 0.020755944773554802\n",
      "Epoch 28, Batch: 194: Training Loss: 0.022338345646858215, Validation Loss: 0.02093120478093624\n",
      "Epoch 28, Batch: 195: Training Loss: 0.019131304696202278, Validation Loss: 0.0241419468075037\n",
      "Epoch 28, Batch: 196: Training Loss: 0.02328476682305336, Validation Loss: 0.024907056242227554\n",
      "Epoch 28, Batch: 197: Training Loss: 0.018567340448498726, Validation Loss: 0.02094012126326561\n",
      "Epoch 28, Batch: 198: Training Loss: 0.020891554653644562, Validation Loss: 0.02359444461762905\n",
      "Epoch 28, Batch: 199: Training Loss: 0.022062860429286957, Validation Loss: 0.021921442821621895\n",
      "Epoch 28, Batch: 200: Training Loss: 0.01770460419356823, Validation Loss: 0.02333550527691841\n",
      "Epoch 28, Batch: 201: Training Loss: 0.02505970560014248, Validation Loss: 0.02312200516462326\n",
      "Epoch 28, Batch: 202: Training Loss: 0.020712848752737045, Validation Loss: 0.022962652146816254\n",
      "Epoch 28, Batch: 203: Training Loss: 0.020235585048794746, Validation Loss: 0.020039260387420654\n",
      "Epoch 28, Batch: 204: Training Loss: 0.02623230777680874, Validation Loss: 0.020562171936035156\n",
      "Epoch 28, Batch: 205: Training Loss: 0.024384982883930206, Validation Loss: 0.02163182571530342\n",
      "Epoch 28, Batch: 206: Training Loss: 0.02292248234152794, Validation Loss: 0.020753875374794006\n",
      "Epoch 28, Batch: 207: Training Loss: 0.022579209879040718, Validation Loss: 0.024260910227894783\n",
      "Epoch 28, Batch: 208: Training Loss: 0.018645744770765305, Validation Loss: 0.02123969793319702\n",
      "Epoch 28, Batch: 209: Training Loss: 0.01788228191435337, Validation Loss: 0.01983257196843624\n",
      "Epoch 28, Batch: 210: Training Loss: 0.022067099809646606, Validation Loss: 0.02256827801465988\n",
      "Epoch 28, Batch: 211: Training Loss: 0.01879909820854664, Validation Loss: 0.024171993136405945\n",
      "Epoch 28, Batch: 212: Training Loss: 0.024529488757252693, Validation Loss: 0.02334114909172058\n",
      "Epoch 28, Batch: 213: Training Loss: 0.0226910263299942, Validation Loss: 0.023193996399641037\n",
      "Epoch 28, Batch: 214: Training Loss: 0.021726280450820923, Validation Loss: 0.022849297150969505\n",
      "Epoch 28, Batch: 215: Training Loss: 0.021411897614598274, Validation Loss: 0.021231332793831825\n",
      "Epoch 28, Batch: 216: Training Loss: 0.021075023338198662, Validation Loss: 0.02123463712632656\n",
      "Epoch 28, Batch: 217: Training Loss: 0.01931454986333847, Validation Loss: 0.021516691893339157\n",
      "Epoch 28, Batch: 218: Training Loss: 0.020784417167305946, Validation Loss: 0.022382929921150208\n",
      "Epoch 28, Batch: 219: Training Loss: 0.019009822979569435, Validation Loss: 0.01970738172531128\n",
      "Epoch 28, Batch: 220: Training Loss: 0.01943720504641533, Validation Loss: 0.022522909566760063\n",
      "Epoch 28, Batch: 221: Training Loss: 0.01861019991338253, Validation Loss: 0.020737143233418465\n",
      "Epoch 28, Batch: 222: Training Loss: 0.020066307857632637, Validation Loss: 0.02213924191892147\n",
      "Epoch 28, Batch: 223: Training Loss: 0.018717942759394646, Validation Loss: 0.02174208126962185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 224: Training Loss: 0.02128451131284237, Validation Loss: 0.02116643823683262\n",
      "Epoch 28, Batch: 225: Training Loss: 0.018918974325060844, Validation Loss: 0.02045113779604435\n",
      "Epoch 28, Batch: 226: Training Loss: 0.02119453065097332, Validation Loss: 0.020772362127900124\n",
      "Epoch 28, Batch: 227: Training Loss: 0.019992602989077568, Validation Loss: 0.021070726215839386\n",
      "Epoch 28, Batch: 228: Training Loss: 0.023760618641972542, Validation Loss: 0.020757421851158142\n",
      "Epoch 28, Batch: 229: Training Loss: 0.02223074436187744, Validation Loss: 0.01993980072438717\n",
      "Epoch 28, Batch: 230: Training Loss: 0.01869351603090763, Validation Loss: 0.019464392215013504\n",
      "Epoch 28, Batch: 231: Training Loss: 0.02431456930935383, Validation Loss: 0.02195795811712742\n",
      "Epoch 28, Batch: 232: Training Loss: 0.02390109933912754, Validation Loss: 0.019380396232008934\n",
      "Epoch 28, Batch: 233: Training Loss: 0.01657664030790329, Validation Loss: 0.02194739319384098\n",
      "Epoch 28, Batch: 234: Training Loss: 0.023589681833982468, Validation Loss: 0.02169838920235634\n",
      "Epoch 28, Batch: 235: Training Loss: 0.021799705922603607, Validation Loss: 0.022575514391064644\n",
      "Epoch 28, Batch: 236: Training Loss: 0.02225741744041443, Validation Loss: 0.02033114992082119\n",
      "Epoch 28, Batch: 237: Training Loss: 0.022280780598521233, Validation Loss: 0.022079506888985634\n",
      "Epoch 28, Batch: 238: Training Loss: 0.020776040852069855, Validation Loss: 0.023385265842080116\n",
      "Epoch 28, Batch: 239: Training Loss: 0.022431809455156326, Validation Loss: 0.022880276665091515\n",
      "Epoch 28, Batch: 240: Training Loss: 0.019321579486131668, Validation Loss: 0.024024154990911484\n",
      "Epoch 28, Batch: 241: Training Loss: 0.021063564345240593, Validation Loss: 0.02312600240111351\n",
      "Epoch 28, Batch: 242: Training Loss: 0.018077287822961807, Validation Loss: 0.022342052310705185\n",
      "Epoch 28, Batch: 243: Training Loss: 0.02001323364675045, Validation Loss: 0.022855114191770554\n",
      "Epoch 28, Batch: 244: Training Loss: 0.02295311912894249, Validation Loss: 0.022263063117861748\n",
      "Epoch 28, Batch: 245: Training Loss: 0.025707364082336426, Validation Loss: 0.023905547335743904\n",
      "Epoch 28, Batch: 246: Training Loss: 0.020868493244051933, Validation Loss: 0.02516719326376915\n",
      "Epoch 28, Batch: 247: Training Loss: 0.022132249549031258, Validation Loss: 0.02098056860268116\n",
      "Epoch 28, Batch: 248: Training Loss: 0.021451041102409363, Validation Loss: 0.021656373515725136\n",
      "Epoch 28, Batch: 249: Training Loss: 0.02395026385784149, Validation Loss: 0.022302791476249695\n",
      "Epoch 28, Batch: 250: Training Loss: 0.02026926539838314, Validation Loss: 0.02365204505622387\n",
      "Epoch 28, Batch: 251: Training Loss: 0.018665220588445663, Validation Loss: 0.023537801578640938\n",
      "Epoch 28, Batch: 252: Training Loss: 0.020903820171952248, Validation Loss: 0.02451443113386631\n",
      "Epoch 28, Batch: 253: Training Loss: 0.02343982830643654, Validation Loss: 0.023232463747262955\n",
      "Epoch 28, Batch: 254: Training Loss: 0.017523007467389107, Validation Loss: 0.022297168150544167\n",
      "Epoch 28, Batch: 255: Training Loss: 0.0228185523301363, Validation Loss: 0.022609887644648552\n",
      "Epoch 28, Batch: 256: Training Loss: 0.02344495803117752, Validation Loss: 0.02319209650158882\n",
      "Epoch 28, Batch: 257: Training Loss: 0.024329159408807755, Validation Loss: 0.022365154698491096\n",
      "Epoch 28, Batch: 258: Training Loss: 0.024830032140016556, Validation Loss: 0.021959837526082993\n",
      "Epoch 28, Batch: 259: Training Loss: 0.020222770050168037, Validation Loss: 0.021140163764357567\n",
      "Epoch 28, Batch: 260: Training Loss: 0.022263741120696068, Validation Loss: 0.01958051696419716\n",
      "Epoch 28, Batch: 261: Training Loss: 0.02452234737575054, Validation Loss: 0.020304860547184944\n",
      "Epoch 28, Batch: 262: Training Loss: 0.020081814378499985, Validation Loss: 0.021296950057148933\n",
      "Epoch 28, Batch: 263: Training Loss: 0.01883867010474205, Validation Loss: 0.020719004794955254\n",
      "Epoch 28, Batch: 264: Training Loss: 0.017135459929704666, Validation Loss: 0.022010572254657745\n",
      "Epoch 28, Batch: 265: Training Loss: 0.019929522648453712, Validation Loss: 0.02299489825963974\n",
      "Epoch 28, Batch: 266: Training Loss: 0.018857579678297043, Validation Loss: 0.021582821384072304\n",
      "Epoch 28, Batch: 267: Training Loss: 0.02124854177236557, Validation Loss: 0.01928611472249031\n",
      "Epoch 28, Batch: 268: Training Loss: 0.019282327964901924, Validation Loss: 0.022022493183612823\n",
      "Epoch 28, Batch: 269: Training Loss: 0.020459895953536034, Validation Loss: 0.02065836451947689\n",
      "Epoch 28, Batch: 270: Training Loss: 0.019706834107637405, Validation Loss: 0.020049085840582848\n",
      "Epoch 28, Batch: 271: Training Loss: 0.01970580592751503, Validation Loss: 0.020931022241711617\n",
      "Epoch 28, Batch: 272: Training Loss: 0.01831546612083912, Validation Loss: 0.019632069393992424\n",
      "Epoch 28, Batch: 273: Training Loss: 0.01831810735166073, Validation Loss: 0.022807884961366653\n",
      "Saving new best model w/ loss: 0.017567411065101624\n",
      "Epoch 28, Batch: 274: Training Loss: 0.020875588059425354, Validation Loss: 0.017567411065101624\n",
      "Epoch 28, Batch: 275: Training Loss: 0.01970405876636505, Validation Loss: 0.022357139736413956\n",
      "Epoch 28, Batch: 276: Training Loss: 0.019458705559372902, Validation Loss: 0.021346502006053925\n",
      "Epoch 28, Batch: 277: Training Loss: 0.01933780498802662, Validation Loss: 0.02105199173092842\n",
      "Epoch 28, Batch: 278: Training Loss: 0.01869366690516472, Validation Loss: 0.022229982540011406\n",
      "Epoch 28, Batch: 279: Training Loss: 0.020520241931080818, Validation Loss: 0.01882227696478367\n",
      "Epoch 28, Batch: 280: Training Loss: 0.017820674926042557, Validation Loss: 0.02145879901945591\n",
      "Epoch 28, Batch: 281: Training Loss: 0.019916627556085587, Validation Loss: 0.02047630585730076\n",
      "Epoch 28, Batch: 282: Training Loss: 0.020220618695020676, Validation Loss: 0.02109251543879509\n",
      "Epoch 28, Batch: 283: Training Loss: 0.018540844321250916, Validation Loss: 0.01962391659617424\n",
      "Epoch 28, Batch: 284: Training Loss: 0.022387998178601265, Validation Loss: 0.018946973606944084\n",
      "Epoch 28, Batch: 285: Training Loss: 0.020462142303586006, Validation Loss: 0.023118946701288223\n",
      "Epoch 28, Batch: 286: Training Loss: 0.019202500581741333, Validation Loss: 0.022318877279758453\n",
      "Epoch 28, Batch: 287: Training Loss: 0.021884366869926453, Validation Loss: 0.021132763475179672\n",
      "Epoch 28, Batch: 288: Training Loss: 0.02189449593424797, Validation Loss: 0.02353006601333618\n",
      "Epoch 28, Batch: 289: Training Loss: 0.022486113011837006, Validation Loss: 0.025222530588507652\n",
      "Epoch 28, Batch: 290: Training Loss: 0.019168531522154808, Validation Loss: 0.021107234060764313\n",
      "Epoch 28, Batch: 291: Training Loss: 0.021584782749414444, Validation Loss: 0.020649975165724754\n",
      "Epoch 28, Batch: 292: Training Loss: 0.021753203123807907, Validation Loss: 0.02139837108552456\n",
      "Epoch 28, Batch: 293: Training Loss: 0.022089555859565735, Validation Loss: 0.023197725415229797\n",
      "Epoch 28, Batch: 294: Training Loss: 0.016512682661414146, Validation Loss: 0.023810721933841705\n",
      "Epoch 28, Batch: 295: Training Loss: 0.020232921466231346, Validation Loss: 0.02440110221505165\n",
      "Epoch 28, Batch: 296: Training Loss: 0.0232459157705307, Validation Loss: 0.023865070194005966\n",
      "Epoch 28, Batch: 297: Training Loss: 0.019300075247883797, Validation Loss: 0.020678726956248283\n",
      "Epoch 28, Batch: 298: Training Loss: 0.022813793271780014, Validation Loss: 0.02369396947324276\n",
      "Epoch 28, Batch: 299: Training Loss: 0.021175723522901535, Validation Loss: 0.024892611429095268\n",
      "Epoch 28, Batch: 300: Training Loss: 0.019023742526769638, Validation Loss: 0.022938773036003113\n",
      "Epoch 28, Batch: 301: Training Loss: 0.017185229808092117, Validation Loss: 0.022553985938429832\n",
      "Epoch 28, Batch: 302: Training Loss: 0.019375374540686607, Validation Loss: 0.02052673138678074\n",
      "Epoch 28, Batch: 303: Training Loss: 0.02086913026869297, Validation Loss: 0.023016953840851784\n",
      "Epoch 28, Batch: 304: Training Loss: 0.019941823557019234, Validation Loss: 0.021330812945961952\n",
      "Epoch 28, Batch: 305: Training Loss: 0.017754599452018738, Validation Loss: 0.019907832145690918\n",
      "Epoch 28, Batch: 306: Training Loss: 0.021408136934041977, Validation Loss: 0.021815916523337364\n",
      "Epoch 28, Batch: 307: Training Loss: 0.0191420279443264, Validation Loss: 0.021419312804937363\n",
      "Epoch 28, Batch: 308: Training Loss: 0.019026122987270355, Validation Loss: 0.021755022928118706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 309: Training Loss: 0.02008901908993721, Validation Loss: 0.021489303559064865\n",
      "Epoch 28, Batch: 310: Training Loss: 0.02006932906806469, Validation Loss: 0.02048146352171898\n",
      "Epoch 28, Batch: 311: Training Loss: 0.019558127969503403, Validation Loss: 0.02003556303679943\n",
      "Epoch 28, Batch: 312: Training Loss: 0.022356046363711357, Validation Loss: 0.01897486113011837\n",
      "Epoch 28, Batch: 313: Training Loss: 0.021065618842840195, Validation Loss: 0.020556392148137093\n",
      "Epoch 28, Batch: 314: Training Loss: 0.018856419250369072, Validation Loss: 0.02055286429822445\n",
      "Epoch 28, Batch: 315: Training Loss: 0.02052542194724083, Validation Loss: 0.02101837657392025\n",
      "Epoch 28, Batch: 316: Training Loss: 0.01849820837378502, Validation Loss: 0.020932260900735855\n",
      "Epoch 28, Batch: 317: Training Loss: 0.018668118864297867, Validation Loss: 0.02034059353172779\n",
      "Epoch 28, Batch: 318: Training Loss: 0.02117322012782097, Validation Loss: 0.02322191186249256\n",
      "Epoch 28, Batch: 319: Training Loss: 0.018790941685438156, Validation Loss: 0.02058648131787777\n",
      "Epoch 28, Batch: 320: Training Loss: 0.020459067076444626, Validation Loss: 0.022082701325416565\n",
      "Epoch 28, Batch: 321: Training Loss: 0.018490394577383995, Validation Loss: 0.02125769667327404\n",
      "Epoch 28, Batch: 322: Training Loss: 0.019189033657312393, Validation Loss: 0.02012043260037899\n",
      "Epoch 28, Batch: 323: Training Loss: 0.019893523305654526, Validation Loss: 0.021595261991024017\n",
      "Epoch 28, Batch: 324: Training Loss: 0.019612226635217667, Validation Loss: 0.021460365504026413\n",
      "Epoch 28, Batch: 325: Training Loss: 0.018365008756518364, Validation Loss: 0.023243973031640053\n",
      "Epoch 28, Batch: 326: Training Loss: 0.021856283769011497, Validation Loss: 0.022777948528528214\n",
      "Epoch 28, Batch: 327: Training Loss: 0.016604403033852577, Validation Loss: 0.022457508370280266\n",
      "Epoch 28, Batch: 328: Training Loss: 0.024227717891335487, Validation Loss: 0.02193867601454258\n",
      "Epoch 28, Batch: 329: Training Loss: 0.02218448929488659, Validation Loss: 0.024225547909736633\n",
      "Epoch 28, Batch: 330: Training Loss: 0.02018078602850437, Validation Loss: 0.027094241231679916\n",
      "Epoch 28, Batch: 331: Training Loss: 0.023214980959892273, Validation Loss: 0.025553621351718903\n",
      "Epoch 28, Batch: 332: Training Loss: 0.020672449842095375, Validation Loss: 0.026220809668302536\n",
      "Epoch 28, Batch: 333: Training Loss: 0.021987497806549072, Validation Loss: 0.0267717856913805\n",
      "Epoch 28, Batch: 334: Training Loss: 0.021326778456568718, Validation Loss: 0.023143408820033073\n",
      "Epoch 28, Batch: 335: Training Loss: 0.019658563658595085, Validation Loss: 0.02225719578564167\n",
      "Epoch 28, Batch: 336: Training Loss: 0.021441197022795677, Validation Loss: 0.02398456446826458\n",
      "Epoch 28, Batch: 337: Training Loss: 0.017166461795568466, Validation Loss: 0.022895347326993942\n",
      "Epoch 28, Batch: 338: Training Loss: 0.016433486714959145, Validation Loss: 0.02130102552473545\n",
      "Epoch 28, Batch: 339: Training Loss: 0.021861637011170387, Validation Loss: 0.024018017575144768\n",
      "Epoch 28, Batch: 340: Training Loss: 0.023254353553056717, Validation Loss: 0.02316032350063324\n",
      "Epoch 28, Batch: 341: Training Loss: 0.020756658166646957, Validation Loss: 0.02191818132996559\n",
      "Epoch 28, Batch: 342: Training Loss: 0.02029401808977127, Validation Loss: 0.02279285155236721\n",
      "Epoch 28, Batch: 343: Training Loss: 0.021873433142900467, Validation Loss: 0.02332082763314247\n",
      "Epoch 28, Batch: 344: Training Loss: 0.021025795489549637, Validation Loss: 0.02146330289542675\n",
      "Epoch 28, Batch: 345: Training Loss: 0.021443206816911697, Validation Loss: 0.021717756986618042\n",
      "Epoch 28, Batch: 346: Training Loss: 0.020196251571178436, Validation Loss: 0.021051499992609024\n",
      "Epoch 28, Batch: 347: Training Loss: 0.017781341448426247, Validation Loss: 0.023973938077688217\n",
      "Epoch 28, Batch: 348: Training Loss: 0.02141381986439228, Validation Loss: 0.023752784356474876\n",
      "Epoch 28, Batch: 349: Training Loss: 0.02234235219657421, Validation Loss: 0.021360499784350395\n",
      "Epoch 28, Batch: 350: Training Loss: 0.018863918259739876, Validation Loss: 0.022997800260782242\n",
      "Epoch 28, Batch: 351: Training Loss: 0.02155916392803192, Validation Loss: 0.02404596284031868\n",
      "Epoch 28, Batch: 352: Training Loss: 0.018156269565224648, Validation Loss: 0.023928385227918625\n",
      "Epoch 28, Batch: 353: Training Loss: 0.022585459053516388, Validation Loss: 0.023359045386314392\n",
      "Epoch 28, Batch: 354: Training Loss: 0.023281747475266457, Validation Loss: 0.02290988340973854\n",
      "Epoch 28, Batch: 355: Training Loss: 0.019053302705287933, Validation Loss: 0.023221980780363083\n",
      "Epoch 28, Batch: 356: Training Loss: 0.021631723269820213, Validation Loss: 0.02150002308189869\n",
      "Epoch 28, Batch: 357: Training Loss: 0.02067074179649353, Validation Loss: 0.02199769765138626\n",
      "Epoch 28, Batch: 358: Training Loss: 0.019324593245983124, Validation Loss: 0.022922364994883537\n",
      "Epoch 28, Batch: 359: Training Loss: 0.016298238188028336, Validation Loss: 0.02124253660440445\n",
      "Epoch 28, Batch: 360: Training Loss: 0.022006357088685036, Validation Loss: 0.02301969937980175\n",
      "Epoch 28, Batch: 361: Training Loss: 0.022726600989699364, Validation Loss: 0.020973455160856247\n",
      "Epoch 28, Batch: 362: Training Loss: 0.01754169352352619, Validation Loss: 0.022129638120532036\n",
      "Epoch 28, Batch: 363: Training Loss: 0.02255704067647457, Validation Loss: 0.021858811378479004\n",
      "Epoch 28, Batch: 364: Training Loss: 0.02174207754433155, Validation Loss: 0.02211221680045128\n",
      "Epoch 28, Batch: 365: Training Loss: 0.020275283604860306, Validation Loss: 0.02169007807970047\n",
      "Epoch 28, Batch: 366: Training Loss: 0.023478176444768906, Validation Loss: 0.02043561264872551\n",
      "Epoch 28, Batch: 367: Training Loss: 0.022483253851532936, Validation Loss: 0.02306537888944149\n",
      "Epoch 28, Batch: 368: Training Loss: 0.019301431253552437, Validation Loss: 0.020817050710320473\n",
      "Epoch 28, Batch: 369: Training Loss: 0.020244253799319267, Validation Loss: 0.021259773522615433\n",
      "Epoch 28, Batch: 370: Training Loss: 0.02082226797938347, Validation Loss: 0.020741060376167297\n",
      "Epoch 28, Batch: 371: Training Loss: 0.01941942237317562, Validation Loss: 0.023935073986649513\n",
      "Epoch 28, Batch: 372: Training Loss: 0.018648285418748856, Validation Loss: 0.025073854252696037\n",
      "Epoch 28, Batch: 373: Training Loss: 0.021087784320116043, Validation Loss: 0.02289780229330063\n",
      "Epoch 28, Batch: 374: Training Loss: 0.021163197234272957, Validation Loss: 0.021064186468720436\n",
      "Epoch 28, Batch: 375: Training Loss: 0.019092928618192673, Validation Loss: 0.021229969337582588\n",
      "Epoch 28, Batch: 376: Training Loss: 0.01895865425467491, Validation Loss: 0.020083190873265266\n",
      "Epoch 28, Batch: 377: Training Loss: 0.019323090091347694, Validation Loss: 0.022267917171120644\n",
      "Epoch 28, Batch: 378: Training Loss: 0.01745782233774662, Validation Loss: 0.023728828877210617\n",
      "Epoch 28, Batch: 379: Training Loss: 0.022276101633906364, Validation Loss: 0.023461706936359406\n",
      "Epoch 28, Batch: 380: Training Loss: 0.0220680870115757, Validation Loss: 0.02022750861942768\n",
      "Epoch 28, Batch: 381: Training Loss: 0.01983434520661831, Validation Loss: 0.02238897979259491\n",
      "Epoch 28, Batch: 382: Training Loss: 0.021462516859173775, Validation Loss: 0.022432299330830574\n",
      "Epoch 28, Batch: 383: Training Loss: 0.02163652516901493, Validation Loss: 0.023769769817590714\n",
      "Epoch 28, Batch: 384: Training Loss: 0.02297159843146801, Validation Loss: 0.023491833359003067\n",
      "Epoch 28, Batch: 385: Training Loss: 0.020326772704720497, Validation Loss: 0.024632666260004044\n",
      "Epoch 28, Batch: 386: Training Loss: 0.019232699647545815, Validation Loss: 0.023630687966942787\n",
      "Epoch 28, Batch: 387: Training Loss: 0.021301446482539177, Validation Loss: 0.023951463401317596\n",
      "Epoch 28, Batch: 388: Training Loss: 0.02370918169617653, Validation Loss: 0.02362682856619358\n",
      "Epoch 28, Batch: 389: Training Loss: 0.022358732298016548, Validation Loss: 0.022330746054649353\n",
      "Epoch 28, Batch: 390: Training Loss: 0.0214358139783144, Validation Loss: 0.022195851430296898\n",
      "Epoch 28, Batch: 391: Training Loss: 0.021590979769825935, Validation Loss: 0.024870485067367554\n",
      "Epoch 28, Batch: 392: Training Loss: 0.018253982067108154, Validation Loss: 0.024252846837043762\n",
      "Epoch 28, Batch: 393: Training Loss: 0.021272551268339157, Validation Loss: 0.024470927193760872\n",
      "Epoch 28, Batch: 394: Training Loss: 0.020739387720823288, Validation Loss: 0.022186411544680595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 395: Training Loss: 0.02048501744866371, Validation Loss: 0.02189716510474682\n",
      "Epoch 28, Batch: 396: Training Loss: 0.021213389933109283, Validation Loss: 0.022688983008265495\n",
      "Epoch 28, Batch: 397: Training Loss: 0.018330145627260208, Validation Loss: 0.024066587910056114\n",
      "Epoch 28, Batch: 398: Training Loss: 0.02138829417526722, Validation Loss: 0.02322615124285221\n",
      "Epoch 28, Batch: 399: Training Loss: 0.019390126690268517, Validation Loss: 0.023864012211561203\n",
      "Epoch 28, Batch: 400: Training Loss: 0.023076718673110008, Validation Loss: 0.022946910932660103\n",
      "Epoch 28, Batch: 401: Training Loss: 0.019369402900338173, Validation Loss: 0.02400158904492855\n",
      "Epoch 28, Batch: 402: Training Loss: 0.021776089444756508, Validation Loss: 0.024553745985031128\n",
      "Epoch 28, Batch: 403: Training Loss: 0.02270543947815895, Validation Loss: 0.02349594235420227\n",
      "Epoch 28, Batch: 404: Training Loss: 0.017660845071077347, Validation Loss: 0.022536590695381165\n",
      "Epoch 28, Batch: 405: Training Loss: 0.02050151489675045, Validation Loss: 0.023277277126908302\n",
      "Epoch 28, Batch: 406: Training Loss: 0.01939229667186737, Validation Loss: 0.02142942324280739\n",
      "Epoch 28, Batch: 407: Training Loss: 0.0187740009278059, Validation Loss: 0.024074004963040352\n",
      "Epoch 28, Batch: 408: Training Loss: 0.017284775152802467, Validation Loss: 0.02429542876780033\n",
      "Epoch 28, Batch: 409: Training Loss: 0.019977154210209846, Validation Loss: 0.02357211522758007\n",
      "Epoch 28, Batch: 410: Training Loss: 0.022315071895718575, Validation Loss: 0.022911589592695236\n",
      "Epoch 28, Batch: 411: Training Loss: 0.017864927649497986, Validation Loss: 0.024258168414235115\n",
      "Epoch 28, Batch: 412: Training Loss: 0.021350348368287086, Validation Loss: 0.023654991760849953\n",
      "Epoch 28, Batch: 413: Training Loss: 0.02261367067694664, Validation Loss: 0.023662807419896126\n",
      "Epoch 28, Batch: 414: Training Loss: 0.02048369124531746, Validation Loss: 0.024232378229498863\n",
      "Epoch 28, Batch: 415: Training Loss: 0.017996294423937798, Validation Loss: 0.023914039134979248\n",
      "Epoch 28, Batch: 416: Training Loss: 0.020226895809173584, Validation Loss: 0.02366790734231472\n",
      "Epoch 28, Batch: 417: Training Loss: 0.01939719170331955, Validation Loss: 0.02401399053633213\n",
      "Epoch 28, Batch: 418: Training Loss: 0.018772680312395096, Validation Loss: 0.023637091740965843\n",
      "Epoch 28, Batch: 419: Training Loss: 0.019967302680015564, Validation Loss: 0.021980643272399902\n",
      "Epoch 28, Batch: 420: Training Loss: 0.02223418653011322, Validation Loss: 0.024974770843982697\n",
      "Epoch 28, Batch: 421: Training Loss: 0.020852521061897278, Validation Loss: 0.020484086126089096\n",
      "Epoch 28, Batch: 422: Training Loss: 0.022429591044783592, Validation Loss: 0.02241065353155136\n",
      "Epoch 28, Batch: 423: Training Loss: 0.020479273051023483, Validation Loss: 0.022119004279375076\n",
      "Epoch 28, Batch: 424: Training Loss: 0.021434681490063667, Validation Loss: 0.019209763035178185\n",
      "Epoch 28, Batch: 425: Training Loss: 0.019633876159787178, Validation Loss: 0.021747317165136337\n",
      "Epoch 28, Batch: 426: Training Loss: 0.01761033944785595, Validation Loss: 0.022266464307904243\n",
      "Epoch 28, Batch: 427: Training Loss: 0.019286412745714188, Validation Loss: 0.022563880309462547\n",
      "Epoch 28, Batch: 428: Training Loss: 0.019987765699625015, Validation Loss: 0.02274283394217491\n",
      "Epoch 28, Batch: 429: Training Loss: 0.019007308408617973, Validation Loss: 0.020592840388417244\n",
      "Epoch 28, Batch: 430: Training Loss: 0.021014010533690453, Validation Loss: 0.02171647921204567\n",
      "Epoch 28, Batch: 431: Training Loss: 0.01853671483695507, Validation Loss: 0.021138343960046768\n",
      "Epoch 28, Batch: 432: Training Loss: 0.020838452503085136, Validation Loss: 0.02342991903424263\n",
      "Epoch 28, Batch: 433: Training Loss: 0.01735294796526432, Validation Loss: 0.021593300625681877\n",
      "Epoch 28, Batch: 434: Training Loss: 0.021323371678590775, Validation Loss: 0.02254972793161869\n",
      "Epoch 28, Batch: 435: Training Loss: 0.021772829815745354, Validation Loss: 0.022827837616205215\n",
      "Epoch 28, Batch: 436: Training Loss: 0.019784828647971153, Validation Loss: 0.02288750372827053\n",
      "Epoch 28, Batch: 437: Training Loss: 0.017249682918190956, Validation Loss: 0.023232026025652885\n",
      "Epoch 28, Batch: 438: Training Loss: 0.021685030311346054, Validation Loss: 0.02325390838086605\n",
      "Epoch 28, Batch: 439: Training Loss: 0.018603727221488953, Validation Loss: 0.025425530970096588\n",
      "Epoch 28, Batch: 440: Training Loss: 0.023541659116744995, Validation Loss: 0.023493532091379166\n",
      "Epoch 28, Batch: 441: Training Loss: 0.01980423927307129, Validation Loss: 0.025025879964232445\n",
      "Epoch 28, Batch: 442: Training Loss: 0.019280284643173218, Validation Loss: 0.02240135334432125\n",
      "Epoch 28, Batch: 443: Training Loss: 0.019249796867370605, Validation Loss: 0.02133850008249283\n",
      "Epoch 28, Batch: 444: Training Loss: 0.016946488991379738, Validation Loss: 0.020918164402246475\n",
      "Epoch 28, Batch: 445: Training Loss: 0.017609862610697746, Validation Loss: 0.02182837203145027\n",
      "Epoch 28, Batch: 446: Training Loss: 0.02233281545341015, Validation Loss: 0.022437123581767082\n",
      "Epoch 28, Batch: 447: Training Loss: 0.019437436014413834, Validation Loss: 0.02294476330280304\n",
      "Epoch 28, Batch: 448: Training Loss: 0.020607464015483856, Validation Loss: 0.021785855293273926\n",
      "Epoch 28, Batch: 449: Training Loss: 0.022261234000325203, Validation Loss: 0.023456797003746033\n",
      "Epoch 28, Batch: 450: Training Loss: 0.0193813294172287, Validation Loss: 0.02188974618911743\n",
      "Epoch 28, Batch: 451: Training Loss: 0.021277502179145813, Validation Loss: 0.02286524511873722\n",
      "Epoch 28, Batch: 452: Training Loss: 0.022832397371530533, Validation Loss: 0.021620117127895355\n",
      "Epoch 28, Batch: 453: Training Loss: 0.019045032560825348, Validation Loss: 0.02057037688791752\n",
      "Epoch 28, Batch: 454: Training Loss: 0.021657440811395645, Validation Loss: 0.01988888718187809\n",
      "Epoch 28, Batch: 455: Training Loss: 0.020031949505209923, Validation Loss: 0.020565299317240715\n",
      "Epoch 28, Batch: 456: Training Loss: 0.018076667562127113, Validation Loss: 0.020535798743367195\n",
      "Epoch 28, Batch: 457: Training Loss: 0.02123877964913845, Validation Loss: 0.021232085302472115\n",
      "Epoch 28, Batch: 458: Training Loss: 0.017807669937610626, Validation Loss: 0.022088175639510155\n",
      "Epoch 28, Batch: 459: Training Loss: 0.019674671813845634, Validation Loss: 0.022389868274331093\n",
      "Epoch 28, Batch: 460: Training Loss: 0.01751830242574215, Validation Loss: 0.02354135736823082\n",
      "Epoch 28, Batch: 461: Training Loss: 0.020232385024428368, Validation Loss: 0.020604046061635017\n",
      "Epoch 28, Batch: 462: Training Loss: 0.018280774354934692, Validation Loss: 0.02255302295088768\n",
      "Epoch 28, Batch: 463: Training Loss: 0.020328626036643982, Validation Loss: 0.022628404200077057\n",
      "Epoch 28, Batch: 464: Training Loss: 0.020772835239768028, Validation Loss: 0.022471193224191666\n",
      "Epoch 28, Batch: 465: Training Loss: 0.021968219429254532, Validation Loss: 0.02168961800634861\n",
      "Epoch 28, Batch: 466: Training Loss: 0.017965631559491158, Validation Loss: 0.023043671622872353\n",
      "Epoch 28, Batch: 467: Training Loss: 0.02728434093296528, Validation Loss: 0.022344417870044708\n",
      "Epoch 28, Batch: 468: Training Loss: 0.021444937214255333, Validation Loss: 0.021789418533444405\n",
      "Epoch 28, Batch: 469: Training Loss: 0.02078089863061905, Validation Loss: 0.024012066423892975\n",
      "Epoch 28, Batch: 470: Training Loss: 0.01954140141606331, Validation Loss: 0.02141202986240387\n",
      "Epoch 28, Batch: 471: Training Loss: 0.01945509761571884, Validation Loss: 0.022025050595402718\n",
      "Epoch 28, Batch: 472: Training Loss: 0.020379653200507164, Validation Loss: 0.021390799432992935\n",
      "Epoch 28, Batch: 473: Training Loss: 0.01764807291328907, Validation Loss: 0.021239828318357468\n",
      "Epoch 28, Batch: 474: Training Loss: 0.01733659952878952, Validation Loss: 0.023128099739551544\n",
      "Epoch 28, Batch: 475: Training Loss: 0.025423821061849594, Validation Loss: 0.02032620459794998\n",
      "Epoch 28, Batch: 476: Training Loss: 0.021405596286058426, Validation Loss: 0.023165371268987656\n",
      "Epoch 28, Batch: 477: Training Loss: 0.02323003113269806, Validation Loss: 0.022954365238547325\n",
      "Epoch 28, Batch: 478: Training Loss: 0.020295925438404083, Validation Loss: 0.023279329761862755\n",
      "Epoch 28, Batch: 479: Training Loss: 0.016736259683966637, Validation Loss: 0.022964924573898315\n",
      "Epoch 28, Batch: 480: Training Loss: 0.024202831089496613, Validation Loss: 0.02028682641685009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch: 481: Training Loss: 0.018787510693073273, Validation Loss: 0.02315908670425415\n",
      "Epoch 28, Batch: 482: Training Loss: 0.025262057781219482, Validation Loss: 0.02449599653482437\n",
      "Epoch 28, Batch: 483: Training Loss: 0.019784007221460342, Validation Loss: 0.023417208343744278\n",
      "Epoch 28, Batch: 484: Training Loss: 0.018018940463662148, Validation Loss: 0.021549036726355553\n",
      "Epoch 28, Batch: 485: Training Loss: 0.02068288065493107, Validation Loss: 0.02249876596033573\n",
      "Epoch 28, Batch: 486: Training Loss: 0.023503204807639122, Validation Loss: 0.022472936660051346\n",
      "Epoch 28, Batch: 487: Training Loss: 0.017868874594569206, Validation Loss: 0.021409902721643448\n",
      "Epoch 28, Batch: 488: Training Loss: 0.019958164542913437, Validation Loss: 0.02275630086660385\n",
      "Epoch 28, Batch: 489: Training Loss: 0.023998072370886803, Validation Loss: 0.02358940988779068\n",
      "Epoch 28, Batch: 490: Training Loss: 0.021691186353564262, Validation Loss: 0.02249317802488804\n",
      "Epoch 28, Batch: 491: Training Loss: 0.016363538801670074, Validation Loss: 0.021162375807762146\n",
      "Epoch 28, Batch: 492: Training Loss: 0.020905911922454834, Validation Loss: 0.02162117138504982\n",
      "Epoch 28, Batch: 493: Training Loss: 0.019720999523997307, Validation Loss: 0.021365048363804817\n",
      "Epoch 28, Batch: 494: Training Loss: 0.020359843969345093, Validation Loss: 0.02174077369272709\n",
      "Epoch 28, Batch: 495: Training Loss: 0.01870810240507126, Validation Loss: 0.023044371977448463\n",
      "Epoch 28, Batch: 496: Training Loss: 0.021878862753510475, Validation Loss: 0.02307106927037239\n",
      "Epoch 28, Batch: 497: Training Loss: 0.017601342871785164, Validation Loss: 0.022220004349946976\n",
      "Epoch 28, Batch: 498: Training Loss: 0.024442903697490692, Validation Loss: 0.02166253887116909\n",
      "Epoch 28, Batch: 499: Training Loss: 0.017119579017162323, Validation Loss: 0.022824963554739952\n",
      "Epoch 29, Batch: 0: Training Loss: 0.020436163991689682, Validation Loss: 0.02406175620853901\n",
      "Epoch 29, Batch: 1: Training Loss: 0.02012912929058075, Validation Loss: 0.022813692688941956\n",
      "Epoch 29, Batch: 2: Training Loss: 0.02141903154551983, Validation Loss: 0.022069595754146576\n",
      "Epoch 29, Batch: 3: Training Loss: 0.018316712230443954, Validation Loss: 0.022522443905472755\n",
      "Epoch 29, Batch: 4: Training Loss: 0.016607875004410744, Validation Loss: 0.024457914754748344\n",
      "Epoch 29, Batch: 5: Training Loss: 0.01837138831615448, Validation Loss: 0.021811867132782936\n",
      "Epoch 29, Batch: 6: Training Loss: 0.01993132010102272, Validation Loss: 0.020997019484639168\n",
      "Epoch 29, Batch: 7: Training Loss: 0.015514880418777466, Validation Loss: 0.023710399866104126\n",
      "Epoch 29, Batch: 8: Training Loss: 0.017710328102111816, Validation Loss: 0.02092195861041546\n",
      "Epoch 29, Batch: 9: Training Loss: 0.020088758319616318, Validation Loss: 0.02162191830575466\n",
      "Epoch 29, Batch: 10: Training Loss: 0.02147573046386242, Validation Loss: 0.023662220686674118\n",
      "Epoch 29, Batch: 11: Training Loss: 0.0220962967723608, Validation Loss: 0.022516004741191864\n",
      "Epoch 29, Batch: 12: Training Loss: 0.021414440125226974, Validation Loss: 0.020799599587917328\n",
      "Epoch 29, Batch: 13: Training Loss: 0.02272285707294941, Validation Loss: 0.021952787414193153\n",
      "Epoch 29, Batch: 14: Training Loss: 0.02082364447414875, Validation Loss: 0.021641863510012627\n",
      "Epoch 29, Batch: 15: Training Loss: 0.019726520404219627, Validation Loss: 0.02033916860818863\n",
      "Epoch 29, Batch: 16: Training Loss: 0.022457076236605644, Validation Loss: 0.02139933779835701\n",
      "Epoch 29, Batch: 17: Training Loss: 0.018113665282726288, Validation Loss: 0.022806715220212936\n",
      "Epoch 29, Batch: 18: Training Loss: 0.016392972320318222, Validation Loss: 0.0226936973631382\n",
      "Epoch 29, Batch: 19: Training Loss: 0.019917689263820648, Validation Loss: 0.021907715126872063\n",
      "Epoch 29, Batch: 20: Training Loss: 0.01851402036845684, Validation Loss: 0.02039000764489174\n",
      "Epoch 29, Batch: 21: Training Loss: 0.021660663187503815, Validation Loss: 0.021493298932909966\n",
      "Epoch 29, Batch: 22: Training Loss: 0.019409939646720886, Validation Loss: 0.02240033447742462\n",
      "Epoch 29, Batch: 23: Training Loss: 0.01946413703262806, Validation Loss: 0.020477691665291786\n",
      "Epoch 29, Batch: 24: Training Loss: 0.02204832248389721, Validation Loss: 0.01838025450706482\n",
      "Epoch 29, Batch: 25: Training Loss: 0.019370319321751595, Validation Loss: 0.018857600167393684\n",
      "Epoch 29, Batch: 26: Training Loss: 0.019520698115229607, Validation Loss: 0.021487275138497353\n",
      "Epoch 29, Batch: 27: Training Loss: 0.01911955326795578, Validation Loss: 0.019997794181108475\n",
      "Epoch 29, Batch: 28: Training Loss: 0.022238221019506454, Validation Loss: 0.020495714619755745\n",
      "Epoch 29, Batch: 29: Training Loss: 0.019822200760245323, Validation Loss: 0.020739514380693436\n",
      "Epoch 29, Batch: 30: Training Loss: 0.02020156756043434, Validation Loss: 0.022307340055704117\n",
      "Epoch 29, Batch: 31: Training Loss: 0.022810187190771103, Validation Loss: 0.022503845393657684\n",
      "Epoch 29, Batch: 32: Training Loss: 0.02097761072218418, Validation Loss: 0.019274210557341576\n",
      "Epoch 29, Batch: 33: Training Loss: 0.017981834709644318, Validation Loss: 0.020185090601444244\n",
      "Epoch 29, Batch: 34: Training Loss: 0.01909199357032776, Validation Loss: 0.0206611305475235\n",
      "Epoch 29, Batch: 35: Training Loss: 0.020240306854248047, Validation Loss: 0.02012607641518116\n",
      "Epoch 29, Batch: 36: Training Loss: 0.019649038091301918, Validation Loss: 0.01895921491086483\n",
      "Epoch 29, Batch: 37: Training Loss: 0.022299306467175484, Validation Loss: 0.020384127274155617\n",
      "Epoch 29, Batch: 38: Training Loss: 0.021617915481328964, Validation Loss: 0.020480213686823845\n",
      "Epoch 29, Batch: 39: Training Loss: 0.02203795686364174, Validation Loss: 0.02144424058496952\n",
      "Epoch 29, Batch: 40: Training Loss: 0.022640377283096313, Validation Loss: 0.021668419241905212\n",
      "Epoch 29, Batch: 41: Training Loss: 0.019396711140871048, Validation Loss: 0.021584250032901764\n",
      "Epoch 29, Batch: 42: Training Loss: 0.020658129826188087, Validation Loss: 0.020911946892738342\n",
      "Epoch 29, Batch: 43: Training Loss: 0.018451282754540443, Validation Loss: 0.020922334864735603\n",
      "Epoch 29, Batch: 44: Training Loss: 0.019878683611750603, Validation Loss: 0.021976079791784286\n",
      "Epoch 29, Batch: 45: Training Loss: 0.02058502286672592, Validation Loss: 0.023093387484550476\n",
      "Epoch 29, Batch: 46: Training Loss: 0.02023100107908249, Validation Loss: 0.0208582766354084\n",
      "Epoch 29, Batch: 47: Training Loss: 0.02200903370976448, Validation Loss: 0.023007096722722054\n",
      "Epoch 29, Batch: 48: Training Loss: 0.021183738484978676, Validation Loss: 0.02031397446990013\n",
      "Epoch 29, Batch: 49: Training Loss: 0.02533785067498684, Validation Loss: 0.021971050649881363\n",
      "Epoch 29, Batch: 50: Training Loss: 0.018269069492816925, Validation Loss: 0.021277861669659615\n",
      "Epoch 29, Batch: 51: Training Loss: 0.021128138527274132, Validation Loss: 0.023391008377075195\n",
      "Epoch 29, Batch: 52: Training Loss: 0.019839534536004066, Validation Loss: 0.01973820850253105\n",
      "Epoch 29, Batch: 53: Training Loss: 0.0202159583568573, Validation Loss: 0.021798649802803993\n",
      "Epoch 29, Batch: 54: Training Loss: 0.02064235508441925, Validation Loss: 0.02417735569179058\n",
      "Epoch 29, Batch: 55: Training Loss: 0.022053254768252373, Validation Loss: 0.021186387166380882\n",
      "Epoch 29, Batch: 56: Training Loss: 0.022816825658082962, Validation Loss: 0.022833706811070442\n",
      "Epoch 29, Batch: 57: Training Loss: 0.022948769852519035, Validation Loss: 0.02087302878499031\n",
      "Epoch 29, Batch: 58: Training Loss: 0.018315188586711884, Validation Loss: 0.020145993679761887\n",
      "Epoch 29, Batch: 59: Training Loss: 0.017005935311317444, Validation Loss: 0.018680669367313385\n",
      "Epoch 29, Batch: 60: Training Loss: 0.019450711086392403, Validation Loss: 0.019578438252210617\n",
      "Epoch 29, Batch: 61: Training Loss: 0.019615478813648224, Validation Loss: 0.02116992510855198\n",
      "Epoch 29, Batch: 62: Training Loss: 0.01779581606388092, Validation Loss: 0.02170516923069954\n",
      "Epoch 29, Batch: 63: Training Loss: 0.022718654945492744, Validation Loss: 0.022230077534914017\n",
      "Epoch 29, Batch: 64: Training Loss: 0.020778734236955643, Validation Loss: 0.021011456847190857\n",
      "Epoch 29, Batch: 65: Training Loss: 0.023377113044261932, Validation Loss: 0.021187080070376396\n",
      "Epoch 29, Batch: 66: Training Loss: 0.02128484472632408, Validation Loss: 0.02182076685130596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 67: Training Loss: 0.017160193994641304, Validation Loss: 0.021690189838409424\n",
      "Epoch 29, Batch: 68: Training Loss: 0.02323809452354908, Validation Loss: 0.021192582324147224\n",
      "Epoch 29, Batch: 69: Training Loss: 0.021225225180387497, Validation Loss: 0.01961585506796837\n",
      "Epoch 29, Batch: 70: Training Loss: 0.021198881790041924, Validation Loss: 0.020762722939252853\n",
      "Epoch 29, Batch: 71: Training Loss: 0.025256561115384102, Validation Loss: 0.019321877509355545\n",
      "Epoch 29, Batch: 72: Training Loss: 0.020588912069797516, Validation Loss: 0.021535588428378105\n",
      "Epoch 29, Batch: 73: Training Loss: 0.01864522695541382, Validation Loss: 0.02272799424827099\n",
      "Epoch 29, Batch: 74: Training Loss: 0.01836162991821766, Validation Loss: 0.023332590237259865\n",
      "Epoch 29, Batch: 75: Training Loss: 0.016136284917593002, Validation Loss: 0.022402210161089897\n",
      "Epoch 29, Batch: 76: Training Loss: 0.01978338323533535, Validation Loss: 0.02308291755616665\n",
      "Epoch 29, Batch: 77: Training Loss: 0.02295655570924282, Validation Loss: 0.02285793237388134\n",
      "Epoch 29, Batch: 78: Training Loss: 0.022836538031697273, Validation Loss: 0.02563174068927765\n",
      "Epoch 29, Batch: 79: Training Loss: 0.020308081060647964, Validation Loss: 0.022480063140392303\n",
      "Epoch 29, Batch: 80: Training Loss: 0.01886853389441967, Validation Loss: 0.022614650428295135\n",
      "Epoch 29, Batch: 81: Training Loss: 0.020463814958930016, Validation Loss: 0.02272368222475052\n",
      "Epoch 29, Batch: 82: Training Loss: 0.0198492631316185, Validation Loss: 0.022086815908551216\n",
      "Epoch 29, Batch: 83: Training Loss: 0.01978355273604393, Validation Loss: 0.025057708844542503\n",
      "Epoch 29, Batch: 84: Training Loss: 0.021828221157193184, Validation Loss: 0.021169763058423996\n",
      "Epoch 29, Batch: 85: Training Loss: 0.020180610939860344, Validation Loss: 0.02202301286160946\n",
      "Epoch 29, Batch: 86: Training Loss: 0.02112841233611107, Validation Loss: 0.02093018963932991\n",
      "Epoch 29, Batch: 87: Training Loss: 0.023203613236546516, Validation Loss: 0.019799454137682915\n",
      "Epoch 29, Batch: 88: Training Loss: 0.018495775759220123, Validation Loss: 0.02167234569787979\n",
      "Epoch 29, Batch: 89: Training Loss: 0.024160966277122498, Validation Loss: 0.020896945148706436\n",
      "Epoch 29, Batch: 90: Training Loss: 0.01925790123641491, Validation Loss: 0.020836975425481796\n",
      "Epoch 29, Batch: 91: Training Loss: 0.02126973867416382, Validation Loss: 0.02385527826845646\n",
      "Epoch 29, Batch: 92: Training Loss: 0.0234281737357378, Validation Loss: 0.022143030539155006\n",
      "Epoch 29, Batch: 93: Training Loss: 0.025929315015673637, Validation Loss: 0.023040588945150375\n",
      "Epoch 29, Batch: 94: Training Loss: 0.022410519421100616, Validation Loss: 0.023729607462882996\n",
      "Epoch 29, Batch: 95: Training Loss: 0.019303200766444206, Validation Loss: 0.02448168210685253\n",
      "Epoch 29, Batch: 96: Training Loss: 0.01854109577834606, Validation Loss: 0.0230296328663826\n",
      "Epoch 29, Batch: 97: Training Loss: 0.021679801866412163, Validation Loss: 0.020782122388482094\n",
      "Epoch 29, Batch: 98: Training Loss: 0.022587383165955544, Validation Loss: 0.02236304245889187\n",
      "Epoch 29, Batch: 99: Training Loss: 0.023298151791095734, Validation Loss: 0.023378057405352592\n",
      "Epoch 29, Batch: 100: Training Loss: 0.022079940885305405, Validation Loss: 0.022847888991236687\n",
      "Epoch 29, Batch: 101: Training Loss: 0.01915021985769272, Validation Loss: 0.022423261776566505\n",
      "Epoch 29, Batch: 102: Training Loss: 0.024093758314847946, Validation Loss: 0.02229631133377552\n",
      "Epoch 29, Batch: 103: Training Loss: 0.022370019927620888, Validation Loss: 0.022244412451982498\n",
      "Epoch 29, Batch: 104: Training Loss: 0.019700733944773674, Validation Loss: 0.022252630442380905\n",
      "Epoch 29, Batch: 105: Training Loss: 0.01805865205824375, Validation Loss: 0.023307830095291138\n",
      "Epoch 29, Batch: 106: Training Loss: 0.019074562937021255, Validation Loss: 0.022442879155278206\n",
      "Epoch 29, Batch: 107: Training Loss: 0.02077203057706356, Validation Loss: 0.02127201110124588\n",
      "Epoch 29, Batch: 108: Training Loss: 0.019279129803180695, Validation Loss: 0.02219022810459137\n",
      "Epoch 29, Batch: 109: Training Loss: 0.02133467048406601, Validation Loss: 0.022864483296871185\n",
      "Epoch 29, Batch: 110: Training Loss: 0.02428210899233818, Validation Loss: 0.022771824151277542\n",
      "Epoch 29, Batch: 111: Training Loss: 0.018213260918855667, Validation Loss: 0.023727351799607277\n",
      "Epoch 29, Batch: 112: Training Loss: 0.019773507490754128, Validation Loss: 0.02377416007220745\n",
      "Epoch 29, Batch: 113: Training Loss: 0.017608514055609703, Validation Loss: 0.02349832095205784\n",
      "Epoch 29, Batch: 114: Training Loss: 0.01994112879037857, Validation Loss: 0.023095428943634033\n",
      "Epoch 29, Batch: 115: Training Loss: 0.025816570967435837, Validation Loss: 0.023503677919507027\n",
      "Epoch 29, Batch: 116: Training Loss: 0.02175464667379856, Validation Loss: 0.02426617033779621\n",
      "Epoch 29, Batch: 117: Training Loss: 0.018987733870744705, Validation Loss: 0.02633287012577057\n",
      "Epoch 29, Batch: 118: Training Loss: 0.020680662244558334, Validation Loss: 0.025086387991905212\n",
      "Epoch 29, Batch: 119: Training Loss: 0.021194176748394966, Validation Loss: 0.023489488288760185\n",
      "Epoch 29, Batch: 120: Training Loss: 0.018396135419607162, Validation Loss: 0.024169445037841797\n",
      "Epoch 29, Batch: 121: Training Loss: 0.019934259355068207, Validation Loss: 0.02145790308713913\n",
      "Epoch 29, Batch: 122: Training Loss: 0.021820412948727608, Validation Loss: 0.024162663146853447\n",
      "Epoch 29, Batch: 123: Training Loss: 0.016406448557972908, Validation Loss: 0.02306145243346691\n",
      "Epoch 29, Batch: 124: Training Loss: 0.01798168197274208, Validation Loss: 0.024076757952570915\n",
      "Epoch 29, Batch: 125: Training Loss: 0.017996544018387794, Validation Loss: 0.021410398185253143\n",
      "Epoch 29, Batch: 126: Training Loss: 0.017818717285990715, Validation Loss: 0.02201802469789982\n",
      "Epoch 29, Batch: 127: Training Loss: 0.02072601392865181, Validation Loss: 0.023214925080537796\n",
      "Epoch 29, Batch: 128: Training Loss: 0.019394442439079285, Validation Loss: 0.021066786721348763\n",
      "Epoch 29, Batch: 129: Training Loss: 0.016825424507260323, Validation Loss: 0.0218935776501894\n",
      "Epoch 29, Batch: 130: Training Loss: 0.02175416797399521, Validation Loss: 0.023829882964491844\n",
      "Epoch 29, Batch: 131: Training Loss: 0.018749959766864777, Validation Loss: 0.02376091107726097\n",
      "Epoch 29, Batch: 132: Training Loss: 0.018132079392671585, Validation Loss: 0.022661754861474037\n",
      "Epoch 29, Batch: 133: Training Loss: 0.018479635939002037, Validation Loss: 0.02246527560055256\n",
      "Epoch 29, Batch: 134: Training Loss: 0.01988932117819786, Validation Loss: 0.02123837359249592\n",
      "Epoch 29, Batch: 135: Training Loss: 0.018563969060778618, Validation Loss: 0.02398691140115261\n",
      "Epoch 29, Batch: 136: Training Loss: 0.019605400040745735, Validation Loss: 0.022156566381454468\n",
      "Epoch 29, Batch: 137: Training Loss: 0.01569296233355999, Validation Loss: 0.021519498899579048\n",
      "Epoch 29, Batch: 138: Training Loss: 0.01889684610068798, Validation Loss: 0.021879907697439194\n",
      "Epoch 29, Batch: 139: Training Loss: 0.01905355043709278, Validation Loss: 0.022273970767855644\n",
      "Epoch 29, Batch: 140: Training Loss: 0.023882688954472542, Validation Loss: 0.023170894011855125\n",
      "Epoch 29, Batch: 141: Training Loss: 0.02104766108095646, Validation Loss: 0.022969793528318405\n",
      "Epoch 29, Batch: 142: Training Loss: 0.01738888770341873, Validation Loss: 0.023138854652643204\n",
      "Epoch 29, Batch: 143: Training Loss: 0.020375963300466537, Validation Loss: 0.01948896050453186\n",
      "Epoch 29, Batch: 144: Training Loss: 0.017105940729379654, Validation Loss: 0.02212142013013363\n",
      "Epoch 29, Batch: 145: Training Loss: 0.019146719947457314, Validation Loss: 0.023620005697011948\n",
      "Epoch 29, Batch: 146: Training Loss: 0.02115490287542343, Validation Loss: 0.023673653602600098\n",
      "Epoch 29, Batch: 147: Training Loss: 0.021050408482551575, Validation Loss: 0.02148202434182167\n",
      "Epoch 29, Batch: 148: Training Loss: 0.019109470769762993, Validation Loss: 0.021955661475658417\n",
      "Epoch 29, Batch: 149: Training Loss: 0.023280469700694084, Validation Loss: 0.02191954478621483\n",
      "Epoch 29, Batch: 150: Training Loss: 0.02045658975839615, Validation Loss: 0.02349943481385708\n",
      "Epoch 29, Batch: 151: Training Loss: 0.022031988948583603, Validation Loss: 0.021556464955210686\n",
      "Epoch 29, Batch: 152: Training Loss: 0.019022362306714058, Validation Loss: 0.022268477827310562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 153: Training Loss: 0.020081061869859695, Validation Loss: 0.021390654146671295\n",
      "Epoch 29, Batch: 154: Training Loss: 0.022412801161408424, Validation Loss: 0.0215953029692173\n",
      "Epoch 29, Batch: 155: Training Loss: 0.02378753572702408, Validation Loss: 0.020705172792077065\n",
      "Epoch 29, Batch: 156: Training Loss: 0.018033180385828018, Validation Loss: 0.020920243114233017\n",
      "Epoch 29, Batch: 157: Training Loss: 0.019779320806264877, Validation Loss: 0.022628646343946457\n",
      "Epoch 29, Batch: 158: Training Loss: 0.01899859309196472, Validation Loss: 0.02184966579079628\n",
      "Epoch 29, Batch: 159: Training Loss: 0.021322617307305336, Validation Loss: 0.021142732352018356\n",
      "Epoch 29, Batch: 160: Training Loss: 0.019114844501018524, Validation Loss: 0.022640084847807884\n",
      "Epoch 29, Batch: 161: Training Loss: 0.02259781025350094, Validation Loss: 0.02290952205657959\n",
      "Epoch 29, Batch: 162: Training Loss: 0.022391952574253082, Validation Loss: 0.02166088856756687\n",
      "Epoch 29, Batch: 163: Training Loss: 0.0197773277759552, Validation Loss: 0.01958593726158142\n",
      "Epoch 29, Batch: 164: Training Loss: 0.020019974559545517, Validation Loss: 0.022995475679636\n",
      "Epoch 29, Batch: 165: Training Loss: 0.02147904969751835, Validation Loss: 0.021989496424794197\n",
      "Epoch 29, Batch: 166: Training Loss: 0.017046144232153893, Validation Loss: 0.022895000874996185\n",
      "Epoch 29, Batch: 167: Training Loss: 0.01850190758705139, Validation Loss: 0.02114873379468918\n",
      "Epoch 29, Batch: 168: Training Loss: 0.022228684276342392, Validation Loss: 0.021702900528907776\n",
      "Epoch 29, Batch: 169: Training Loss: 0.021620573475956917, Validation Loss: 0.02037692256271839\n",
      "Epoch 29, Batch: 170: Training Loss: 0.019725937396287918, Validation Loss: 0.022948209196329117\n",
      "Epoch 29, Batch: 171: Training Loss: 0.018423546105623245, Validation Loss: 0.020194988697767258\n",
      "Epoch 29, Batch: 172: Training Loss: 0.020468611270189285, Validation Loss: 0.022636927664279938\n",
      "Epoch 29, Batch: 173: Training Loss: 0.018353469669818878, Validation Loss: 0.020300615578889847\n",
      "Epoch 29, Batch: 174: Training Loss: 0.020666485652327538, Validation Loss: 0.020236870273947716\n",
      "Epoch 29, Batch: 175: Training Loss: 0.020938202738761902, Validation Loss: 0.020785823464393616\n",
      "Epoch 29, Batch: 176: Training Loss: 0.024142039939761162, Validation Loss: 0.02232433296740055\n",
      "Epoch 29, Batch: 177: Training Loss: 0.01563108153641224, Validation Loss: 0.024714525789022446\n",
      "Epoch 29, Batch: 178: Training Loss: 0.024433793500065804, Validation Loss: 0.020757244899868965\n",
      "Epoch 29, Batch: 179: Training Loss: 0.018303832039237022, Validation Loss: 0.02196606434881687\n",
      "Epoch 29, Batch: 180: Training Loss: 0.020003408193588257, Validation Loss: 0.023036694154143333\n",
      "Epoch 29, Batch: 181: Training Loss: 0.0193340927362442, Validation Loss: 0.02221694216132164\n",
      "Epoch 29, Batch: 182: Training Loss: 0.021103236824274063, Validation Loss: 0.022554617375135422\n",
      "Epoch 29, Batch: 183: Training Loss: 0.017381977289915085, Validation Loss: 0.021134043112397194\n",
      "Epoch 29, Batch: 184: Training Loss: 0.022319022566080093, Validation Loss: 0.020631475374102592\n",
      "Epoch 29, Batch: 185: Training Loss: 0.023852823302149773, Validation Loss: 0.02378358505666256\n",
      "Epoch 29, Batch: 186: Training Loss: 0.021034540608525276, Validation Loss: 0.022614069283008575\n",
      "Epoch 29, Batch: 187: Training Loss: 0.018010394647717476, Validation Loss: 0.02154470980167389\n",
      "Epoch 29, Batch: 188: Training Loss: 0.02286347933113575, Validation Loss: 0.021923530846834183\n",
      "Epoch 29, Batch: 189: Training Loss: 0.018787870183587074, Validation Loss: 0.02109379880130291\n",
      "Epoch 29, Batch: 190: Training Loss: 0.023204144090414047, Validation Loss: 0.021708540618419647\n",
      "Epoch 29, Batch: 191: Training Loss: 0.02214994467794895, Validation Loss: 0.021946076303720474\n",
      "Epoch 29, Batch: 192: Training Loss: 0.02177668735384941, Validation Loss: 0.022512784227728844\n",
      "Epoch 29, Batch: 193: Training Loss: 0.021494979038834572, Validation Loss: 0.019512493163347244\n",
      "Epoch 29, Batch: 194: Training Loss: 0.018483778461813927, Validation Loss: 0.02171272598206997\n",
      "Epoch 29, Batch: 195: Training Loss: 0.019025422632694244, Validation Loss: 0.02257937379181385\n",
      "Epoch 29, Batch: 196: Training Loss: 0.023440517485141754, Validation Loss: 0.02250610664486885\n",
      "Epoch 29, Batch: 197: Training Loss: 0.017805103212594986, Validation Loss: 0.020907191559672356\n",
      "Epoch 29, Batch: 198: Training Loss: 0.01887090504169464, Validation Loss: 0.020455360412597656\n",
      "Epoch 29, Batch: 199: Training Loss: 0.01786227524280548, Validation Loss: 0.022257674485445023\n",
      "Epoch 29, Batch: 200: Training Loss: 0.015414590016007423, Validation Loss: 0.023341940715909004\n",
      "Epoch 29, Batch: 201: Training Loss: 0.022478090599179268, Validation Loss: 0.023463137447834015\n",
      "Epoch 29, Batch: 202: Training Loss: 0.01860274374485016, Validation Loss: 0.02110440470278263\n",
      "Epoch 29, Batch: 203: Training Loss: 0.018500102683901787, Validation Loss: 0.02499230206012726\n",
      "Epoch 29, Batch: 204: Training Loss: 0.019759276881814003, Validation Loss: 0.025010352954268456\n",
      "Epoch 29, Batch: 205: Training Loss: 0.023470763117074966, Validation Loss: 0.02168138511478901\n",
      "Epoch 29, Batch: 206: Training Loss: 0.021695371717214584, Validation Loss: 0.021908152848482132\n",
      "Epoch 29, Batch: 207: Training Loss: 0.020978132262825966, Validation Loss: 0.021656585857272148\n",
      "Epoch 29, Batch: 208: Training Loss: 0.017477963119745255, Validation Loss: 0.022452203556895256\n",
      "Epoch 29, Batch: 209: Training Loss: 0.020027397200465202, Validation Loss: 0.02166033163666725\n",
      "Epoch 29, Batch: 210: Training Loss: 0.020465228706598282, Validation Loss: 0.021253546699881554\n",
      "Epoch 29, Batch: 211: Training Loss: 0.01806650310754776, Validation Loss: 0.024596892297267914\n",
      "Epoch 29, Batch: 212: Training Loss: 0.020327290520071983, Validation Loss: 0.021428264677524567\n",
      "Epoch 29, Batch: 213: Training Loss: 0.022939734160900116, Validation Loss: 0.022499525919556618\n",
      "Epoch 29, Batch: 214: Training Loss: 0.019079534336924553, Validation Loss: 0.022515587508678436\n",
      "Epoch 29, Batch: 215: Training Loss: 0.020599771291017532, Validation Loss: 0.02198825590312481\n",
      "Epoch 29, Batch: 216: Training Loss: 0.01991254650056362, Validation Loss: 0.02256317064166069\n",
      "Epoch 29, Batch: 217: Training Loss: 0.023184845224022865, Validation Loss: 0.022901741787791252\n",
      "Epoch 29, Batch: 218: Training Loss: 0.020114723592996597, Validation Loss: 0.023242663592100143\n",
      "Epoch 29, Batch: 219: Training Loss: 0.01911459118127823, Validation Loss: 0.025848235934972763\n",
      "Epoch 29, Batch: 220: Training Loss: 0.01777653768658638, Validation Loss: 0.021197794005274773\n",
      "Epoch 29, Batch: 221: Training Loss: 0.01794484257698059, Validation Loss: 0.02386007271707058\n",
      "Epoch 29, Batch: 222: Training Loss: 0.02008349634706974, Validation Loss: 0.02236037887632847\n",
      "Epoch 29, Batch: 223: Training Loss: 0.019121000543236732, Validation Loss: 0.02090582065284252\n",
      "Epoch 29, Batch: 224: Training Loss: 0.023313160985708237, Validation Loss: 0.022527365013957024\n",
      "Epoch 29, Batch: 225: Training Loss: 0.020093748345971107, Validation Loss: 0.022023700177669525\n",
      "Epoch 29, Batch: 226: Training Loss: 0.019046593457460403, Validation Loss: 0.02291453443467617\n",
      "Epoch 29, Batch: 227: Training Loss: 0.0207208339124918, Validation Loss: 0.023752283304929733\n",
      "Epoch 29, Batch: 228: Training Loss: 0.02431601844727993, Validation Loss: 0.023298073559999466\n",
      "Epoch 29, Batch: 229: Training Loss: 0.022209081798791885, Validation Loss: 0.02569296397268772\n",
      "Epoch 29, Batch: 230: Training Loss: 0.016288692131638527, Validation Loss: 0.02153017185628414\n",
      "Epoch 29, Batch: 231: Training Loss: 0.021363813430070877, Validation Loss: 0.022657742723822594\n",
      "Epoch 29, Batch: 232: Training Loss: 0.022135088220238686, Validation Loss: 0.02176952175796032\n",
      "Epoch 29, Batch: 233: Training Loss: 0.018646566197276115, Validation Loss: 0.02490193210542202\n",
      "Epoch 29, Batch: 234: Training Loss: 0.02130451239645481, Validation Loss: 0.022366179153323174\n",
      "Epoch 29, Batch: 235: Training Loss: 0.021676883101463318, Validation Loss: 0.022437607869505882\n",
      "Epoch 29, Batch: 236: Training Loss: 0.022967413067817688, Validation Loss: 0.022481946274638176\n",
      "Epoch 29, Batch: 237: Training Loss: 0.01870140992105007, Validation Loss: 0.024086715653538704\n",
      "Epoch 29, Batch: 238: Training Loss: 0.021291738376021385, Validation Loss: 0.02546360343694687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 239: Training Loss: 0.021591072902083397, Validation Loss: 0.022988295182585716\n",
      "Epoch 29, Batch: 240: Training Loss: 0.020154796540737152, Validation Loss: 0.02462957426905632\n",
      "Epoch 29, Batch: 241: Training Loss: 0.01778906211256981, Validation Loss: 0.023998189717531204\n",
      "Epoch 29, Batch: 242: Training Loss: 0.019219644367694855, Validation Loss: 0.02396666444838047\n",
      "Epoch 29, Batch: 243: Training Loss: 0.021347567439079285, Validation Loss: 0.02211904712021351\n",
      "Epoch 29, Batch: 244: Training Loss: 0.020679475739598274, Validation Loss: 0.024913065135478973\n",
      "Epoch 29, Batch: 245: Training Loss: 0.02094232104718685, Validation Loss: 0.022288616746664047\n",
      "Epoch 29, Batch: 246: Training Loss: 0.020175453275442123, Validation Loss: 0.02226262539625168\n",
      "Epoch 29, Batch: 247: Training Loss: 0.023634329438209534, Validation Loss: 0.02045842818915844\n",
      "Epoch 29, Batch: 248: Training Loss: 0.019532086327672005, Validation Loss: 0.023418812081217766\n",
      "Epoch 29, Batch: 249: Training Loss: 0.018033131957054138, Validation Loss: 0.021931689232587814\n",
      "Epoch 29, Batch: 250: Training Loss: 0.01879606768488884, Validation Loss: 0.023185689002275467\n",
      "Epoch 29, Batch: 251: Training Loss: 0.020628418773412704, Validation Loss: 0.02315344847738743\n",
      "Epoch 29, Batch: 252: Training Loss: 0.02000686526298523, Validation Loss: 0.022260446101427078\n",
      "Epoch 29, Batch: 253: Training Loss: 0.02407713606953621, Validation Loss: 0.020585179328918457\n",
      "Epoch 29, Batch: 254: Training Loss: 0.020558390766382217, Validation Loss: 0.02317069284617901\n",
      "Epoch 29, Batch: 255: Training Loss: 0.020578186959028244, Validation Loss: 0.021183393895626068\n",
      "Epoch 29, Batch: 256: Training Loss: 0.02182895690202713, Validation Loss: 0.02180269919335842\n",
      "Epoch 29, Batch: 257: Training Loss: 0.02110842429101467, Validation Loss: 0.021229736506938934\n",
      "Epoch 29, Batch: 258: Training Loss: 0.022459132596850395, Validation Loss: 0.02006491832435131\n",
      "Epoch 29, Batch: 259: Training Loss: 0.019338231533765793, Validation Loss: 0.022787660360336304\n",
      "Epoch 29, Batch: 260: Training Loss: 0.02062552236020565, Validation Loss: 0.022742073982954025\n",
      "Epoch 29, Batch: 261: Training Loss: 0.022358788177371025, Validation Loss: 0.022478248924016953\n",
      "Epoch 29, Batch: 262: Training Loss: 0.019948912784457207, Validation Loss: 0.022821661084890366\n",
      "Epoch 29, Batch: 263: Training Loss: 0.019129235297441483, Validation Loss: 0.0226504597812891\n",
      "Epoch 29, Batch: 264: Training Loss: 0.019207362085580826, Validation Loss: 0.019459202885627747\n",
      "Epoch 29, Batch: 265: Training Loss: 0.020165804773569107, Validation Loss: 0.02426900900900364\n",
      "Epoch 29, Batch: 266: Training Loss: 0.016742002218961716, Validation Loss: 0.020817961543798447\n",
      "Epoch 29, Batch: 267: Training Loss: 0.019639678299427032, Validation Loss: 0.020698172971606255\n",
      "Epoch 29, Batch: 268: Training Loss: 0.018937386572360992, Validation Loss: 0.022781742736697197\n",
      "Epoch 29, Batch: 269: Training Loss: 0.02016966976225376, Validation Loss: 0.02036110870540142\n",
      "Epoch 29, Batch: 270: Training Loss: 0.01998518593609333, Validation Loss: 0.020607057958841324\n",
      "Epoch 29, Batch: 271: Training Loss: 0.01975637674331665, Validation Loss: 0.025316348299384117\n",
      "Epoch 29, Batch: 272: Training Loss: 0.019398104399442673, Validation Loss: 0.02419731579720974\n",
      "Epoch 29, Batch: 273: Training Loss: 0.02150687202811241, Validation Loss: 0.021633805707097054\n",
      "Epoch 29, Batch: 274: Training Loss: 0.023397956043481827, Validation Loss: 0.02144690416753292\n",
      "Epoch 29, Batch: 275: Training Loss: 0.020612075924873352, Validation Loss: 0.0210921261459589\n",
      "Epoch 29, Batch: 276: Training Loss: 0.01855693757534027, Validation Loss: 0.02139003574848175\n",
      "Epoch 29, Batch: 277: Training Loss: 0.018526311963796616, Validation Loss: 0.023612339049577713\n",
      "Epoch 29, Batch: 278: Training Loss: 0.019911378622055054, Validation Loss: 0.02354283072054386\n",
      "Epoch 29, Batch: 279: Training Loss: 0.019586872309446335, Validation Loss: 0.025097902864217758\n",
      "Epoch 29, Batch: 280: Training Loss: 0.017999347299337387, Validation Loss: 0.022456293925642967\n",
      "Epoch 29, Batch: 281: Training Loss: 0.017983563244342804, Validation Loss: 0.021763255819678307\n",
      "Epoch 29, Batch: 282: Training Loss: 0.02021086774766445, Validation Loss: 0.0193561390042305\n",
      "Epoch 29, Batch: 283: Training Loss: 0.020437372848391533, Validation Loss: 0.021304767578840256\n",
      "Epoch 29, Batch: 284: Training Loss: 0.020281611010432243, Validation Loss: 0.020769162103533745\n",
      "Epoch 29, Batch: 285: Training Loss: 0.01875017024576664, Validation Loss: 0.02089970000088215\n",
      "Epoch 29, Batch: 286: Training Loss: 0.02005089446902275, Validation Loss: 0.02328808419406414\n",
      "Epoch 29, Batch: 287: Training Loss: 0.020769450813531876, Validation Loss: 0.023959750309586525\n",
      "Epoch 29, Batch: 288: Training Loss: 0.018779154866933823, Validation Loss: 0.022295096889138222\n",
      "Epoch 29, Batch: 289: Training Loss: 0.023153090849518776, Validation Loss: 0.0206186193972826\n",
      "Epoch 29, Batch: 290: Training Loss: 0.02092130482196808, Validation Loss: 0.02339424379169941\n",
      "Epoch 29, Batch: 291: Training Loss: 0.01856503263115883, Validation Loss: 0.02643514610826969\n",
      "Epoch 29, Batch: 292: Training Loss: 0.022870225831866264, Validation Loss: 0.02532382868230343\n",
      "Epoch 29, Batch: 293: Training Loss: 0.023233886808156967, Validation Loss: 0.023418687283992767\n",
      "Epoch 29, Batch: 294: Training Loss: 0.019064202904701233, Validation Loss: 0.021231170743703842\n",
      "Epoch 29, Batch: 295: Training Loss: 0.019089967012405396, Validation Loss: 0.02246738038957119\n",
      "Epoch 29, Batch: 296: Training Loss: 0.023212051019072533, Validation Loss: 0.021356772631406784\n",
      "Epoch 29, Batch: 297: Training Loss: 0.022334348410367966, Validation Loss: 0.022137682884931564\n",
      "Epoch 29, Batch: 298: Training Loss: 0.020914152264595032, Validation Loss: 0.022211648523807526\n",
      "Epoch 29, Batch: 299: Training Loss: 0.0218276958912611, Validation Loss: 0.021121812984347343\n",
      "Epoch 29, Batch: 300: Training Loss: 0.02091737650334835, Validation Loss: 0.022623535245656967\n",
      "Epoch 29, Batch: 301: Training Loss: 0.02209649048745632, Validation Loss: 0.022261081263422966\n",
      "Epoch 29, Batch: 302: Training Loss: 0.018881337717175484, Validation Loss: 0.023783227428793907\n",
      "Epoch 29, Batch: 303: Training Loss: 0.018904078751802444, Validation Loss: 0.02247600443661213\n",
      "Epoch 29, Batch: 304: Training Loss: 0.016599128022789955, Validation Loss: 0.02187175676226616\n",
      "Epoch 29, Batch: 305: Training Loss: 0.021423300728201866, Validation Loss: 0.021414581686258316\n",
      "Epoch 29, Batch: 306: Training Loss: 0.020966650918126106, Validation Loss: 0.02099107950925827\n",
      "Epoch 29, Batch: 307: Training Loss: 0.017754308879375458, Validation Loss: 0.021271467208862305\n",
      "Epoch 29, Batch: 308: Training Loss: 0.022651785984635353, Validation Loss: 0.022515878081321716\n",
      "Epoch 29, Batch: 309: Training Loss: 0.02240539714694023, Validation Loss: 0.02292332611978054\n",
      "Epoch 29, Batch: 310: Training Loss: 0.020286064594984055, Validation Loss: 0.020913392305374146\n",
      "Epoch 29, Batch: 311: Training Loss: 0.0198000930249691, Validation Loss: 0.024449948221445084\n",
      "Epoch 29, Batch: 312: Training Loss: 0.021041790023446083, Validation Loss: 0.022504858672618866\n",
      "Epoch 29, Batch: 313: Training Loss: 0.01864585094153881, Validation Loss: 0.02229924127459526\n",
      "Epoch 29, Batch: 314: Training Loss: 0.020011795684695244, Validation Loss: 0.022601556032896042\n",
      "Epoch 29, Batch: 315: Training Loss: 0.020740492269396782, Validation Loss: 0.022478710860013962\n",
      "Epoch 29, Batch: 316: Training Loss: 0.017730114981532097, Validation Loss: 0.020874617621302605\n",
      "Epoch 29, Batch: 317: Training Loss: 0.01900683529675007, Validation Loss: 0.020252835005521774\n",
      "Epoch 29, Batch: 318: Training Loss: 0.02004077658057213, Validation Loss: 0.02345525473356247\n",
      "Epoch 29, Batch: 319: Training Loss: 0.02213672734797001, Validation Loss: 0.02075626328587532\n",
      "Epoch 29, Batch: 320: Training Loss: 0.022908570244908333, Validation Loss: 0.024291163310408592\n",
      "Epoch 29, Batch: 321: Training Loss: 0.019628802314400673, Validation Loss: 0.023310938850045204\n",
      "Epoch 29, Batch: 322: Training Loss: 0.019664950668811798, Validation Loss: 0.023540817201137543\n",
      "Epoch 29, Batch: 323: Training Loss: 0.019759396091103554, Validation Loss: 0.024429820477962494\n",
      "Epoch 29, Batch: 324: Training Loss: 0.02052806317806244, Validation Loss: 0.022989407181739807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 325: Training Loss: 0.018938785418868065, Validation Loss: 0.02490069530904293\n",
      "Epoch 29, Batch: 326: Training Loss: 0.02405114471912384, Validation Loss: 0.022584987804293633\n",
      "Epoch 29, Batch: 327: Training Loss: 0.018192334100604057, Validation Loss: 0.023837456479668617\n",
      "Epoch 29, Batch: 328: Training Loss: 0.020386474207043648, Validation Loss: 0.02220204286277294\n",
      "Epoch 29, Batch: 329: Training Loss: 0.020348841324448586, Validation Loss: 0.023067567497491837\n",
      "Epoch 29, Batch: 330: Training Loss: 0.020163996145129204, Validation Loss: 0.02200627699494362\n",
      "Epoch 29, Batch: 331: Training Loss: 0.021482253447175026, Validation Loss: 0.024573229253292084\n",
      "Epoch 29, Batch: 332: Training Loss: 0.021761734038591385, Validation Loss: 0.024447325617074966\n",
      "Epoch 29, Batch: 333: Training Loss: 0.021632634103298187, Validation Loss: 0.027045143768191338\n",
      "Epoch 29, Batch: 334: Training Loss: 0.021377794444561005, Validation Loss: 0.026585977524518967\n",
      "Epoch 29, Batch: 335: Training Loss: 0.018732450902462006, Validation Loss: 0.023743536323308945\n",
      "Epoch 29, Batch: 336: Training Loss: 0.01943231374025345, Validation Loss: 0.02503873035311699\n",
      "Epoch 29, Batch: 337: Training Loss: 0.017486367374658585, Validation Loss: 0.02428281679749489\n",
      "Epoch 29, Batch: 338: Training Loss: 0.020342765375971794, Validation Loss: 0.02617664262652397\n",
      "Epoch 29, Batch: 339: Training Loss: 0.021560464054346085, Validation Loss: 0.02429329976439476\n",
      "Epoch 29, Batch: 340: Training Loss: 0.01907172240316868, Validation Loss: 0.02347223088145256\n",
      "Epoch 29, Batch: 341: Training Loss: 0.0187830850481987, Validation Loss: 0.02307070791721344\n",
      "Epoch 29, Batch: 342: Training Loss: 0.01951359398663044, Validation Loss: 0.022799592465162277\n",
      "Epoch 29, Batch: 343: Training Loss: 0.022230781614780426, Validation Loss: 0.02275734767317772\n",
      "Epoch 29, Batch: 344: Training Loss: 0.02113628014922142, Validation Loss: 0.022670546546578407\n",
      "Epoch 29, Batch: 345: Training Loss: 0.020159678533673286, Validation Loss: 0.024345537647604942\n",
      "Epoch 29, Batch: 346: Training Loss: 0.02399255521595478, Validation Loss: 0.023262185975909233\n",
      "Epoch 29, Batch: 347: Training Loss: 0.019697634503245354, Validation Loss: 0.02326361835002899\n",
      "Epoch 29, Batch: 348: Training Loss: 0.019467143341898918, Validation Loss: 0.02209479548037052\n",
      "Epoch 29, Batch: 349: Training Loss: 0.022689225152134895, Validation Loss: 0.02155790664255619\n",
      "Epoch 29, Batch: 350: Training Loss: 0.01901341788470745, Validation Loss: 0.02237076312303543\n",
      "Epoch 29, Batch: 351: Training Loss: 0.026479562744498253, Validation Loss: 0.022724825888872147\n",
      "Epoch 29, Batch: 352: Training Loss: 0.020386120304465294, Validation Loss: 0.02228461019694805\n",
      "Epoch 29, Batch: 353: Training Loss: 0.020490283146500587, Validation Loss: 0.02107897400856018\n",
      "Epoch 29, Batch: 354: Training Loss: 0.021282803267240524, Validation Loss: 0.02594398520886898\n",
      "Epoch 29, Batch: 355: Training Loss: 0.01740991324186325, Validation Loss: 0.023345068097114563\n",
      "Epoch 29, Batch: 356: Training Loss: 0.021585704758763313, Validation Loss: 0.02297390066087246\n",
      "Epoch 29, Batch: 357: Training Loss: 0.017616480588912964, Validation Loss: 0.024354074150323868\n",
      "Epoch 29, Batch: 358: Training Loss: 0.01887749508023262, Validation Loss: 0.022950423881411552\n",
      "Epoch 29, Batch: 359: Training Loss: 0.021474678069353104, Validation Loss: 0.020524632185697556\n",
      "Epoch 29, Batch: 360: Training Loss: 0.022177308797836304, Validation Loss: 0.024050060659646988\n",
      "Epoch 29, Batch: 361: Training Loss: 0.019196292385458946, Validation Loss: 0.024173278361558914\n",
      "Epoch 29, Batch: 362: Training Loss: 0.020303521305322647, Validation Loss: 0.02341539040207863\n",
      "Epoch 29, Batch: 363: Training Loss: 0.025692513212561607, Validation Loss: 0.024284766986966133\n",
      "Epoch 29, Batch: 364: Training Loss: 0.01993042789399624, Validation Loss: 0.022698814049363136\n",
      "Epoch 29, Batch: 365: Training Loss: 0.020622121170163155, Validation Loss: 0.021992653608322144\n",
      "Epoch 29, Batch: 366: Training Loss: 0.021769322454929352, Validation Loss: 0.021618366241455078\n",
      "Epoch 29, Batch: 367: Training Loss: 0.017941497266292572, Validation Loss: 0.023700425401329994\n",
      "Epoch 29, Batch: 368: Training Loss: 0.019020983949303627, Validation Loss: 0.022917579859495163\n",
      "Epoch 29, Batch: 369: Training Loss: 0.021164659410715103, Validation Loss: 0.022843357175588608\n",
      "Epoch 29, Batch: 370: Training Loss: 0.019988836720585823, Validation Loss: 0.02150355465710163\n",
      "Epoch 29, Batch: 371: Training Loss: 0.017055246978998184, Validation Loss: 0.023191876709461212\n",
      "Epoch 29, Batch: 372: Training Loss: 0.01892111450433731, Validation Loss: 0.020723646506667137\n",
      "Epoch 29, Batch: 373: Training Loss: 0.024696221575140953, Validation Loss: 0.024061819538474083\n",
      "Epoch 29, Batch: 374: Training Loss: 0.019533833488821983, Validation Loss: 0.022207913920283318\n",
      "Epoch 29, Batch: 375: Training Loss: 0.02072097733616829, Validation Loss: 0.022624503821134567\n",
      "Epoch 29, Batch: 376: Training Loss: 0.017934519797563553, Validation Loss: 0.022751839831471443\n",
      "Epoch 29, Batch: 377: Training Loss: 0.02052178978919983, Validation Loss: 0.022549815475940704\n",
      "Epoch 29, Batch: 378: Training Loss: 0.019684556871652603, Validation Loss: 0.021859312430024147\n",
      "Epoch 29, Batch: 379: Training Loss: 0.021261494606733322, Validation Loss: 0.022860540077090263\n",
      "Epoch 29, Batch: 380: Training Loss: 0.023404832929372787, Validation Loss: 0.02540525048971176\n",
      "Epoch 29, Batch: 381: Training Loss: 0.021675189957022667, Validation Loss: 0.02551894821226597\n",
      "Epoch 29, Batch: 382: Training Loss: 0.020376885309815407, Validation Loss: 0.02137860096991062\n",
      "Epoch 29, Batch: 383: Training Loss: 0.018989767879247665, Validation Loss: 0.02438785322010517\n",
      "Epoch 29, Batch: 384: Training Loss: 0.023361936211586, Validation Loss: 0.020226826891303062\n",
      "Epoch 29, Batch: 385: Training Loss: 0.02084452100098133, Validation Loss: 0.024278566241264343\n",
      "Epoch 29, Batch: 386: Training Loss: 0.01702125556766987, Validation Loss: 0.02185419574379921\n",
      "Epoch 29, Batch: 387: Training Loss: 0.020080020651221275, Validation Loss: 0.021443936973810196\n",
      "Epoch 29, Batch: 388: Training Loss: 0.017682747915387154, Validation Loss: 0.023348741233348846\n",
      "Epoch 29, Batch: 389: Training Loss: 0.02058197371661663, Validation Loss: 0.02168145217001438\n",
      "Epoch 29, Batch: 390: Training Loss: 0.02029624581336975, Validation Loss: 0.022153586149215698\n",
      "Epoch 29, Batch: 391: Training Loss: 0.019506948068737984, Validation Loss: 0.021330982446670532\n",
      "Epoch 29, Batch: 392: Training Loss: 0.01871783472597599, Validation Loss: 0.02103775553405285\n",
      "Epoch 29, Batch: 393: Training Loss: 0.020350368693470955, Validation Loss: 0.024641774594783783\n",
      "Epoch 29, Batch: 394: Training Loss: 0.019004113972187042, Validation Loss: 0.02191149815917015\n",
      "Epoch 29, Batch: 395: Training Loss: 0.019389081746339798, Validation Loss: 0.02262848988175392\n",
      "Epoch 29, Batch: 396: Training Loss: 0.022530244663357735, Validation Loss: 0.022798465564846992\n",
      "Epoch 29, Batch: 397: Training Loss: 0.02179415337741375, Validation Loss: 0.022197188809514046\n",
      "Epoch 29, Batch: 398: Training Loss: 0.022991618141531944, Validation Loss: 0.023230256512761116\n",
      "Epoch 29, Batch: 399: Training Loss: 0.019835833460092545, Validation Loss: 0.022616304457187653\n",
      "Epoch 29, Batch: 400: Training Loss: 0.024017849937081337, Validation Loss: 0.02388591878116131\n",
      "Epoch 29, Batch: 401: Training Loss: 0.02226138301193714, Validation Loss: 0.02119891718029976\n",
      "Epoch 29, Batch: 402: Training Loss: 0.01998339779675007, Validation Loss: 0.023062530905008316\n",
      "Epoch 29, Batch: 403: Training Loss: 0.020175641402602196, Validation Loss: 0.02145248092710972\n",
      "Epoch 29, Batch: 404: Training Loss: 0.01764768734574318, Validation Loss: 0.021278636530041695\n",
      "Epoch 29, Batch: 405: Training Loss: 0.021365150809288025, Validation Loss: 0.022250236943364143\n",
      "Epoch 29, Batch: 406: Training Loss: 0.02156655490398407, Validation Loss: 0.022178690880537033\n",
      "Epoch 29, Batch: 407: Training Loss: 0.01895606331527233, Validation Loss: 0.022487012669444084\n",
      "Epoch 29, Batch: 408: Training Loss: 0.017812462523579597, Validation Loss: 0.020042307674884796\n",
      "Epoch 29, Batch: 409: Training Loss: 0.020138543099164963, Validation Loss: 0.021146323531866074\n",
      "Epoch 29, Batch: 410: Training Loss: 0.02105112560093403, Validation Loss: 0.02299468033015728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 411: Training Loss: 0.017659414559602737, Validation Loss: 0.020467683672904968\n",
      "Epoch 29, Batch: 412: Training Loss: 0.024415647611021996, Validation Loss: 0.022450460121035576\n",
      "Epoch 29, Batch: 413: Training Loss: 0.02402261272072792, Validation Loss: 0.023185959085822105\n",
      "Epoch 29, Batch: 414: Training Loss: 0.021540045738220215, Validation Loss: 0.022694941610097885\n",
      "Epoch 29, Batch: 415: Training Loss: 0.02172316424548626, Validation Loss: 0.02161610871553421\n",
      "Epoch 29, Batch: 416: Training Loss: 0.02016184665262699, Validation Loss: 0.021885452792048454\n",
      "Epoch 29, Batch: 417: Training Loss: 0.0190887451171875, Validation Loss: 0.021236512809991837\n",
      "Epoch 29, Batch: 418: Training Loss: 0.016850829124450684, Validation Loss: 0.020543411374092102\n",
      "Epoch 29, Batch: 419: Training Loss: 0.02066209726035595, Validation Loss: 0.021825307980179787\n",
      "Epoch 29, Batch: 420: Training Loss: 0.02239273674786091, Validation Loss: 0.02065286785364151\n",
      "Epoch 29, Batch: 421: Training Loss: 0.02120593748986721, Validation Loss: 0.022709431126713753\n",
      "Epoch 29, Batch: 422: Training Loss: 0.023115135729312897, Validation Loss: 0.021134354174137115\n",
      "Epoch 29, Batch: 423: Training Loss: 0.020670417696237564, Validation Loss: 0.021198062226176262\n",
      "Epoch 29, Batch: 424: Training Loss: 0.020378796383738518, Validation Loss: 0.020703932270407677\n",
      "Epoch 29, Batch: 425: Training Loss: 0.018555592745542526, Validation Loss: 0.021363845095038414\n",
      "Epoch 29, Batch: 426: Training Loss: 0.01949583925306797, Validation Loss: 0.021552929654717445\n",
      "Epoch 29, Batch: 427: Training Loss: 0.01937609165906906, Validation Loss: 0.02001064270734787\n",
      "Epoch 29, Batch: 428: Training Loss: 0.021377354860305786, Validation Loss: 0.0218726247549057\n",
      "Epoch 29, Batch: 429: Training Loss: 0.018967732787132263, Validation Loss: 0.02003387361764908\n",
      "Epoch 29, Batch: 430: Training Loss: 0.01837928406894207, Validation Loss: 0.0198761485517025\n",
      "Epoch 29, Batch: 431: Training Loss: 0.020246002823114395, Validation Loss: 0.020532162860035896\n",
      "Epoch 29, Batch: 432: Training Loss: 0.018646346405148506, Validation Loss: 0.021410975605249405\n",
      "Epoch 29, Batch: 433: Training Loss: 0.02012336440384388, Validation Loss: 0.022353434935212135\n",
      "Epoch 29, Batch: 434: Training Loss: 0.021699372678995132, Validation Loss: 0.021633414551615715\n",
      "Epoch 29, Batch: 435: Training Loss: 0.024146132171154022, Validation Loss: 0.022254088893532753\n",
      "Epoch 29, Batch: 436: Training Loss: 0.018036071211099625, Validation Loss: 0.02440384030342102\n",
      "Epoch 29, Batch: 437: Training Loss: 0.018178652971982956, Validation Loss: 0.021685468032956123\n",
      "Epoch 29, Batch: 438: Training Loss: 0.01956489495933056, Validation Loss: 0.019989844411611557\n",
      "Epoch 29, Batch: 439: Training Loss: 0.01951298490166664, Validation Loss: 0.020562198013067245\n",
      "Epoch 29, Batch: 440: Training Loss: 0.022392716258764267, Validation Loss: 0.0217866413295269\n",
      "Epoch 29, Batch: 441: Training Loss: 0.017310529947280884, Validation Loss: 0.021123431622982025\n",
      "Epoch 29, Batch: 442: Training Loss: 0.01843392103910446, Validation Loss: 0.023765096440911293\n",
      "Epoch 29, Batch: 443: Training Loss: 0.019385408610105515, Validation Loss: 0.0227640513330698\n",
      "Epoch 29, Batch: 444: Training Loss: 0.016252094879746437, Validation Loss: 0.021963519975543022\n",
      "Epoch 29, Batch: 445: Training Loss: 0.017492923885583878, Validation Loss: 0.02053609862923622\n",
      "Epoch 29, Batch: 446: Training Loss: 0.02391473948955536, Validation Loss: 0.019731061533093452\n",
      "Epoch 29, Batch: 447: Training Loss: 0.019559776410460472, Validation Loss: 0.022962365299463272\n",
      "Epoch 29, Batch: 448: Training Loss: 0.018343336880207062, Validation Loss: 0.020819565281271935\n",
      "Epoch 29, Batch: 449: Training Loss: 0.02049110271036625, Validation Loss: 0.019576290622353554\n",
      "Epoch 29, Batch: 450: Training Loss: 0.016646480187773705, Validation Loss: 0.021464262157678604\n",
      "Epoch 29, Batch: 451: Training Loss: 0.02229836955666542, Validation Loss: 0.02005394548177719\n",
      "Epoch 29, Batch: 452: Training Loss: 0.01967070810496807, Validation Loss: 0.02166012115776539\n",
      "Epoch 29, Batch: 453: Training Loss: 0.017439747229218483, Validation Loss: 0.019291499629616737\n",
      "Epoch 29, Batch: 454: Training Loss: 0.019390851259231567, Validation Loss: 0.021828001365065575\n",
      "Epoch 29, Batch: 455: Training Loss: 0.01702486164867878, Validation Loss: 0.02037588506937027\n",
      "Epoch 29, Batch: 456: Training Loss: 0.019559690728783607, Validation Loss: 0.022993283346295357\n",
      "Epoch 29, Batch: 457: Training Loss: 0.02345059998333454, Validation Loss: 0.02333400957286358\n",
      "Epoch 29, Batch: 458: Training Loss: 0.018590357154607773, Validation Loss: 0.020030636340379715\n",
      "Epoch 29, Batch: 459: Training Loss: 0.018347330391407013, Validation Loss: 0.02345983497798443\n",
      "Epoch 29, Batch: 460: Training Loss: 0.018915213644504547, Validation Loss: 0.022880541160702705\n",
      "Epoch 29, Batch: 461: Training Loss: 0.018655631691217422, Validation Loss: 0.023066790774464607\n",
      "Epoch 29, Batch: 462: Training Loss: 0.018484121188521385, Validation Loss: 0.019966380670666695\n",
      "Epoch 29, Batch: 463: Training Loss: 0.018787330016493797, Validation Loss: 0.023342430591583252\n",
      "Epoch 29, Batch: 464: Training Loss: 0.01911664381623268, Validation Loss: 0.022964874282479286\n",
      "Epoch 29, Batch: 465: Training Loss: 0.02343636378645897, Validation Loss: 0.023240264505147934\n",
      "Epoch 29, Batch: 466: Training Loss: 0.017468633130192757, Validation Loss: 0.021861739456653595\n",
      "Epoch 29, Batch: 467: Training Loss: 0.02333654649555683, Validation Loss: 0.020823346450924873\n",
      "Epoch 29, Batch: 468: Training Loss: 0.02492307499051094, Validation Loss: 0.02042931318283081\n",
      "Epoch 29, Batch: 469: Training Loss: 0.019597066566348076, Validation Loss: 0.022807009518146515\n",
      "Epoch 29, Batch: 470: Training Loss: 0.018498258665204048, Validation Loss: 0.020717404782772064\n",
      "Epoch 29, Batch: 471: Training Loss: 0.02274148352444172, Validation Loss: 0.019335558637976646\n",
      "Epoch 29, Batch: 472: Training Loss: 0.019161297008395195, Validation Loss: 0.019656898453831673\n",
      "Epoch 29, Batch: 473: Training Loss: 0.01928734965622425, Validation Loss: 0.019486093893647194\n",
      "Epoch 29, Batch: 474: Training Loss: 0.018520187586545944, Validation Loss: 0.02072550170123577\n",
      "Epoch 29, Batch: 475: Training Loss: 0.020554708316922188, Validation Loss: 0.020315222442150116\n",
      "Epoch 29, Batch: 476: Training Loss: 0.02021525800228119, Validation Loss: 0.021409571170806885\n",
      "Epoch 29, Batch: 477: Training Loss: 0.020978376269340515, Validation Loss: 0.023002989590168\n",
      "Epoch 29, Batch: 478: Training Loss: 0.019930630922317505, Validation Loss: 0.022262681275606155\n",
      "Epoch 29, Batch: 479: Training Loss: 0.020832736045122147, Validation Loss: 0.020573796704411507\n",
      "Epoch 29, Batch: 480: Training Loss: 0.022569259628653526, Validation Loss: 0.02094295248389244\n",
      "Epoch 29, Batch: 481: Training Loss: 0.018034063279628754, Validation Loss: 0.022884292528033257\n",
      "Epoch 29, Batch: 482: Training Loss: 0.02180783450603485, Validation Loss: 0.0246629249304533\n",
      "Epoch 29, Batch: 483: Training Loss: 0.021015243604779243, Validation Loss: 0.02090604417026043\n",
      "Epoch 29, Batch: 484: Training Loss: 0.01884535513818264, Validation Loss: 0.01955721713602543\n",
      "Epoch 29, Batch: 485: Training Loss: 0.020301641896367073, Validation Loss: 0.02056366764008999\n",
      "Epoch 29, Batch: 486: Training Loss: 0.020526740700006485, Validation Loss: 0.02163831703364849\n",
      "Epoch 29, Batch: 487: Training Loss: 0.017838140949606895, Validation Loss: 0.0212581604719162\n",
      "Epoch 29, Batch: 488: Training Loss: 0.0206564012914896, Validation Loss: 0.020299594849348068\n",
      "Epoch 29, Batch: 489: Training Loss: 0.022487886250019073, Validation Loss: 0.019850483164191246\n",
      "Epoch 29, Batch: 490: Training Loss: 0.02107253484427929, Validation Loss: 0.021624140441417694\n",
      "Epoch 29, Batch: 491: Training Loss: 0.01891208626329899, Validation Loss: 0.022331569343805313\n",
      "Epoch 29, Batch: 492: Training Loss: 0.021449264138936996, Validation Loss: 0.019609209150075912\n",
      "Epoch 29, Batch: 493: Training Loss: 0.022137586027383804, Validation Loss: 0.022212238982319832\n",
      "Epoch 29, Batch: 494: Training Loss: 0.020395943894982338, Validation Loss: 0.019917959347367287\n",
      "Epoch 29, Batch: 495: Training Loss: 0.022355714812874794, Validation Loss: 0.022128574550151825\n",
      "Epoch 29, Batch: 496: Training Loss: 0.018998101353645325, Validation Loss: 0.022605985403060913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch: 497: Training Loss: 0.017931364476680756, Validation Loss: 0.019995441660284996\n",
      "Epoch 29, Batch: 498: Training Loss: 0.021890785545110703, Validation Loss: 0.022021468728780746\n",
      "Epoch 29, Batch: 499: Training Loss: 0.019081076607108116, Validation Loss: 0.023012831807136536\n",
      "Epoch 30, Batch: 0: Training Loss: 0.021528588607907295, Validation Loss: 0.02149469219148159\n",
      "Epoch 30, Batch: 1: Training Loss: 0.018679417669773102, Validation Loss: 0.020839016884565353\n",
      "Epoch 30, Batch: 2: Training Loss: 0.02388349361717701, Validation Loss: 0.019214587286114693\n",
      "Epoch 30, Batch: 3: Training Loss: 0.018972650170326233, Validation Loss: 0.02150486782193184\n",
      "Epoch 30, Batch: 4: Training Loss: 0.01838752254843712, Validation Loss: 0.020591024309396744\n",
      "Epoch 30, Batch: 5: Training Loss: 0.021304145455360413, Validation Loss: 0.021250838413834572\n",
      "Epoch 30, Batch: 6: Training Loss: 0.016102340072393417, Validation Loss: 0.018628982827067375\n",
      "Epoch 30, Batch: 7: Training Loss: 0.018104687333106995, Validation Loss: 0.018551796674728394\n",
      "Epoch 30, Batch: 8: Training Loss: 0.021881921216845512, Validation Loss: 0.019200900569558144\n",
      "Epoch 30, Batch: 9: Training Loss: 0.016793645918369293, Validation Loss: 0.022209586575627327\n",
      "Epoch 30, Batch: 10: Training Loss: 0.01914113014936447, Validation Loss: 0.022477727383375168\n",
      "Epoch 30, Batch: 11: Training Loss: 0.02168811671435833, Validation Loss: 0.02050470933318138\n",
      "Epoch 30, Batch: 12: Training Loss: 0.02171611599624157, Validation Loss: 0.021356796845793724\n",
      "Epoch 30, Batch: 13: Training Loss: 0.024767016991972923, Validation Loss: 0.022452788427472115\n",
      "Epoch 30, Batch: 14: Training Loss: 0.02406127005815506, Validation Loss: 0.022843554615974426\n",
      "Epoch 30, Batch: 15: Training Loss: 0.02254338562488556, Validation Loss: 0.022384582087397575\n",
      "Epoch 30, Batch: 16: Training Loss: 0.023711424320936203, Validation Loss: 0.020156295970082283\n",
      "Epoch 30, Batch: 17: Training Loss: 0.018231231719255447, Validation Loss: 0.02368057519197464\n",
      "Epoch 30, Batch: 18: Training Loss: 0.02146046794950962, Validation Loss: 0.02097758837044239\n",
      "Epoch 30, Batch: 19: Training Loss: 0.019924461841583252, Validation Loss: 0.02184523642063141\n",
      "Epoch 30, Batch: 20: Training Loss: 0.02109190635383129, Validation Loss: 0.024209389463067055\n",
      "Epoch 30, Batch: 21: Training Loss: 0.022400084882974625, Validation Loss: 0.022098036482930183\n",
      "Epoch 30, Batch: 22: Training Loss: 0.02151625044643879, Validation Loss: 0.024046724662184715\n",
      "Epoch 30, Batch: 23: Training Loss: 0.020836813375353813, Validation Loss: 0.021931735798716545\n",
      "Epoch 30, Batch: 24: Training Loss: 0.020367706194519997, Validation Loss: 0.02387242391705513\n",
      "Epoch 30, Batch: 25: Training Loss: 0.019014298915863037, Validation Loss: 0.023284845054149628\n",
      "Epoch 30, Batch: 26: Training Loss: 0.02274959906935692, Validation Loss: 0.019553018733859062\n",
      "Epoch 30, Batch: 27: Training Loss: 0.01977309212088585, Validation Loss: 0.023696187883615494\n",
      "Epoch 30, Batch: 28: Training Loss: 0.019602401182055473, Validation Loss: 0.021467512473464012\n",
      "Epoch 30, Batch: 29: Training Loss: 0.023405293002724648, Validation Loss: 0.022786719724535942\n",
      "Epoch 30, Batch: 30: Training Loss: 0.02093505673110485, Validation Loss: 0.023528937250375748\n",
      "Epoch 30, Batch: 31: Training Loss: 0.02178020216524601, Validation Loss: 0.022702530026435852\n",
      "Epoch 30, Batch: 32: Training Loss: 0.02038159966468811, Validation Loss: 0.02174116112291813\n",
      "Epoch 30, Batch: 33: Training Loss: 0.01861782930791378, Validation Loss: 0.022346846759319305\n",
      "Epoch 30, Batch: 34: Training Loss: 0.01747741736471653, Validation Loss: 0.021289750933647156\n",
      "Epoch 30, Batch: 35: Training Loss: 0.019069740548729897, Validation Loss: 0.02251664362847805\n",
      "Epoch 30, Batch: 36: Training Loss: 0.020046429708600044, Validation Loss: 0.020474154502153397\n",
      "Epoch 30, Batch: 37: Training Loss: 0.020958460867404938, Validation Loss: 0.02250143326818943\n",
      "Epoch 30, Batch: 38: Training Loss: 0.024620601907372475, Validation Loss: 0.021436553448438644\n",
      "Epoch 30, Batch: 39: Training Loss: 0.01977362297475338, Validation Loss: 0.02008790522813797\n",
      "Epoch 30, Batch: 40: Training Loss: 0.020962191745638847, Validation Loss: 0.020534314215183258\n",
      "Epoch 30, Batch: 41: Training Loss: 0.020969530567526817, Validation Loss: 0.021397020667791367\n",
      "Epoch 30, Batch: 42: Training Loss: 0.021349355578422546, Validation Loss: 0.021308938041329384\n",
      "Epoch 30, Batch: 43: Training Loss: 0.016375944018363953, Validation Loss: 0.02179664373397827\n",
      "Epoch 30, Batch: 44: Training Loss: 0.022175924852490425, Validation Loss: 0.020864296704530716\n",
      "Epoch 30, Batch: 45: Training Loss: 0.018670300021767616, Validation Loss: 0.019267387688159943\n",
      "Epoch 30, Batch: 46: Training Loss: 0.024636847898364067, Validation Loss: 0.020434226840734482\n",
      "Epoch 30, Batch: 47: Training Loss: 0.019211506471037865, Validation Loss: 0.019537294283509254\n",
      "Epoch 30, Batch: 48: Training Loss: 0.0214395634829998, Validation Loss: 0.02131025865674019\n",
      "Epoch 30, Batch: 49: Training Loss: 0.025299226865172386, Validation Loss: 0.019373809918761253\n",
      "Epoch 30, Batch: 50: Training Loss: 0.019452489912509918, Validation Loss: 0.02180839702486992\n",
      "Epoch 30, Batch: 51: Training Loss: 0.023154107853770256, Validation Loss: 0.02408912405371666\n",
      "Epoch 30, Batch: 52: Training Loss: 0.019282011315226555, Validation Loss: 0.02161330357193947\n",
      "Epoch 30, Batch: 53: Training Loss: 0.017591554671525955, Validation Loss: 0.02265472710132599\n",
      "Epoch 30, Batch: 54: Training Loss: 0.019319305196404457, Validation Loss: 0.02299932762980461\n",
      "Epoch 30, Batch: 55: Training Loss: 0.018850576132535934, Validation Loss: 0.02425977773964405\n",
      "Epoch 30, Batch: 56: Training Loss: 0.021503452211618423, Validation Loss: 0.023258620873093605\n",
      "Epoch 30, Batch: 57: Training Loss: 0.02193313278257847, Validation Loss: 0.023009393364191055\n",
      "Epoch 30, Batch: 58: Training Loss: 0.023323437198996544, Validation Loss: 0.02245142310857773\n",
      "Epoch 30, Batch: 59: Training Loss: 0.01631380245089531, Validation Loss: 0.025162992998957634\n",
      "Epoch 30, Batch: 60: Training Loss: 0.017822308465838432, Validation Loss: 0.023095086216926575\n",
      "Epoch 30, Batch: 61: Training Loss: 0.022897593677043915, Validation Loss: 0.024569092318415642\n",
      "Epoch 30, Batch: 62: Training Loss: 0.018376579508185387, Validation Loss: 0.024928035214543343\n",
      "Epoch 30, Batch: 63: Training Loss: 0.02108401618897915, Validation Loss: 0.022914372384548187\n",
      "Epoch 30, Batch: 64: Training Loss: 0.021278686821460724, Validation Loss: 0.022712284699082375\n",
      "Epoch 30, Batch: 65: Training Loss: 0.022200385108590126, Validation Loss: 0.021781133487820625\n",
      "Epoch 30, Batch: 66: Training Loss: 0.019820544868707657, Validation Loss: 0.022694721817970276\n",
      "Epoch 30, Batch: 67: Training Loss: 0.01778891310095787, Validation Loss: 0.02116379700601101\n",
      "Epoch 30, Batch: 68: Training Loss: 0.020427796989679337, Validation Loss: 0.021314071491360664\n",
      "Epoch 30, Batch: 69: Training Loss: 0.02291877008974552, Validation Loss: 0.022418513894081116\n",
      "Epoch 30, Batch: 70: Training Loss: 0.023607641458511353, Validation Loss: 0.020646777004003525\n",
      "Epoch 30, Batch: 71: Training Loss: 0.01879398338496685, Validation Loss: 0.01998157612979412\n",
      "Epoch 30, Batch: 72: Training Loss: 0.017194528132677078, Validation Loss: 0.02244524285197258\n",
      "Epoch 30, Batch: 73: Training Loss: 0.0191179309040308, Validation Loss: 0.02485050819814205\n",
      "Epoch 30, Batch: 74: Training Loss: 0.017448708415031433, Validation Loss: 0.021309515461325645\n",
      "Epoch 30, Batch: 75: Training Loss: 0.01405656710267067, Validation Loss: 0.02271842397749424\n",
      "Epoch 30, Batch: 76: Training Loss: 0.018535003066062927, Validation Loss: 0.02126522921025753\n",
      "Epoch 30, Batch: 77: Training Loss: 0.024313442409038544, Validation Loss: 0.021008703857660294\n",
      "Epoch 30, Batch: 78: Training Loss: 0.0254509374499321, Validation Loss: 0.021814659237861633\n",
      "Epoch 30, Batch: 79: Training Loss: 0.02084161899983883, Validation Loss: 0.019549598917365074\n",
      "Epoch 30, Batch: 80: Training Loss: 0.0176845695823431, Validation Loss: 0.024078553542494774\n",
      "Epoch 30, Batch: 81: Training Loss: 0.023535648360848427, Validation Loss: 0.020782068371772766\n",
      "Epoch 30, Batch: 82: Training Loss: 0.0205849576741457, Validation Loss: 0.020969444885849953\n",
      "Epoch 30, Batch: 83: Training Loss: 0.018773168325424194, Validation Loss: 0.021672161296010017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch: 84: Training Loss: 0.022401344031095505, Validation Loss: 0.02415590174496174\n",
      "Epoch 30, Batch: 85: Training Loss: 0.02016258053481579, Validation Loss: 0.021172571927309036\n",
      "Epoch 30, Batch: 86: Training Loss: 0.021238408982753754, Validation Loss: 0.019909853115677834\n",
      "Epoch 30, Batch: 87: Training Loss: 0.022969301789999008, Validation Loss: 0.023316342383623123\n",
      "Epoch 30, Batch: 88: Training Loss: 0.022380312904715538, Validation Loss: 0.021846668794751167\n",
      "Epoch 30, Batch: 89: Training Loss: 0.021911369636654854, Validation Loss: 0.0217001810669899\n",
      "Epoch 30, Batch: 90: Training Loss: 0.0211136806756258, Validation Loss: 0.02220776304602623\n",
      "Epoch 30, Batch: 91: Training Loss: 0.023603618144989014, Validation Loss: 0.023781349882483482\n",
      "Epoch 30, Batch: 92: Training Loss: 0.022901514545083046, Validation Loss: 0.022135451436042786\n",
      "Epoch 30, Batch: 93: Training Loss: 0.023369625210762024, Validation Loss: 0.022061822935938835\n",
      "Epoch 30, Batch: 94: Training Loss: 0.021272946149110794, Validation Loss: 0.02118007466197014\n",
      "Epoch 30, Batch: 95: Training Loss: 0.019059989601373672, Validation Loss: 0.022808728739619255\n",
      "Epoch 30, Batch: 96: Training Loss: 0.017496256157755852, Validation Loss: 0.022007063031196594\n",
      "Epoch 30, Batch: 97: Training Loss: 0.019749589264392853, Validation Loss: 0.023056423291563988\n",
      "Epoch 30, Batch: 98: Training Loss: 0.02376742847263813, Validation Loss: 0.020987054333090782\n",
      "Epoch 30, Batch: 99: Training Loss: 0.019975898787379265, Validation Loss: 0.02194785326719284\n",
      "Epoch 30, Batch: 100: Training Loss: 0.02244408242404461, Validation Loss: 0.022410377860069275\n",
      "Epoch 30, Batch: 101: Training Loss: 0.018592018634080887, Validation Loss: 0.022537559270858765\n",
      "Epoch 30, Batch: 102: Training Loss: 0.02255077473819256, Validation Loss: 0.022702306509017944\n",
      "Epoch 30, Batch: 103: Training Loss: 0.022927748039364815, Validation Loss: 0.022574370726943016\n",
      "Epoch 30, Batch: 104: Training Loss: 0.017724016681313515, Validation Loss: 0.02213491126894951\n",
      "Epoch 30, Batch: 105: Training Loss: 0.018227890133857727, Validation Loss: 0.02340647205710411\n",
      "Epoch 30, Batch: 106: Training Loss: 0.02120111882686615, Validation Loss: 0.02283361181616783\n",
      "Epoch 30, Batch: 107: Training Loss: 0.019762294366955757, Validation Loss: 0.020436115562915802\n",
      "Epoch 30, Batch: 108: Training Loss: 0.019946398213505745, Validation Loss: 0.02177080512046814\n",
      "Epoch 30, Batch: 109: Training Loss: 0.022757988423109055, Validation Loss: 0.022624816745519638\n",
      "Epoch 30, Batch: 110: Training Loss: 0.02169113978743553, Validation Loss: 0.021955052390694618\n",
      "Epoch 30, Batch: 111: Training Loss: 0.020684178918600082, Validation Loss: 0.020552800968289375\n",
      "Epoch 30, Batch: 112: Training Loss: 0.020948773249983788, Validation Loss: 0.020201990380883217\n",
      "Epoch 30, Batch: 113: Training Loss: 0.016793476417660713, Validation Loss: 0.02499455027282238\n",
      "Epoch 30, Batch: 114: Training Loss: 0.020649364218115807, Validation Loss: 0.02303512953221798\n",
      "Epoch 30, Batch: 115: Training Loss: 0.023741334676742554, Validation Loss: 0.024408617988228798\n",
      "Epoch 30, Batch: 116: Training Loss: 0.020217714831233025, Validation Loss: 0.02232055552303791\n",
      "Epoch 30, Batch: 117: Training Loss: 0.020749328657984734, Validation Loss: 0.024982575327157974\n",
      "Epoch 30, Batch: 118: Training Loss: 0.02044370397925377, Validation Loss: 0.022327633574604988\n",
      "Epoch 30, Batch: 119: Training Loss: 0.02234097197651863, Validation Loss: 0.024728793650865555\n",
      "Epoch 30, Batch: 120: Training Loss: 0.019518578425049782, Validation Loss: 0.02177158184349537\n",
      "Epoch 30, Batch: 121: Training Loss: 0.022584863007068634, Validation Loss: 0.02238951250910759\n",
      "Epoch 30, Batch: 122: Training Loss: 0.01957916095852852, Validation Loss: 0.020785866305232048\n",
      "Epoch 30, Batch: 123: Training Loss: 0.01721120811998844, Validation Loss: 0.02293766289949417\n",
      "Epoch 30, Batch: 124: Training Loss: 0.018659040331840515, Validation Loss: 0.022053273394703865\n",
      "Epoch 30, Batch: 125: Training Loss: 0.019048580899834633, Validation Loss: 0.024662494659423828\n",
      "Epoch 30, Batch: 126: Training Loss: 0.016721639782190323, Validation Loss: 0.023983633145689964\n",
      "Epoch 30, Batch: 127: Training Loss: 0.02093132957816124, Validation Loss: 0.021822454407811165\n",
      "Epoch 30, Batch: 128: Training Loss: 0.018190795555710793, Validation Loss: 0.023464828729629517\n",
      "Epoch 30, Batch: 129: Training Loss: 0.019855348393321037, Validation Loss: 0.02248796634376049\n",
      "Epoch 30, Batch: 130: Training Loss: 0.022816091775894165, Validation Loss: 0.021612420678138733\n",
      "Epoch 30, Batch: 131: Training Loss: 0.02295016311109066, Validation Loss: 0.02235892415046692\n",
      "Epoch 30, Batch: 132: Training Loss: 0.021045135334134102, Validation Loss: 0.02344890497624874\n",
      "Epoch 30, Batch: 133: Training Loss: 0.016809886321425438, Validation Loss: 0.024120861664414406\n",
      "Epoch 30, Batch: 134: Training Loss: 0.020232314243912697, Validation Loss: 0.02224505878984928\n",
      "Epoch 30, Batch: 135: Training Loss: 0.02228454127907753, Validation Loss: 0.0228345338255167\n",
      "Epoch 30, Batch: 136: Training Loss: 0.019623082131147385, Validation Loss: 0.021964948624372482\n",
      "Epoch 30, Batch: 137: Training Loss: 0.01820342428982258, Validation Loss: 0.02549338899552822\n",
      "Epoch 30, Batch: 138: Training Loss: 0.02043280005455017, Validation Loss: 0.022508755326271057\n",
      "Epoch 30, Batch: 139: Training Loss: 0.018120087683200836, Validation Loss: 0.020830504596233368\n",
      "Epoch 30, Batch: 140: Training Loss: 0.022777900099754333, Validation Loss: 0.02234889566898346\n",
      "Epoch 30, Batch: 141: Training Loss: 0.02257349155843258, Validation Loss: 0.022229108959436417\n",
      "Epoch 30, Batch: 142: Training Loss: 0.020015588030219078, Validation Loss: 0.022011449560523033\n",
      "Epoch 30, Batch: 143: Training Loss: 0.019814876839518547, Validation Loss: 0.02285958081483841\n",
      "Epoch 30, Batch: 144: Training Loss: 0.018445748835802078, Validation Loss: 0.021884413436055183\n",
      "Epoch 30, Batch: 145: Training Loss: 0.02256721444427967, Validation Loss: 0.02166476845741272\n",
      "Epoch 30, Batch: 146: Training Loss: 0.02317962981760502, Validation Loss: 0.023249400779604912\n",
      "Epoch 30, Batch: 147: Training Loss: 0.02068842574954033, Validation Loss: 0.02333681471645832\n",
      "Epoch 30, Batch: 148: Training Loss: 0.021003277972340584, Validation Loss: 0.02393168769776821\n",
      "Epoch 30, Batch: 149: Training Loss: 0.02174551971256733, Validation Loss: 0.02202504314482212\n",
      "Epoch 30, Batch: 150: Training Loss: 0.024469221010804176, Validation Loss: 0.018726401031017303\n",
      "Epoch 30, Batch: 151: Training Loss: 0.020982692018151283, Validation Loss: 0.020595310255885124\n",
      "Epoch 30, Batch: 152: Training Loss: 0.020072927698493004, Validation Loss: 0.019962545484304428\n",
      "Epoch 30, Batch: 153: Training Loss: 0.02249700017273426, Validation Loss: 0.021695228293538094\n",
      "Epoch 30, Batch: 154: Training Loss: 0.01936923898756504, Validation Loss: 0.021252555772662163\n",
      "Epoch 30, Batch: 155: Training Loss: 0.022476712241768837, Validation Loss: 0.02008882164955139\n",
      "Epoch 30, Batch: 156: Training Loss: 0.019403738901019096, Validation Loss: 0.022303618490695953\n",
      "Epoch 30, Batch: 157: Training Loss: 0.019240790978074074, Validation Loss: 0.022794149816036224\n",
      "Epoch 30, Batch: 158: Training Loss: 0.018747711554169655, Validation Loss: 0.02067944034934044\n",
      "Epoch 30, Batch: 159: Training Loss: 0.019589198753237724, Validation Loss: 0.021045736968517303\n",
      "Epoch 30, Batch: 160: Training Loss: 0.01842225342988968, Validation Loss: 0.021765045821666718\n",
      "Epoch 30, Batch: 161: Training Loss: 0.022417237982153893, Validation Loss: 0.019652176648378372\n",
      "Epoch 30, Batch: 162: Training Loss: 0.02373466268181801, Validation Loss: 0.022722791880369186\n",
      "Epoch 30, Batch: 163: Training Loss: 0.020740952342748642, Validation Loss: 0.019930090755224228\n",
      "Epoch 30, Batch: 164: Training Loss: 0.02054719440639019, Validation Loss: 0.021481025964021683\n",
      "Epoch 30, Batch: 165: Training Loss: 0.021258989349007607, Validation Loss: 0.021075667813420296\n",
      "Epoch 30, Batch: 166: Training Loss: 0.02043023519217968, Validation Loss: 0.022206241264939308\n",
      "Epoch 30, Batch: 167: Training Loss: 0.019041232764720917, Validation Loss: 0.02242446504533291\n",
      "Epoch 30, Batch: 168: Training Loss: 0.020441746339201927, Validation Loss: 0.02064538188278675\n",
      "Epoch 30, Batch: 169: Training Loss: 0.021617380902171135, Validation Loss: 0.02072659693658352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch: 170: Training Loss: 0.02116682380437851, Validation Loss: 0.02353065088391304\n",
      "Epoch 30, Batch: 171: Training Loss: 0.018003879114985466, Validation Loss: 0.022837340831756592\n",
      "Epoch 30, Batch: 172: Training Loss: 0.019722409546375275, Validation Loss: 0.02365134097635746\n",
      "Epoch 30, Batch: 173: Training Loss: 0.017732126638293266, Validation Loss: 0.021594127640128136\n",
      "Epoch 30, Batch: 174: Training Loss: 0.02262861467897892, Validation Loss: 0.019850678741931915\n",
      "Epoch 30, Batch: 175: Training Loss: 0.02260635420680046, Validation Loss: 0.020534425973892212\n",
      "Epoch 30, Batch: 176: Training Loss: 0.021038731560111046, Validation Loss: 0.023455413058400154\n",
      "Epoch 30, Batch: 177: Training Loss: 0.021234633401036263, Validation Loss: 0.021762222051620483\n",
      "Epoch 30, Batch: 178: Training Loss: 0.026978077366948128, Validation Loss: 0.022017277777194977\n",
      "Epoch 30, Batch: 179: Training Loss: 0.020171139389276505, Validation Loss: 0.020580647513270378\n",
      "Epoch 30, Batch: 180: Training Loss: 0.021517165005207062, Validation Loss: 0.020868627354502678\n",
      "Epoch 30, Batch: 181: Training Loss: 0.019266415387392044, Validation Loss: 0.021410025656223297\n",
      "Epoch 30, Batch: 182: Training Loss: 0.023055605590343475, Validation Loss: 0.021870017051696777\n",
      "Epoch 30, Batch: 183: Training Loss: 0.022269127890467644, Validation Loss: 0.020198697224259377\n",
      "Epoch 30, Batch: 184: Training Loss: 0.02045666053891182, Validation Loss: 0.0230513084679842\n",
      "Epoch 30, Batch: 185: Training Loss: 0.020872442051768303, Validation Loss: 0.0196393895894289\n",
      "Epoch 30, Batch: 186: Training Loss: 0.021495921537280083, Validation Loss: 0.019117966294288635\n",
      "Epoch 30, Batch: 187: Training Loss: 0.020114034414291382, Validation Loss: 0.01849222369492054\n",
      "Epoch 30, Batch: 188: Training Loss: 0.020455213263630867, Validation Loss: 0.022983163595199585\n",
      "Epoch 30, Batch: 189: Training Loss: 0.01946449466049671, Validation Loss: 0.0193476565182209\n",
      "Epoch 30, Batch: 190: Training Loss: 0.021769162267446518, Validation Loss: 0.022907527163624763\n",
      "Epoch 30, Batch: 191: Training Loss: 0.021767931059002876, Validation Loss: 0.02312288247048855\n",
      "Epoch 30, Batch: 192: Training Loss: 0.021471111103892326, Validation Loss: 0.021520499140024185\n",
      "Epoch 30, Batch: 193: Training Loss: 0.022431181743741035, Validation Loss: 0.021527409553527832\n",
      "Epoch 30, Batch: 194: Training Loss: 0.019538871943950653, Validation Loss: 0.021648548543453217\n",
      "Epoch 30, Batch: 195: Training Loss: 0.016715157777071, Validation Loss: 0.02091442234814167\n",
      "Epoch 30, Batch: 196: Training Loss: 0.021482808515429497, Validation Loss: 0.023221543058753014\n",
      "Epoch 30, Batch: 197: Training Loss: 0.01728806644678116, Validation Loss: 0.020404737442731857\n",
      "Epoch 30, Batch: 198: Training Loss: 0.018441572785377502, Validation Loss: 0.02155669406056404\n",
      "Epoch 30, Batch: 199: Training Loss: 0.01927141286432743, Validation Loss: 0.022102823480963707\n",
      "Epoch 30, Batch: 200: Training Loss: 0.017774460837244987, Validation Loss: 0.02320450358092785\n",
      "Epoch 30, Batch: 201: Training Loss: 0.02369431033730507, Validation Loss: 0.02163989283144474\n",
      "Epoch 30, Batch: 202: Training Loss: 0.021378057077527046, Validation Loss: 0.021653812378644943\n",
      "Epoch 30, Batch: 203: Training Loss: 0.01797298900783062, Validation Loss: 0.021541815251111984\n",
      "Epoch 30, Batch: 204: Training Loss: 0.020790930837392807, Validation Loss: 0.021146051585674286\n",
      "Epoch 30, Batch: 205: Training Loss: 0.023381125181913376, Validation Loss: 0.02353900484740734\n",
      "Epoch 30, Batch: 206: Training Loss: 0.021111363545060158, Validation Loss: 0.020725080743432045\n",
      "Epoch 30, Batch: 207: Training Loss: 0.025237038731575012, Validation Loss: 0.022134490311145782\n",
      "Epoch 30, Batch: 208: Training Loss: 0.02116093784570694, Validation Loss: 0.0221641156822443\n",
      "Epoch 30, Batch: 209: Training Loss: 0.021980656310915947, Validation Loss: 0.022784456610679626\n",
      "Epoch 30, Batch: 210: Training Loss: 0.01884767785668373, Validation Loss: 0.02236362174153328\n",
      "Epoch 30, Batch: 211: Training Loss: 0.02442292496562004, Validation Loss: 0.02357688918709755\n",
      "Epoch 30, Batch: 212: Training Loss: 0.02186562865972519, Validation Loss: 0.021072842180728912\n",
      "Epoch 30, Batch: 213: Training Loss: 0.01962604932487011, Validation Loss: 0.021687792614102364\n",
      "Epoch 30, Batch: 214: Training Loss: 0.018886027857661247, Validation Loss: 0.020756421610713005\n",
      "Epoch 30, Batch: 215: Training Loss: 0.018930383026599884, Validation Loss: 0.020702524110674858\n",
      "Epoch 30, Batch: 216: Training Loss: 0.021719837561249733, Validation Loss: 0.021372230723500252\n",
      "Epoch 30, Batch: 217: Training Loss: 0.022402893751859665, Validation Loss: 0.022662512958049774\n",
      "Epoch 30, Batch: 218: Training Loss: 0.0219615139067173, Validation Loss: 0.023056333884596825\n",
      "Epoch 30, Batch: 219: Training Loss: 0.01846211589872837, Validation Loss: 0.022414425387978554\n",
      "Epoch 30, Batch: 220: Training Loss: 0.021646330133080482, Validation Loss: 0.021087290719151497\n",
      "Epoch 30, Batch: 221: Training Loss: 0.021621741354465485, Validation Loss: 0.022405356168746948\n",
      "Epoch 30, Batch: 222: Training Loss: 0.02183435671031475, Validation Loss: 0.02191350609064102\n",
      "Epoch 30, Batch: 223: Training Loss: 0.02094210870563984, Validation Loss: 0.021103311330080032\n",
      "Epoch 30, Batch: 224: Training Loss: 0.02215915359556675, Validation Loss: 0.01887974515557289\n",
      "Epoch 30, Batch: 225: Training Loss: 0.021096013486385345, Validation Loss: 0.020557833835482597\n",
      "Epoch 30, Batch: 226: Training Loss: 0.019680047407746315, Validation Loss: 0.01989828795194626\n",
      "Epoch 30, Batch: 227: Training Loss: 0.0189141146838665, Validation Loss: 0.020795397460460663\n",
      "Epoch 30, Batch: 228: Training Loss: 0.019923120737075806, Validation Loss: 0.021969446912407875\n",
      "Epoch 30, Batch: 229: Training Loss: 0.02156291902065277, Validation Loss: 0.022103311493992805\n",
      "Epoch 30, Batch: 230: Training Loss: 0.018395788967609406, Validation Loss: 0.021342214196920395\n",
      "Epoch 30, Batch: 231: Training Loss: 0.02393985353410244, Validation Loss: 0.019948597997426987\n",
      "Epoch 30, Batch: 232: Training Loss: 0.021986596286296844, Validation Loss: 0.023569677025079727\n",
      "Epoch 30, Batch: 233: Training Loss: 0.01787000149488449, Validation Loss: 0.02267281524837017\n",
      "Epoch 30, Batch: 234: Training Loss: 0.02250177040696144, Validation Loss: 0.02201027236878872\n",
      "Epoch 30, Batch: 235: Training Loss: 0.021645065397024155, Validation Loss: 0.021410752087831497\n",
      "Epoch 30, Batch: 236: Training Loss: 0.01984163373708725, Validation Loss: 0.02340802736580372\n",
      "Epoch 30, Batch: 237: Training Loss: 0.021955261006951332, Validation Loss: 0.025212280452251434\n",
      "Epoch 30, Batch: 238: Training Loss: 0.0206796545535326, Validation Loss: 0.022027330473065376\n",
      "Epoch 30, Batch: 239: Training Loss: 0.022582653909921646, Validation Loss: 0.026327980682253838\n",
      "Epoch 30, Batch: 240: Training Loss: 0.02171245776116848, Validation Loss: 0.0230691060423851\n",
      "Epoch 30, Batch: 241: Training Loss: 0.018547547981142998, Validation Loss: 0.02417786791920662\n",
      "Epoch 30, Batch: 242: Training Loss: 0.019804859533905983, Validation Loss: 0.023685459047555923\n",
      "Epoch 30, Batch: 243: Training Loss: 0.022939682006835938, Validation Loss: 0.022885112091898918\n",
      "Epoch 30, Batch: 244: Training Loss: 0.02005472593009472, Validation Loss: 0.022491859272122383\n",
      "Epoch 30, Batch: 245: Training Loss: 0.024434998631477356, Validation Loss: 0.022065673023462296\n",
      "Epoch 30, Batch: 246: Training Loss: 0.020307736471295357, Validation Loss: 0.02301652915775776\n",
      "Epoch 30, Batch: 247: Training Loss: 0.022665608674287796, Validation Loss: 0.020333534106612206\n",
      "Epoch 30, Batch: 248: Training Loss: 0.01909494586288929, Validation Loss: 0.022374670952558517\n",
      "Epoch 30, Batch: 249: Training Loss: 0.01823968067765236, Validation Loss: 0.02276408113539219\n",
      "Epoch 30, Batch: 250: Training Loss: 0.021681929007172585, Validation Loss: 0.020700430497527122\n",
      "Epoch 30, Batch: 251: Training Loss: 0.018555404618382454, Validation Loss: 0.020889317616820335\n",
      "Epoch 30, Batch: 252: Training Loss: 0.020094361156225204, Validation Loss: 0.021581148728728294\n",
      "Epoch 30, Batch: 253: Training Loss: 0.023007897660136223, Validation Loss: 0.02279350534081459\n",
      "Epoch 30, Batch: 254: Training Loss: 0.01915150322020054, Validation Loss: 0.023052284494042397\n",
      "Epoch 30, Batch: 255: Training Loss: 0.02224293164908886, Validation Loss: 0.023314205929636955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch: 256: Training Loss: 0.020148562267422676, Validation Loss: 0.022441908717155457\n",
      "Epoch 30, Batch: 257: Training Loss: 0.022133730351924896, Validation Loss: 0.022732142359018326\n",
      "Epoch 30, Batch: 258: Training Loss: 0.022004060447216034, Validation Loss: 0.02260221168398857\n",
      "Epoch 30, Batch: 259: Training Loss: 0.01918184943497181, Validation Loss: 0.023264065384864807\n",
      "Epoch 30, Batch: 260: Training Loss: 0.021178267896175385, Validation Loss: 0.023789267987012863\n",
      "Epoch 30, Batch: 261: Training Loss: 0.0205198023468256, Validation Loss: 0.024249661713838577\n",
      "Epoch 30, Batch: 262: Training Loss: 0.01915736496448517, Validation Loss: 0.022300731390714645\n",
      "Epoch 30, Batch: 263: Training Loss: 0.01867479272186756, Validation Loss: 0.022297564893960953\n",
      "Epoch 30, Batch: 264: Training Loss: 0.017570186406373978, Validation Loss: 0.0237598717212677\n",
      "Epoch 30, Batch: 265: Training Loss: 0.021703315898776054, Validation Loss: 0.02150285802781582\n",
      "Epoch 30, Batch: 266: Training Loss: 0.01734962686896324, Validation Loss: 0.021717140451073647\n",
      "Epoch 30, Batch: 267: Training Loss: 0.0187198705971241, Validation Loss: 0.020778849720954895\n",
      "Epoch 30, Batch: 268: Training Loss: 0.018168600276112556, Validation Loss: 0.020657625049352646\n",
      "Epoch 30, Batch: 269: Training Loss: 0.02004517987370491, Validation Loss: 0.019633891060948372\n",
      "Epoch 30, Batch: 270: Training Loss: 0.018144628033041954, Validation Loss: 0.02000373601913452\n",
      "Epoch 30, Batch: 271: Training Loss: 0.019448162987828255, Validation Loss: 0.02188951149582863\n",
      "Epoch 30, Batch: 272: Training Loss: 0.019601861014962196, Validation Loss: 0.020838063210248947\n",
      "Epoch 30, Batch: 273: Training Loss: 0.01983981393277645, Validation Loss: 0.019130319356918335\n",
      "Epoch 30, Batch: 274: Training Loss: 0.02094263583421707, Validation Loss: 0.019311800599098206\n",
      "Epoch 30, Batch: 275: Training Loss: 0.020321074873209, Validation Loss: 0.02131945639848709\n",
      "Epoch 30, Batch: 276: Training Loss: 0.019017525017261505, Validation Loss: 0.020916739478707314\n",
      "Epoch 30, Batch: 277: Training Loss: 0.02192811109125614, Validation Loss: 0.019241483882069588\n",
      "Epoch 30, Batch: 278: Training Loss: 0.01654919795691967, Validation Loss: 0.02031341753900051\n",
      "Epoch 30, Batch: 279: Training Loss: 0.021142134442925453, Validation Loss: 0.022265346720814705\n",
      "Epoch 30, Batch: 280: Training Loss: 0.015268178656697273, Validation Loss: 0.020269185304641724\n",
      "Epoch 30, Batch: 281: Training Loss: 0.018979573622345924, Validation Loss: 0.02155156619846821\n",
      "Epoch 30, Batch: 282: Training Loss: 0.020294303074479103, Validation Loss: 0.02095632627606392\n",
      "Epoch 30, Batch: 283: Training Loss: 0.02111385203897953, Validation Loss: 0.023597190156579018\n",
      "Epoch 30, Batch: 284: Training Loss: 0.0224141925573349, Validation Loss: 0.019662238657474518\n",
      "Epoch 30, Batch: 285: Training Loss: 0.020058346912264824, Validation Loss: 0.020795142278075218\n",
      "Epoch 30, Batch: 286: Training Loss: 0.021894317120313644, Validation Loss: 0.022126855328679085\n",
      "Epoch 30, Batch: 287: Training Loss: 0.020073046907782555, Validation Loss: 0.019648857414722443\n",
      "Epoch 30, Batch: 288: Training Loss: 0.021456168964505196, Validation Loss: 0.023686503991484642\n",
      "Epoch 30, Batch: 289: Training Loss: 0.02144777402281761, Validation Loss: 0.021766437217593193\n",
      "Epoch 30, Batch: 290: Training Loss: 0.021369386464357376, Validation Loss: 0.022805985063314438\n",
      "Epoch 30, Batch: 291: Training Loss: 0.01808236725628376, Validation Loss: 0.021689575165510178\n",
      "Epoch 30, Batch: 292: Training Loss: 0.02373567409813404, Validation Loss: 0.021519791334867477\n",
      "Epoch 30, Batch: 293: Training Loss: 0.021489620208740234, Validation Loss: 0.021348794922232628\n",
      "Epoch 30, Batch: 294: Training Loss: 0.020156627520918846, Validation Loss: 0.02117558754980564\n",
      "Epoch 30, Batch: 295: Training Loss: 0.01790039986371994, Validation Loss: 0.019639024510979652\n",
      "Epoch 30, Batch: 296: Training Loss: 0.021980121731758118, Validation Loss: 0.021681813523173332\n",
      "Epoch 30, Batch: 297: Training Loss: 0.018086982890963554, Validation Loss: 0.020643584430217743\n",
      "Epoch 30, Batch: 298: Training Loss: 0.019886434078216553, Validation Loss: 0.020186014473438263\n",
      "Epoch 30, Batch: 299: Training Loss: 0.017025217413902283, Validation Loss: 0.021168220788240433\n",
      "Epoch 30, Batch: 300: Training Loss: 0.02112964168190956, Validation Loss: 0.020941579714417458\n",
      "Epoch 30, Batch: 301: Training Loss: 0.01719038560986519, Validation Loss: 0.02158297225832939\n",
      "Epoch 30, Batch: 302: Training Loss: 0.01673467457294464, Validation Loss: 0.023661838844418526\n",
      "Epoch 30, Batch: 303: Training Loss: 0.019772009924054146, Validation Loss: 0.021749788895249367\n",
      "Epoch 30, Batch: 304: Training Loss: 0.018985861912369728, Validation Loss: 0.019958041608333588\n",
      "Epoch 30, Batch: 305: Training Loss: 0.017052801325917244, Validation Loss: 0.022343028336763382\n",
      "Epoch 30, Batch: 306: Training Loss: 0.018453655764460564, Validation Loss: 0.022958293557167053\n",
      "Epoch 30, Batch: 307: Training Loss: 0.017461244016885757, Validation Loss: 0.0206292774528265\n",
      "Epoch 30, Batch: 308: Training Loss: 0.01659204065799713, Validation Loss: 0.021075719967484474\n",
      "Epoch 30, Batch: 309: Training Loss: 0.017928408458828926, Validation Loss: 0.020891966298222542\n",
      "Epoch 30, Batch: 310: Training Loss: 0.018006721511483192, Validation Loss: 0.019840596243739128\n",
      "Epoch 30, Batch: 311: Training Loss: 0.019366266205906868, Validation Loss: 0.021417878568172455\n",
      "Epoch 30, Batch: 312: Training Loss: 0.01850329525768757, Validation Loss: 0.02314842864871025\n",
      "Epoch 30, Batch: 313: Training Loss: 0.02273738384246826, Validation Loss: 0.021949773654341698\n",
      "Epoch 30, Batch: 314: Training Loss: 0.021195732057094574, Validation Loss: 0.02442639321088791\n",
      "Epoch 30, Batch: 315: Training Loss: 0.018167763948440552, Validation Loss: 0.020712874829769135\n",
      "Epoch 30, Batch: 316: Training Loss: 0.019264016300439835, Validation Loss: 0.022409597411751747\n",
      "Epoch 30, Batch: 317: Training Loss: 0.018524963408708572, Validation Loss: 0.022215653210878372\n",
      "Epoch 30, Batch: 318: Training Loss: 0.019044050946831703, Validation Loss: 0.022055936977267265\n",
      "Epoch 30, Batch: 319: Training Loss: 0.018507590517401695, Validation Loss: 0.02382207103073597\n",
      "Epoch 30, Batch: 320: Training Loss: 0.023941928520798683, Validation Loss: 0.02111765183508396\n",
      "Epoch 30, Batch: 321: Training Loss: 0.017823675647377968, Validation Loss: 0.024468233808875084\n",
      "Epoch 30, Batch: 322: Training Loss: 0.02047203667461872, Validation Loss: 0.023717675358057022\n",
      "Epoch 30, Batch: 323: Training Loss: 0.019771913066506386, Validation Loss: 0.023465419188141823\n",
      "Epoch 30, Batch: 324: Training Loss: 0.022392524406313896, Validation Loss: 0.021361472085118294\n",
      "Epoch 30, Batch: 325: Training Loss: 0.020382322371006012, Validation Loss: 0.021625813096761703\n",
      "Epoch 30, Batch: 326: Training Loss: 0.024776121601462364, Validation Loss: 0.025045625865459442\n",
      "Epoch 30, Batch: 327: Training Loss: 0.02033674158155918, Validation Loss: 0.021606309339404106\n",
      "Epoch 30, Batch: 328: Training Loss: 0.021374983713030815, Validation Loss: 0.02203969657421112\n",
      "Epoch 30, Batch: 329: Training Loss: 0.01895144023001194, Validation Loss: 0.02387341298162937\n",
      "Epoch 30, Batch: 330: Training Loss: 0.01955607905983925, Validation Loss: 0.023937391117215157\n",
      "Epoch 30, Batch: 331: Training Loss: 0.019401876255869865, Validation Loss: 0.02321748249232769\n",
      "Epoch 30, Batch: 332: Training Loss: 0.023125970736145973, Validation Loss: 0.022176938131451607\n",
      "Epoch 30, Batch: 333: Training Loss: 0.020421495661139488, Validation Loss: 0.02451767399907112\n",
      "Epoch 30, Batch: 334: Training Loss: 0.023758092895150185, Validation Loss: 0.02421727403998375\n",
      "Epoch 30, Batch: 335: Training Loss: 0.021818237379193306, Validation Loss: 0.022541701793670654\n",
      "Epoch 30, Batch: 336: Training Loss: 0.01954757422208786, Validation Loss: 0.02333223633468151\n",
      "Epoch 30, Batch: 337: Training Loss: 0.018872546032071114, Validation Loss: 0.023211251944303513\n",
      "Epoch 30, Batch: 338: Training Loss: 0.017862360924482346, Validation Loss: 0.0227296631783247\n",
      "Epoch 30, Batch: 339: Training Loss: 0.02161707729101181, Validation Loss: 0.020390789955854416\n",
      "Epoch 30, Batch: 340: Training Loss: 0.022713765501976013, Validation Loss: 0.01861514337360859\n",
      "Epoch 30, Batch: 341: Training Loss: 0.018493855372071266, Validation Loss: 0.019631532952189445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch: 342: Training Loss: 0.018782470375299454, Validation Loss: 0.020467042922973633\n",
      "Epoch 30, Batch: 343: Training Loss: 0.01958446390926838, Validation Loss: 0.02047419175505638\n",
      "Epoch 30, Batch: 344: Training Loss: 0.021338505670428276, Validation Loss: 0.02173495478928089\n",
      "Epoch 30, Batch: 345: Training Loss: 0.019073031842708588, Validation Loss: 0.02191530540585518\n",
      "Epoch 30, Batch: 346: Training Loss: 0.02045925334095955, Validation Loss: 0.019014490768313408\n",
      "Epoch 30, Batch: 347: Training Loss: 0.018624018877744675, Validation Loss: 0.018854770809412003\n",
      "Epoch 30, Batch: 348: Training Loss: 0.021234853193163872, Validation Loss: 0.019674258306622505\n",
      "Epoch 30, Batch: 349: Training Loss: 0.022994080558419228, Validation Loss: 0.02024897374212742\n",
      "Epoch 30, Batch: 350: Training Loss: 0.019898345693945885, Validation Loss: 0.0197893138974905\n",
      "Epoch 30, Batch: 351: Training Loss: 0.02047513984143734, Validation Loss: 0.019711416214704514\n",
      "Epoch 30, Batch: 352: Training Loss: 0.018580565229058266, Validation Loss: 0.023311566561460495\n",
      "Epoch 30, Batch: 353: Training Loss: 0.024198245257139206, Validation Loss: 0.023175079375505447\n",
      "Epoch 30, Batch: 354: Training Loss: 0.0215130764991045, Validation Loss: 0.02212231419980526\n",
      "Epoch 30, Batch: 355: Training Loss: 0.017491796985268593, Validation Loss: 0.023885684087872505\n",
      "Epoch 30, Batch: 356: Training Loss: 0.02048325166106224, Validation Loss: 0.02424415946006775\n",
      "Epoch 30, Batch: 357: Training Loss: 0.018425174057483673, Validation Loss: 0.023794006556272507\n",
      "Epoch 30, Batch: 358: Training Loss: 0.020241329446434975, Validation Loss: 0.02190343849360943\n",
      "Epoch 30, Batch: 359: Training Loss: 0.02264251746237278, Validation Loss: 0.02085939235985279\n",
      "Epoch 30, Batch: 360: Training Loss: 0.021168220788240433, Validation Loss: 0.022700833156704903\n",
      "Epoch 30, Batch: 361: Training Loss: 0.019682612270116806, Validation Loss: 0.02153131738305092\n",
      "Epoch 30, Batch: 362: Training Loss: 0.021066788583993912, Validation Loss: 0.022192401811480522\n",
      "Epoch 30, Batch: 363: Training Loss: 0.02746415324509144, Validation Loss: 0.02133803442120552\n",
      "Epoch 30, Batch: 364: Training Loss: 0.022381054237484932, Validation Loss: 0.024383358657360077\n",
      "Epoch 30, Batch: 365: Training Loss: 0.018469884991645813, Validation Loss: 0.021750558167696\n",
      "Epoch 30, Batch: 366: Training Loss: 0.020160801708698273, Validation Loss: 0.020245440304279327\n",
      "Epoch 30, Batch: 367: Training Loss: 0.020521100610494614, Validation Loss: 0.020573196932673454\n",
      "Epoch 30, Batch: 368: Training Loss: 0.021757714450359344, Validation Loss: 0.01874820701777935\n",
      "Epoch 30, Batch: 369: Training Loss: 0.020073451101779938, Validation Loss: 0.021975748240947723\n",
      "Epoch 30, Batch: 370: Training Loss: 0.019026095047593117, Validation Loss: 0.022425634786486626\n",
      "Epoch 30, Batch: 371: Training Loss: 0.02067606709897518, Validation Loss: 0.020850736647844315\n",
      "Epoch 30, Batch: 372: Training Loss: 0.01756148412823677, Validation Loss: 0.02043081820011139\n",
      "Epoch 30, Batch: 373: Training Loss: 0.023159056901931763, Validation Loss: 0.01839572563767433\n",
      "Epoch 30, Batch: 374: Training Loss: 0.01937638223171234, Validation Loss: 0.02079388126730919\n",
      "Epoch 30, Batch: 375: Training Loss: 0.020507102832198143, Validation Loss: 0.018852200359106064\n",
      "Epoch 30, Batch: 376: Training Loss: 0.01882927305996418, Validation Loss: 0.019324274733662605\n",
      "Epoch 30, Batch: 377: Training Loss: 0.020760249346494675, Validation Loss: 0.017578447237610817\n",
      "Epoch 30, Batch: 378: Training Loss: 0.018452219665050507, Validation Loss: 0.020428067073225975\n",
      "Epoch 30, Batch: 379: Training Loss: 0.022780735045671463, Validation Loss: 0.0207747183740139\n",
      "Epoch 30, Batch: 380: Training Loss: 0.022247400134801865, Validation Loss: 0.0203716941177845\n",
      "Epoch 30, Batch: 381: Training Loss: 0.019662892445921898, Validation Loss: 0.018528485670685768\n",
      "Epoch 30, Batch: 382: Training Loss: 0.02075052075088024, Validation Loss: 0.018988478928804398\n",
      "Epoch 30, Batch: 383: Training Loss: 0.018643472343683243, Validation Loss: 0.019971784204244614\n",
      "Epoch 30, Batch: 384: Training Loss: 0.021283915266394615, Validation Loss: 0.01916850544512272\n",
      "Epoch 30, Batch: 385: Training Loss: 0.018462253734469414, Validation Loss: 0.0207658801227808\n",
      "Epoch 30, Batch: 386: Training Loss: 0.015371258370578289, Validation Loss: 0.019306933507323265\n",
      "Epoch 30, Batch: 387: Training Loss: 0.022946706041693687, Validation Loss: 0.020245378836989403\n",
      "Epoch 30, Batch: 388: Training Loss: 0.019452983513474464, Validation Loss: 0.019902126863598824\n",
      "Epoch 30, Batch: 389: Training Loss: 0.018840016797184944, Validation Loss: 0.020800529047846794\n",
      "Epoch 30, Batch: 390: Training Loss: 0.017410770058631897, Validation Loss: 0.018908130005002022\n",
      "Epoch 30, Batch: 391: Training Loss: 0.01948241889476776, Validation Loss: 0.020242128521203995\n",
      "Epoch 30, Batch: 392: Training Loss: 0.01997421495616436, Validation Loss: 0.018697839230298996\n",
      "Epoch 30, Batch: 393: Training Loss: 0.021379567682743073, Validation Loss: 0.01782957836985588\n",
      "Epoch 30, Batch: 394: Training Loss: 0.019595639780163765, Validation Loss: 0.02112402953207493\n",
      "Epoch 30, Batch: 395: Training Loss: 0.019492097198963165, Validation Loss: 0.0199901070445776\n",
      "Epoch 30, Batch: 396: Training Loss: 0.01835385523736477, Validation Loss: 0.022519441321492195\n",
      "Epoch 30, Batch: 397: Training Loss: 0.022372394800186157, Validation Loss: 0.022335851565003395\n",
      "Epoch 30, Batch: 398: Training Loss: 0.01984221860766411, Validation Loss: 0.01998492144048214\n",
      "Epoch 30, Batch: 399: Training Loss: 0.02119322307407856, Validation Loss: 0.02293839491903782\n",
      "Epoch 30, Batch: 400: Training Loss: 0.024049967527389526, Validation Loss: 0.02039620280265808\n",
      "Epoch 30, Batch: 401: Training Loss: 0.01840180531144142, Validation Loss: 0.021055633202195168\n",
      "Epoch 30, Batch: 402: Training Loss: 0.02234390564262867, Validation Loss: 0.02088354527950287\n",
      "Epoch 30, Batch: 403: Training Loss: 0.025376664474606514, Validation Loss: 0.021070571616292\n",
      "Epoch 30, Batch: 404: Training Loss: 0.019899841398000717, Validation Loss: 0.020636234432458878\n",
      "Epoch 30, Batch: 405: Training Loss: 0.01958174630999565, Validation Loss: 0.018988825380802155\n",
      "Epoch 30, Batch: 406: Training Loss: 0.02115803025662899, Validation Loss: 0.01950482837855816\n",
      "Epoch 30, Batch: 407: Training Loss: 0.018372472375631332, Validation Loss: 0.0192470233887434\n",
      "Epoch 30, Batch: 408: Training Loss: 0.01679069921374321, Validation Loss: 0.02232753112912178\n",
      "Epoch 30, Batch: 409: Training Loss: 0.01774672232568264, Validation Loss: 0.020577600225806236\n",
      "Epoch 30, Batch: 410: Training Loss: 0.02143448404967785, Validation Loss: 0.021842556074261665\n",
      "Epoch 30, Batch: 411: Training Loss: 0.016581116244196892, Validation Loss: 0.02023439109325409\n",
      "Epoch 30, Batch: 412: Training Loss: 0.024320345371961594, Validation Loss: 0.022757744416594505\n",
      "Epoch 30, Batch: 413: Training Loss: 0.02261137031018734, Validation Loss: 0.02084219455718994\n",
      "Epoch 30, Batch: 414: Training Loss: 0.01929263398051262, Validation Loss: 0.021541735157370567\n",
      "Epoch 30, Batch: 415: Training Loss: 0.021531056612730026, Validation Loss: 0.021007264032959938\n",
      "Epoch 30, Batch: 416: Training Loss: 0.020211441442370415, Validation Loss: 0.018641076982021332\n",
      "Epoch 30, Batch: 417: Training Loss: 0.021490424871444702, Validation Loss: 0.02084030956029892\n",
      "Epoch 30, Batch: 418: Training Loss: 0.017547309398651123, Validation Loss: 0.02129102125763893\n",
      "Epoch 30, Batch: 419: Training Loss: 0.01647256128489971, Validation Loss: 0.024296404793858528\n",
      "Epoch 30, Batch: 420: Training Loss: 0.02214779146015644, Validation Loss: 0.02306876704096794\n",
      "Epoch 30, Batch: 421: Training Loss: 0.021957900375127792, Validation Loss: 0.02312575839459896\n",
      "Epoch 30, Batch: 422: Training Loss: 0.02163822017610073, Validation Loss: 0.02195729687809944\n",
      "Epoch 30, Batch: 423: Training Loss: 0.021071963012218475, Validation Loss: 0.020005227997899055\n",
      "Epoch 30, Batch: 424: Training Loss: 0.020853251218795776, Validation Loss: 0.02086815983057022\n",
      "Epoch 30, Batch: 425: Training Loss: 0.020206140354275703, Validation Loss: 0.022895917296409607\n",
      "Epoch 30, Batch: 426: Training Loss: 0.01917504332959652, Validation Loss: 0.023902064189314842\n",
      "Epoch 30, Batch: 427: Training Loss: 0.01999056525528431, Validation Loss: 0.02122528851032257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch: 428: Training Loss: 0.02043698914349079, Validation Loss: 0.023083215579390526\n",
      "Epoch 30, Batch: 429: Training Loss: 0.0176479984074831, Validation Loss: 0.022624658420681953\n",
      "Epoch 30, Batch: 430: Training Loss: 0.019528118893504143, Validation Loss: 0.02095838636159897\n",
      "Epoch 30, Batch: 431: Training Loss: 0.019646739587187767, Validation Loss: 0.02161199040710926\n",
      "Epoch 30, Batch: 432: Training Loss: 0.018584484234452248, Validation Loss: 0.02321099303662777\n",
      "Epoch 30, Batch: 433: Training Loss: 0.01763065718114376, Validation Loss: 0.02166280522942543\n",
      "Epoch 30, Batch: 434: Training Loss: 0.01888624206185341, Validation Loss: 0.021861210465431213\n",
      "Epoch 30, Batch: 435: Training Loss: 0.018519707024097443, Validation Loss: 0.02285131625831127\n",
      "Epoch 30, Batch: 436: Training Loss: 0.020286355167627335, Validation Loss: 0.02175418846309185\n",
      "Epoch 30, Batch: 437: Training Loss: 0.017821408808231354, Validation Loss: 0.02304191142320633\n",
      "Epoch 30, Batch: 438: Training Loss: 0.022805245593190193, Validation Loss: 0.02295304834842682\n",
      "Epoch 30, Batch: 439: Training Loss: 0.020142097026109695, Validation Loss: 0.022952666506171227\n",
      "Epoch 30, Batch: 440: Training Loss: 0.02227727323770523, Validation Loss: 0.02146518975496292\n",
      "Epoch 30, Batch: 441: Training Loss: 0.016877368092536926, Validation Loss: 0.02317008562386036\n",
      "Epoch 30, Batch: 442: Training Loss: 0.025773538276553154, Validation Loss: 0.022487450391054153\n",
      "Epoch 30, Batch: 443: Training Loss: 0.02018021047115326, Validation Loss: 0.02219836227595806\n",
      "Epoch 30, Batch: 444: Training Loss: 0.0213016327470541, Validation Loss: 0.021891070529818535\n",
      "Epoch 30, Batch: 445: Training Loss: 0.01962307281792164, Validation Loss: 0.02213982120156288\n",
      "Epoch 30, Batch: 446: Training Loss: 0.022646313533186913, Validation Loss: 0.024164199829101562\n",
      "Epoch 30, Batch: 447: Training Loss: 0.020643623545765877, Validation Loss: 0.021575260907411575\n",
      "Epoch 30, Batch: 448: Training Loss: 0.01908666454255581, Validation Loss: 0.020326102152466774\n",
      "Epoch 30, Batch: 449: Training Loss: 0.02076238952577114, Validation Loss: 0.02023186720907688\n",
      "Epoch 30, Batch: 450: Training Loss: 0.01881646178662777, Validation Loss: 0.022921191528439522\n",
      "Epoch 30, Batch: 451: Training Loss: 0.023387737572193146, Validation Loss: 0.02153169736266136\n",
      "Epoch 30, Batch: 452: Training Loss: 0.021408449858427048, Validation Loss: 0.02353566326200962\n",
      "Epoch 30, Batch: 453: Training Loss: 0.021716400980949402, Validation Loss: 0.021349485963582993\n",
      "Epoch 30, Batch: 454: Training Loss: 0.02561301365494728, Validation Loss: 0.02318229340016842\n",
      "Epoch 30, Batch: 455: Training Loss: 0.018403666093945503, Validation Loss: 0.023903800174593925\n",
      "Epoch 30, Batch: 456: Training Loss: 0.020953983068466187, Validation Loss: 0.023233231157064438\n",
      "Epoch 30, Batch: 457: Training Loss: 0.020375754684209824, Validation Loss: 0.02377069741487503\n",
      "Epoch 30, Batch: 458: Training Loss: 0.018402859568595886, Validation Loss: 0.023143433034420013\n",
      "Epoch 30, Batch: 459: Training Loss: 0.019637329503893852, Validation Loss: 0.02223842777311802\n",
      "Epoch 30, Batch: 460: Training Loss: 0.021080177277326584, Validation Loss: 0.02195749245584011\n",
      "Epoch 30, Batch: 461: Training Loss: 0.023537518456578255, Validation Loss: 0.0222176406532526\n",
      "Epoch 30, Batch: 462: Training Loss: 0.020691603422164917, Validation Loss: 0.022674957290291786\n",
      "Epoch 30, Batch: 463: Training Loss: 0.023379160091280937, Validation Loss: 0.022308632731437683\n",
      "Epoch 30, Batch: 464: Training Loss: 0.01952282339334488, Validation Loss: 0.022269463166594505\n",
      "Epoch 30, Batch: 465: Training Loss: 0.021443676203489304, Validation Loss: 0.021335648372769356\n",
      "Epoch 30, Batch: 466: Training Loss: 0.018610825762152672, Validation Loss: 0.023843251168727875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_237254/785033935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Compute validation loss across all validation samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HEIGHT = batch_size\n",
    "WIDTH = 1000\n",
    "DEPTH = 4\n",
    "\n",
    "num_epochs = 50\n",
    "best_validation_loss = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, training_batch in enumerate(training_samples):\n",
    "        \n",
    "    net.zero_grad()\n",
    "    output = net(training_batch)\n",
    "\n",
    "    ds_ts = deepsea(training_batch.reshape((100, 4, 1, 1000)))\n",
    "    ds_o = deepsea(output.reshape((100, 4, 1, 1000)))\n",
    "    \n",
    "    # Training loss to minimize DeepSEA distances\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    training_loss = criterion(ds_ts, ds_o)\n",
    "\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    training_loss = training_loss.item()\n",
    "\n",
    "    # Compute validation loss across all validation samples\n",
    "    output_val = net(validation_samples[0])\n",
    "    \n",
    "    ds_val = deepsea(validation_samples[0].reshape((100, 4, 1, 1000)))\n",
    "    ds_oval = deepsea(output_val.reshape((100, 4, 1, 1000)))\n",
    "\n",
    "    validation_loss = criterion(ds_val, ds_oval).item()\n",
    "    \n",
    "    loss_log[\"val\"].append(training_loss)\n",
    "    loss_log[\"training\"].append(validation_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        torch.save(net.state_dict(), 'gencoder_weights.pt')\n",
    "        best_validation_loss = validation_loss\n",
    "        print(f\"Saving new best model w/ loss: {validation_loss}\")\n",
    "\n",
    "    print(f\"Epoch {epoch}, Batch: {i}: Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
    "    \n",
    "    with open(\"loss_log.pkl\", \"wb+\") as log:\n",
    "        pkl.dump(loss_log, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install matplotlib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = range(19*500)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"GENcoder training progression\")\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Training step number')\n",
    "\n",
    "ax.plot(loss_log[\"val\"])\n",
    "ax.plot(loss_log[\"training\"])\n",
    "\n",
    "plt.savefig(\"gencoder_progress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WOkGODCaslXF"
   ],
   "machine_shape": "hm",
   "name": "GANome.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
