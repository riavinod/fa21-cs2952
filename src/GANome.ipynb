{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kaq6k4pJJlCy"
   },
   "source": [
    "We use DeepSEA as the biological critic. It's input is encoded A, G, C, T and takes in sequences of length 1000.\n",
    "\n",
    "We set up our generator to also create one-hot encoded strings of A, G, C, T of length 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HNMdNj-sj3w"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uQgH_WaUoqBm"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from textwrap import wrap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rKujdKwr1dNR"
   },
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc_c14qBqODX"
   },
   "source": [
    "Split reference genome into 131-kb samples to match the input size of Basenji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lu3cUhnIyKe"
   },
   "source": [
    "We also convert lower-case soft-masked regions of the DNA to uppercase for proper one-hot encoding \n",
    "\n",
    "See: https://bioinformatics.stackexchange.com/questions/225/uppercase-vs-lowercase-letters-in-reference-genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "489qIdH_ld_o"
   },
   "outputs": [],
   "source": [
    "sample_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YizUz5bJqE1T"
   },
   "outputs": [],
   "source": [
    "# nucleic_acids = \"ACGT\"\n",
    "# samples = []\n",
    "\n",
    "# if len(samples) == 0:\n",
    "#   for i, record in enumerate(SeqIO.parse(\"/content/GCF_000001405.39_GRCh38.p13_genomic.fna\", \"fasta\")):\n",
    "#     if i <= 5:\n",
    "#       sequence = str(record.seq)\n",
    "#       for sample in wrap(sequence, sample_length):\n",
    "#         sample = sample.upper()\n",
    "#         if set(sample) == set(nucleic_acids) and len(sample) == sample_length: # Ensure string only contains proper nucleic acids\n",
    "#           samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BQIWmsuQY67F"
   },
   "outputs": [],
   "source": [
    "nucleic_acids = \"AGCT\"\n",
    "\n",
    "num_training_samples = 100\n",
    "validation_samples = []\n",
    "num_validation_samples = 100\n",
    "\n",
    "def generate_samples(num_samples):\n",
    "  samples = []\n",
    "  for i in range(num_samples):\n",
    "    sample = \"\"\n",
    "    for i in range(sample_length):\n",
    "      base_selection = int(random.random()*4)\n",
    "      sample += nucleic_acids[base_selection]\n",
    "    samples.append(sample)\n",
    "  return samples\n",
    "\n",
    "raw_training_samples = generate_samples(num_training_samples)\n",
    "\n",
    "raw_validation_samples = generate_samples(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T460uOphOUjX"
   },
   "source": [
    "Initialize one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOu3wO6EJgt5",
    "outputId": "4db34047-a368-4843-cef6-d5606d7d5a0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "integer_encoded = label_encoder.fit_transform(list(nucleic_acids))\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder.fit(np.asarray(list(nucleic_acids)).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3l78vTb2OKMO"
   },
   "outputs": [],
   "source": [
    "# One-hot encode each input string\n",
    "batch_size = 100\n",
    "use_channels = False\n",
    "\n",
    "def encode_and_resize_samples(samples):\n",
    "  one_hot_samples = np.array([np.array(onehot_encoder.transform(np.asarray(list(sample)).reshape(-1, 1))) for sample in samples])\n",
    "  if use_channels:\n",
    "    one_hot_samples2 = np.swapaxes(np.swapaxes(np.expand_dims(one_hot_samples, axis=3), 1, 2), 2, 3)\n",
    "  else:\n",
    "    one_hot_samples2 = np.swapaxes(np.expand_dims(one_hot_samples, axis=3), 1, 3)\n",
    "\n",
    "  return one_hot_samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z8Phw0Oyy35A"
   },
   "outputs": [],
   "source": [
    "resized_training_samples = encode_and_resize_samples(raw_training_samples)\n",
    "resized_validation_samples = encode_and_resize_samples(raw_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6cGYtcRKKXU",
    "outputId": "926b0d30-015d-4384-c7c7-443833741c21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_training_samples[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxP9OvEmbcXq"
   },
   "source": [
    "Reshape and batch one-hot encoded input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "q7VzIsEQ-orE"
   },
   "outputs": [],
   "source": [
    "def batchify(samples: np.ndarray, batch_size: int) -> np.ndarray:\n",
    "    for i in list(range(0, len(samples), batch_size)):\n",
    "      yield samples[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5YsMdkaX-vdM"
   },
   "outputs": [],
   "source": [
    "batched_training_samples = list(batchify(resized_training_samples, batch_size))\n",
    "batched_validation_samples = list(batchify(resized_validation_samples, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3byRyOmeWhQq",
    "outputId": "91082937-e30f-4508-97b1-781bbe3f9f2c"
   },
   "outputs": [],
   "source": [
    "# Convert one-hot encoded genome samples to PyTorch tensors\n",
    "training_samples = torch.as_tensor(batched_training_samples).to(device, dtype=torch.float) \n",
    "validation_samples = torch.as_tensor(batched_validation_samples).to(device, dtype=torch.float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjbPBAPCWyEC"
   },
   "source": [
    "# Set up Pytorch for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3q3ZcqaTW0uz"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSdlYCVCspu"
   },
   "source": [
    "# Train DeepSEA-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pzll5euDXLi1"
   },
   "outputs": [],
   "source": [
    "# Format DeepSEA training data\n",
    "import h5py\n",
    "h5f = h5py.File(\"/gpfs/home/crsmall/CS2952G/ganome/data/deepsea_train/train.mat\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dfsvyx4gdnyL"
   },
   "outputs": [],
   "source": [
    "ds_trainx = h5f[\"trainxdata\"][:]\n",
    "ds_trainy = h5f[\"traindata\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "kg5X6ol5juVS",
    "outputId": "98b855d4-abea-4707-bc59-933f892c449b"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "ds_trainx = torch.as_tensor(ds_trainx.reshape((-1, batch_size, 1, 4, 1000))).to(device, dtype=torch.float)\n",
    "ds_trainy = torch.as_tensor(ds_trainy.reshape((-1, batch_size, 919))).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEMx-FwhONir"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSEA architecture (Zhou & Troyanskaya, 2015).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DeepSEA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepSEA, self).__init__()\n",
    "        kernel_size = 8\n",
    "        num_channels = 1\n",
    "        n_deepsea_features = 256\n",
    "        n_output_features = 919\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(num_channels, 256, (4, kernel_size)),\n",
    "          nn.LeakyReLU(0.2 ,inplace=True),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.MaxPool2d((1, 8)),\n",
    "          nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(256, 512, (1,kernel_size)), # <- This kernel size has to be 1,x for some reason\n",
    "          nn.LeakyReLU(0.2 ,inplace=True),\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.MaxPool2d((1,8)),\n",
    "          nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "          nn.Conv2d(512, 1028, (1,kernel_size)),\n",
    "          nn.LeakyReLU(0.2, inplace=True),\n",
    "          nn.BatchNorm2d(1028),\n",
    "          nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "          nn.Linear(7196, 919),\n",
    "          nn.ReLU(True),\n",
    "          nn.Linear(919, 919),\n",
    "#           nn.ReLU(True),\n",
    "#           nn.Linear(1216, n_output_features),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.layer1(x)\n",
    "        f2 = self.layer2(f1)\n",
    "        f3 = self.layer3(f2)\n",
    "        f3 = f3.view(f3.size(0), -1)\n",
    "        f4 = self.linear(f3)\n",
    "        return f4\n",
    "    \n",
    "    def print(self, x):\n",
    "        f1 = self.layer1(x)\n",
    "        print(f1.size())\n",
    "        f2 = self.layer2(f1)\n",
    "        print(f2.size())\n",
    "        f3 = self.layer3(f2)\n",
    "        print(f3.size())\n",
    "        f3 = f3.view(f3.size(0), -1)\n",
    "        print(f3.size())\n",
    "        f4 = self.linear(f3)\n",
    "        print(f4.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icP_n7oDXAk2"
   },
   "outputs": [],
   "source": [
    "deepsea = DeepSEA().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deepsea.print(ds_trainx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "ygBGfwncXGeM"
   },
   "outputs": [],
   "source": [
    "ds_criterion = nn.BCELoss(reduction=\"mean\")\n",
    "ds_optimizer = optim.Adam(deepsea.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "QnOI-YydZBL5",
    "outputId": "c1334217-993c-4b87-9098-0aace7c7b233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 0: Training Loss: 0.3015921115875244\n",
      "Epoch 0, Batch: 1: Training Loss: 0.18967653810977936\n",
      "Epoch 0, Batch: 2: Training Loss: 0.2592916786670685\n",
      "Epoch 0, Batch: 3: Training Loss: 0.32434913516044617\n",
      "Epoch 0, Batch: 4: Training Loss: 0.3612569272518158\n",
      "Epoch 0, Batch: 5: Training Loss: 0.284496009349823\n",
      "Epoch 0, Batch: 6: Training Loss: 0.23889963328838348\n",
      "Epoch 0, Batch: 7: Training Loss: 0.17854973673820496\n",
      "Epoch 0, Batch: 8: Training Loss: 0.28352242708206177\n",
      "Epoch 0, Batch: 9: Training Loss: 0.33259519934654236\n",
      "Epoch 0, Batch: 10: Training Loss: 0.398405522108078\n",
      "Epoch 0, Batch: 11: Training Loss: 0.28916820883750916\n",
      "Epoch 0, Batch: 12: Training Loss: 0.46474334597587585\n",
      "Epoch 0, Batch: 13: Training Loss: 0.25340384244918823\n",
      "Epoch 0, Batch: 14: Training Loss: 0.1586865484714508\n",
      "Epoch 0, Batch: 15: Training Loss: 0.36715051531791687\n",
      "Epoch 0, Batch: 16: Training Loss: 0.31817567348480225\n",
      "Epoch 0, Batch: 17: Training Loss: 0.24917292594909668\n",
      "Epoch 0, Batch: 18: Training Loss: 0.27588996291160583\n",
      "Epoch 0, Batch: 19: Training Loss: 0.0940152108669281\n",
      "Epoch 0, Batch: 20: Training Loss: 0.21048621833324432\n",
      "Epoch 0, Batch: 21: Training Loss: 0.26643630862236023\n",
      "Epoch 0, Batch: 22: Training Loss: 0.2226417362689972\n",
      "Epoch 0, Batch: 23: Training Loss: 0.22834211587905884\n",
      "Epoch 0, Batch: 24: Training Loss: 0.2792815864086151\n",
      "Epoch 0, Batch: 25: Training Loss: 0.21528954803943634\n",
      "Epoch 0, Batch: 26: Training Loss: 0.2562713027000427\n",
      "Epoch 0, Batch: 27: Training Loss: 0.32936805486679077\n",
      "Epoch 0, Batch: 28: Training Loss: 0.3543943464756012\n",
      "Epoch 0, Batch: 29: Training Loss: 0.28049153089523315\n",
      "Epoch 0, Batch: 30: Training Loss: 0.23586925864219666\n",
      "Epoch 0, Batch: 31: Training Loss: 0.19521845877170563\n",
      "Epoch 0, Batch: 32: Training Loss: 0.2817683219909668\n",
      "Epoch 0, Batch: 33: Training Loss: 0.3529253602027893\n",
      "Epoch 0, Batch: 34: Training Loss: 0.37766754627227783\n",
      "Epoch 0, Batch: 35: Training Loss: 0.3173618018627167\n",
      "Epoch 0, Batch: 36: Training Loss: 0.4554927349090576\n",
      "Epoch 0, Batch: 37: Training Loss: 0.24516186118125916\n",
      "Epoch 0, Batch: 38: Training Loss: 0.163620725274086\n",
      "Epoch 0, Batch: 39: Training Loss: 0.3616067171096802\n",
      "Epoch 0, Batch: 40: Training Loss: 0.33343541622161865\n",
      "Epoch 0, Batch: 41: Training Loss: 0.24932898581027985\n",
      "Epoch 0, Batch: 42: Training Loss: 0.25446197390556335\n",
      "Epoch 0, Batch: 43: Training Loss: 0.09389495104551315\n",
      "Epoch 0, Batch: 44: Training Loss: 0.2081819325685501\n",
      "Epoch 0, Batch: 45: Training Loss: 0.27541178464889526\n",
      "Epoch 0, Batch: 46: Training Loss: 0.21726864576339722\n",
      "Epoch 0, Batch: 47: Training Loss: 0.2135256975889206\n",
      "Epoch 0, Batch: 48: Training Loss: 0.2966231405735016\n",
      "Epoch 0, Batch: 49: Training Loss: 0.29684701561927795\n",
      "Epoch 0, Batch: 50: Training Loss: 0.26217415928840637\n",
      "Epoch 0, Batch: 51: Training Loss: 0.28940460085868835\n",
      "Epoch 0, Batch: 52: Training Loss: 0.2936009168624878\n",
      "Epoch 0, Batch: 53: Training Loss: 0.3167978823184967\n",
      "Epoch 0, Batch: 54: Training Loss: 0.2681919038295746\n",
      "Epoch 0, Batch: 55: Training Loss: 0.25484833121299744\n",
      "Epoch 0, Batch: 56: Training Loss: 0.2926633059978485\n",
      "Epoch 0, Batch: 57: Training Loss: 0.251427561044693\n",
      "Epoch 0, Batch: 58: Training Loss: 0.2970350682735443\n",
      "Epoch 0, Batch: 59: Training Loss: 0.2689383924007416\n",
      "Epoch 0, Batch: 60: Training Loss: 0.2854292094707489\n",
      "Epoch 0, Batch: 61: Training Loss: 0.257500559091568\n",
      "Epoch 0, Batch: 62: Training Loss: 0.2861135005950928\n",
      "Epoch 0, Batch: 63: Training Loss: 0.23027081787586212\n",
      "Epoch 0, Batch: 64: Training Loss: 0.25687775015830994\n",
      "Epoch 0, Batch: 65: Training Loss: 0.2801099121570587\n",
      "Epoch 0, Batch: 66: Training Loss: 0.27520930767059326\n",
      "Epoch 0, Batch: 67: Training Loss: 0.2660942077636719\n",
      "Epoch 0, Batch: 68: Training Loss: 0.2515227794647217\n",
      "Epoch 0, Batch: 69: Training Loss: 0.27488547563552856\n",
      "Epoch 0, Batch: 70: Training Loss: 0.2748200297355652\n",
      "Epoch 0, Batch: 71: Training Loss: 0.2654722034931183\n",
      "Epoch 0, Batch: 72: Training Loss: 0.29428791999816895\n",
      "Epoch 0, Batch: 73: Training Loss: 0.2947632968425751\n",
      "Epoch 0, Batch: 74: Training Loss: 0.2654779851436615\n",
      "Epoch 0, Batch: 75: Training Loss: 0.2819466292858124\n",
      "Epoch 0, Batch: 76: Training Loss: 0.29709601402282715\n",
      "Epoch 0, Batch: 77: Training Loss: 0.31412407755851746\n",
      "Epoch 0, Batch: 78: Training Loss: 0.26242783665657043\n",
      "Epoch 0, Batch: 79: Training Loss: 0.2545155882835388\n",
      "Epoch 0, Batch: 80: Training Loss: 0.28975456953048706\n",
      "Epoch 0, Batch: 81: Training Loss: 0.2568656802177429\n",
      "Epoch 0, Batch: 82: Training Loss: 0.2934595048427582\n",
      "Epoch 0, Batch: 83: Training Loss: 0.2721303403377533\n",
      "Epoch 0, Batch: 84: Training Loss: 0.2843826413154602\n",
      "Epoch 0, Batch: 85: Training Loss: 0.2519618272781372\n",
      "Epoch 0, Batch: 86: Training Loss: 0.28579917550086975\n",
      "Epoch 0, Batch: 87: Training Loss: 0.22562848031520844\n",
      "Epoch 0, Batch: 88: Training Loss: 0.2568044364452362\n",
      "Epoch 0, Batch: 89: Training Loss: 0.2781711518764496\n",
      "Epoch 0, Batch: 90: Training Loss: 0.27260294556617737\n",
      "Epoch 0, Batch: 91: Training Loss: 0.2574153244495392\n",
      "Epoch 0, Batch: 92: Training Loss: 0.2513570189476013\n",
      "Epoch 0, Batch: 93: Training Loss: 0.27625107765197754\n",
      "Epoch 0, Batch: 94: Training Loss: 0.275591641664505\n",
      "Epoch 0, Batch: 95: Training Loss: 0.2931389808654785\n",
      "Epoch 0, Batch: 96: Training Loss: 0.24191780388355255\n",
      "Epoch 0, Batch: 97: Training Loss: 0.21656271815299988\n",
      "Epoch 0, Batch: 98: Training Loss: 0.19456055760383606\n",
      "Epoch 0, Batch: 99: Training Loss: 0.26714497804641724\n",
      "Epoch 1, Batch: 0: Training Loss: 0.3082194924354553\n",
      "Epoch 1, Batch: 1: Training Loss: 0.18825514614582062\n",
      "Epoch 1, Batch: 2: Training Loss: 0.25696924328804016\n",
      "Epoch 1, Batch: 3: Training Loss: 0.3218129575252533\n",
      "Epoch 1, Batch: 4: Training Loss: 0.3591405749320984\n",
      "Epoch 1, Batch: 5: Training Loss: 0.2819623351097107\n",
      "Epoch 1, Batch: 6: Training Loss: 0.2375827133655548\n",
      "Epoch 1, Batch: 7: Training Loss: 0.1772671490907669\n",
      "Epoch 1, Batch: 8: Training Loss: 0.280284583568573\n",
      "Epoch 1, Batch: 9: Training Loss: 0.3291115164756775\n",
      "Epoch 1, Batch: 10: Training Loss: 0.3936963677406311\n",
      "Epoch 1, Batch: 11: Training Loss: 0.2860885560512543\n",
      "Epoch 1, Batch: 12: Training Loss: 0.46009108424186707\n",
      "Epoch 1, Batch: 13: Training Loss: 0.2504495680332184\n",
      "Epoch 1, Batch: 14: Training Loss: 0.1569509506225586\n",
      "Epoch 1, Batch: 15: Training Loss: 0.3626752197742462\n",
      "Epoch 1, Batch: 16: Training Loss: 0.31338244676589966\n",
      "Epoch 1, Batch: 17: Training Loss: 0.2451775074005127\n",
      "Epoch 1, Batch: 18: Training Loss: 0.27120569348335266\n",
      "Epoch 1, Batch: 19: Training Loss: 0.09208206832408905\n",
      "Epoch 1, Batch: 20: Training Loss: 0.20694567263126373\n",
      "Epoch 1, Batch: 21: Training Loss: 0.26158809661865234\n",
      "Epoch 1, Batch: 22: Training Loss: 0.21760372817516327\n",
      "Epoch 1, Batch: 23: Training Loss: 0.22334043681621552\n",
      "Epoch 1, Batch: 24: Training Loss: 0.2742513418197632\n",
      "Epoch 1, Batch: 25: Training Loss: 0.21076679229736328\n",
      "Epoch 1, Batch: 26: Training Loss: 0.25056514143943787\n",
      "Epoch 1, Batch: 27: Training Loss: 0.3232554793357849\n",
      "Epoch 1, Batch: 28: Training Loss: 0.34727564454078674\n",
      "Epoch 1, Batch: 29: Training Loss: 0.2743198573589325\n",
      "Epoch 1, Batch: 30: Training Loss: 0.231044739484787\n",
      "Epoch 1, Batch: 31: Training Loss: 0.19088946282863617\n",
      "Epoch 1, Batch: 32: Training Loss: 0.2754282057285309\n",
      "Epoch 1, Batch: 33: Training Loss: 0.3461510241031647\n",
      "Epoch 1, Batch: 34: Training Loss: 0.3709961175918579\n",
      "Epoch 1, Batch: 35: Training Loss: 0.310578316450119\n",
      "Epoch 1, Batch: 36: Training Loss: 0.44656530022621155\n",
      "Epoch 1, Batch: 37: Training Loss: 0.24094544351100922\n",
      "Epoch 1, Batch: 38: Training Loss: 0.16072040796279907\n",
      "Epoch 1, Batch: 39: Training Loss: 0.3546617925167084\n",
      "Epoch 1, Batch: 40: Training Loss: 0.32718899846076965\n",
      "Epoch 1, Batch: 41: Training Loss: 0.24402406811714172\n",
      "Epoch 1, Batch: 42: Training Loss: 0.24960504472255707\n",
      "Epoch 1, Batch: 43: Training Loss: 0.09199836105108261\n",
      "Epoch 1, Batch: 44: Training Loss: 0.20354817807674408\n",
      "Epoch 1, Batch: 45: Training Loss: 0.26854366064071655\n",
      "Epoch 1, Batch: 46: Training Loss: 0.21259216964244843\n",
      "Epoch 1, Batch: 47: Training Loss: 0.20930945873260498\n",
      "Epoch 1, Batch: 48: Training Loss: 0.2895963490009308\n",
      "Epoch 1, Batch: 49: Training Loss: 0.2907394766807556\n",
      "Epoch 1, Batch: 50: Training Loss: 0.2558757960796356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch: 51: Training Loss: 0.28364601731300354\n",
      "Epoch 1, Batch: 52: Training Loss: 0.28666940331459045\n",
      "Epoch 1, Batch: 53: Training Loss: 0.3105241656303406\n",
      "Epoch 1, Batch: 54: Training Loss: 0.2626662254333496\n",
      "Epoch 1, Batch: 55: Training Loss: 0.24946840107440948\n",
      "Epoch 1, Batch: 56: Training Loss: 0.28590089082717896\n",
      "Epoch 1, Batch: 57: Training Loss: 0.24682219326496124\n",
      "Epoch 1, Batch: 58: Training Loss: 0.29141950607299805\n",
      "Epoch 1, Batch: 59: Training Loss: 0.263651579618454\n",
      "Epoch 1, Batch: 60: Training Loss: 0.2795623242855072\n",
      "Epoch 1, Batch: 61: Training Loss: 0.25269731879234314\n",
      "Epoch 1, Batch: 62: Training Loss: 0.2804722785949707\n",
      "Epoch 1, Batch: 63: Training Loss: 0.22540879249572754\n",
      "Epoch 1, Batch: 64: Training Loss: 0.2522360682487488\n",
      "Epoch 1, Batch: 65: Training Loss: 0.27502039074897766\n",
      "Epoch 1, Batch: 66: Training Loss: 0.2695813775062561\n",
      "Epoch 1, Batch: 67: Training Loss: 0.26053300499916077\n",
      "Epoch 1, Batch: 68: Training Loss: 0.24668380618095398\n",
      "Epoch 1, Batch: 69: Training Loss: 0.269359827041626\n",
      "Epoch 1, Batch: 70: Training Loss: 0.27004608511924744\n",
      "Epoch 1, Batch: 71: Training Loss: 0.260405033826828\n",
      "Epoch 1, Batch: 72: Training Loss: 0.2892904281616211\n",
      "Epoch 1, Batch: 73: Training Loss: 0.2888430655002594\n",
      "Epoch 1, Batch: 74: Training Loss: 0.26033324003219604\n",
      "Epoch 1, Batch: 75: Training Loss: 0.2769531011581421\n",
      "Epoch 1, Batch: 76: Training Loss: 0.29218703508377075\n",
      "Epoch 1, Batch: 77: Training Loss: 0.30847787857055664\n",
      "Epoch 1, Batch: 78: Training Loss: 0.257876455783844\n",
      "Epoch 1, Batch: 79: Training Loss: 0.25009608268737793\n",
      "Epoch 1, Batch: 80: Training Loss: 0.28403517603874207\n",
      "Epoch 1, Batch: 81: Training Loss: 0.2526601254940033\n",
      "Epoch 1, Batch: 82: Training Loss: 0.2890695035457611\n",
      "Epoch 1, Batch: 83: Training Loss: 0.26837900280952454\n",
      "Epoch 1, Batch: 84: Training Loss: 0.27962860465049744\n",
      "Epoch 1, Batch: 85: Training Loss: 0.2482653111219406\n",
      "Epoch 1, Batch: 86: Training Loss: 0.2815648913383484\n",
      "Epoch 1, Batch: 87: Training Loss: 0.2225557267665863\n",
      "Epoch 1, Batch: 88: Training Loss: 0.253545343875885\n",
      "Epoch 1, Batch: 89: Training Loss: 0.274649053812027\n",
      "Epoch 1, Batch: 90: Training Loss: 0.2695326805114746\n",
      "Epoch 1, Batch: 91: Training Loss: 0.2542034983634949\n",
      "Epoch 1, Batch: 92: Training Loss: 0.24822451174259186\n",
      "Epoch 1, Batch: 93: Training Loss: 0.27336758375167847\n",
      "Epoch 1, Batch: 94: Training Loss: 0.2724060118198395\n",
      "Epoch 1, Batch: 95: Training Loss: 0.2895142138004303\n",
      "Epoch 1, Batch: 96: Training Loss: 0.23902666568756104\n",
      "Epoch 1, Batch: 97: Training Loss: 0.21425503492355347\n",
      "Epoch 1, Batch: 98: Training Loss: 0.19250327348709106\n",
      "Epoch 1, Batch: 99: Training Loss: 0.2639555037021637\n",
      "Epoch 2, Batch: 0: Training Loss: 0.3056159019470215\n",
      "Epoch 2, Batch: 1: Training Loss: 0.18694402277469635\n",
      "Epoch 2, Batch: 2: Training Loss: 0.2552681267261505\n",
      "Epoch 2, Batch: 3: Training Loss: 0.3195149898529053\n",
      "Epoch 2, Batch: 4: Training Loss: 0.35704484581947327\n",
      "Epoch 2, Batch: 5: Training Loss: 0.2803187370300293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_122819/183905120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mdeepsea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepsea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_122819/1544430144.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_ds_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_ds_epochs):\n",
    "  for i in range(0, batch_size):\n",
    "      training_batch = ds_trainx[i]\n",
    "      training_labels = ds_trainy[i]\n",
    "\n",
    "      deepsea.zero_grad()\n",
    "      output = deepsea.forward(training_batch)\n",
    "        \n",
    "      training_loss = ds_criterion(output, training_labels)\n",
    "\n",
    "      training_loss.backward()\n",
    "      ds_optimizer.step()\n",
    "\n",
    "      training_loss = training_loss.item()\n",
    "\n",
    "      print(f\"Epoch {epoch}, Batch: {i}: Training Loss: {training_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Frk9og9socs"
   },
   "source": [
    "# Simple NN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "gYH0Jpkrsr9P"
   },
   "outputs": [],
   "source": [
    "num_channels = 1  # Only one channel for genomic data\n",
    "n_generator_features = 256  # Size of feature maps in generator\n",
    "n_discriminator_features = 64  # Size of feature maps in discriminator\n",
    "kernel_size = (4, 10)\n",
    "input_size = sample_length\n",
    "ngpu = 1\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, ngpu):\n",
    "    super(Net, self).__init__()\n",
    "    self.ngpu = ngpu\n",
    "    self.main = nn.Sequential(\n",
    "        # First layer\n",
    "        nn.Conv2d(num_channels, n_generator_features, kernel_size),\n",
    "        nn.BatchNorm2d(n_generator_features),\n",
    "        nn.LeakyReLU(0.2 ,inplace=True),\n",
    "        # Second layer\n",
    "        nn.Conv2d(n_generator_features, int(n_generator_features/2), (1,10)), # <- This kernel size has to be 1,x for some reason\n",
    "        nn.BatchNorm2d(int(n_generator_features/2)),\n",
    "        nn.LeakyReLU(0.2 ,inplace=True),\n",
    "        nn.ConvTranspose2d(int(n_generator_features/2), n_generator_features, (1, 10)),\n",
    "        nn.BatchNorm2d(n_generator_features),\n",
    "        nn.ReLU(True),\n",
    "        # Output Layer\n",
    "        nn.ConvTranspose2d(n_generator_features, num_channels, kernel_size),\n",
    "        nn.Softmax(dim=2)\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "a4Uu6927riwJ"
   },
   "outputs": [],
   "source": [
    "# Arbitrarily taken from: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN3CYBtJzgHP",
    "outputId": "281fd575-78b2-47d0-cecd-831a859c63c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(256, 128, kernel_size=(1, 10), stride=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): ConvTranspose2d(128, 256, kernel_size=(1, 10), stride=(1, 1))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(256, 1, kernel_size=(4, 10), stride=(1, 1))\n",
      "    (10): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our model with the detected GPU\n",
    "net = Net(1).to(device)\n",
    "\n",
    "# Initialize the weights in our model\n",
    "# net.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmXLvMJ3uBF5"
   },
   "outputs": [],
   "source": [
    "# # Visualize our model\n",
    "# from torchviz import make_dot\n",
    "# y = net(one_hot_samples3)\n",
    "# make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "jx894V0sz86X"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='sum')\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "Pcc6Hy_pFvUa",
    "outputId": "08fb72fd-e626-46cd-eba3-e096d463e464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch: 0: Training Loss: 8.971385767608808e-08, Validation Loss: 0.003022681688889861\n",
      "[1,     1] loss: 0.000\n",
      "Epoch 1, Batch: 0: Training Loss: 6.767019300468746e-08, Validation Loss: 0.0030712741427123547\n",
      "[2,     1] loss: 0.000\n",
      "Epoch 2, Batch: 0: Training Loss: 6.954193310093615e-08, Validation Loss: 0.003102491609752178\n",
      "[3,     1] loss: 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_122819/1140161730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mds_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepsea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_122819/1610218336.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/ganome_env/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HEIGHT = batch_size\n",
    "WIDTH = 1000\n",
    "DEPTH = 4\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, training_batch in enumerate(training_samples):\n",
    "        \n",
    "    net.zero_grad()\n",
    "    output = net(training_batch)\n",
    "\n",
    "    ds_ts = deepsea(training_batch)\n",
    "    ds_o = deepsea(output)\n",
    "    \n",
    "    # Training loss to minimize DeepSEA distances\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    training_loss = criterion(ds_ts, ds_o)\n",
    "\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    training_loss = training_loss.item()\n",
    "\n",
    "    # Compute validation loss across all validation samples\n",
    "    validation_total_loss = 0\n",
    "    for validation_batch in validation_samples:\n",
    "      validation_output = net(validation_batch)\n",
    "      validation_total_loss += criterion(validation_output, validation_batch)\n",
    "\n",
    "    validation_loss = validation_total_loss/num_validation_samples\n",
    "\n",
    "    print(f\"Epoch {epoch}, Batch: {i}: Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss))\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf8kchviM-A9"
   },
   "outputs": [],
   "source": [
    "y = net(validation_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpBjewi-WKC8"
   },
   "outputs": [],
   "source": [
    "torch.argmax(validation_samples[0][0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a79aXLjWWOXB"
   },
   "outputs": [],
   "source": [
    "torch.argmax(y[0], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkOAUV5KLdir"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrPZ3F05L6Eb"
   },
   "outputs": [],
   "source": [
    "o = output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFAik34AL6hj"
   },
   "outputs": [],
   "source": [
    "(o == o.max(axis=2)[:,None]).astype(int).reshape(1, 100, 4, 1000).squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71a5OWLrMZ9n"
   },
   "outputs": [],
   "source": [
    "deepsea_training_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIr2j4k4PQok"
   },
   "outputs": [],
   "source": [
    "deepsea_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvpuh223PR-Q"
   },
   "outputs": [],
   "source": [
    "len(training_pred_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3vduYz5PcUL"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/deepsea_train.zip /content/deepsea_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNFZVjD-dk5P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WOkGODCaslXF"
   ],
   "machine_shape": "hm",
   "name": "GANome.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
